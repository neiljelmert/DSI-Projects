id,created_at,link,text,retweets,favorites,abstract
803773449739128832,2016-11-30 01:31:38,https://t.co/rAXdVIsvtS,The Emergence of Organizing Structure in Conceptual Representation. (arXiv:1611.09384v1 [cs.LG]) https://t.co/rAXdVIsvtS,0,0," Abstract: Both scientists and children make important structural discoveries, yet their computational underpinnings are not well understood. Structure discovery has previously been formalized as probabilistic inference about the right structural form --- where form could be a tree, ring, chain, grid, etc. [Kemp & Tenenbaum (2008). The discovery of structural form. PNAS, 105(3), 10687-10692]. While this approach can learn intuitive organizations, including a tree for animals and a ring for the color circle, it assumes a strong inductive bias that considers only these particular forms, and each form is explicitly provided as initial knowledge. Here we introduce a new computational model of how organizing structure can be discovered, utilizing a broad hypothesis space with a preference for sparse connectivity. Given that the inductive bias is more general, the model's initial knowledge shows little qualitative resemblance to some of the discoveries it supports. As a consequence, the model can also learn complex structures for domains that lack intuitive description, as well as predict human property induction judgments without explicit structural forms. By allowing form to emerge from sparsity, our approach clarifies how both the richness and flexibility of human conceptual organization can coexist. "
803773448468230145,2016-11-30 01:31:38,https://t.co/5euYtX0mU3,Simultaneous Clustering and Estimation of Heterogeneous Graphical Models. (arXiv:1611.09391v1 [stat.ML]) https://t.co/5euYtX0mU3,0,0," Abstract: We consider joint estimation of multiple graphical models arising from heterogeneous and high-dimensional observations. Unlike most previous approaches which assume that the cluster structure is given in advance, an appealing feature of our method is to learn cluster structure while estimating heterogeneous graphical models. This is achieved via a high dimensional version of Expectation Conditional Maximization (ECM) algorithm (Meng and Rubin, 1993). A joint graphical lasso penalty is imposed in the conditional maximization step to extract both homogeneity and heterogeneity components across all clusters. Our algorithm is computationally efficient due to fast sparse learning routines and can be implemented without unsupervised learning knowledge. The superior performance of our method is demonstrated by extensive experiments and its application to a Glioblastoma cancer dataset reveals some new insights in understanding the Glioblastoma cancer. In theory, a non-asymptotic error bound is established for the output directly from our high dimensional ECM algorithm, and it consists of two quantities: statistical error (statistical accuracy) and optimization error (computational complexity). Such a result gives a theoretical guideline in terminating our ECM iterations. "
803773447142785024,2016-11-30 01:31:37,https://t.co/K8fCpjliHY,The empirical size of trained neural networks. (arXiv:1611.09444v1 [stat.ML]) https://t.co/K8fCpjliHY,0,1," Abstract: ReLU neural networks define piecewise linear functions of their inputs. However, initializing and training a neural network is very different from fitting a linear spline. In this paper, we expand empirically upon previous theoretical work to demonstrate features of trained neural networks. Standard network initialization and training produce networks vastly simpler than a naive parameter count would suggest and can impart odd features to the trained network. However, we also show the forced simplicity is beneficial and, indeed, critical for the wide success of these networks. "
803773445477699585,2016-11-30 01:31:37,https://t.co/Hm8uoSqKLb,The Upper Bound on Knots in Neural Networks. (arXiv:1611.09448v1 [stat.ML]) https://t.co/Hm8uoSqKLb,1,5," Abstract: Neural networks with rectified linear unit activations are essentially multivariate linear splines. As such, one of many ways to measure the ""complexity"" or ""expressivity"" of a neural network is to count the number of knots in the spline model. We study the number of knots in fully-connected feedforward neural networks with rectified linear unit activation functions. We intentionally keep the neural networks very simple, so as to make theoretical analyses more approachable. An induction on the number of layers $l$ reveals a tight upper bound on the number of knots in $\mathbb{R} \to \mathbb{R}^p$ deep neural networks. With $n_i \gg 1$ neurons in layer $i = 1, \dots, l$, the upper bound is approximately $n_1 \dots n_l$. We then show that the exact upper bound is tight, and we demonstrate the upper bound with an example. The purpose of these analyses is to pave a path for understanding the behavior of general $\mathbb{R}^q \to \mathbb{R}^p$ neural networks. "
803773443946717188,2016-11-30 01:31:37,https://t.co/vN5XAkeHEE,Graph-Based Manifold Frequency Analysis for Denoising. (arXiv:1611.09510v1 [cs.LG]) https://t.co/vN5XAkeHEE,0,3," Abstract: We propose a new framework for manifold denoising based on processing in the graph Fourier frequency domain, derived from the spectral decomposition of the discrete graph Laplacian. Our approach uses the Spectral Graph Wavelet transform in order to per- form non-iterative denoising directly in the graph frequency domain, an approach inspired by conventional wavelet-based signal denoising methods. We theoretically justify our approach, based on the fact that for smooth manifolds the coordinate information energy is localized in the low spectral graph wavelet sub-bands, while the noise affects all frequency bands in a similar way. Experimental results show that our proposed manifold frequency denoising (MFD) approach significantly outperforms the state of the art denoising meth- ods, and is robust to a wide range of parameter selections, e.g., the choice of k nearest neighbor connectivity of the graph. "
803773442663260160,2016-11-30 01:31:36,https://t.co/myTqMJBsUG,Associative Memory using Dictionary Learning and Expander Decoding. (arXiv:1611.09621v1 [stat.ML]) https://t.co/myTqMJBsUG,0,1," Abstract: An associative memory is a framework of content-addressable memory that stores a collection of message vectors (or a dataset) over a neural network while enabling a neurally feasible mechanism to recover any message in the dataset from its noisy version. Designing an associative memory requires addressing two main tasks: 1) learning phase: given a dataset, learn a concise representation of the dataset in the form of a graphical model (or a neural network), 2) recall phase: given a noisy version of a message vector from the dataset, output the correct message vector via a neurally feasible algorithm over the network learnt during the learning phase. This paper studies the problem of designing a class of neural associative memories which learns a network representation for a large dataset that ensures correction against a large number of adversarial errors during the recall phase. Specifically, the associative memories designed in this paper can store dataset containing $\exp(n)$ $n$-length message vectors over a network with $O(n)$ nodes and can tolerate $\Omega(\frac{n}{{\rm polylog} n})$ adversarial errors. This paper carries out this memory design by mapping the learning phase and recall phase to the tasks of dictionary learning with a square dictionary and iterative error correction in an expander code, respectively. "
803773440926760960,2016-11-30 01:31:36,https://t.co/mY6MJTTCD8,Improving Variational Auto-Encoders using Householder Flow. (arXiv:1611.09630v1 [cs.LG]) https://t.co/mY6MJTTCD8,0,3," Abstract: Variational auto-encoders (VAE) are scalable and powerful generative models. However, the choice of the variational posterior determines tractability and flexibility of the VAE. Commonly, latent variables are modeled using the normal distribution with a diagonal covariance matrix. This results in computational efficiency but typically it is not flexible enough to match the true posterior distribution. One fashion of enriching the variational posterior distribution is application of normalizing flows, i.e., a series of invertible transformations to latent variables with a simple posterior. In this paper, we follow this line of thinking and propose a volume-preserving flow that uses a series of Householder transformations. We show empirically on MNIST dataset and histopathology data that the proposed flow allows to obtain more flexible variational posterior and highly competitive results comparing to other normalizing flows. "
803773438863081472,2016-11-30 01:31:35,https://t.co/ZKssrUP1aM,Probabilistic map-matching using particle filters. (arXiv:1611.09706v1 [stat.ML]) https://t.co/ZKssrUP1aM,0,0," Abstract: Increasing availability of vehicle GPS data has created potentially transformative opportunities for traffic management, route planning and other location-based services. Critical to the utility of the data is their accuracy. Map-matching is the process of improving the accuracy by aligning GPS data with the road network. In this paper, we propose a purely probabilistic approach to map-matching based on a sequential Monte Carlo algorithm known as particle filters. The approach performs map-matching by producing a range of candidate solutions, each with an associated probability score. We outline implementation details and thoroughly validate the technique on GPS data of varied quality. "
803773436262748160,2016-11-30 01:31:35,https://t.co/Vb6hXIPsth,Gossip training for deep learning. (arXiv:1611.09726v1 [cs.CV]) https://t.co/Vb6hXIPsth,0,1," Abstract: We address the issue of speeding up the training of convolutional networks. Here we study a distributed method adapted to stochastic gradient descent (SGD). The parallel optimization setup uses several threads, each applying individual gradient descents on a local variable. We propose a new way to share information between different threads inspired by gossip algorithms and showing good consensus convergence properties. Our method called GoSGD has the advantage to be fully asynchronous and decentralized. We compared our method to the recent EASGD in \cite{elastic} on CIFAR-10 show encouraging results. "
803773434652200960,2016-11-30 01:31:34,https://t.co/AA8CDEjsjj,On the Existence of Synchrostates in Multichannel EEG Signals during Face-perception Tasks. (arXiv:1611.09791v1 [p… https://t.co/AA8CDEjsjj,0,1," Abstract: Phase synchronisation in multichannel EEG is known as the manifestation of functional brain connectivity. Traditional phase synchronisation studies are mostly based on time average synchrony measures hence do not preserve the temporal evolution of the phase difference. Here we propose a new method to show the existence of a small set of unique phase synchronised patterns or ""states"" in multi-channel EEG recordings, each ""state"" being stable of the order of ms, from typical and pathological subjects during face perception tasks. The proposed methodology bridges the concepts of EEG microstates and phase synchronisation in time and frequency domain respectively. The analysis is reported for four groups of children including typical, Autism Spectrum Disorder (ASD), low and high anxiety subjects - a total of 44 subjects. In all cases, we observe consistent existence of these states - termed as synchrostates - within specific cognition related frequency bands (beta and gamma bands), though the topographies of these synchrostates differ for different subject groups with different pathological conditions. The inter-synchrostate switching follows a well-defined sequence capturing the underlying inter-electrode phase relation dynamics in stimulus- and person-centric manner. Our study is motivated from the well-known EEG microstate exhibiting stable potential maps over the scalp. However, here we report a similar observation of quasi-stable phase synchronised states in multichannel EEG. The existence of the synchrostates coupled with their unique switching sequence characteristics could be considered as a potentially new field over contemporary EEG phase synchronisation studies. "
803773433012064256,2016-11-30 01:31:34,https://t.co/XKue1TE7jZ,A Primal-dual Three-operator Splitting Scheme. (arXiv:1611.09805v1 [math.OC]) https://t.co/XKue1TE7jZ,0,0," Abstract: In this paper, we propose a new primal-dual algorithm for minimizing $f(x)+g(x)+h(Ax)$, where $f$, $g$, and $h$ are convex functions, $f$ is differentiable with a Lipschitz continuous gradient, and $A$ is a bounded linear operator. It has some famous primal-dual algorithms for minimizing the sum of two functions as special cases. For example, it reduces to the Chambolle-Pock algorithm when $f=0$ and a primal-dual fixed-point algorithm in [P. Chen, J. Huang, and X. Zhang, A primal-dual fixed-point algorithm for convex separable minimization with applications to image restoration, Inverse Problems, 29 (2013), p.025011] when $g=0$. In addition, it recovers the three-operator splitting scheme in [D. Davis and W. Yin, A three-operator splitting scheme and its optimization applications, arXiv:1504.01032, (2015)] when $A$ is the identity operator. We prove the convergence of this new algorithm for the general case by showing that the iteration is a nonexpansive operator and derive the linear convergence rate with additional assumptions. Comparing to other primal-dual algorithms for solving the same problem, this algorithm extends the range of acceptable parameters to ensure the convergence and has a smaller per-iteration cost. The numerical experiments show the efficiency of this new algorithm by comparing to other primal-dual algorithms. "
803773431044927490,2016-11-30 01:31:34,https://t.co/pVaY8Mcrqr,Co-adaptive learning over a countable space. (arXiv:1611.09816v1 [stat.ML]) https://t.co/pVaY8Mcrqr,0,1," Abstract: Co-adaptation is a special form of on-line learning where an algorithm $\mathcal{A}$ must assist an unknown algorithm $\mathcal{B}$ to perform some task. This is a general framework and has applications in recommendation systems, search, education, and much more. Today, the most common use of co-adaptive algorithms is in brain-computer interfacing (BCI), where algorithms help patients gain and maintain control over prosthetic devices. While previous studies have shown strong empirical results Kowalski et al. (2013); Orsborn et al. (2014) or have been studied in specific examples Merel et al. (2013, 2015), there is no general analysis of the co-adaptive learning problem. Here we will study the co-adaptive learning problem in the online, closed-loop setting. We will prove that, with high probability, co-adaptive learning is guaranteed to outperform learning with a fixed decoder as long as a particular condition is met. "
803773429157601280,2016-11-30 01:31:33,https://t.co/GCaZk9Tvpg,Measuring and modeling the perception of natural and unconstrained gaze in humans and machines. (arXiv:1611.09819v… https://t.co/GCaZk9Tvpg,0,0," Abstract: Humans are remarkably adept at interpreting the gaze direction of other individuals in their surroundings. This skill is at the core of the ability to engage in joint visual attention, which is essential for establishing social interactions. How accurate are humans in determining the gaze direction of others in lifelike scenes, when they can move their heads and eyes freely, and what are the sources of information for the underlying perceptual processes? These questions pose a challenge from both empirical and computational perspectives, due to the complexity of the visual input in real-life situations. Here we measure empirically human accuracy in perceiving the gaze direction of others in lifelike scenes, and study computationally the sources of information and representations underlying this cognitive capacity. We show that humans perform better in face-to-face conditions compared with recorded conditions, and that this advantage is not due to the availability of input dynamics. We further show that humans are still performing well when only the eyes-region is visible, rather than the whole face. We develop a computational model, which replicates the pattern of human performance, including the finding that the eyes-region contains on its own, the required information for estimating both head orientation and direction of gaze. Consistent with neurophysiological findings on task-specific face regions in the brain, the learned computational representations reproduce perceptual effects such as the Wollaston illusion, when trained to estimate direction of gaze, but not when trained to recognize objects or faces. "
803773426997620736,2016-11-30 01:31:33,https://t.co/1vnNrEHmPJ,Exploring Strategies for Classification of External Stimuli Using Statistical Features of the Plant Electrical Res… https://t.co/1vnNrEHmPJ,0,0," Abstract: Plants sense their environment by producing electrical signals which in essence represent changes in underlying physiological processes. These electrical signals, when monitored, show both stochastic and deterministic dynamics. In this paper, we compute 11 statistical features from the raw non-stationary plant electrical signal time series to classify the stimulus applied (causing the electrical signal). By using different discriminant analysis based classification techniques, we successfully establish that there is enough information in the raw electrical signal to classify the stimuli. In the process, we also propose two standard features which consistently give good classification results for three types of stimuli - Sodium Chloride (NaCl), Sulphuric Acid (H2SO4) and Ozone (O3). This may facilitate reduction in the complexity involved in computing all the features for online classification of similar external stimuli in future. "
803773425064050692,2016-11-30 01:31:32,https://t.co/R9OJiDJEPr,Learning Features of Music from Scratch. (arXiv:1611.09827v1 [stat.ML]) https://t.co/R9OJiDJEPr,2,3," Abstract: We introduce a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. We define a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol. We benchmark several machine learning architectures for this task: i) learning from ""hand-crafted"" spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. We show that several end-to-end learning proposals outperform approaches based on learning from hand-crafted audio features. "
803773422970884096,2016-11-30 01:31:32,https://t.co/pyDLf6PRbw,Hierarchical Sparse Modeling: A Choice of Two Group Lasso Formulations. (arXiv:1512.01631v2 [stat.ME] UPDATED) https://t.co/pyDLf6PRbw,0,3," Abstract: Demanding sparsity in estimated models has become a routine practice in statistics. In many situations, we wish to require that the sparsity patterns attained honor certain problem-specific constraints. Hierarchical sparse modeling (HSM) refers to situations in which these constraints specify that one set of parameters be set to zero whenever another is set to zero. In recent years, numerous papers have developed convex regularizers for this form of sparsity structure, which arises in many areas of statistics including interaction modeling, time series analysis, and covariance estimation. In this paper, we observe that these methods fall into two frameworks, the group lasso (GL) and latent overlapping group lasso (LOG), which have not been systematically compared in the context of HSM. The purpose of this paper is to provide a side-by-side comparison of these two frameworks for HSM in terms of their statistical properties and computational efficiency. We call special attention to GL's more aggressive shrinkage of parameters deep in the hierarchy, a property not shared by LOG. In terms of computation, we introduce a finite-step algorithm that exactly solves the proximal operator of LOG for a certain simple HSM structure; we later exploit this to develop a novel path-based BCD scheme for general HSM structures. Both algorithms greatly improve the computational performance of LOG. Finally, we compare the two methods in the context of covariance estimation, where we introduce a new sparsely-banded estimator using LOG, which we show achieves the statistical advantages of an existing GL-based method but is simpler to express and more efficient to compute. "
803773420794019840,2016-11-30 01:31:31,https://t.co/o59WJAaP0I,RSG: Beating Subgradient Method without Smoothness and Strong Convexity. (arXiv:1512.03107v12 [math.OC] UPDATED) https://t.co/o59WJAaP0I,0,1," Abstract: In this paper, we study the efficiency of a {\bf R}estarted {\bf S}ub{\bf G}radient (RSG) method that periodically restarts the standard subgradient method (SG). We show that, when applied to a broad class of convex optimization problems, RSG method can find an $\epsilon$-optimal solution with a low complexity than SG method. In particular, we first show that RSG can reduce the dependence of SG's iteration complexity on the distance between the initial solution and the optimal set to that between the $\epsilon$-level set and the optimal set. In addition, we show the advantages of RSG over SG in solving three different families of convex optimization problems. (a) For the problems whose epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with local quadratic growth property, RSG has an $O(\frac{1}{\epsilon}\log(\frac{1}{\epsilon}))$ iteration complexity. (c) For the problems that admit a local Kurdyka-\L ojasiewicz property with a power constant of $\beta\in[0,1)$, RSG has an $O(\frac{1}{\epsilon^{2\beta}}\log(\frac{1}{\epsilon}))$ iteration complexity. On the contrary, with only the standard analysis, the iteration complexity of SG is known to be $O(\frac{1}{\epsilon^2})$ for these three classes of problems. The novelty of our analysis lies at exploiting the lower bound of the first-order optimality residual at the $\epsilon$-level set. It is this novelty that allows us to explore the local properties of functions (e.g., local quadratic growth property, local Kurdyka-\L ojasiewicz property, more generally local error bounds) to develop the improved convergence of RSG. We demonstrate the effectiveness of the proposed algorithms on several machine learning tasks including regression and classification. "
803773418982121472,2016-11-30 01:31:31,https://t.co/3dJNRlJrPq,Achieving Budget-optimality with Adaptive Schemes in Crowdsourcing. (arXiv:1602.03481v2 [cs.LG] UPDATED) https://t.co/3dJNRlJrPq,0,0," Abstract: Adaptive schemes, where tasks are assigned based on the data collected thus far, are widely used in practical crowdsourcing systems to efficiently allocate the budget. However, existing theoretical analyses of crowdsourcing systems suggest that the gain of adaptive task assignments is minimal. To bridge this gap, we investigate this question under a strictly more general probabilistic model, which has been recently introduced to model practical crowdsourcing datasets. Under this generalized Dawid-Skene model, we characterize the fundamental trade-off between budget and accuracy. We introduce a novel adaptive scheme that matches this fundamental limit. A given budget is allocated over multiple rounds. In each round, a subset of tasks with high enough confidence are classified, and increasing budget is allocated on remaining ones that are potentially more difficult. On each round, decisions are made based on the leading eigenvector of (weighted) non-backtracking operator corresponding to the bipartite assignment graph. We further quantify the gain of adaptivity, by comparing the tradeoff with the one for non-adaptive schemes, and confirm that the gain is significant and can be made arbitrarily large depending on the distribution of the difficulty level of the tasks at hand. "
803773417229090816,2016-11-30 01:31:30,https://t.co/8iIKffyZLQ,Learning to Abstain from Binary Prediction. (arXiv:1602.08151v2 [cs.LG] UPDATED) https://t.co/8iIKffyZLQ,0,0," Abstract: A binary classifier capable of abstaining from making a label prediction has two goals in tension: minimizing errors, and avoiding abstaining unnecessarily often. In this work, we exactly characterize the best achievable tradeoff between these two goals in a general semi-supervised setting, given an ensemble of predictors of varying competence as well as unlabeled data on which we wish to predict or abstain. We give an algorithm for learning a classifier in this setting which trades off its errors with abstentions in a minimax optimal manner, is as efficient as linear learning and prediction, and is demonstrably practical. Our analysis extends to a large class of loss functions and other scenarios, including ensembles comprised of specialists that can themselves abstain. "
803773415337304064,2016-11-30 01:31:30,https://t.co/toR7bou8WQ,Reconstructing undirected graphs from eigenspaces. (arXiv:1603.08113v2 [math.ST] UPDATED) https://t.co/toR7bou8WQ,0,1," Abstract: In this paper, we aim at recovering an undirected weighted graph of N vertices from the knowledge of a perturbed version of the eigenspaces of its adjacency matrix W. Our approach is based on minimizing a cost function given by the Frobenius norm of the commutator AB-BA between symmetric matrices A and B. In the Erd\H{o}s-R\'enyi model with no self-loops, we show that identifiability (i.e., the ability to reconstruct W from the knowledge of its eigenspaces) follows a sharp phase transition on the expected number of edges with threshold function N\log N/2. Given an estimation of the eigenspaces based on a n-sample, we provide support selection procedures from theoretical and practical point of views. In particular, when deleting an edge from the active support, our study unveils that our test statistic is the order of O(1/n) when we overestimate the true support and lower bounded by a positive constant when the estimated support is smaller than the true support. This feature leads to a powerful practical support estimation procedure when properly thresholding. Simulated and real life numerical experiments assert our new methodology. "
803773413189881856,2016-11-30 01:31:29,https://t.co/2213lFAPFP,Causality on Longitudinal Data: Stable Specification Search in Constrained Structural Equation Modeling. (arXiv:16… https://t.co/2213lFAPFP,0,1," Abstract: A typical problem in causal modeling is the instability of model structure learning, i.e., small changes in finite data can result in completely different optimal models. The present work introduces a novel causal modeling algorithm for longitudinal data, that is robust for finite samples based on recent advances in stability selection using subsampling and selection algorithms. Our approach uses exploratory search but allows incorporation of prior knowledge, e.g., that causal relationships do not go back in time. We represent causal relationships using structural equation models. Models are scored along two objectives: the model fit and the model complexity. Since both objectives are often conflicting we apply a multi-objective evolutionary algorithm to search for Pareto optimal models. To handle the instability of small finite data samples, we repeatedly subsample the data and select those substructures (from the optimal models) that are both stable and parsimonious. These substructures can be visualized through a causal graph. Our more exploratory approach outperforms state-of-the-art alternative approaches on a simulated data set with a known ground truth. We also present the results of our method on three real-world longitudinal data sets on chronic fatigue syndrome, Alzheimer disease, and chronic kidney disease. The findings obtained with our approach are generally in line with results from more hypothesis-driven analyses in earlier studies and do suggest some novel relationships that deserve further research. "
803773410429894656,2016-11-30 01:31:29,https://t.co/V1eTFAVCTe,A Unifying Framework for Sparse Gaussian Process Approximation using Power Expectation Propagation. (arXiv:1605.07… https://t.co/V1eTFAVCTe,0,0," Abstract: Gaussian processes (GPs) are flexible distributions over functions that enable high-level assumptions about unknown functions to be encoded in a parsimonious, flexible and general way. Although elegant, the application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. In this paper we develop a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that unifies a large number of these pseudo-point approximations. Unlike much of the previous venerable work in this area, the new framework is built on standard methods for approximate inference (variational free-energy, EP and power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way all of approximation is performed at `inference time' rather than at `modelling time' resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression, classification and state space modelling tasks. "
803773408521650177,2016-11-30 01:31:28,https://t.co/797yd2E4s3,Integrated perception with recurrent multi-task neural networks. (arXiv:1606.01735v2 [stat.ML] UPDATED) https://t.co/797yd2E4s3,0,1," Abstract: Modern discriminative predictors have been shown to match natural intelligences in specific perceptual tasks in image classification, object and part detection, boundary extraction, etc. However, a major advantage that natural intelligences still have is that they work well for ""all"" perceptual problems together, solving them efficiently and coherently in an ""integrated manner"". In order to capture some of these advantages in machine perception, we ask two questions: whether deep neural networks can learn universal image representations, useful not only for a single task but for all of them, and how the solutions to the different tasks can be integrated in this framework. We answer by proposing a new architecture, which we call ""MultiNet"", in which not only deep image features are shared between tasks, but where tasks can interact in a recurrent manner by encoding the results of their analysis in a common shared representation of the data. In this manner, we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then, more significantly, by integrating their solutions in the common representation. "
803773406533521408,2016-11-30 01:31:28,https://t.co/hyoA8YTV4w,Column Networks for Collective Classification. (arXiv:1609.04508v2 [cs.LG] UPDATED) https://t.co/hyoA8YTV4w,0,1," Abstract: Relational learning deals with data that are characterized by relational structures. An important task is collective classification, which is to jointly classify networked objects. While it holds a great promise to produce a better accuracy than non-collective classifiers, collective classification is computational challenging and has not leveraged on the recent breakthroughs of deep learning. We present Column Network (CLN), a novel deep learning model for collective classification in multi-relational domains. CLN has many desirable theoretical properties: (i) it encodes multi-relations between any two instances; (ii) it is deep and compact, allowing complex functions to be approximated at the network level with a small set of free parameters; (iii) local and relational features are learned simultaneously; (iv) long-range, higher-order dependencies between instances are supported naturally; and (v) crucially, learning and inference are efficient, linear in the size of the network and the number of relations. We evaluate CLN on multiple real-world applications: (a) delay prediction in software projects, (b) PubMed Diabetes publication classification and (c) film genre classification. In all applications, CLN demonstrates a higher accuracy than state-of-the-art rivals. "
803773405174628352,2016-11-30 01:31:27,https://t.co/wqJLy3U0Kb,Removal of Batch Effects using Distribution-Matching Residual Networks. (arXiv:1610.04181v3 [stat.ML] UPDATED) https://t.co/wqJLy3U0Kb,0,1," Abstract: Sources of variability in experimentally derived data include measurement error in addition to the physical phenomena of interest. This measurement error is a combination of systematic components, originating from the measuring instrument, and random measurement errors. Several novel biological technologies, such as mass cytometry and single-cell RNA-seq, are plagued with systematic errors that may severely affect statistical analysis if the data is not properly calibrated. Here, we propose a novel deep learning approach for removing systematic batch effects. Our method is based on a residual network, trained to minimize the Maximum Mean Discrepancy (MMD) between the multivariate distributions of two replicates, measured in different batches. We apply our method to mass cytometry and single-cell RNA-seq datasets, and demonstrate that it effectively attenuates batch effects, and outperforms several popular methods. "
803773403777728512,2016-11-30 01:31:27,https://t.co/TuRTmaym8a,Variational Bayes In Private Settings (VIPS). (arXiv:1611.00340v2 [stat.ML] UPDATED) https://t.co/TuRTmaym8a,0,0," Abstract: We provide a general framework for privacy-preserving variational Bayes (VB) for a large class of probabilistic models, called the conjugate exponential (CE) family. Our primary observation is that when models are in the CE family, we can privatise the variational posterior distributions simply by perturbing the expected sufficient statistics of the complete-data likelihood. For widely used non-CE models with binomial likelihoods (e.g., logistic regression), we exploit the P{\'o}lya-Gamma data augmentation scheme to bring such models into the CE family, such that inferences in the modified model resemble the original (private) variational Bayes algorithm as closely as possible. The iterative nature of variational Bayes presents a further challenge for privacy preservation, as each iteration increases the amount of noise needed. We overcome this challenge by combining: (1) a relaxed notion of differential privacy, called {\it{concentrated differential privacy}}, which provides a tight bound on the privacy cost of multiple VB iterations and thus significantly decreases the amount of additive noise; and (2) the privacy amplification effect resulting from subsampling mini-batches from large-scale data in stochastic learning. We empirically demonstrate the effectiveness of our method in CE and non-CE models including latent Dirichlet allocation (LDA) and Bayesian logistic regression, evaluated on real-world datasets. "
803412052735827968,2016-11-29 01:35:34,https://t.co/DOmN5H1sFV,A Benchmark and Comparison of Active Learning for Logistic Regression. (arXiv:1611.08618v1 [stat.ML]) https://t.co/DOmN5H1sFV,0,6," Abstract: Various active learning methods based on logistic regression have been proposed. In this paper, we investigate seven state-of-the-art strategies, present an extensive benchmark, and provide a better understanding of their underlying characteristics. Experiments are carried out both on 3 synthetic datasets and 43 real-world datasets, providing insights into the behaviour of these active learning methods with respect to classification accuracy and their computational cost. "
803412051175600128,2016-11-29 01:35:34,https://t.co/xZ27A5fGhs,Patient-Driven Privacy Control through Generalized Distillation. (arXiv:1611.08648v1 [cs.CR]) https://t.co/xZ27A5fGhs,0,4," Abstract: The introduction of data analytics into medicine has changed the nature of treatment. In this, patients are asked to disclose personal information such as genetic markers, lifestyle habits, and clinical history. This data is then used by statistical models to predict personalized treatments. However, due to privacy concerns, patients often desire to withhold sensitive information. This self-censorship can impede proper diagnosis and treatment, which may lead to serious health complications and even death. In this paper, we present privacy distillation, a mechanism which allows patients to control the type and amount of information they wish to disclose to the healthcare providers for use in statistical models. Meanwhile, it retains the accuracy of models that have access to all patient data under a sufficient but not full set of privacy-relevant information. We validate privacy distillation using a corpus of patients prescribed to warfarin for a personalized dosage. We use a deep neural network to implement privacy distillation for training and making dose predictions. We find that privacy distillation with sufficient privacy-relevant information i) retains accuracy almost as good as having all patient data (only 3% worse), and ii) is effective at preventing errors that introduce health-related risks (yielding on average 3.9% of under- or over-prescriptions). "
803412048604446721,2016-11-29 01:35:33,https://t.co/fIu4bc0OiL,Learning without recall in directed circles and rooted trees. (arXiv:1611.08791v1 [stat.AP]) https://t.co/fIu4bc0OiL,0,1," Abstract: This work investigates the case of a network of agents that attempt to learn some unknown state of the world amongst the finitely many possibilities. At each time step, agents all receive random, independently distributed private signals whose distributions are dependent on the unknown state of the world. However, it may be the case that some or any of the agents cannot distinguish between two or more of the possible states based only on their private observations, as when several states result in the same distribution of the private signals. In our model, the agents form some initial belief (probability distribution) about the unknown state and then refine their beliefs in accordance with their private observations, as well as the beliefs of their neighbors. An agent learns the unknown state when her belief converges to a point mass that is concentrated at the true state. A rational agent would use the Bayes' rule to incorporate her neighbors' beliefs and own private signals over time. While such repeated applications of the Bayes' rule in networks can become computationally intractable, in this paper, we show that in the canonical cases of directed star, circle or path networks and their combinations, one can derive a class of memoryless update rules that replicate that of a single Bayesian agent but replace the self beliefs with the beliefs of the neighbors. This way, one can realize an exponentially fast rate of learning similar to the case of Bayesian (fully rational) agents. The proposed rules are a special case of the Learning without Recall. "
803412046851219456,2016-11-29 01:35:33,https://t.co/wH9kgDOoZ8,Learning a Natural Language Interface with Neural Programmer. (arXiv:1611.08945v1 [cs.CL]) https://t.co/wH9kgDOoZ8,3,2," Abstract: Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.2% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser. "
803412045156777984,2016-11-29 01:35:32,https://t.co/bo9DGsUCAC,Prediction of Video Popularity in the Absence of Reliable Data from Video Hosting Services: Utility of Traces Left… https://t.co/bo9DGsUCAC,0,1," Abstract: With the growth of user-generated content, we observe the constant rise of the number of companies, such as search engines, content aggregators, etc., that operate with tremendous amounts of web content not being the services hosting it. Thus, aiming to locate the most important content and promote it to the users, they face the need of estimating the current and predicting the future content popularity. In this paper, we approach the problem of video popularity prediction not from the side of a video hosting service, as done in all previous studies, but from the side of an operating company, which provides a popular video search service that aggregates content from different video hosting websites. We investigate video popularity prediction based on features from three primary sources available for a typical operating company: first, the content hosting provider may deliver its data via its API, second, the operating company makes use of its own search and browsing logs, third, the company crawls information about embeds of a video and links to a video page from publicly available resources on the Web. We show that video popularity prediction based on the embed and link data coupled with the internal search and browsing data significantly improves video popularity prediction based only on the data provided by the video hosting and can even adequately replace the API data in the cases when it is partly or completely unavailable. "
803412043726548992,2016-11-29 01:35:32,https://t.co/ZAP0jNYNSj,Generalizing the Kelly strategy. (arXiv:1611.09130v1 [stat.ML]) https://t.co/ZAP0jNYNSj,0,2," Abstract: Prompted by a recent experiment by Victor Haghani and Richard Dewey, this note generalises the Kelly strategy (optimal for simple investment games with log utility) to a large class of practical utility functions and including the effect of extraneous wealth. A counterintuitive result is proved : for any continuous, concave, differentiable utility function, the optimal choice at every point depends only on the probability of reaching that point. The practical calculation of the optimal action at every stage is made possible through use of the binomial expansion, reducing the problem size from exponential to quadratic. Applications include (better) automatic investing and risk taking under uncertainty. "
803412042380115968,2016-11-29 01:35:32,https://t.co/lhXTKCIJRz,Proceedings of NIPS 2016 Workshop on Interpretable Machine Learning for Complex Systems. (arXiv:1611.09139v1 [stat… https://t.co/lhXTKCIJRz,6,27," Abstract: This is the Proceedings of NIPS 2016 Workshop on Interpretable Machine Learning for Complex Systems, held in Barcelona, Spain on December 9, 2016 "
803412040618541056,2016-11-29 01:35:31,https://t.co/7rqLeC3HLn,AutoMOS: Learning a non-intrusive assessor of naturalness-of-speech. (arXiv:1611.09207v1 [cs.CL]) https://t.co/7rqLeC3HLn,0,2," Abstract: Developers of text-to-speech synthesizers (TTS) often make use of human raters to assess the quality of synthesized speech. We demonstrate that we can model human raters' mean opinion scores (MOS) of synthesized speech using a deep recurrent neural network whose inputs consist solely of a raw waveform. Our best models provide utterance-level estimates of MOS only moderately inferior to sampled human ratings, as shown by Pearson and Spearman correlations. When multiple utterances are scored and averaged, a scenario common in synthesizer quality assessment, AutoMOS achieves correlations approaching those of human raters. The AutoMOS model has a number of applications, such as the ability to explore the parameter space of a speech synthesizer without requiring a human-in-the-loop. "
803412039276302336,2016-11-29 01:35:31,https://t.co/HxCfB8YzQ4,Efficient Convolutional Auto-Encoding via Random Convexification and Frequency-Domain Minimization. (arXiv:1611.09… https://t.co/HxCfB8YzQ4,1,5," Abstract: The omnipresence of deep learning architectures such as deep convolutional neural networks (CNN)s is fueled by the synergistic combination of ever-increasing labeled datasets and specialized hardware. Despite the indisputable success, the reliance on huge amounts of labeled data and specialized hardware can be a limiting factor when approaching new applications. To help alleviating these limitations, we propose an efficient learning strategy for layer-wise unsupervised training of deep CNNs on conventional hardware in acceptable time. Our proposed strategy consists of randomly convexifying the reconstruction contractive auto-encoding (RCAE) learning objective and solving the resulting large-scale convex minimization problem in the frequency domain via coordinate descent (CD). The main advantages of our proposed learning strategy are: (1) single tunable optimization parameter; (2) fast and guaranteed convergence; (3) possibilities for full parallelization. Numerical experiments show that our proposed learning strategy scales (in the worst case) linearly with image size, number of filters and filter size. "
803412037623828480,2016-11-29 01:35:31,https://t.co/xuPR3xt5oN,Accelerated Gradient Temporal Difference Learning. (arXiv:1611.09328v1 [cs.AI]) https://t.co/xuPR3xt5oN,0,5," Abstract: The family of temporal difference (TD) methods span a spectrum from computationally frugal linear methods like TD({\lambda}) to data efficient least squares methods. Least square methods make the best use of available data directly computing the TD solution and thus do not require tuning a typically highly sensitive learning rate parameter, but require quadratic computation and storage. Recent algorithmic developments have yielded several sub-quadratic methods that use an approximation to the least squares TD solution, but incur bias. In this paper, we propose a new family of accelerated gradient TD (ATD) methods that (1) provide similar data efficiency benefits to least-squares methods, at a fraction of the computation and storage (2) significantly reduce parameter sensitivity compared to linear TD methods, and (3) are asymptotically unbiased. We illustrate these claims with a proof of convergence in expectation and experiments on several benchmark domains and a large-scale industrial energy allocation domain. "
803412036273274880,2016-11-29 01:35:30,https://t.co/s9UHjlOwQ1,Quantum Machine Learning. (arXiv:1611.09347v1 [quant-ph]) https://t.co/s9UHjlOwQ1,1,11," Abstract: Recent progress implies that a crossover between machine learning and quantum information processing benefits both fields. Traditional machine learning has dramatically improved the benchmarking and control of experimental quantum computing systems, including adaptive quantum phase estimation and designing quantum computing gates. On the other hand, quantum mechanics offers tantalizing prospects to enhance machine learning, ranging from reduced computational complexity to improved generalization performance. The most notable examples include quantum enhanced algorithms for principal component analysis, quantum support vector machines, and quantum Boltzmann machines. Progress has been rapid, fostered by demonstrations of midsized quantum optimizers which are predicted to soon outperform their classical counterparts. Further, we are witnessing the emergence of a physical theory pinpointing the fundamental and natural limitations of learning. Here we survey the cutting edge of this merger and list several open problems. "
803412034423582724,2016-11-29 01:35:30,https://t.co/0cGDfXEGBP,Bethe Projections for Non-Local Inference. (arXiv:1503.01397v3 [stat.ML] UPDATED) https://t.co/0cGDfXEGBP,0,3," Abstract: Many inference problems in structured prediction are naturally solved by augmenting a tractable dependency structure with complex, non-local auxiliary objectives. This includes the mean field family of variational inference algorithms, soft- or hard-constrained inference using Lagrangian relaxation or linear programming, collective graphical models, and forms of semi-supervised learning such as posterior regularization. We present a method to discriminatively learn broad families of inference objectives, capturing powerful non-local statistics of the latent variables, while maintaining tractable and provably fast inference using non-Euclidean projected gradient descent with a distance-generating function given by the Bethe entropy. We demonstrate the performance and flexibility of our method by (1) extracting structured citations from research papers by learning soft global constraints, (2) achieving state-of-the-art results on a widely-used handwriting recognition task using a novel learned non-convex inference procedure, and (3) providing a fast and highly scalable algorithm for the challenging problem of inference in a collective graphical model applied to bird migration. "
803412033169461248,2016-11-29 01:35:30,https://t.co/uFsa4flLGi],Layered Adaptive Importance Sampling. (arXiv:1505.04732v4 [https://t.co/uFsa4flLGi] UPDATED) https://t.co/QjWpdlCBBk,0,3,INDEXERROR
803412031810535424,2016-11-29 01:35:29,https://t.co/MXFbi1d9FH,A General Retraining Framework for Scalable Adversarial Classification. (arXiv:1604.02606v2 [cs.GT] UPDATED) https://t.co/MXFbi1d9FH,0,3," Abstract: Traditional classification algorithms assume that training and test data come from similar distributions. This assumption is violated in adversarial settings, where malicious actors modify instances to evade detection. A number of custom methods have been developed for both adversarial evasion attacks and robust learning. We propose the first systematic and general-purpose retraining framework which can: a) boost robustness of an \emph{arbitrary} learning algorithm, in the face of b) a broader class of adversarial models than any prior methods. We show that, under natural conditions, the retraining framework minimizes an upper bound on optimal adversarial risk, and show how to extend this result to account for approximations of evasion attacks. Extensive experimental evaluation demonstrates that our retraining methods are nearly indistinguishable from state-of-the-art algorithms for optimizing adversarial risk, but are more general and far more scalable. The experiments also confirm that without retraining, our adversarial framework dramatically reduces the effectiveness of learning. In contrast, retraining significantly boosts robustness to evasion attacks without significantly compromising overall accuracy. "
803412030422155264,2016-11-29 01:35:29,https://t.co/veg0asBYNs,Unified Scalable Equivalent Formulations for Schatten Quasi-Norms. (arXiv:1606.00668v2 [cs.IT] UPDATED) https://t.co/veg0asBYNs,0,3," Abstract: The Schatten quasi-norm can be used to bridge the gap between the nuclear norm and rank function, and is the tighter approximation to matrix rank. However, most existing Schatten quasi-norm minimization (SQNM) algorithms, as well as for nuclear norm minimization, are too slow or even impractical for large-scale problems, due to the SVD or EVD of the whole matrix in each iteration. In this paper, we rigorously prove that for any p, p1, p2>0 satisfying 1/p=1/p1+1/p2, the Schatten-p quasi-norm of any matrix is equivalent to minimizing the product of the Schatten-p1 norm (or quasi-norm) and Schatten-p2 norm (or quasi-norm) of its two factor matrices. Then we present and prove the equivalence relationship between the product formula of the Schatten quasi-norm and its weighted sum formula for the two cases of p1 and p2: p1=p2 and p1\neq p2. In particular, when p>1/2, there is an equivalence between the Schatten-p quasi-norm of any matrix and the Schatten-2p norms of its two factor matrices, where the widely used equivalent formulation of the nuclear norm can be viewed as a special case. That is, various SQNM problems with p>1/2 can be transformed into the one only involving smooth, convex norms of two factor matrices, which can lead to simpler and more efficient algorithms than conventional methods. We further extend the theoretical results of two factor matrices to the cases of three and more factor matrices, from which we can see that for any 0<p<1, the Schatten-p quasi-norm of any matrix is the minimization of the mean of the Schatten-(p3+1)p norms of all factor matrices, where p3 denotes the largest integer not exceeding 1/p. In other words, for any 0<p<1, the SQNM problem can be transformed into an optimization problem only involving the smooth, convex norms of multiple factor matrices. "
803412028455059456,2016-11-29 01:35:28,https://t.co/aeuVhFANRD,Learning in Quantum Control: High-Dimensional Global Optimization for Noisy Quantum Dynamics. (arXiv:1607.03428v3 … https://t.co/aeuVhFANRD,0,3," Abstract: Quantum control is valuable for various quantum technologies such as high-fidelity gates for universal quantum computing, adaptive quantum-enhanced metrology, and ultra-cold atom manipulation. Although supervised machine learning and reinforcement learning are widely used for optimizing control parameters in classical systems, quantum control for parameter optimization is mainly pursued via gradient-based greedy algorithms. Although the quantum fitness landscape is often compatible with greedy algorithms, sometimes greedy algorithms yield poor results, especially for large-dimensional quantum systems. We employ differential evolution algorithms to circumvent the stagnation problem of non-convex optimization. We improve quantum control fidelity for noisy system by averaging over the objective function. To reduce computational cost, we introduce heuristics for early termination of runs and for adaptive selection of search subspaces. Our implementation is massively parallel and vectorized to reduce run time even further. We demonstrate our methods with two examples, namely quantum phase estimation and quantum gate design, for which we achieve superior fidelity and scalability than obtained using greedy algorithms. "
803412026957643776,2016-11-29 01:35:28,https://t.co/a3n0ztYfXk,Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition. (arXiv:1607.06017v2 [math.OC] UPDATE… https://t.co/a3n0ztYfXk,0,4," Abstract: We study $k$-GenEV, the problem of finding the top $k$ generalized eigenvectors, and $k$-CCA, the problem of finding the top $k$ vectors in canonical-correlation analysis. We propose algorithms $\mathtt{LazyEV}$ and $\mathtt{LazyCCA}$ to solve the two problems with running times linearly dependent on the input size and on $k$. Furthermore, our algorithms are DOUBLY-ACCELERATED: our running times depend only on the square root of the matrix condition number, and on the square root of the eigengap. This is the first such result for both $k$-GenEV or $k$-CCA. We also provide the first gap-free results, which provide running times that depend on $1/\sqrt{\varepsilon}$ rather than the eigengap. "
803412025439375360,2016-11-29 01:35:28,https://t.co/AOxUnkK4Zm,Sequence Graph Transform (SGT): A Feature Extraction Function for Sequence Data Mining. (arXiv:1608.03533v6 [stat.… https://t.co/AOxUnkK4Zm,0,5," Abstract: The ubiquitous presence of sequence data across fields such as the web, healthcare, bioinformatics, and text mining has made sequence mining a vital research area. However, sequence mining is particularly challenging because of difficulty in finding (dis)similarity/distance between sequences. This is because a distance measure between sequences is not obvious due to their unstructuredness---arbitrary strings of arbitrary length. Feature representations, such as n-grams, are often used but they either compromise on extracting both short- and long-term sequence patterns or have a high computation. We propose a new function, Sequence Graph Transform (SGT), that extracts the short- and long-term sequence features and embeds them in a finite-dimensional feature space. Importantly, SGT has low computation and can extract any amount of short- to long-term patterns without any increase in the computation, also proved theoretically in this paper. Due to this, SGT yields superior result with significantly higher accuracy and lower computation compared to the existing methods. We show it via several experimentation and SGT's real world application for clustering, classification, search and visualization as examples. "
803412024113922048,2016-11-29 01:35:27,https://t.co/AbEoB9pXG2,Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model. (arXiv:1609.00680v6 [q-bio.BM] UP… https://t.co/AbEoB9pXG2,0,3," Abstract: Recently exciting progress has been made on protein contact prediction, but the predicted contacts for proteins without many sequence homologs is still of low quality and not very useful for de novo structure prediction. This paper presents a new deep learning method that predicts contacts by integrating both evolutionary coupling (EC) and sequence conservation information through an ultra-deep neural network formed by two deep residual networks. This deep neural network allows us to model very complex sequence-contact relationship as well as long-range inter-contact correlation. Our method greatly outperforms existing contact prediction methods and leads to much more accurate contact-assisted protein folding. Tested on three datasets of 579 proteins, the average top L long-range prediction accuracy obtained our method, the representative EC method CCMpred and the CASP11 winner MetaPSICOV is 0.47, 0.21 and 0.30, respectively; the average top L/10 long-range accuracy of our method, CCMpred and MetaPSICOV is 0.77, 0.47 and 0.59, respectively. Ab initio folding using our predicted contacts as restraints can yield correct folds (i.e., TMscore>0.6) for 203 test proteins, while that using MetaPSICOV- and CCMpred-predicted contacts can do so for only 79 and 62 proteins, respectively. Further, our contact-assisted models have much better quality than template-based models. Using our predicted contacts as restraints, we can (ab initio) fold 208 of the 398 membrane proteins with TMscore>0.5. By contrast, when the training proteins of our method are used as templates, homology modeling can only do so for 10 of them. One interesting finding is that even if we do not train our prediction models with any membrane proteins, our method works very well on membrane protein prediction. Finally, in recent blind CAMEO benchmark our method successfully folded 5 test proteins with a novel fold. "
803412022675247104,2016-11-29 01:35:27,https://t.co/TYlpn1nR9z,Private Topic Modeling. (arXiv:1609.04120v2 [stat.ML] UPDATED) https://t.co/TYlpn1nR9z,0,5," Abstract: We develop a privatised stochastic variational inference method for Latent Dirichlet Allocation (LDA). The iterative nature of stochastic variational inference presents challenges: multiple iterations are required to obtain accurate posterior distributions, yet each iteration increases the amount of noise that must be added to achieve a reasonable degree of privacy. We propose a practical algorithm that overcomes this challenge by combining: (1) A relaxed notion of the differential privacy, called concentrated differential privacy, which provides high probability bounds for cumulative privacy loss, which is well suited for iterative algorithms, rather than focusing on single-query loss; and (2) Privacy amplification resulting from subsampling of large-scale data. Focusing on conjugate exponential family models, in our private variational inference, all the posterior distributions will be privatised by simply perturbing expected sufficient statistics. Using Wikipedia data, we illustrate the effectiveness of our algorithm for large-scale data. "
803412021253378048,2016-11-29 01:35:27,https://t.co/zsMXxgTEiS,Understanding Deep Neural Networks with Rectified Linear Units. (arXiv:1611.01491v3 [cs.LG] UPDATED) https://t.co/zsMXxgTEiS,1,22," Abstract: In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give the first-ever polynomial time (in the size of data) algorithm to train a ReLU DNN with one hidden layer to {\em global optimality}. This follows from our complete characterization of the ReLU DNN function class whereby we show that a $\R^n \to \R$ function is representable by a ReLU DNN {\em if and only if} it is a continuous piecewise linear function. The main tool used to prove this characterization is an elegant result from tropical geometry. Further, for the $n=1$ case, we show that a single hidden layer suffices to express all piecewise linear functions, and we give tight bounds for the size of such a ReLU DNN. We follow up with gap results showing that there is a smoothly parameterized family of $\R\to \R$ ""hard"" functions that lead to an exponential blow-up in size, if the number of layers is decreased by a small amount. An example consequence of our gap theorem is that for every natural number $N$, there exists a function representable by a ReLU DNN with depth $N^2+1$ and total size $N^3$, such that any ReLU DNN with depth at most $N + 1$ will require at least $\frac12N^{N+1}-1$ total nodes. Finally, we construct a family of $\R^n\to \R$ functions for $n\geq 2$ (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension $n$ at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on $n$ has not been achieved by previous families of ""hard"" functions in the neural nets literature. This construction utilizes the theory of zonotopes from polyhedral theory. "
803412020104151040,2016-11-29 01:35:26,https://t.co/FBtJjIMDc8,Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning. (arXiv:1611.01722… https://t.co/FBtJjIMDc8,0,3," Abstract: We propose a simple algorithm to train stochastic neural networks to draw samples from given target distributions for probabilistic inference. Our method is based on iteratively adjusting the neural network parameters so that the output changes along a Stein variational gradient that maximumly decreases the KL divergence with the target distribution. Our method works for any target distribution specified by their unnormalized density function, and can train any black-box architectures that are differentiable in terms of the parameters we want to adapt. As an application of our method, we propose an amortized MLE algorithm for training deep energy model, where a neural sampler is adaptively trained to approximate the likelihood function. Our method mimics an adversarial game between the deep energy model and the neural sampler, and obtains realistic-looking images competitive with the state-of-the-art results. "
803412016815865856,2016-11-29 01:35:26,https://t.co/U4M7C5mEI5,Deep Learning with Sets and Point Clouds. (arXiv:1611.04500v2 [stat.ML] UPDATED) https://t.co/U4M7C5mEI5,0,4," Abstract: We study a simple notion of structural invariance that readily suggests a parameter-sharing scheme in deep neural networks. In particular, we define structure as a collection of relations, and derive graph convolution and recurrent neural networks as special cases. We study composition of basic structures in defining models that are invariant to more complex ""product"" structures such as graph of graphs, sets of images or sequence of sets. For demonstration, our experimental results are focused on the setting where the discrete structure of interest is a set. We present results on several novel and non-trivial problems on sets, including semi-supervised learning using clustering information, point-cloud classification and set outlier detection. "
803412015326838784,2016-11-29 01:35:25,https://t.co/9ZAZUXJU3H,Stochastic Gradient Descent in Continuous Time. (arXiv:1611.05545v2 [math.PR] UPDATED) https://t.co/9ZAZUXJU3H,1,11," Abstract: We consider stochastic gradient descent for continuous-time models. Traditional approaches for the statistical estimation of continuous-time models, such as batch optimization, can be impractical for large datasets where observations occur over a long period of time. Stochastic gradient descent provides a computationally efficient method for such statistical learning problems. The stochastic gradient descent algorithm performs an online parameter update in continuous time, with the parameter updates $\theta_t$ satisfying a stochastic differential equation. We prove that $\lim_{t \rightarrow \infty} \nabla \bar g(\theta_t) = 0$ where $\bar g$ is a natural objective function for the estimation of the continuous-time dynamics. The convergence proof leverages ergodicity by using an appropriate Poisson equation to help describe the evolution of the parameters for large times. Numerical analysis of the stochastic gradient descent algorithm is presented for several examples, including the Ornstein-Uhlenbeck process, Burger's stochastic partial differential equation, and reinforcement learning. "
803412013674352640,2016-11-29 01:35:25,https://t.co/ZV7IndKL9y,Support Vector Algorithms for Optimizing the Partial Area Under the ROC Curve. (arXiv:1605.04337v1 [cs.LG] CROSS L… https://t.co/ZV7IndKL9y,1,8," Abstract: The area under the ROC curve (AUC) is a widely used performance measure in machine learning. Increasingly, however, in several applications, ranging from ranking to biometric screening to medicine, performance is measured not in terms of the full area under the ROC curve, but in terms of the \emph{partial} area under the ROC curve between two false positive rates. In this paper, we develop support vector algorithms for directly optimizing the partial AUC between any two false positive rates. Our methods are based on minimizing a suitable proxy or surrogate objective for the partial AUC error. In the case of the full AUC, one can readily construct and optimize convex surrogates by expressing the performance measure as a summation of pairwise terms. The partial AUC, on the other hand, does not admit such a simple decomposable structure, making it more challenging to design and optimize (tight) convex surrogates for this measure. Our approach builds on the structural SVM framework of Joachims (2005) to design convex surrogates for partial AUC, and solves the resulting optimization problem using a cutting plane solver. Unlike the full AUC, where the combinatorial optimization needed in each iteration of the cutting plane solver can be decomposed and solved efficiently, the corresponding problem for the partial AUC is harder to decompose. One of our main contributions is a polynomial time algorithm for solving the combinatorial optimization problem associated with partial AUC. We also develop an approach for optimizing a tighter non-convex hinge loss based surrogate for the partial AUC using difference-of-convex programming. Our experiments on a variety of real-world and benchmark tasks confirm the efficacy of the proposed methods. "
803049624822919169,2016-11-28 01:35:25,https://t.co/bfyMd2s4GS,EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces. (arXiv:1611.08024v1 [cs.LG]) https://t.co/bfyMd2s4GS,8,26," Abstract: Objective: Brain-Computer Interface technologies (BCI) enable the direct communication between humans and computers by analyzing brain measurements, such as electroencephalography (EEG). These technologies have been applied to a variety of domains, including neuroprosthetic control and the monitoring of epileptic seizures. Existing BCI systems primarily use a priori knowledge of EEG features of interest to build machine learning models. Recently, convolutional networks have been used for automatic feature extraction of large image databases, where they have obtained state-of-the-art results. In this work we introduce EEGNet, a compact fully convolutional network for EEG-based BCIs developed using Deep Learning approaches. Methods: EEGNet is a 4-layer convolutional network that uses filter factorization for learning a compact representation of EEG time series. EEGNet is one of the smallest convolutional networks to date, having less than 2200 parameters for a binary classification. Results: We show state-of-the-art classification performance across four different BCI paradigms: P300 event-related potential, error-related negativity, movement-related cortical potential, and sensory motor rhythm, with as few as 500 EEG trials. We also show that adding more trials reduces the error variance of prediction rather than improving classification performance. Conclusion: We provide preliminary evidence suggesting that our model can be used with small EEG databases while improving upon the state-of-the-art performance across several tasks and across subjects. Significance: The EEGNet neural network architecture provides state-of-the-art performance across several tasks and across subjects, challenging the notion that large datasets are required to obtain optimal performance. "
803049623354908672,2016-11-28 01:35:24,https://t.co/z7dRF1DLX9,Survey of Expressivity in Deep Neural Networks. (arXiv:1611.08083v1 [stat.ML]) https://t.co/z7dRF1DLX9,1,18," Abstract: We survey results on neural network expressivity described in ""On the Expressive Power of Deep Neural Networks"". The paper motivates and develops three natural measures of expressiveness, which all display an exponential dependence on the depth of the network. In fact, all of these measures are related to a fourth quantity, trajectory length. This quantity grows exponentially in the depth of the network, and is responsible for the depth sensitivity observed. These results translate to consequences for networks during and after training. They suggest that parameters earlier in a network have greater influence on its expressive power -- in particular, given a layer, its influence on expressivity is determined by the remaining depth of the network after that layer. This is verified with experiments on MNIST and CIFAR-10. We also explore the effect of training on the input-output map, and find that it trades off between the stability and expressivity. "
803049621672980480,2016-11-28 01:35:24,https://t.co/cgQW3kletu,Quantum Enhanced Inference in Markov Logic Networks. (arXiv:1611.08104v1 [stat.ML]) https://t.co/cgQW3kletu,0,4," Abstract: Markov logic networks (MLNs) reconcile two opposing schools in machine learning and artificial intelligence: causal networks, which account for uncertainty extremely well, and first-order logic, which allows for formal deduction. An MLN is essentially a first-order logic template to generate Markov networks. Inference in MLNs is probabilistic and it is often performed by approximate methods such as Markov chain Monte Carlo (MCMC) Gibbs sampling. An MLN has many regular, symmetric structures that can be exploited at both first-order level and in the generated Markov network. We analyze the graph structures that are produced by various lifting methods and investigate the extent to which quantum protocols can be used to speed up Gibbs sampling with state preparation and measurement schemes. We review different such approaches, discuss their advantages, theoretical limitations, and their appeal to implementations. We find that a straightforward application of a recent result yields exponential speedup compared to classical heuristics in approximate probabilistic inference, thereby demonstrating another example where advanced quantum resources can potentially prove useful in machine learning. "
803049619924123649,2016-11-28 01:35:23,https://t.co/WWutEvv0v9,Interpreting the Predictions of Complex ML Models by Layer-wise Relevance Propagation. (arXiv:1611.08191v1 [stat.M… https://t.co/WWutEvv0v9,0,5," Abstract: Complex nonlinear models such as deep neural network (DNNs) have become an important tool for image classification, speech recognition, natural language processing, and many other fields of application. These models however lack transparency due to their complex nonlinear structure and to the complex data distributions to which they typically apply. As a result, it is difficult to fully characterize what makes these models reach a particular decision for a given input. This lack of transparency can be a drawback, especially in the context of sensitive applications such as medical analysis or security. In this short paper, we summarize a recent technique introduced by Bach et al. [1] that explains predictions by decomposing the classification decision of DNN models in terms of input variables. "
803049618485444608,2016-11-28 01:35:23,https://t.co/XevVaHbapI,Texture Synthesis with Spatial Generative Adversarial Networks. (arXiv:1611.08207v1 [cs.CV]) https://t.co/XevVaHbapI,0,4," Abstract: Generative adversarial networks (GANs) are a recent approach to train generative models of data, which have been shown to work particularly well on image data. In the current paper we introduce a new model for texture synthesis based on GAN learning. By extending the input noise distribution space from a single vector to a whole spatial tensor, we create an architecture with properties well suited to the task of texture synthesis, which we call spatial GAN (SGAN). To our knowledge, this is the first successful completely data-driven texture synthesis method based on GANs. Our method has the following features which make it a state of the art algorithm for texture synthesis: high image quality of the generated textures, very high scalability w.r.t. the output texture size, fast real-time forward generation, the ability to fuse multiple diverse source images in complex textures. To illustrate these capabilities we present multiple experiments with different classes of texture images and use cases. We also discuss some limitations of our method with respect to the types of texture images it can synthesize, and compare it to other neural techniques for texture generation. "
803049617067806721,2016-11-28 01:35:23,https://t.co/QkJQ9B0DpW,Fast Orthonormal Sparsifying Transforms Based on Householder Reflectors. (arXiv:1611.08229v1 [cs.LG]) https://t.co/QkJQ9B0DpW,0,3," Abstract: Dictionary learning is the task of determining a data-dependent transform that yields a sparse representation of some observed data. The dictionary learning problem is non-convex, and usually solved via computationally complex iterative algorithms. Furthermore, the resulting transforms obtained generally lack structure that permits their fast application to data. To address this issue, this paper develops a framework for learning orthonormal dictionaries which are built from products of a few Householder reflectors. Two algorithms are proposed to learn the reflector coefficients: one that considers a sequential update of the reflectors and one with a simultaneous update of all reflectors that imposes an additional internal orthogonal constraint. The proposed methods have low computational complexity and are shown to converge to local minimum points which can be described in terms of the spectral properties of the matrices involved. The resulting dictionaries balance between the computational complexity and the quality of the sparse representations by controlling the number of Householder reflectors in their product. Simulations of the proposed algorithms are shown in the image processing setting where well-known fast transforms are available for comparisons. The proposed algorithms have favorable reconstruction error and the advantage of a fast implementation relative to the classical, unstructured, dictionaries. "
803049615285243904,2016-11-28 01:35:22,https://t.co/m571eHr1Zi,The Inverse Bagging Algorithm: Anomaly Detection by Inverse Bootstrap Aggregating. (arXiv:1611.08256v1 [stat.ML]) https://t.co/m571eHr1Zi,3,11," Abstract: For data sets populated by a very well modeled process and by another process of unknown probability density function (PDF), a desired feature when manipulating the fraction of the unknown process (either for enhancing it or suppressing it) consists in avoiding to modify the kinematic distributions of the well modeled one. A bootstrap technique is used to identify sub-samples rich in the well modeled process, and classify each event according to the frequency of it being part of such sub-samples. Comparisons with general MVA algorithms will be shown, as well as a study of the asymptotic properties of the method, making use of a public domain data set that models a typical search for new physics as performed at hadronic colliders such as the Large Hadron Collider (LHC). "
803049613603307520,2016-11-28 01:35:22,https://t.co/zGYBFRMXWr,Identifying Significant Predictive Bias in Classifiers. (arXiv:1611.08292v1 [stat.ML]) https://t.co/zGYBFRMXWr,0,3," Abstract: We present a novel subset scan method to detect if a probabilistic binary classifier has statistically significant bias -- over or under predicting the risk -- for some subgroup, and identify the characteristics of this subgroup. This form of model checking and goodness-of-fit test provides a way to interpretably detect the presence of classifier bias and poor classifier fit, not just in one or two dimensions of features of a priori interest, but in the space of all possible feature subgroups. We use subset scan and parametric bootstrap methods to efficiently address the difficulty of assessing the exponentially many possible subgroups. We also suggest several useful extensions of this method for increasing interpretability of predictive models and prediction performance. "
803049611002777600,2016-11-28 01:35:21,https://t.co/u20gno1b0m,An Overview on Data Representation Learning: From Traditional Feature Learning to Recent Deep Learning. (arXiv:161… https://t.co/u20gno1b0m,1,16," Abstract: Since about 100 years ago, to learn the intrinsic structure of data, many representation learning approaches have been proposed, including both linear ones and nonlinear ones, supervised ones and unsupervised ones. Particularly, deep architectures are widely applied for representation learning in recent years, and have delivered top results in many tasks, such as image classification, object detection and speech recognition. In this paper, we review the development of data representation learning methods. Specifically, we investigate both traditional feature learning algorithms and state-of-the-art deep learning models. The history of data representation learning is introduced, while available resources (e.g. online course, tutorial and book information) and toolboxes are provided. Finally, we conclude this paper with remarks and some interesting research directions on data representation learning. "
803049609652224001,2016-11-28 01:35:21,https://t.co/KNgd464Vzu,Local Discriminant Hyperalignment for multi-subject fMRI data alignment. (arXiv:1611.08366v1 [stat.ML]) https://t.co/KNgd464Vzu,0,2," Abstract: Multivariate Pattern (MVP) classification can map different cognitive states to the brain tasks. One of the main challenges in MVP analysis is validating the generated results across subjects. However, analyzing multi-subject fMRI data requires accurate functional alignments between neuronal activities of different subjects, which can rapidly increase the performance and robustness of the final results. Hyperalignment (HA) is one of the most effective functional alignment methods, which can be mathematically formulated by the Canonical Correlation Analysis (CCA) methods. Since HA mostly uses the unsupervised CCA techniques, its solution may not be optimized for MVP analysis. By incorporating the idea of Local Discriminant Analysis (LDA) into CCA, this paper proposes Local Discriminant Hyperalignment (LDHA) as a novel supervised HA method, which can provide better functional alignment for MVP analysis. Indeed, the locality is defined based on the stimuli categories in the train-set, where the correlation between all stimuli in the same category will be maximized and the correlation between distinct categories of stimuli approaches to near zero. Experimental studies on multi-subject MVP analysis confirm that the LDHA method achieves superior performance to other state-of-the-art HA algorithms. "
803049608230342656,2016-11-28 01:35:21,https://t.co/uaWrLseqUb,A Unified Convex Surrogate for the Schatten-$p$ Norm. (arXiv:1611.08372v1 [stat.ML]) https://t.co/uaWrLseqUb,0,6," Abstract: The Schatten-$p$ norm ($0<p<1$) has been widely used to replace the nuclear norm for better approximating the rank function. However, existing methods are either 1) not scalable for large scale problems due to relying on singular value decomposition (SVD) in every iteration, or 2) specific to some $p$ values, e.g., $1/2$, and $2/3$. In this paper, we show that for any $p$, $p_1$, and $p_2 >0$ satisfying $1/p=1/p_1+1/p_2$, there is an equivalence between the Schatten-$p$ norm of one matrix and the Schatten-$p_1$ and the Schatten-$p_2$ norms of its two factor matrices. We further extend the equivalence to multiple factor matrices and show that all the factor norms can be convex and smooth for any $p>0$. In contrast, the original Schatten-$p$ norm for $0<p<1$ is non-convex and non-smooth. As an example we conduct experiments on matrix completion. To utilize the convexity of the factor matrix norms, we adopt the accelerated proximal alternating linearized minimization algorithm and establish its sequence convergence. Experiments on both synthetic and real datasets exhibit its superior performance over the state-of-the-art methods. Its speed is also highly competitive. "
803049606615592960,2016-11-28 01:35:20,https://t.co/sytknJ4rxK,Bidirectional LSTM-CRF for Clinical Concept Extraction. (arXiv:1611.08373v1 [stat.ML]) https://t.co/sytknJ4rxK,3,11," Abstract: Automated extraction of concepts from patient clinical records is an essential facilitator of clinical research. For this reason, the 2010 i2b2/VA Natural Language Processing Challenges for Clinical Records introduced a concept extraction task aimed at identifying and classifying concepts into predefined categories (i.e., treatments, tests and problems). State-of-the-art concept extraction approaches heavily rely on handcrafted features and domain-specific resources which are hard to collect and define. For this reason, this paper proposes an alternative, streamlined approach: a recurrent neural network (the bidirectional LSTM with CRF decoding) initialized with general-purpose, off-the-shelf word embeddings. The experimental results achieved on the 2010 i2b2/VA reference corpora using the proposed framework outperform all recent methods and ranks closely to the best submission from the original 2010 i2b2/VA challenge. "
803049605252468736,2016-11-28 01:35:20,https://t.co/bV4ccZWXoN,Distributed Optimization of Multi-Class SVMs. (arXiv:1611.08480v1 [stat.ML]) https://t.co/bV4ccZWXoN,2,6," Abstract: Training of one-vs.-rest SVMs can be parallelized over the number of classes in a straight forward way. Given enough computational resources, one-vs.-rest SVMs can thus be trained on data involving a large number of classes. The same cannot be stated, however, for the so-called all-in-one SVMs, which require solving a quadratic program of size quadratically in the number of classes. We develop distributed algorithms for two all-in-one SVM formulations (Lee et al. and Weston and Watkins) that parallelize the computation evenly over the number of classes. This allows us to compare these models to one-vs.-rest SVMs on unprecedented scale. The results indicate superior accuracy on text classification data. "
803049603507421184,2016-11-28 01:35:20,https://t.co/QcyMnjCgzk,Bottleneck Conditional Density Estimation. (arXiv:1611.08568v1 [stat.ML]) https://t.co/QcyMnjCgzk,0,2," Abstract: We propose a neural network framework for high-dimensional conditional density estimation. The Bottleneck Conditional Density Estimator (BCDE) is a variant of the conditional variational autoencoder (CVAE) that employs layer(s) of stochastic variables as the bottleneck between the input x and target y, where both are high-dimensional. The key to effectively train BCDEs is the hybrid blending of the conditional generative model with a joint generative model that leverages unlabeled data to regularize the conditional model. We show that the BCDE significantly outperforms the CVAE in MNIST quadrant prediction benchmarks in the fully supervised case and establishes new benchmarks in the semi-supervised case. "
803049602005934081,2016-11-28 01:35:19,https://t.co/Y54F3Xi3fC,Concept Stability for Constructing Taxonomies of Web-site Users. (arXiv:0905.1424v2 [cs.CY] UPDATED) https://t.co/Y54F3Xi3fC,1,3," Abstract: Owners of a web-site are often interested in analysis of groups of users of their site. Information on these groups can help optimizing the structure and contents of the site. In this paper we use an approach based on formal concepts for constructing taxonomies of user groups. For decreasing the huge amount of concepts that arise in applications, we employ stability index of a concept, which describes how a group given by a concept extent differs from other such groups. We analyze resulting taxonomies of user groups for three target websites. "
803049600328175616,2016-11-28 01:35:19,https://t.co/6tuwNH6l2f,"Asynchrony begets Momentum, with an Application to Deep Learning. (arXiv:1605.09774v2 [stat.ML] UPDATED) https://t.co/6tuwNH6l2f",0,3," Abstract: Asynchronous methods are widely used in deep learning, but have limited theoretical justification when applied to non-convex problems. We show that running stochastic gradient descent (SGD) in an asynchronous manner can be viewed as adding a momentum-like term to the SGD iteration. Our result does not assume convexity of the objective function, so it is applicable to deep learning systems. We observe that a standard queuing model of asynchrony results in a form of momentum that is commonly used by deep learning practitioners. This forges a link between queuing theory and asynchrony in deep learning systems, which could be useful for systems builders. For convolutional neural networks, we experimentally validate that the degree of asynchrony directly correlates with the momentum, confirming our main result. An important implication is that tuning the momentum parameter is important when considering different levels of asynchrony. We assert that properly tuned momentum reduces the number of steps required for convergence. Finally, our theory suggests new ways of counteracting the adverse effects of asynchrony: a simple mechanism like using negative algorithmic momentum can improve performance under high asynchrony. Since asynchronous methods have better hardware efficiency, this result may shed light on when asynchronous execution is more efficient for deep learning systems. "
803049598331842560,2016-11-28 01:35:18,https://t.co/VfcPi9Z2t6,Effects of Additional Data on Bayesian Clustering. (arXiv:1607.03574v2 [stat.ML] UPDATED) https://t.co/VfcPi9Z2t6,0,3," Abstract: Hierarchical probabilistic models, such as mixture models, are used for cluster analysis. These models have two types of variables: observable and latent. In cluster analysis, the latent variable is estimated, and it is expected that additional information will improve the accuracy of the estimation of the latent variable. Many proposed learning methods are able to use additional data; these include semi-supervised learning and transfer learning. However, from a statistical point of view, a complex probabilistic model that encompasses both the initial and additional data might be less accurate due to having a higher-dimensional parameter. The present paper presents a theoretical analysis of the accuracy of such a model and clarifies which factor has the greatest effect on its accuracy, the advantages of obtaining additional data, and the disadvantages of increasing the complexity. "
803049596956143620,2016-11-28 01:35:18,https://t.co/3PQNdmkLYA,One-Trial Correction of Legacy AI Systems and Stochastic Separation Theorems. (arXiv:1610.00494v2 [stat.ML] UPDATE… https://t.co/3PQNdmkLYA,0,2," Abstract: We consider the problem of efficient ""on the fly"" tuning of existing, or {\it legacy}, Artificial Intelligence (AI) systems. The legacy AI systems are allowed to be of arbitrary class, albeit the data they are using for computing interim or final decision responses should posses an underlying structure of a high-dimensional topological real vector space. The tuning method that we propose enables dealing with errors without the need to re-train the system. Instead of re-training a simple cascade of perceptron nodes is added to the legacy system. The added cascade modulates the AI legacy system's decisions. If applied repeatedly, the process results in a network of modulating rules ""dressing up"" and improving performance of existing AI systems. Mathematical rationale behind the method is based on the fundamental property of measure concentration in high dimensional spaces. The method is illustrated with an example of fine-tuning a deep convolutional network that has been pre-trained to detect pedestrians in images. "
803049595152384000,2016-11-28 01:35:18,https://t.co/7Ga5H1jf1L,Algebraic multigrid support vector machines. (arXiv:1611.05487v2 [stat.ML] UPDATED) https://t.co/7Ga5H1jf1L,0,3," Abstract: The support vector machine is a flexible optimization-based technique widely used for classification problems. In practice, its training part becomes computationally expensive on large-scale data sets because of such reasons as the complexity and number of iterations in parameter fitting methods, underlying optimization solvers, and nonlinearity of kernels. We introduce a fast multilevel framework for solving support vector machine models that is inspired by the algebraic multigrid. Significant improvement in the running has been achieved without any loss in the quality. The proposed technique is highly beneficial on imbalanced sets. We demonstrate computational results on publicly available and industrial data sets. "
803049593311203328,2016-11-28 01:35:17,https://t.co/njkawx6HGl,Learning to reinforcement learn. (arXiv:1611.05763v2 [cs.LG] UPDATED) https://t.co/njkawx6HGl,2,5," Abstract: In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience. "
801605099348967428,2016-11-24 01:55:23,https://t.co/aDoNKMMG3k,Randomized Distributed Mean Estimation: Accuracy vs Communication. (arXiv:1611.07555v1 [cs.DC]) https://t.co/aDoNKMMG3k,0,4," Abstract: We consider the problem of estimating the arithmetic average of a finite collection of real vectors stored in a distributed fashion across several compute nodes subject to a communication budget constraint. Our analysis does not rely on any statistical assumptions about the source of the vectors. This problem arises as a subproblem in many applications, including reduce-all operations within algorithms for distributed and federated optimization and learning. We propose a flexible family of randomized algorithms exploring the trade-off between expected communication cost and estimation error. Our family contains the full-communication and zero-error method on one extreme, and an $\epsilon$-bit communication and ${\cal O}\left(1/(\epsilon n)\right)$ error method on the opposite extreme. In the special case where we communicate, in expectation, a single bit per coordinate of each vector, we improve upon existing results by obtaining $\mathcal{O}(r/n)$ error, where $r$ is the number of bits used to represent a floating point value. "
801605098115842051,2016-11-24 01:55:23,https://t.co/Tkif3m2tSz,Feature Importance Measure for Non-linear Learning Algorithms. (arXiv:1611.07567v1 [cs.AI]) https://t.co/Tkif3m2tSz,2,17," Abstract: Complex problems may require sophisticated, non-linear learning methods such as kernel machines or deep neural networks to achieve state of the art prediction accuracies. However, high prediction accuracies are not the only objective to consider when solving problems using machine learning. Instead, particular scientific applications require some explanation of the learned prediction function. Unfortunately, most methods do not come with out of the box straight forward interpretation. Even linear prediction functions are not straight forward to explain if features exhibit complex correlation structure. In this paper, we propose the Measure of Feature Importance (MFI). MFI is general and can be applied to any arbitrary learning machine (including kernel machines and deep learning). MFI is intrinsically non-linear and can detect features that by itself are inconspicuous and only impact the prediction function through their interaction with other features. Lastly, MFI can be used for both --- model-based feature importance and instance-based feature importance (i.e, measuring the importance of a feature for a particular data point). "
801605096089993216,2016-11-24 01:55:22,https://t.co/Y16ePZFuo5,Programs as Black-Box Explanations. (arXiv:1611.07579v1 [stat.ML]) https://t.co/Y16ePZFuo5,0,7," Abstract: Recent work in model-agnostic explanations of black-box machine learning has demonstrated that interpretability of complex models does not have to come at the cost of accuracy or model flexibility. However, it is not clear what kind of explanations, such as linear models, decision trees, and rule lists, are the appropriate family to consider, and different tasks and models may benefit from different kinds of explanations. Instead of picking a single family of representations, in this work we propose to use ""programs"" as model-agnostic explanations. We show that small programs can be expressive yet intuitive as explanations, and generalize over a number of existing interpretable families. We propose a prototype program induction method based on simulated annealing that approximates the local behavior of black-box classifiers around a specific prediction using random perturbations. Finally, we present preliminary application on small datasets and show that the generated explanations are intuitive and accurate for a number of classifiers. "
801605094697484294,2016-11-24 01:55:22,https://t.co/cfReiZA1fw,A Neural Network Model to Classify Liver Cancer Patients Using Data Expansion and Compression. (arXiv:1611.07588v1… https://t.co/cfReiZA1fw,4,3, Abstract: We develop a neural network model to classify liver cancer patients into high-risk and low-risk groups using genomic data. Our approach provides a novel technique to classify big data sets using neural network models. We preprocess the data before training the neural network models. We first expand the data using wavelet analysis. We then compress the wavelet coefficients by mapping them onto a new scaled orthonormal coordinate system. Then the data is used to train a neural network model that enables us to classify cancer patients into two different classes of high-risk and low-risk patients. We use the leave-one-out approach to build a neural network model. This neural network model enables us to classify a patient using genomic data as a high-risk or low-risk patient without any information about the survival time of the patient. The results from genomic data analysis are compared with survival time analysis. It is shown that the expansion and compression of data using wavelet analysis and singular value decomposition (SVD) is essential to train the neural network model. 
801605093271408640,2016-11-24 01:55:21,https://t.co/wxfxysmLGX,Adaptive Accelerated Gradient Converging Methods under Holderian Error Bound Condition. (arXiv:1611.07609v1 [math.… https://t.co/wxfxysmLGX,0,5," Abstract: In this paper, we focus our study on the convergence of (proximal) gradient methods and accelerated (proximal) gradient methods for smooth (composite) optimization under a H\""{o}lderian error bound (HEB) condition. We first show that proximal gradient (PG) method is automatically adaptive to HEB while accelerated proximal gradient (APG) method can be adaptive to HEB by restart with an improved iteration complexity. However, the number of iterations to restart APG hinges on a possibly unknown parameter. To address this issue, we propose to develop adaptive gradient converging methods, i.e., using the magnitude of gradient as a criterion for restart and termination. We develop adaptive accelerated gradient converging (adaAGC) methods for solving smooth (composite) optimization under the HEB condition with an explicit exponent $\theta$, and establish an better iteration complexity than PG. Furthermore, we demonstrate that these results have important implication and applications in machine learning: (i) if the considered objective function is coercive and semi-algebraic, PG's convergence speed is essentially $o(\frac{1}{t})$, where $t$ is the total number of iterations; (ii) if the objective function consists of an $\ell_1$, $\ell_\infty$, $\ell_{1,\infty}$, or huber norm regularization and a convex smooth piecewise quadratic loss, which includes least-squares loss, smoothed hinge loss and huber loss, the proposed adaAGC is parameter-free and enjoys a faster linear convergence than PG without any other assumptions (e.g., restricted eigen-value condition). It is notable that for all aforementioned problems, our linear convergence results are global instead of local. To the best of our knowledge, these improved results are the first shown in this work. "
801605091828580354,2016-11-24 01:55:21,https://t.co/SATcNYhJ18,Interpretation of Prediction Models Using the Input Gradient. (arXiv:1611.07634v1 [stat.ML]) https://t.co/SATcNYhJ18,0,8," Abstract: State of the art machine learning algorithms are highly optimized to provide the optimal prediction possible, naturally resulting in complex models. While these models often outperform simpler more interpretable models by order of magnitudes, in terms of understanding the way the model functions, we are often facing a ""black box"". In this paper we suggest a simple method to interpret the behavior of any predictive model, both for regression and classification. Given a particular model, the information required to interpret it can be obtained by studying the partial derivatives of the model with respect to the input. We exemplify this insight by interpreting convolutional and multi-layer neural networks in the field of natural language processing. "
801605090398322690,2016-11-24 01:55:21,https://t.co/a3RRZiHbAX,Learning Cost-Effective and Interpretable Regimes for Treatment Recommendation. (arXiv:1611.07663v1 [stat.ML]) https://t.co/a3RRZiHbAX,0,5," Abstract: Decision makers, such as doctors and judges, make crucial decisions such as recommending treatments to patients, and granting bails to defendants on a daily basis. Such decisions typically involve weighting the potential benefits of taking an action against the costs involved. In this work, we aim to automate this task of learning {cost-effective, interpretable and actionable treatment regimes. We formulate this as a problem of learning a decision list -- a sequence of if-then-else rules -- which maps characteristics of subjects (eg., diagnostic test results of patients) to treatments. We propose a novel objective to construct a decision list which maximizes outcomes for the population, and minimizes overall costs. We model the problem of learning such a list as a Markov Decision Process (MDP) and employ a variant of the Upper Confidence Bound for Trees (UCT) strategy which leverages customized checks for pruning the search space effectively. Experimental results on real world observational data capturing treatment recommendations for asthma patients demonstrate the effectiveness of our approach. "
801605089165197312,2016-11-24 01:55:21,https://t.co/sns7bdW6CR,Spatio-Temporal Modeling of Check-ins in Location-Based Social Networks. (arXiv:1611.07710v1 [cs.SI]) https://t.co/sns7bdW6CR,2,4," Abstract: Social networks are getting closer to our real physical world. People share the exact location and time of their check-ins and are influenced by their friends. Modeling the spatio-temporal behavior of users in social networks is of great importance for predicting the future behavior of users, controlling the users' movements, and finding the latent influence network. It is observed that users have periodic patterns in their movements. Also, they are influenced by the locations that their close friends recently visited. Leveraging these two observations, we propose a probabilistic model based on a doubly stochastic point process with a periodic decaying kernel for the time of check-ins and a time-varying multinomial distribution for the location of check-ins of users in the location-based social networks. We learn the model parameters by using an efficient EM algorithm, which distributes over the users. Experiments on synthetic and real data gathered from Foursquare show that the proposed inference algorithm learns the parameters efficiently and our method models the real data better than other alternatives. "
801605087567167490,2016-11-24 01:55:20,https://t.co/a7ecO7a9Bc,iCaRL: Incremental Classifier and Representation Learning. (arXiv:1611.07725v1 [cs.CV]) https://t.co/a7ecO7a9Bc,1,14," Abstract: A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on the CIFAR-100 and ImageNet ILSVRC 2012 datasets that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail. "
801605086015275010,2016-11-24 01:55:20,https://t.co/AbmEedEhqI,Tunable Sensitivity to Large Errors in Neural Network Training. (arXiv:1611.07743v1 [stat.ML]) https://t.co/AbmEedEhqI,0,8," Abstract: When humans learn a new concept, they might ignore examples that they cannot make sense of at first, and only later focus on such examples, when they are more useful for learning. We propose incorporating this idea of tunable sensitivity for hard examples in neural network learning, using a new generalization of the cross-entropy gradient step, which can be used in place of the gradient in any gradient-based training method. The generalized gradient is parameterized by a value that controls the sensitivity of the training process to harder training examples. We tested our method on several benchmark datasets. We propose, and corroborate in our experiments, that the optimal level of sensitivity to hard example is positively correlated with the depth of the network. Moreover, the test prediction error obtained by our method is generally lower than that of the vanilla cross-entropy gradient learner. We therefore conclude that tunable sensitivity can be helpful for neural network learning. "
801605084857696265,2016-11-24 01:55:19,https://t.co/vSPww9igZl,Infinite Variational Autoencoder for Semi-Supervised Learning. (arXiv:1611.07800v1 [cs.LG]) https://t.co/vSPww9igZl,2,18," Abstract: This paper presents an infinite variational autoencoder (VAE) whose capacity adapts to suit the input data. This is achieved using a mixture model where the mixing coefficients are modeled by a Dirichlet process, allowing us to integrate over the coefficients when performing inference. Critically, this then allows us to automatically vary the number of autoencoders in the mixture based on the data. Experiments show the flexibility of our method, particularly for semi-supervised learning, where only a small number of training samples are available. "
801605083528052738,2016-11-24 01:55:19,https://t.co/jGPI0aMOic,Robust Unsupervised Transient Detection With Invariant Representation based on the Scattering Network. (arXiv:1611… https://t.co/jGPI0aMOic,0,3," Abstract: We present a sparse and invariant representation with low asymptotic complexity for robust unsupervised transient and onset zone detection in noisy environments. This unsupervised approach is based on wavelet transforms and leverages the scattering network from Mallat et al. by deriving frequency invariance. This frequency invariance is a key concept to enforce robust representations of transients in presence of possible frequency shifts and perturbations occurring in the original signal. Implementation details as well as complexity analysis are provided in addition of the theoretical framework and the invariance properties. In this work, our primary application consists of predicting the onset of seizure in epileptic patients from subdural recordings as well as detecting inter-ictal spikes. "
801600381809737729,2016-11-24 01:36:38,https://t.co/y9sjQnHlPT,A Projection Based Conditional Dependence Measure with Applications to High-dimensional Undirected Graphical Model… https://t.co/y9sjQnHlPT,0,5," Abstract: Measuring conditional dependence is an important topic in statistics with broad applications including graphical models. Under a factor model setting, a new conditional dependence measure based on projection is proposed. The corresponding conditional independence test is developed with the asymptotic null distribution unveiled where the number of factors could be high-dimensional. It is also shown that the new test has control over the asymptotic significance level and can be calculated efficiently. A generic method for building dependency graphs without Gaussian assumption using the new test is elaborated. Numerical results and real data analysis show the superiority of the new method. "
801600380337537024,2016-11-24 01:36:38,https://t.co/qTA6XeHuL6,A Randomized Rounding Algorithm for Sparse PCA. (arXiv:1508.03337v5 [cs.DS] UPDATED) https://t.co/qTA6XeHuL6,0,9," Abstract: We present and analyze a simple, two-step algorithm to approximate the optimal solution of the sparse PCA problem. Our approach first solves a L1 penalized version of the NP-hard sparse PCA optimization problem and then uses a randomized rounding strategy to sparsify the resulting dense solution. Our main theoretical result guarantees an additive error approximation and provides a tradeoff between sparsity and accuracy. Our experimental evaluation indicates that our approach is competitive in practice, even compared to state-of-the-art toolboxes such as Spasm. "
801600378999554049,2016-11-24 01:36:38,https://t.co/lOIycdftKx,Parsimonious modeling with Information Filtering Networks. (arXiv:1602.07349v3 [cs.IT] UPDATED) https://t.co/lOIycdftKx,0,2," Abstract: We introduce a methodology to construct parsimonious probabilistic models. This method makes use of Information Filtering Networks to produce a robust estimate of the global sparse inverse covariance from a simple sum of local inverse covariances computed on small sub-parts of the network. Being based on local and low-dimensional inversions, this method is computationally very efficient and statistically robust even for the estimation of inverse covariance of high-dimensional, noisy and short time-series. Applied to financial data our method results computationally more efficient than state-of-the-art methodologies such as Glasso producing, in a fraction of the computation time, models that can have equivalent or better performances but with a sparser inference structure. We also discuss performances with sparse factor models where we notice that relative performances decrease with the number of factors. The local nature of this approach allows us to perform computations in parallel and provides a tool for dynamical adaptation by partial updating when the properties of some variables change without the need of recomputing the whole model. This makes this approach particularly suitable to handle big datasets with large numbers of variables. Examples of practical application for forecasting, stress testing and risk allocation in financial systems are also provided. "
801600377728679936,2016-11-24 01:36:37,https://t.co/Ize2EaRLyF,Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization. (arXiv:1603.06560v3 [cs.LG] UPDATED) https://t.co/Ize2EaRLyF,4,16," Abstract: Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While current methods offer efficiencies by adaptively choosing new configurations to train, an alternative strategy is to adaptively allocate resources across the selected configurations. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinitely many armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce Hyperband for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with state-of-the-art methods on a suite of hyperparameter optimization problems. We observe that Hyperband provides five times to thirty times speedup over state-of-the-art Bayesian optimization algorithms on a variety of deep-learning and kernel-based learning problems. "
801600376537497605,2016-11-24 01:36:37,https://t.co/JJH5WuDmyB,Iterative Smoothing Proximal Gradient for Regression with Structured Sparsity. (arXiv:1605.09658v4 [stat.ML] UPDAT… https://t.co/JJH5WuDmyB,0,5," Abstract: In the context high-dimensionnal predictive models, we consider the problem of optimizing the sum of a smooth convex loss and a non-smooth convex penalty, whose proximal operator is known, and a non-smooth convex structured penalties such as total variation, or overlapping group lasso. We propose to smooth the structured penalty, since it allows a generic framework in which a large range of non-smooth convex structured penalties can be minimized without computing their proximal operators that are either not known or expensive to compute. The problem can be minimized with an accelerated proximal gradient method to benefit of (non-smoothed) sparsity-inducing penalties. We propose an expression of the duality gap to control the convergence of the global non-smooth problem. This expression is applicable to a large range of structured penalties. However, smoothing methods have many limitations that the proposed solver aims to overcome. Therefore, we propose a continuation algorithm, called CONESTA, that dynamically generates a decreasing sequence of smoothing parameters in order to maintain the optimal convergence speed towards any globally desired precision. At each continuation step, the aforementioned duality gap provides the current error and thus the next smaller prescribed precision. Given this precision, we propose a expression to calculate the optimal smoothing parameter, that minimizes the number of iterations to reach such precision. We demonstrate that CONESTA achieves an improved convergence rate compared to classical (without continuation) proximal gradient smoothing. Moreover, experiments conducted on both simulated and high-dimensional neuroimaging (MRI) data, exhibit that CONESTA significantly outperforms the excessive gap method, ADMM, classical proximal gradient smoothing and inexact FISTA in terms of convergence speed and/or precision of the solution. "
801600375321137152,2016-11-24 01:36:37,https://t.co/DpFe0TUwBa,VIME: Variational Information Maximizing Exploration. (arXiv:1605.09674v3 [cs.LG] UPDATED) https://t.co/DpFe0TUwBa,2,17," Abstract: Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards. "
801600373815472128,2016-11-24 01:36:36,https://t.co/wkIHE0cqZC,Principled Option Learning in Markov Decision Processes. (arXiv:1609.05524v2 [cs.LG] UPDATED) https://t.co/wkIHE0cqZC,0,8," Abstract: It is well known that options can make planning more efficient, among their many benefits. Thus far, algorithms for autonomously discovering a set of useful options were heuristic. Naturally, a principled way of finding a set of useful options may be more promising and insightful. In this paper we suggest a mathematical characterization of good sets of options using tools from information theory. This characterization enables us to find conditions for a set of options to be optimal and an algorithm that outputs a useful set of options and illustrate the proposed algorithm in simulation. "
801600371705659392,2016-11-24 01:36:36,https://t.co/Ets5DVat9w,The Projected Power Method: An Efficient Algorithm for Joint Alignment from Pairwise Differences. (arXiv:1609.0582… https://t.co/Ets5DVat9w,0,5," Abstract: Various applications involve assigning discrete label values to a collection of objects based on some pairwise noisy data. Due to the discrete---and hence nonconvex---structure of the problem, computing the optimal assignment (e.g.~maximum likelihood assignment) becomes intractable at first sight. This paper makes progress towards efficient computation by focusing on a concrete joint alignment problem---that is, the problem of recovering $n$ discrete variables $x_i \in \{1,\cdots, m\}$, $1\leq i\leq n$ given noisy observations of their modulo differences $\{x_i - x_j~\mathsf{mod}~m\}$. We propose a low-complexity and model-free procedure, which operates in a lifted space by representing distinct label values in orthogonal directions, and which attempts to optimize quadratic functions over hypercubes. Starting with a first guess computed via a spectral method, the algorithm successively refines the iterates via projected power iterations. We prove that for a broad class of statistical models, the proposed projected power method makes no error---and hence converges to the maximum likelihood estimate---in a suitable regime. Numerical experiments have been carried out on both synthetic and real data to demonstrate the practicality of our algorithm. We expect this algorithmic framework to be effective for a broad range of discrete assignment problems. "
801600370355146752,2016-11-24 01:36:35,https://t.co/15JJc4Vnjb,Categorical Reparameterization with Gumbel-Softmax. (arXiv:1611.01144v2 [stat.ML] UPDATED) https://t.co/15JJc4Vnjb,1,10," Abstract: Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification. "
801237666520035329,2016-11-23 01:35:20,https://t.co/drJSXR4PSr,Gaussian Process Structure Learning via Probabilistic Inverse Compilation. (arXiv:1611.07051v1 [stat.ML]) https://t.co/drJSXR4PSr,2,9," Abstract: There is a widespread need for techniques that can learn interpretable models from data. Recent work by Duvenaud et al. (2013) and Lloyd et al. (2014) showed that it is possible to use Gaussian Processes (GPs) to discover symbolic structure in univariate time series. This abstract shows how to reimplement the approach from Duvenaud et al. (2013) using under 100 lines of probabilistic code in Venture (Mansinghka et al., 2014; Lu, 2016), improving on a previous implementation from Schaechtle et al. (2015). The key idea is to formulate structure learning as a kind of probabilistic inverse compilation, where the kernel structure is represented as source code and the resulting GP model is represented as an executable probabilistic program produced by compiling that source code. Figures 1 and 2 give an overview of the inverse compilation framework. Figure 3 shows example kernel structures, including program source, an English summary, and typical data corresponding to the given structure. Figure 4 shows the complete Venture source code for reimplementing the approach from Duvenaud et al. (2013), and Figure 5 shows an application to real-world time series data describing air travel volume. "
801237664926232576,2016-11-23 01:35:20,https://t.co/uFsa4flLGi]),The Recycling Gibbs Sampler for Efficient Learning. (arXiv:1611.07056v1 [https://t.co/uFsa4flLGi]) https://t.co/Pyl8dkk2UL,0,4,INDEXERROR
801237663500083200,2016-11-23 01:35:19,https://t.co/SlRicFHYKZ,Using Empirical Covariance Matrix in Enhancing Prediction Accuracy of Linear Models with Missing Information. (arX… https://t.co/SlRicFHYKZ,0,4," Abstract: Inference and Estimation in Missing Information (MI) scenarios is an important topic in Statistical Learning Theory and Machine Learning (ML). In ML literature, attempts have been carried out to enhance prediction through precise feature selection methods. In sparse linear models, Lasso is well- known in extracting the desired support of the signal and resisting against noisy systems. When sparse models are also suffering from MI, the sparse recovery and inference of the missing models are taken into account simultaneously. In this paper, we will introduce an approach which enjoys sparse regression and covariance matrix estimation in improving matrix completion accuracy, and as a result enhancing feature selection preciseness which leads to reduction in prediction Mean Squared Error (MSE). We will compare the effect of employing covariance matrix in enhancing estimation accuracy to the case it is not used in feature selection. Simulations will show the improvement in the performance as compared to the case we do not use the covariance matrix estimation. "
801237661910519808,2016-11-23 01:35:19,https://t.co/ntlTfTQ2WR,Structured Prediction by Least Squares Estimated Conditional Risk Minimization. (arXiv:1611.07096v1 [stat.ML]) https://t.co/ntlTfTQ2WR,0,2," Abstract: We propose a general approach for supervised learning with structured output spaces, such as combinatorial and polyhedral sets, that is based on minimizing estimated conditional risk functions. Given a loss function defined over pairs of output labels, we first estimate the conditional risk function by solving a (possibly infinite) collection of regularized least squares problems. A prediction is made by solving an auxiliary optimization problem that minimizes the estimated conditional risk function over the output space. We apply this method to a class of problems with discrete combinatorial outputs and additive pairwise losses, and show that the auxiliary problem can be solved efficiently by exact linear programming relaxations in several important cases, including variants of hierarchical multilabel classification and multilabel ranking problems. We demonstrate how the same approach can also be extended to vector regression problems with convex constraints and losses. Evaluations of this approach on hierarchical multilabel classification show that it compares favorably with several existing methods in terms of predictive accuracy, and has computational advantages over them when applied to large hierarchies. "
801237660564148224,2016-11-23 01:35:19,https://t.co/mx0OeGnSWR,Interpreting Finite Automata for Sequential Data. (arXiv:1611.07100v1 [stat.ML]) https://t.co/mx0OeGnSWR,0,2," Abstract: Automaton models are often seen as interpretable models. Interpretability itself is not well defined: it remains unclear what interpretability means without first explicitly specifying objectives or desired attributes. In this paper, we identify the key properties used to interpret automata and propose a modification of a state-merging approach to learn variants of finite state automata. We apply the approach to problems beyond typical grammar inference tasks. Additionally, we cover several use-cases for prediction, classification, and clustering on sequential data in both supervised and unsupervised scenarios to show how the identified key properties are applicable in a wide range of contexts. "
801237658282369024,2016-11-23 01:35:18,https://t.co/uHCa5ICjlA,Generation of discrete random variables in scalable framework. (arXiv:1611.07103v1 [stat.ME]) https://t.co/uHCa5ICjlA,0,6," Abstract: In this paper, we face the problem of simulating discrete random variables with general and varying distribution in a scalable framework, where fully parallelizable operations should be preferred. Compared with classical algorithms, we add randomness, that will be analyzed with a fully parallelizable operation, and we leave the final simulation of the random variable to a single associative operator. We characterize the set of algorithms that work in this way, and some classes of them related to an additive or multiplicative local noise. As a consequence, we could define a natural way to solve some popular simulation problems. "
801237656143335424,2016-11-23 01:35:18,https://t.co/QcBpvmvRiU,Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable. (arXiv:1611.07115v1 [stat.ML]) https://t.co/QcBpvmvRiU,0,5," Abstract: Ensembles of decision trees have good prediction accuracy but suffer from a lack of interpretability. We propose a new approach for interpreting tree ensembles by finding prototypes in tree space, utilizing the naturally-learned similarity measure from the tree ensemble. Demonstrating the method on random forests, we show that the method benefits from unique aspects of tree ensembles by leveraging tree structure to sequentially find prototypes. The method provides good prediction accuracy when found prototypes are used in nearest-prototype classifiers, while using fewer prototypes than competitor methods. We are investigating the sensitivity of the method to different prototype-finding procedures and demonstrating it on higher-dimensional data. "
801237654360780800,2016-11-23 01:35:17,https://t.co/uDh5b87jSe,Max-Margin Deep Generative Models for (Semi-)Supervised Learning. (arXiv:1611.07119v1 [cs.CV]) https://t.co/uDh5b87jSe,1,11," Abstract: Deep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability. However, it is relatively insufficient to empower the discriminative ability of DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs) and a class-conditional variant (mmDCGMs), which explore the strongly discriminative principle of max-margin learning to improve the predictive performance of DGMs in both supervised and semi-supervised learning, while retaining the generative capability. In semi-supervised learning, we use the predictions of a max-margin classifier as the missing labels instead of performing full posterior inference for efficiency; we also introduce additional max-margin and label-balance regularization terms of unlabeled data for effectiveness. We develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objectives in different settings. Empirical results on various datasets demonstrate that: (1) max-margin learning can significantly improve the prediction performance of DGMs and meanwhile retain the generative ability; (2) in supervised learning, mmDGMs are competitive to the best fully discriminative networks when employing convolutional neural networks as the generative and recognition models; and (3) in semi-supervised learning, mmDCGMs can perform efficient inference and achieve state-of-the-art classification results on several benchmarks. "
801237652901101568,2016-11-23 01:35:17,https://t.co/sw9JKymxEV,A new approach to Laplacian solvers and flow problems. (arXiv:1611.07138v1 [math.OC]) https://t.co/sw9JKymxEV,0,7," Abstract: This paper investigates message-passing algorithms for solving systems of linear equations in the Laplacian matrices of graphs and to compute electric flows. These two problems are fundamental primitives that arise in several domains such as computer science, electrical engineering, operations research, and machine learning. Despite the extensive literature on approximately solving these problems in quasi-linear time, the algorithms that have been proposed are typically centralized and involve multiple graph theoretic constructions or sampling mechanisms that make them difficult to implement and analyze. On the other hand, message-passing routines are distributed, simple, and easy to implement. In this paper we establish a framework to analyze message-passing algorithms to solve voltage and flow problems. We characterize the error committed by the algorithms in d-regular graphs with equal weights. We show that the convergence of the algorithms is controlled by the total variation distance between the distributions of non-backtracking random walks that start from neighbor nodes. More broadly, our analysis of message-passing introduces new insights to address generic optimization problems with constraints. "
801237651063996416,2016-11-23 01:35:16,https://t.co/8RZrz6VRay,Optimal Learning for Stochastic Optimization with Nonlinear Parametric Belief Models. (arXiv:1611.07161v1 [stat.ML… https://t.co/8RZrz6VRay,1,4," Abstract: We consider the problem of estimating the expected value of information (the knowledge gradient) for Bayesian learning problems where the belief model is nonlinear in the parameters. Our goal is to maximize some metric, while simultaneously learning the unknown parameters of the nonlinear belief model, by guiding a sequential experimentation process which is expensive. We overcome the problem of computing the expected value of an experiment, which is computationally intractable, by using a sampled approximation, which helps to guide experiments but does not provide an accurate estimate of the unknown parameters. We then introduce a resampling process which allows the sampled model to adapt to new information, exploiting past experiments. We show theoretically that the method converges asymptotically to the true parameters, while simultaneously maximizing our metric. We show empirically that the process exhibits rapid convergence, yielding good results with a very small number of experiments. "
801237648509730816,2016-11-23 01:35:16,https://t.co/R8sllkqwZV,Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery. (arXiv:1611.07252v1 [stat.ML]) https://t.co/R8sllkqwZV,0,10," Abstract: Recurrent neural networks (RNNs) are powerful and effective for processing sequential data. However, RNNs are usually considered ""black box"" models whose internal structure and learned parameters are not interpretable. In this paper, we propose an interpretable RNN based on the sequential iterative soft-thresholding algorithm (SISTA) for solving the sequential sparse recovery problem, which models a sequence of correlated observations with a sequence of sparse latent vectors. The architecture of the resulting SISTA-RNN is implicitly defined by the computational structure of SISTA, which results in a novel stacked RNN architecture. Furthermore, the weights of the SISTA-RNN are perfectly interpretable as the parameters of a principled statistical model, which in this case include a sparsifying dictionary, iterative step size, and regularization parameters. In addition, on a particular sequential compressive sensing task, the SISTA-RNN trains faster and achieves better performance than conventional state-of-the-art black box RNNs, including long-short term memory (LSTM) RNNs. "
801237646848774144,2016-11-23 01:35:15,https://t.co/TAhVnn0QNq,Adaptive Design of Experiments for Conservative Estimation of Excursion Sets. (arXiv:1611.07256v1 [stat.ME]) https://t.co/TAhVnn0QNq,0,2," Abstract: We consider a Gaussian process model trained on few evaluations of an expensive-to-evaluate deterministic function and we study the problem of estimating a fixed excursion set of this function. We review the concept of conservative estimates, recently introduced in this framework, and, in particular, we focus on estimates based on Vorob'ev quantiles. We present a method that sequentially selects new evaluations of the function in order to reduce the uncertainty on such estimates. The sequential strategies are first benchmarked on artificial test cases generated from Gaussian process realizations in two and five dimensions, and then applied to two reliability engineering test cases. "
801237645150060544,2016-11-23 01:35:15,https://t.co/dXb6sEgT3k,Investigating the influence of noise and distractors on the interpretation of neural networks. (arXiv:1611.07270v1… https://t.co/dXb6sEgT3k,4,5," Abstract: Understanding neural networks is becoming increasingly important. Over the last few years different types of visualisation and explanation methods have been proposed. However, none of them explicitly considered the behaviour in the presence of noise and distracting elements. In this work, we will show how noise and distracting dimensions can influence the result of an explanation model. This gives a new theoretical insights to aid selection of the most appropriate explanation model within the deep-Taylor decomposition framework. "
801237643073683457,2016-11-23 01:35:15,https://t.co/kjnqwbunJI,Variational Graph Auto-Encoders. (arXiv:1611.07308v1 [stat.ML]) https://t.co/kjnqwbunJI,5,14," Abstract: We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets. "
801237641324888064,2016-11-23 01:35:14,https://t.co/KyD0v2ahR7,Deep Learning Approximation for Stochastic Control Problems. (arXiv:1611.07422v1 [cs.LG]) https://t.co/KyD0v2ahR7,1,6," Abstract: Many real world stochastic control problems suffer from the ""curse of dimensionality"". To overcome this difficulty, we develop a deep learning approach that directly solves high-dimensional stochastic control problems based on Monte-Carlo sampling. We approximate the time-dependent controls as feedforward neural networks and stack these networks together through model dynamics. The objective function for the control problem plays the role of the loss function for the deep neural network. We test this approach using examples from the areas of optimal trading and energy storage. Our results suggest that the algorithm presented here achieves satisfactory accuracy and at the same time, can handle rather high dimensional problems. "
801237639449878528,2016-11-23 01:35:14,https://t.co/GGZPvLBc5w,TreeView: Peeking into Deep Neural Networks Via Feature-Space Partitioning. (arXiv:1611.07429v1 [stat.ML]) https://t.co/GGZPvLBc5w,0,8," Abstract: With the advent of highly predictive but opaque deep learning models, it has become more important than ever to understand and explain the predictions of such models. Existing approaches define interpretability as the inverse of complexity and achieve interpretability at the cost of accuracy. This introduces a risk of producing interpretable but misleading explanations. As humans, we are prone to engage in this kind of behavior \cite{mythos}. In this paper, we take a step in the direction of tackling the problem of interpretability without compromising the model accuracy. We propose to build a Treeview representation of the complex model via hierarchical partitioning of the feature space, which reveals the iterative rejection of unlikely class labels until the correct association is predicted. "
801237637797478400,2016-11-23 01:35:13,https://t.co/9LXPOphRwR,Mapping chemical performance on molecular structures using locally interpretable explanations. (arXiv:1611.07443v1… https://t.co/9LXPOphRwR,0,6," Abstract: In this work, we present an application of Locally Interpretable Machine-Agnostic Explanations to 2-D chemical structures. Using this framework we are able to provide a structural interpretation for an existing black-box model for classifying biologically produced fuel compounds with regard to Research Octane Number. This method of ""painting"" locally interpretable explanations onto 2-D chemical structures replicates the chemical intuition of synthetic chemists, allowing researchers in the field to directly accept, reject, inform and evaluate decisions underlying inscrutably complex quantitative structure-activity relationship models. "
801237636354637825,2016-11-23 01:35:13,https://t.co/Q3EXBzbyXz,Grad-CAM: Why did you say that?. (arXiv:1611.07450v1 [stat.ML]) https://t.co/Q3EXBzbyXz,3,3," Abstract: We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract. "
801237634823651329,2016-11-23 01:35:13,https://t.co/wvRR49GSLz,Poisson Random Fields for Dynamic Feature Models. (arXiv:1611.07460v1 [stat.ML]) https://t.co/wvRR49GSLz,0,4," Abstract: We present the Wright-Fisher Indian buffet process (WF-IBP), a probabilistic model for time-dependent data assumed to have been generated by an unknown number of latent features. This model is suitable as a prior in Bayesian nonparametric feature allocation models in which the features underlying the observed data exhibit a dependency structure over time. More specifically, we establish a new framework for generating dependent Indian buffet processes, where the Poisson random field model from population genetics is used as a way of constructing dependent beta processes. Inference in the model is complex, and we describe a sophisticated Markov Chain Monte Carlo algorithm for exact posterior simulation. We apply our construction to develop a nonparametric focused topic model for collections of time-stamped text documents and test it on the full corpus of NIPS papers published from 1987 to 2015. "
801237632441139200,2016-11-23 01:35:12,https://t.co/Rr0kv3iSDL,Inducing Interpretable Representations with Variational Autoencoders. (arXiv:1611.07492v1 [stat.ML]) https://t.co/Rr0kv3iSDL,2,15," Abstract: We develop a framework for incorporating structured graphical models in the \emph{encoders} of variational autoencoders (VAEs) that allows us to induce interpretable representations through approximate variational inference. This allows us to both perform reasoning (e.g. classification) under the structural constraints of a given graphical model, and use deep generative models to deal with messy, high-dimensional domains where it is often difficult to model all the variation. Learning in this framework is carried out end-to-end with a variational objective, applying to both unsupervised and semi-supervised schemes. "
801237630495162368,2016-11-23 01:35:12,https://t.co/yFU2OHpKb5,Challenges in Bayesian Adaptive Data Analysis. (arXiv:1604.02492v4 [cs.LG] UPDATED) https://t.co/yFU2OHpKb5,0,6," Abstract: Traditional statistical analysis requires that the analysis process and data are independent. By contrast, the new field of adaptive data analysis hopes to understand and provide algorithms and accuracy guarantees for research as it is commonly performed in practice, as an iterative process of interacting repeatedly with the same data set, such as repeated tests against a holdout set. Previous work has defined a model with a rather strong lower bound on sample complexity in terms of the number of queries, $n\sim\sqrt q$, arguing that adaptive data analysis is much harder than static data analysis, where $n\sim\log q$ is possible. Instead, we argue that those strong lower bounds point to a limitation of the previous model in that it must consider wildly asymmetric scenarios which do not hold in typical applications. To better understand other difficulties of adaptivity, we propose a new Bayesian version of the problem that mandates symmetry. Since the other lower bound techniques are ruled out, we can more effectively see difficulties that might otherwise be overshadowed. As a first contribution to this model, we produce a new problem using error-correcting codes on which a large family of methods, including all previously proposed algorithms, require roughly $n\sim\sqrt[4]q$. These early results illustrate new difficulties in adaptive data analysis regarding slightly correlated queries on problems with concentrated uncertainty. "
801237627508621312,2016-11-23 01:35:11,https://t.co/jXibrgGGlG,Finding Common Characteristics Among NBA Playoff and Championship Teams: A Machine Learning Approach. (arXiv:1604.… https://t.co/jXibrgGGlG,0,3," Abstract: In this paper, we employ machine learning techniques to analyze sixteen seasons of NBA regular season data from every team to determine the common characteristics among NBA playoff teams. Each team was characterized by 42 predictor variables and one binary response variable taking on a value of ""TRUE"" if a team had made the playoffs, and value of ""FALSE"" if a team had missed the playoffs. After fitting an initial classification tree to this problem, this tree was then pruned which decreased the test error rate. Further to this, a random forest of classification trees was grown which provided a very accurate model from which a variable importance plot was generated to determine which predictor variables had the greatest influence on the response variable. The result of this work was the conclusion that the most important factors in characterizing a team's playoff eligibility are the opponent field goal percentage and the opponent points per game. This seems to suggest that \emph{defensive} factors as opposed to offensive factors are the most important characteristics shared among NBA playoff teams. We also perform a classification analysis to determine common characteristics among NBA championship teams. Using an artificial neural network structure, we show that championship teams must be able to have very strong defensive characteristics, in particular, strong perimeter defense characteristics in combination with an effective half-court offense that generates high-percentage two-point shots. A key part of this offensive strategy must also be the ability to draw fouls. This analysis will hopefully dispel the rising notion that an offense geared towards shooting many three point shots is a sufficient and necessary condition for an NBA team to be successful in qualifying for the playoffs and winning a championship. "
801237625201770496,2016-11-23 01:35:10,https://t.co/I6cS1A1NOp,Convergence Analysis for Rectangular Matrix Completion Using Burer-Monteiro Factorization and Gradient Descent. (a… https://t.co/I6cS1A1NOp,1,3," Abstract: We address the rectangular matrix completion problem by lifting the unknown matrix to a positive semidefinite matrix in higher dimension, and optimizing a nonconvex objective over the semidefinite factor using a simple gradient descent scheme. With $O( \mu r^2 \kappa^2 n \max(\mu, \log n))$ random observations of a $n_1 \times n_2$ $\mu$-incoherent matrix of rank $r$ and condition number $\kappa$, where $n = \max(n_1, n_2)$, the algorithm linearly converges to the global optimum with high probability. "
801237623222177792,2016-11-23 01:35:10,https://t.co/6rvmyvPdvR,Clustering with Same-Cluster Queries. (arXiv:1606.02404v2 [cs.LG] UPDATED) https://t.co/6rvmyvPdvR,0,2," Abstract: We propose a framework for Semi-Supervised Active Clustering framework (SSAC), where the learner is allowed to interact with a domain expert, asking whether two given instances belong to the same cluster or not. We study the query and computational complexity of clustering in this framework. We consider a setting where the expert conforms to a center-based clustering with a notion of margin. We show that there is a trade off between computational complexity and query complexity; We prove that for the case of $k$-means clustering (i.e., when the expert conforms to a solution of $k$-means), having access to relatively few such queries allows efficient solutions to otherwise NP hard problems. In particular, we provide a probabilistic polynomial-time (BPP) algorithm for clustering in this setting that asks $O\big(k^2\log k + k\log n)$ same-cluster queries and runs with time complexity $O\big(kn\log n)$ (where $k$ is the number of clusters and $n$ is the number of instances). The algorithm succeeds with high probability for data satisfying margin conditions under which, without queries, we show that the problem is NP hard. We also prove a lower bound on the number of queries needed to have a computationally efficient clustering algorithm in this setting. "
801237621666119680,2016-11-23 01:35:09,https://t.co/VrK9S2Jh8F,Evolutionary Synthesis of Deep Neural Networks via Synaptic Cluster-driven Genetic Encoding. (arXiv:1609.01360v2 [… https://t.co/VrK9S2Jh8F,0,3," Abstract: There has been significant recent interest towards achieving highly efficient deep neural network architectures. A promising paradigm for achieving this is the concept of evolutionary deep intelligence, which attempts to mimic biological evolution processes to synthesize highly-efficient deep neural networks over successive generations. An important aspect of evolutionary deep intelligence is the genetic encoding scheme used to mimic heredity, which can have a significant impact on the quality of offspring deep neural networks. Motivated by the neurobiological phenomenon of synaptic clustering, we introduce a new genetic encoding scheme where synaptic probability is driven towards the formation of a highly sparse set of synaptic clusters. Experimental results for the task of image classification demonstrated that the synthesized offspring networks using this synaptic cluster-driven genetic encoding scheme can achieve state-of-the-art performance while having network architectures that are not only significantly more efficient (with a ~125-fold decrease in synapses for MNIST) compared to the original ancestor network, but also tailored for GPU-accelerated machine learning applications. "
801237618931486720,2016-11-23 01:35:09,https://t.co/dxsyuclBXD,Statistical comparison of classifiers through Bayesian hierarchical modelling. (arXiv:1609.08905v3 [cs.LG] UPDATED) https://t.co/dxsyuclBXD,0,6," Abstract: Usually one compares the accuracy of two competing classifiers via null hypothesis significance tests (nhst). Yet the nhst tests suffer from important shortcomings, which can be overcome by switching to Bayesian hypothesis testing. We propose a Bayesian hierarchical model which jointly analyzes the cross-validation results obtained by two classifiers on multiple data sets. It returns the posterior probability of the accuracies of the two classifiers being practically equivalent or significantly different. A further strength of the hierarchical model is that, by jointly analyzing the results obtained on all data sets, it reduces the estimation error compared to the usual approach of averaging the cross-validation results obtained on a given data set. "
800875313911762945,2016-11-22 01:35:29,https://t.co/Lgvaptgyvt,Deep Clustering and Conventional Networks for Music Separation: Stronger Together. (arXiv:1611.06265v1 [stat.ML]) https://t.co/Lgvaptgyvt,1,12," Abstract: Deep clustering is the first method to handle general audio separation scenarios with multiple sources of the same type and an arbitrary number of sources, performing impressively in speaker-independent speech separation tasks. However, little is known about its effectiveness in other challenging situations such as music source separation. Contrary to conventional networks that directly estimate the source signals, deep clustering generates an embedding for each time-frequency bin, and separates sources by clustering the bins in the embedding space. We show that deep clustering outperforms conventional networks on a singing voice separation task, in both matched and mismatched conditions, even though conventional networks have the advantage of end-to-end training for best signal approximation, presumably because its more flexible objective engenders better regularization. Since the strengths of deep clustering and conventional network architectures appear complementary, we explore combining them in a single hybrid network trained via an approach akin to multi-task learning. Remarkably, the combination significantly outperforms either of its components. "
800875312296972288,2016-11-22 01:35:28,https://t.co/KX6nM3ZJzl,Local minima in training of deep networks. (arXiv:1611.06310v1 [stat.ML]) https://t.co/KX6nM3ZJzl,1,5," Abstract: There has been a lot of recent interest in trying to characterize the error surface of deep models. This stems from a long standing question. Given that deep networks are highly nonlinear systems optimized by local gradient methods, why do they not seem to be affected by bad local minima? It is widely believed that training of deep models using gradient methods works so well because the error surface either has no local minima, or if they exist they need to be close in value to the global minimum. It is known that such results hold under very strong assumptions which are not satisfied by real models. In this paper we present examples showing that for such theorem to be true additional assumptions on the data, initialization schemes and/or the model classes have to be made. We look at the particular case of finite size datasets. We demonstrate that in this scenario one can construct counter-examples (datasets or initialization schemes) when the network does become susceptible to bad local minima over the weight space. "
800875310296100867,2016-11-22 01:35:28,https://t.co/7MbiKh3nHG,Determining the Veracity of Rumours on Twitter. (arXiv:1611.06314v1 [cs.SI]) https://t.co/7MbiKh3nHG,0,2," Abstract: While social networks can provide an ideal platform for up-to-date information from individuals across the world, it has also proved to be a place where rumours fester and accidental or deliberate misinformation often emerges. In this article, we aim to support the task of making sense from social media data, and specifically, seek to build an autonomous message-classifier that filters relevant and trustworthy information from Twitter. For our work, we collected about 100 million public tweets, including users' past tweets, from which we identified 72 rumours (41 true, 31 false). We considered over 80 trustworthiness measures including the authors' profile and past behaviour, the social network connections (graphs), and the content of tweets themselves. We ran modern machine-learning classifiers over those measures to produce trustworthiness scores at various time windows from the outbreak of the rumour. Such time-windows were key as they allowed useful insight into the progression of the rumours. From our findings, we identified that our model was significantly more accurate than similar studies in the literature. We also identified critical attributes of the data that give rise to the trustworthiness scores assigned. Finally we developed a software demonstration that provides a visual user interface to allow the user to examine the analysis. "
800875308790464512,2016-11-22 01:35:27,https://t.co/o70r6uW74c,Conservative Contextual Linear Bandits. (arXiv:1611.06426v1 [stat.ML]) https://t.co/o70r6uW74c,0,4," Abstract: Safety is a desirable property that can immensely increase the applicability of learning algorithms in real-world decision-making problems. It is much easier for a company to deploy an algorithm that is safe, i.e.,~guaranteed to perform at least as well as a baseline. In this paper, we study the issue of safety in contextual linear bandits that have application in many different fields including personalized ad recommendation in online marketing. We formulate a notion of safety for this class of algorithms. We develop a safe contextual linear bandit algorithm, called {\em conservative linear UCB} (CLUCB), that simultaneously minimizes its regret and satisfies the safety constraint, i.e.,~maintains its performance above a fixed percentage of the performance of a baseline strategy, uniformly over time. We prove an upper-bound on the regret of CLUCB and show that it can be decomposed into two terms: {\bf 1)} an upper-bound for the regret of the standard linear UCB algorithm that grows with the time horizon and {\bf 2)} a constant (does not grow with the time horizon) term that accounts for the loss of being conservative in order to satisfy the safety constraint. We empirically show that our algorithm is safe and validate our theoretical analysis. "
800875307104403456,2016-11-22 01:35:27,https://t.co/ATSucyPHKD,Dealing with Range Anxiety in Mean Estimation via Statistical Queries. (arXiv:1611.06475v1 [cs.LG]) https://t.co/ATSucyPHKD,0,2," Abstract: In the statistical query (SQ) model an algorithm has access to an SQ oracle for the input distribution $D$ over $X$ instead of i.i.d.~ samples from $D$. Given a query function $\phi:X \rightarrow [-1,1]$, the oracle returns an estimate of ${\bf E}_{{\bf x}\sim D}[\phi({\bf x})]$ within some tolerance $\tau$. In a variety of natural problems it is necessary to estimate expectations of functions whose standard deviation is much smaller than the range. In this note we describe a nearly optimal algorithm for estimation of such expectations via statistical queries. As applications, we give algorithms for high dimensional mean estimation in the SQ model and in the distributed setting where only a single bit is communicated from each sample. "
800875305703391232,2016-11-22 01:35:27,https://t.co/HaUZuWOIeq,Linear Thompson Sampling Revisited. (arXiv:1611.06534v1 [stat.ML]) https://t.co/HaUZuWOIeq,0,7," Abstract: We derive an alternative proof for the regret of Thompson sampling (\ts) in the stochastic linear bandit setting. While we obtain a regret bound of order $\widetilde{O}(d^{3/2}\sqrt{T})$ as in previous results, the proof sheds new light on the functioning of the \ts. We leverage on the structure of the problem to show how the regret is related to the sensitivity (i.e., the gradient) of the objective function and how selecting optimal arms associated to \textit{optimistic} parameters does control it. Thus we show that \ts can be seen as a generic randomized algorithm where the sampling distribution is designed to have a fixed probability of being optimistic, at the cost of an additional $\sqrt{d}$ regret factor compared to a UCB-like approach. Furthermore, we show that our proof can be readily applied to regularized linear optimization and generalized linear model problems. "
800875302599671810,2016-11-22 01:35:26,https://t.co/BbwNuyFuYT,Variational Boosting: Iteratively Refining Posterior Approximations. (arXiv:1611.06585v1 [stat.ML]) https://t.co/BbwNuyFuYT,2,25," Abstract: We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class. Our method, termed variational boosting, iteratively refines an existing variational approximation by solving a sequence of optimization problems, allowing the practitioner to trade computation time for accuracy. We show how to expand the variational approximating class by incorporating additional covariance structure and by introducing new components to form a mixture. We apply variational boosting to synthetic and real statistical models, and show that resulting posterior inferences compare favorably to existing posterior approximation algorithms in both accuracy and efficiency. "
800875300557033473,2016-11-22 01:35:25,https://t.co/7Ki1Jj4S54,Scalable Adaptive Stochastic Optimization Using Random Projections. (arXiv:1611.06652v1 [stat.ML]) https://t.co/7Ki1Jj4S54,0,7," Abstract: Adaptive stochastic gradient methods such as AdaGrad have gained popularity in particular for training deep neural networks. The most commonly used and studied variant maintains a diagonal matrix approximation to second order information by accumulating past gradients which are used to tune the step size adaptively. In certain situations the full-matrix variant of AdaGrad is expected to attain better performance, however in high dimensions it is computationally impractical. We present Ada-LR and RadaGrad two computationally efficient approximations to full-matrix AdaGrad based on randomized dimensionality reduction. They are able to capture dependencies between features and achieve similar performance to full-matrix AdaGrad but at a much smaller computational cost. We show that the regret of Ada-LR is close to the regret of full-matrix AdaGrad which can have an up-to exponentially smaller dependence on the dimension than the diagonal variant. Empirically, we show that Ada-LR and RadaGrad perform similarly to full-matrix AdaGrad. On the task of training convolutional neural networks as well as recurrent neural networks, RadaGrad achieves faster convergence than diagonal AdaGrad. "
800875299093315585,2016-11-22 01:35:25,https://t.co/uYx1dDqqZ4,Scalable Approximations for Generalized Linear Problems. (arXiv:1611.06686v1 [stat.ML]) https://t.co/uYx1dDqqZ4,0,1," Abstract: In stochastic optimization, the population risk is generally approximated by the empirical risk. However, in the large-scale setting, minimization of the empirical risk may be computationally restrictive. In this paper, we design an efficient algorithm to approximate the population risk minimizer in generalized linear problems such as binary classification with surrogate losses and generalized linear regression models. We focus on large-scale problems, where the iterative minimization of the empirical risk is computationally intractable, i.e., the number of observations $n$ is much larger than the dimension of the parameter $p$, i.e. $n \gg p \gg 1$. We show that under random sub-Gaussian design, the true minimizer of the population risk is approximately proportional to the corresponding ordinary least squares (OLS) estimator. Using this relation, we design an algorithm that achieves the same accuracy as the empirical risk minimizer through iterations that attain up to a cubic convergence rate, and that are cheaper than any batch optimization algorithm by at least a factor of $\mathcal{O}(p)$. We provide theoretical guarantees for our algorithm, and analyze the convergence behavior in terms of data dimensions. Finally, we demonstrate the performance of our algorithm on well-known classification and regression problems, through extensive numerical studies on large-scale datasets, and show that it achieves the highest performance compared to several other widely used and specialized optimization algorithms. "
800875297482637314,2016-11-22 01:35:25,https://t.co/JtkiIEp6zF,Variational Fourier features for Gaussian processes. (arXiv:1611.06740v1 [stat.ML]) https://t.co/JtkiIEp6zF,0,10," Abstract: This work brings together two powerful concepts in Gaussian processes: the variational approach to sparse approximation and the spectral representation of Gaussian processes. This gives rise to an approximation that inherits the benefits of the variational approach but with the representational power and computational scalability of spectral representations. The work hinges on a key result that there exist spectral features related to a finite domain of the Gaussian process which exhibit almost-independent covariances. We derive these expressions for Matern kernels in one dimension, and generalize to more dimensions using kernels with specific structures. Under the assumption of additive Gaussian noise, our method requires only a single pass through the dataset, making for very fast and accurate computation. We fit a model to 4 million training points in just a few minutes on a standard laptop. With non-conjugate likelihoods, our MCMC scheme reduces the cost of computation from O(NM2) (for a sparse Gaussian process) to O(NM) per iteration, where N is the number of data and M is the number of features. "
800875295997890560,2016-11-22 01:35:24,https://t.co/hNqWMPgYb1,Emergence of Compositional Representations in Restricted Boltzmann Machines. (arXiv:1611.06759v1 [physics.data-an]) https://t.co/hNqWMPgYb1,1,4," Abstract: Extracting automatically the complex set of features composing real high-dimensional data is crucial for achieving high performance in machine--learning tasks. Restricted Boltzmann Machines (RBM) are empirically known to be efficient for this purpose, and to be able to generate distributed and graded representations of the data. We characterize the structural conditions (sparsity of the weights, low effective temperature, nonlinearities in the activation functions of hidden units, and adaptation of fields maintaining the activity in the visible layer) allowing RBM to operate in such a compositional phase. Evidence is provided by the replica analysis of an adequate statistical ensemble of random RBMs and by RBM trained on the handwritten digits dataset MNIST. "
800875294349524992,2016-11-22 01:35:24,https://t.co/OEWkqgsjhN,MDL-motivated compression of GLM ensembles increases interpretability and retains predictive power. (arXiv:1611.06… https://t.co/OEWkqgsjhN,0,2," Abstract: Over the years, ensemble methods have become a staple of machine learning. Similarly, generalized linear models (GLMs) have become very popular for a wide variety of statistical inference tasks. The former have been shown to enhance out- of-sample predictive power and the latter possess easy interpretability. Recently, ensembles of GLMs have been proposed as a possibility. On the downside, this approach loses the interpretability that GLMs possess. We show that minimum description length (MDL)-motivated compression of the inferred ensembles can be used to recover interpretability without much, if any, downside to performance and illustrate on a number of standard classification data sets. "
800875292579360769,2016-11-22 01:35:23,https://t.co/TdxxbmcRX7,Joint Hacking and Latent Hazard Rate Estimation. (arXiv:1611.06843v1 [stat.ML]) https://t.co/TdxxbmcRX7,0,3," Abstract: In this paper we describe an algorithm for predicting the websites at risk in a long range hacking activity, while jointly inferring the provenance and evolution of vulnerabilities on websites over continuous time. Specifically, we use hazard regression with a time-varying additive hazard function parameterized in a generalized linear form. The activation coefficients on each feature are continuous-time functions constrained with total variation penalty inspired by hacking campaigns. We show that the optimal solution is a 0th order spline with a finite number of adaptively chosen knots, and can be solved efficiently. Experiments on real data show that our method significantly outperforms classic methods while providing meaningful interpretability. "
800875290645893121,2016-11-22 01:35:23,https://t.co/UQI9hf2df4,Probabilistic structure discovery in time series data. (arXiv:1611.06863v1 [stat.ML]) https://t.co/UQI9hf2df4,1,8," Abstract: Existing methods for structure discovery in time series data construct interpretable, compositional kernels for Gaussian process regression models. While the learned Gaussian process model provides posterior mean and variance estimates, typically the structure is learned via a greedy optimization procedure. This restricts the space of possible solutions and leads to over-confident uncertainty estimates. We introduce a fully Bayesian approach, inferring a full posterior over structures, which more reliably captures the uncertainty of the model. "
800875289030979584,2016-11-22 01:35:23,https://t.co/lsIV82ueTy,Learning From Graph Neighborhoods Using LSTMs. (arXiv:1611.06882v1 [cs.LG]) https://t.co/lsIV82ueTy,0,8," Abstract: Many prediction problems can be phrased as inferences over local neighborhoods of graphs. The graph represents the interaction between entities, and the neighborhood of each entity contains information that allows the inferences or predictions. We present an approach for applying machine learning directly to such graph neighborhoods, yielding predicitons for graph nodes on the basis of the structure of their local neighborhood and the features of the nodes in it. Our approach allows predictions to be learned directly from examples, bypassing the step of creating and tuning an inference model or summarizing the neighborhoods via a fixed set of hand-crafted features. The approach is based on a multi-level architecture built from Long Short-Term Memory neural nets (LSTMs); the LSTMs learn how to summarize the neighborhood from data. We demonstrate the effectiveness of the proposed technique on a synthetic example and on real-world data related to crowdsourced grading, Bitcoin transactions, and Wikipedia edit reversions. "
800875286921285633,2016-11-22 01:35:22,https://t.co/DHTr1wPACw,Memory Lens: How Much Memory Does an Agent Use?. (arXiv:1611.06928v1 [cs.AI]) https://t.co/DHTr1wPACw,0,2," Abstract: We propose a new method to study the internal memory used by reinforcement learning policies. We estimate the amount of relevant past information by estimating mutual information between behavior histories and the current action of an agent. We perform this estimation in the passive setting, that is, we do not intervene but merely observe the natural behavior of the agent. Moreover, we provide a theoretical justification for our approach by showing that it yields an implementation-independent lower bound on the minimal memory capacity of any agent that implement the observed policy. We demonstrate our approach by estimating the use of memory of DQN policies on concatenated Atari frames, demonstrating sharply different use of memory across 49 games. The study of memory as information that flows from the past to the current action opens avenues to understand and improve successful reinforcement learning algorithms. "
800875284597600256,2016-11-22 01:35:22,https://t.co/aRGSPetxMe,Unsupervised Learning for Lexicon-Based Classification. (arXiv:1611.06933v1 [cs.LG]) https://t.co/aRGSPetxMe,6,21," Abstract: In lexicon-based classification, documents are assigned labels by comparing the number of words that appear from two opposed lexicons, such as positive and negative sentiment. Creating such words lists is often easier than labeling instances, and they can be debugged by non-experts if classification performance is unsatisfactory. However, there is little analysis or justification of this classification heuristic. This paper describes a set of assumptions that can be used to derive a probabilistic justification for lexicon-based classification, as well as an analysis of its expected accuracy. One key assumption behind lexicon-based classification is that all words in each lexicon are equally predictive. This is rarely true in practice, which is why lexicon-based approaches are usually outperformed by supervised classifiers that learn distinct weights on each word from labeled instances. This paper shows that it is possible to learn such weights without labeled data, by leveraging co-occurrence statistics across the lexicons. This offers the best of both worlds: light supervision in the form of lexicons, and data-driven classification with higher accuracy than traditional word-counting heuristics. "
800875282664189952,2016-11-22 01:35:21,https://t.co/AI1TGtZ84D,Measuring Sample Quality with Diffusions. (arXiv:1611.06972v1 [stat.ML]) https://t.co/AI1TGtZ84D,0,3," Abstract: Standard Markov chain Monte Carlo diagnostics, like effective sample size, are ineffective for biased sampling procedures that sacrifice asymptotic correctness for computational speed. Recent work addresses this issue for a class of strongly log-concave target distributions by constructing a computable discrepancy measure based on Stein's method that provably determines convergence to the target. We generalize this approach to cover any target with a fast-coupling Ito diffusion by bounding the derivatives of Stein equation solutions in terms of Markov process coupling rates. As example applications, we develop computable and convergence-determining diffusion Stein discrepancies for log-concave, heavy-tailed, and multimodal targets and use these quality measures to select the hyperparameters of biased samplers, compare random and deterministic quadrature rules, and quantify bias-variance tradeoffs in approximate Markov chain Monte Carlo. Our explicit multivariate Stein factor bounds may be of independent interest. "
800875281099550725,2016-11-22 01:35:21,https://t.co/OHz058PlRk,Spatial contrasting for deep unsupervised learning. (arXiv:1611.06996v1 [stat.ML]) https://t.co/OHz058PlRk,0,5," Abstract: Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images. This criterion can be employed within conventional neural networks and trained using standard techniques such as SGD and back-propagation, thus complementing supervised methods. "
800875279249973248,2016-11-22 01:35:20,https://t.co/3ciVBHPVBU,Private Empirical Risk Minimization Beyond the Worst Case: The Effect of the Constraint Set Geometry. (arXiv:1411.… https://t.co/3ciVBHPVBU,0,5," Abstract: Empirical Risk Minimization (ERM) is a standard technique in machine learning, where a model is selected by minimizing a loss function over constraint set. When the training dataset consists of private information, it is natural to use a differentially private ERM algorithm, and this problem has been the subject of a long line of work started with Chaudhuri and Monteleoni 2008. A private ERM algorithm outputs an approximate minimizer of the loss function and its error can be measured as the difference from the optimal value of the loss function. When the constraint set is arbitrary, the required error bounds are fairly well understood \cite{BassilyST14}. In this work, we show that the geometric properties of the constraint set can be used to derive significantly better results. Specifically, we show that a differentially private version of Mirror Descent leads to error bounds of the form $\tilde{O}(G_{\mathcal{C}}/n)$ for a lipschitz loss function, improving on the $\tilde{O}(\sqrt{p}/n)$ bounds in Bassily, Smith and Thakurta 2014. Here $p$ is the dimensionality of the problem, $n$ is the number of data points in the training set, and $G_{\mathcal{C}}$ denotes the Gaussian width of the constraint set that we optimize over. We show similar improvements for strongly convex functions, and for smooth functions. In addition, we show that when the loss function is Lipschitz with respect to the $\ell_1$ norm and $\mathcal{C}$ is $\ell_1$-bounded, a differentially private version of the Frank-Wolfe algorithm gives error bounds of the form $\tilde{O}(n^{-2/3})$. This captures the important and common case of sparse linear regression (LASSO), when the data $x_i$ satisfies $|x_i|_{\infty} \leq 1$ and we optimize over the $\ell_1$ ball. We show new lower bounds for this setting, that together with known bounds, imply that all our upper bounds are tight. "
800875277006110720,2016-11-22 01:35:20,https://t.co/kwxEABpFRF,Estimation and Inference of Heterogeneous Treatment Effects using Random Forests. (arXiv:1510.04342v3 [stat.ME] UP… https://t.co/kwxEABpFRF,0,4," Abstract: Many scientific and engineering challenges -- ranging from personalized medicine to customized marketing recommendations -- require an understanding of treatment effect heterogeneity. In this paper, we develop a non-parametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect, and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates. "
800875275156328452,2016-11-22 01:35:19,https://t.co/zaDlcWQNJy,Neuron's Eye View: Inferring Features of Complex Stimuli from Neural Responses. (arXiv:1512.01408v2 [stat.ML] UPDA… https://t.co/zaDlcWQNJy,0,3," Abstract: Experiments that study neural encoding of stimuli at the level of individual neurons typically choose a small set of features present in the world --- contrast and luminance for vision, pitch and intensity for sound --- and assemble a stimulus set that systematically varies along these dimensions. Subsequent analysis of neural responses to these stimuli typically focuses on regression models, with experimenter-controlled features as predictors and spike counts or firing rates as responses. Unfortunately, this approach requires knowledge in advance about the relevant features coded by a given population of neurons. For domains as complex as social interaction or natural movement, however, the relevant feature space is poorly understood, and an arbitrary \emph{a priori} choice of features may give rise to confirmation bias. Here, we present a Bayesian model for exploratory data analysis that is capable of automatically identifying the features present in unstructured stimuli based solely on neuronal responses. Our approach is unique within the class of latent state space models of neural activity in that it assumes that firing rates of neurons are sensitive to multiple discrete time-varying features tied to the \emph{stimulus}, each of which has Markov (or semi-Markov) dynamics. That is, we are modeling neural activity as driven by multiple simultaneous stimulus features rather than intrinsic neural dynamics. We derive a fast variational Bayesian inference algorithm and show that it correctly recovers hidden features in synthetic data, as well as ground-truth stimulus features in a prototypical neural dataset. To demonstrate the utility of the algorithm, we also apply it to cluster neural responses and demonstrate successful recovery of features corresponding to monkeys and faces in the image set. "
800875273742938112,2016-11-22 01:35:19,https://t.co/gBBKrymm6c,MCMC assisted by Belief Propagaion. (arXiv:1605.09042v5 [stat.ML] UPDATED) https://t.co/gBBKrymm6c,0,7," Abstract: Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most popular algorithms for computational inference in Graphical Models (GM). In principle, MCMC is an exact probabilistic method which, however, often suffers from exponentially slow mixing. In contrast, BP is a deterministic method, which is typically fast, empirically very successful, however in general lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC algorithms correcting the approximation error of BP, i.e., we provide a way to compensate for BP errors via a consecutive BP-aware MCMC. Our framework is based on the Loop Calculus (LC) approach which allows to express the BP error as a sum of weighted generalized loops. Although the full series is computationally intractable, it is known that a truncated series, summing up all 2-regular loops, is computable in polynomial-time for planar pair-wise binary GMs and it also provides a highly accurate approximation empirically. Motivated by this, we first propose a polynomial-time approximation MCMC scheme for the truncated series of general (non-planar) pair-wise binary models. Our main idea here is to use the Worm algorithm, known to provide fast mixing in other (related) problems, and then design an appropriate rejection scheme to sample 2-regular loops. Furthermore, we also design an efficient rejection-free MCMC scheme for approximating the full series. The main novelty underlying our design is in utilizing the concept of cycle basis, which provides an efficient decomposition of the generalized loops. In essence, the proposed MCMC schemes run on transformed GM built upon the non-trivial BP solution, and our experiments show that this synthesis of BP and MCMC outperforms both direct MCMC and bare BP schemes. "
800875271553314818,2016-11-22 01:35:18,https://t.co/HiLQSEMzAi,Nonlinear Statistical Learning with Truncated Gaussian Graphical Models. (arXiv:1606.00906v2 [stat.ML] UPDATED) https://t.co/HiLQSEMzAi,0,3," Abstract: We introduce the truncated Gaussian graphical model (TGGM) as a novel framework for designing statistical models for nonlinear learning. A TGGM is a Gaussian graphical model (GGM) with a subset of variables truncated to be nonnegative. The truncated variables are assumed latent and integrated out to induce a marginal model. We show that the variables in the marginal model are non-Gaussian distributed and their expected relations are nonlinear. We use expectation-maximization to break the inference of the nonlinear model into a sequence of TGGM inference problems, each of which is efficiently solved by using the properties and numerical methods of multivariate Gaussian distributions. We use the TGGM to design models for nonlinear regression and classification, with the performances of these models demonstrated on extensive benchmark datasets and compared to state-of-the-art competing results. "
800875269980569605,2016-11-22 01:35:18,https://t.co/GbPOp1ucST,Exponential Family Embeddings. (arXiv:1608.00778v2 [stat.ML] UPDATED) https://t.co/GbPOp1ucST,2,6," Abstract: Word embeddings are a powerful approach for capturing semantic similarity among terms in a vocabulary. In this paper, we develop exponential family embeddings, a class of methods that extends the idea of word embeddings to other types of high-dimensional data. As examples, we studied neural data with real-valued observations, count data from a market basket analysis, and ratings data from a movie recommendation system. The main idea is to model each observation conditioned on a set of other observations. This set is called the context, and the way the context is defined is a modeling choice that depends on the problem. In language the context is the surrounding words; in neuroscience the context is close-by neurons; in market basket data the context is other items in the shopping cart. Each type of embedding model defines the context, the exponential family of conditional distributions, and how the latent embedding vectors are shared across data. We infer the embeddings with a scalable algorithm based on stochastic gradient descent. On all three applications - neural activity of zebrafish, users' shopping behavior, and movie ratings - we found exponential family embedding models to be more effective than other types of dimension reduction. They better reconstruct held-out data and find interesting qualitative structure. "
800875268399370240,2016-11-22 01:35:18,https://t.co/SfR07dhSMM,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. (arXiv:1609.04802v3 [cs.CV] … https://t.co/SfR07dhSMM,0,7," Abstract: Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method. "
800875266755141636,2016-11-22 01:35:17,https://t.co/2hDybyr3HQ,Dynamic Pricing in High-dimensions. (arXiv:1609.07574v3 [stat.ML] UPDATED) https://t.co/2hDybyr3HQ,1,4," Abstract: We study the pricing problem faced by a firm that sells a large number of products, described via a wide range of features, to customers that arrive over time. This is motivated in part by the prevalence of online marketplaces that allow for real-time pricing. We propose a dynamic policy, called Regularized Maximum Likelihood Pricing (RMLP), that obtains asymptotically optimal revenue. Our policy leverages the structure (sparsity) of a high-dimensional demand space in order to obtain a logarithmic regret compared to the clairvoyant policy that knows the parameters of the demand in advance. More specifically, the regret of our algorithm is of $O(s_0 \log T (\log d + \log T))$, where $d$ and $s_0$ correspond to the dimension of the demand space and its sparsity. Furthermore, we show that no policy can obtain regret better than $O(s_0 (\log d + \log T))$. "
800875265287221248,2016-11-22 01:35:17,https://t.co/W0mdeZHVpY,One-Class SVM with Privileged Information and its Application to Malware Detection. (arXiv:1609.08039v2 [stat.ML] … https://t.co/W0mdeZHVpY,2,4," Abstract: A number of important applied problems in engineering, finance and medicine can be formulated as a problem of anomaly detection. A classical approach to the problem is to describe a normal state using a one-class support vector machine. Then to detect anomalies we quantify a distance from a new observation to the constructed description of the normal class. In this paper we present a new approach to the one-class classification. We formulate a new problem statement and a corresponding algorithm that allow taking into account a privileged information during the training phase. We evaluate performance of the proposed approach using a synthetic dataset, as well as the publicly available Microsoft Malware Classification Challenge dataset. "
800875263676530688,2016-11-22 01:35:17,https://t.co/GWuPufJjM5,Stochastic Alternating Direction Method of Multipliers with Variance Reduction for Nonconvex Optimization. (arXiv:… https://t.co/GWuPufJjM5,0,6," Abstract: In this work, we study the stochastic alternating direction method of multipliers (ADMM) method for optimizing nonconvex problems, and propose two classes of nonconvex stochastic ADMM with variance reduction. The first class is the nonconvex stochastic variance reduced gradient ADMM (SVRG-ADMM), which uses a multi-stage strategy to progressively reduce the variance of stochastic gradients. The second is the nonconvex stochastic average gradient ADMM (SAGA-ADMM), which additionally uses the old gradients estimated in the previous iteration. Theoretically, we analyze convergence of the SVRG-ADMM and SAGA-ADMM, and prove that they enjoy the iteration complexity bound of $O(1/\epsilon)$ to reach an $\epsilon$-stationary solution. In particular, we provide a general framework to analyze convergence and iteration complexity of the nonconvex stochastic ADMM with variance reduction. In addition, we prove that the simple stochastic ADMM (S-ADMM), in which the variance of the stochastic gradients is free, is divergent under some conditions. Finally, the experimental results on some real datasets back up our theoretical results. To the best of our knowledge, this is the first study of iteration complexity of the stochastic ADMM for the noncovex problems. "
800875262032416768,2016-11-22 01:35:16,https://t.co/6P79LkqSaL,Statistics of Robust Optimization: A Generalized Empirical Likelihood Approach. (arXiv:1610.03425v2 [stat.ML] UPDA… https://t.co/6P79LkqSaL,2,8," Abstract: We study statistical inference and robust solution methods for stochastic optimization problems, focusing on giving calibrated and adaptive confidence intervals for optimal values and solutions for a range of stochastic problems. As part of this, we develop a generalized empirical likelihood framework---based on distributional uncertainty sets constructed from nonparametric $f$-divergence balls---for Hadamard differentiable functionals, and in particular, stochastic optimization problems. As consequences of this theory, we provide principled methods of choosing distributional uncertainty regions so as to provide calibrated one- and two-sided confidence intervals. We also give an asymptotic expansion for our distributionally robust formulation, showing how robustification regularizes problems by their variance. Finally, we show that optimizers of the distributionally robust formulations we study enjoy (essentially) the same consistency properties as those in classical sample average approximations. "
800512915807801344,2016-11-21 01:35:26,https://t.co/UDTTlFXmTa,"""Influence Sketching"": Finding Influential Samples In Large-Scale Regressions. (arXiv:1611.05923v1 [stat.ML]) https://t.co/UDTTlFXmTa",0,4," Abstract: There is an especially strong need in modern large-scale data analysis to prioritize samples for manual inspection. For example, the inspection could target important mislabeled samples or key vulnerabilities exploitable by an adversarial attack. In order to solve the ""needle in the haystack"" problem of which samples to inspect, we develop a new scalable version of Cook's distance, a classical statistical technique for identifying samples which unusually strongly impact the fit of a regression model (and its downstream predictions). In order to scale this technique up to very large and high-dimensional datasets, we introduce a new algorithm which we call ""influence sketching."" Influence sketching embeds random projections within the influence computation; in particular, the influence score is calculated using the randomly projected pseudo-dataset from the post-convergence General Linear Model (GLM). We validate that influence sketching can reliably and successfully discover influential samples by applying the technique to a malware detection dataset of over 2 million executable files, each represented with almost 100,000 features. For example, we find that randomly deleting approximately 10% of training samples reduces predictive accuracy only slightly from 99.47% to 99.45%, whereas deleting the same number of samples with high influence sketch scores reduces predictive accuracy all the way down to 90.24%. Moreover, we find that influential samples are especially likely to be mislabeled. In the case study, we manually inspect the most influential samples, and find that influence sketching pointed us to new, previously unidentified pieces of malware. "
800512914360647680,2016-11-21 01:35:26,https://t.co/2TpAO57AlT,Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models. (arXiv:1611.05934v1 [stat… https://t.co/2TpAO57AlT,0,28," Abstract: As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks, state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining a long short-term memory (LSTM) model with a hidden Markov model (HMM), a simpler and more transparent model. We add the HMM state probabilities to the output layer of the LSTM, and then train the HMM and LSTM either sequentially or jointly. The LSTM can make use of the information from the HMM, and fill in the gaps when the HMM is not performing well. A small hybrid model usually performs better than a standalone LSTM of the same size, especially on smaller data sets. We test the algorithms on text data and medical time series data, and find that the LSTM and HMM learn complementary information about the features in the text. "
800512912901242882,2016-11-21 01:35:25,https://t.co/rn6UBBTyTJ,Finding Alternate Features in Lasso. (arXiv:1611.05940v1 [stat.ML]) https://t.co/rn6UBBTyTJ,2,14," Abstract: We propose a method for finding alternate features missing in the Lasso optimal solution. In ordinary Lasso problem, one global optimum is obtained and the resulting features are interpreted as task-relevant features. However, this can overlook possibly relevant features not selected by the Lasso. With the proposed method, we can provide not only the Lasso optimal solution but also possible alternate features to the Lasso solution. We show that such alternate features can be computed efficiently by avoiding redundant computations. We also demonstrate how the proposed method works in the 20 newsgroup data, which shows that reasonable features are found as alternate features. "
800512911575683072,2016-11-21 01:35:25,https://t.co/MvmB9poD3f,Robust and Scalable Column/Row Sampling from Corrupted Big Data. (arXiv:1611.05977v1 [cs.LG]) https://t.co/MvmB9poD3f,0,3," Abstract: Conventional sampling techniques fall short of drawing descriptive sketches of the data when the data is grossly corrupted as such corruptions break the low rank structure required for them to perform satisfactorily. In this paper, we present new sampling algorithms which can locate the informative columns in presence of severe data corruptions. In addition, we develop new scalable randomized designs of the proposed algorithms. The proposed approach is simultaneously robust to sparse corruption and outliers and substantially outperforms the state-of-the-art robust sampling algorithms as demonstrated by experiments conducted using both real and synthetic data. "
800512909746970624,2016-11-21 01:35:25,https://t.co/PneGBvKHQR,Deriving reproducible biomarkers from multi-site resting-state data: An Autism-based example. (arXiv:1611.06066v1 … https://t.co/PneGBvKHQR,0,3," Abstract: Resting-state functional Magnetic Resonance Imaging (R-fMRI) holds the promise to reveal functional biomarkers of neuropsychiatric disorders. However, extracting such biomarkers is challenging for complex multi-faceted neuropatholo-gies, such as autism spectrum disorders. Large multi-site datasets increase sample sizes to compensate for this complexity, at the cost of uncontrolled heterogeneity. This heterogeneity raises new challenges, akin to those face in realistic diagnostic applications. Here, we demonstrate the feasibility of inter-site classification of neuropsychiatric status, with an application to the Autism Brain Imaging Data Exchange (ABIDE) database, a large (N=871) multi-site autism dataset. For this purpose, we investigate pipelines that extract the most predictive biomarkers from the data. These R-fMRI pipelines build participant-specific connectomes from functionally-defined brain areas. Connectomes are then compared across participants to learn patterns of connectivity that differentiate typical controls from individuals with autism. We predict this neuropsychiatric status for participants from the same acquisition sites or different, unseen, ones. Good choices of methods for the various steps of the pipeline lead to 67% prediction accuracy on the full ABIDE data, which is significantly better than previously reported results. We perform extensive validation on multiple subsets of the data defined by different inclusion criteria. These enables detailed analysis of the factors contributing to successful connectome-based prediction. First, prediction accuracy improves as we include more subjects, up to the maximum amount of subjects available. Second, the definition of functional brain areas is of paramount importance for biomarker discovery: brain areas extracted from large R-fMRI datasets outperform reference atlases in the classification tasks. "
800512908388134912,2016-11-21 01:35:24,https://t.co/cgz9cj9hJP,A Generalized Stochastic Variational Bayesian Hyperparameter Learning Framework for Sparse Spectrum Gaussian Proce… https://t.co/cgz9cj9hJP,1,4," Abstract: While much research effort has been dedicated to scaling up sparse Gaussian process (GP) models based on inducing variables for big data, little attention is afforded to the other less explored class of low-rank GP approximations that exploit the sparse spectral representation of a GP kernel. This paper presents such an effort to advance the state of the art of sparse spectrum GP models to achieve competitive predictive performance for massive datasets. Our generalized framework of stochastic variational Bayesian sparse spectrum GP (sVBSSGP) models addresses their shortcomings by adopting a Bayesian treatment of the spectral frequencies to avoid overfitting, modeling these frequencies jointly in its variational distribution to enable their interaction a posteriori, and exploiting local data for boosting the predictive performance. However, such structural improvements result in a variational lower bound that is intractable to be optimized. To resolve this, we exploit a variational parameterization trick to make it amenable to stochastic optimization. Interestingly, the resulting stochastic gradient has a linearly decomposable structure that can be exploited to refine our stochastic optimization method to incur constant time per iteration while preserving its property of being an unbiased estimator of the exact gradient of the variational lower bound. Empirical evaluation on real-world datasets shows that sVBSSGP outperforms state-of-the-art stochastic implementations of sparse GP models. "
800512906072911874,2016-11-21 01:35:24,https://t.co/GZJDboUTjV,Generalizing diffuse interface methods on graphs: non-smooth potentials and hypergraphs. (arXiv:1611.06094v1 [stat… https://t.co/GZJDboUTjV,0,2," Abstract: Diffuse interface methods have recently been introduced for the task of semi-supervised learning. The underlying model is well-known in materials science but was extended to graphs using a Ginzburg--Landau functional and the graph Laplacian. We here generalize the previously proposed model by a non-smooth potential function. Additionally, we show that the diffuse interface method can be used for the segmentation of data coming from hypergraphs. For this we show that the graph Laplacian in almost all cases is derived from hypergraph information. Additionally, we show that the formerly introduced hypergraph Laplacian coming from a relaxed optimization problem is well suited to be used within the diffuse interface method. We present computational experiments for graph and hypergraph Laplacians. "
800512904382423042,2016-11-21 01:35:23,https://t.co/2VZZzflzDw,Faster variational inducing input Gaussian process classification. (arXiv:1611.06132v1 [cs.LG]) https://t.co/2VZZzflzDw,0,2," Abstract: Gaussian processes (GP) provide a prior over functions and allow finding complex regularities in data. Gaussian processes are successfully used for classification/regression problems and dimensionality reduction. In this work we consider the classification problem only. The complexity of standard methods for GP-classification scales cubically with the size of the training dataset. This complexity makes them inapplicable to big data problems. Therefore, a variety of methods were introduced to overcome this limitation. In the paper we focus on methods based on so called inducing inputs. This approach is based on variational inference and proposes a particular lower bound for marginal likelihood (evidence). This bound is then maximized w.r.t. parameters of kernel function of the Gaussian process, thus fitting the model to data. The computational complexity of this method is $O(nm^2)$, where $m$ is the number of inducing inputs used by the model and is assumed to be substantially smaller than the size of the dataset $n$. Recently, a new evidence lower bound for GP-classification problem was introduced. It allows using stochastic optimization, which makes it suitable for big data problems. However, the new lower bound depends on $O(m^2)$ variational parameter, which makes optimization challenging in case of big m. In this work we develop a new approach for training inducing input GP models for classification problems. Here we use quadratic approximation of several terms in the aforementioned evidence lower bound, obtaining analytical expressions for optimal values of most of the parameters in the optimization, thus sufficiently reducing the dimension of optimization space. In our experiments we achieve as well or better results, compared to the existing method. Moreover, our method doesn't require the user to manually set the learning rate, making it more practical, than the existing method. "
800512902935511040,2016-11-21 01:35:23,https://t.co/YN1Hm7TkLm,Compacting Neural Network Classifiers via Dropout Training. (arXiv:1611.06148v1 [stat.ML]) https://t.co/YN1Hm7TkLm,0,7," Abstract: We introduce dropout compaction, a novel method for training feed-forward neural networks which realizes the performance gains of training a large model with dropout regularization, yet extracts a compact neural network for run-time efficiency. In the proposed method, we introduce a sparsity-inducing prior on the per unit dropout retention probability so that the optimizer can effectively prune hidden units during training. By changing the prior hyperparameters, we can control the size of the resulting network. We performed a systematic comparison of dropout compaction and competing methods on several real-world speech recognition tasks and found that dropout compaction achieved comparable accuracy with fewer than 50% of the hidden units, translating to a 2.5x speedup in run-time. "
800512901417148416,2016-11-21 01:35:23,https://t.co/zPz94G1bX8,Parallelizing Word2Vec in Multi-Core and Many-Core Architectures. (arXiv:1611.06172v1 [cs.DC]) https://t.co/zPz94G1bX8,0,4," Abstract: Word2vec is a widely used algorithm for extracting low-dimensional vector representations of words. State-of-the-art algorithms including those by Mikolov et al. have been parallelized for multi-core CPU architectures, but are based on vector-vector operations with ""Hogwild"" updates that are memory-bandwidth intensive and do not efficiently use computational resources. In this paper, we propose ""HogBatch"" by improving reuse of various data structures in the algorithm through the use of minibatching and negative sample sharing, hence allowing us to express the problem using matrix multiply operations. We also explore different techniques to distribute word2vec computation across nodes in a compute cluster, and demonstrate good strong scalability up to 32 nodes. The new algorithm is particularly suitable for modern multi-core/many-core architectures, especially Intel's latest Knights Landing processors, and allows us to scale up the computation near linearly across cores and nodes, and process hundreds of millions of words per second, which is the fastest word2vec implementation to the best of our knowledge. "
800512899970199552,2016-11-21 01:35:22,https://t.co/c2jKyXiK2s,Learning Interpretability for Visualizations using Adapted Cox Models through a User Experiment. (arXiv:1611.06175… https://t.co/c2jKyXiK2s,0,4," Abstract: In order to be useful, visualizations need to be interpretable. This paper uses a user-based approach to combine and assess quality measures in order to better model user preferences. Results show that cluster separability measures are outperformed by a neighborhood conservation measure, even though the former are usually considered as intuitively representative of user motives. Moreover, combining measures, as opposed to using a single measure, further improves prediction performances. "
800512898627817474,2016-11-21 01:35:22,https://t.co/Y0r13W1Ywy,Variable Computation in Recurrent Neural Networks. (arXiv:1611.06188v1 [stat.ML]) https://t.co/Y0r13W1Ywy,0,7," Abstract: Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step, which can be detrimental to both speed and model capacity. In this paper, we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step, without prior knowledge of the sequence's time structure. We show experimentally that not only is our model more computationally efficient, it also leads to better performance overall on our evaluation tasks. "
800512896786571264,2016-11-21 01:35:22,https://t.co/OmtY3SVO8M,Expert Gate: Lifelong Learning with a Network of Experts. (arXiv:1611.06194v1 [cs.CV]) https://t.co/OmtY3SVO8M,0,7," Abstract: In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process, data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision of which expert to deploy at test time. We introduce a gating autoencoder that learns a representation for the task at hand, and is used at test time to automatically forward the test sample to the relevant expert. This has the added advantage of being memory efficient as only one expert network has to be loaded into memory at any given time. Further, the autoencoders inherently capture the relatedness of one task to another, based on which the most relevant prior model to be used for training a new expert with fine-tuning or learning-without-forgetting can be selected. We evaluate our system on image classification and video prediction problems. "
800512895343804416,2016-11-21 01:35:21,https://t.co/sVByOsmo85,GaDei: On Scale-up Training As A Service For Deep Learning. (arXiv:1611.06213v1 [stat.ML]) https://t.co/sVByOsmo85,1,2," Abstract: Deep learning (DL) training-as-a-service (TaaS) is an important emerging industrial workload. The unique challenge of TaaS is that it must satisfy a wide range of customers who have no experience and resources to tune DL hyper-parameters, and meticulous tuning for each user's dataset is prohibitively expensive. Therefore, TaaS hyper-parameters must be fixed with values that are applicable to all users. IBM Watson Natural Language Classifier (NLC) service, the most popular IBM cognitive service used by thousands of enterprise-level clients around the globe, is a typical TaaS service. By evaluating the NLC workloads, we show that only the conservative hyper-parameter setup (e.g., small mini-batch size and small learning rate) can guarantee acceptable model accuracy for a wide range of customers. We further justify theoretically why such a setup guarantees better model convergence in general. Unfortunately, the small mini-batch size causes a high volume of communication traffic in a parameter-server based system. We characterize the high communication bandwidth requirement of TaaS using representative industrial deep learning workloads and demonstrate that none of the state-of-the-art scale-up or scale-out solutions can satisfy such a requirement. We then present GaDei, an optimized shared-memory based scale-up parameter server design. We prove that the designed protocol is deadlock-free and it processes each gradient exactly once. Our implementation is evaluated on both commercial benchmarks and public benchmarks to demonstrate that it significantly outperforms the state-of-the-art parameter-server based implementation while maintaining the required accuracy and our implementation reaches near the best possible runtime performance, constrained only by the hardware limitation. Furthermore, to the best of our knowledge, GaDei is the only scale-up DL system that provides fault-tolerance. "
800512893972320256,2016-11-21 01:35:21,https://t.co/vFRzRE6gae,A regularization scheme for structured output problems: an application to facial landmark detection. (arXiv:1504.0… https://t.co/vFRzRE6gae,0,5," Abstract: Facial landmark detection is an important step for many perception tasks. In this paper, we address facial landmark detection as a structured output regression problem, where we exploit the strong dependencies that lie between the facial landmarks. For this, we propose a generic multi task regression framework for structured output problems. The learning of the output structure is achieved through a regularization of the supervised task, in an unsupervised way. Therefore, the proposed framework allows the use of unlabeled input and/or label only output data. In this article, the formulation is instantiated as a deep architecture, and evaluated on two public challenging datasets: LFPW and HELEN. We show that our regularization scheme improves the generalization of deep neural networks, and accelerates their training. The use of unlabeled data is also explored, showing an additional improvement of the results. An opensource implementation of our approach is provided. "
800512891904462849,2016-11-21 01:35:20,https://t.co/AJtPDDsYDt,Communication Efficient Distributed Agnostic Boosting. (arXiv:1506.06318v2 [cs.LG] UPDATED) https://t.co/AJtPDDsYDt,0,3," Abstract: We consider the problem of learning from distributed data in the agnostic setting, i.e., in the presence of arbitrary forms of noise. Our main contribution is a general distributed boosting-based procedure for learning an arbitrary concept space, that is simultaneously noise tolerant, communication efficient, and computationally efficient. This improves significantly over prior works that were either communication efficient only in noise-free scenarios or computationally prohibitive. Empirical results on large synthetic and real-world datasets demonstrate the effectiveness and scalability of the proposed approach. "
800512890432262144,2016-11-21 01:35:20,https://t.co/4iBdD3FtqG,Neural Simpletrons - Minimalistic Directed Generative Networks for Learning with Few Labels. (arXiv:1506.08448v4 [… https://t.co/4iBdD3FtqG,0,3," Abstract: Classifiers for the semi-supervised setting often combine strong supervised models with additional learning objectives to make use of unlabeled data. This results in powerful though very complex models that are hard to train and that demand additional labels for optimal parameter tuning, which are often not given when labeled data is very sparse. We here study a minimalistic multi-layer generative neural network for semi-supervised learning in a form and setting as similar to standard discriminative networks as possible. Based on normalized Poisson mixtures, we derive compact and local learning and neural activation rules. Learning and inference in the network can be scaled using standard deep learning tools for parallelized GPU implementation. With the single objective of likelihood optimization, both labeled and unlabeled data are naturally incorporated into learning. Empirical evaluations on standard benchmarks show, that for datasets with few labels the derived minimalistic network improves on all classical deep learning approaches and is competitive with their recent variants without the need of additional labels for parameter tuning. Furthermore, we find that the studied network is the best performing monolithic (`non-hybrid') system for few labels, and that it can be applied in the limit of very few labels, where no other system has been reported to operate so far. "
800512888607752192,2016-11-21 01:35:20,https://t.co/N79qPobDNY,Testing for Differences in Gaussian Graphical Models: Applications to Brain Connectivity. (arXiv:1512.08643v2 [sta… https://t.co/N79qPobDNY,1,3," Abstract: Functional brain networks are well described and estimated from data with Gaussian Graphical Models (GGMs), e.g. using sparse inverse covariance estimators. Comparing functional connectivity of subjects in two populations calls for comparing these estimated GGMs. Our goal is to identify differences in GGMs known to have similar structure. We characterize the uncertainty of differences with confidence intervals obtained using a parametric distribution on parameters of a sparse estimator. Sparse penalties enable statistical guarantees and interpretable models even in high-dimensional and low-sample settings. Characterizing the distributions of sparse models is inherently challenging as the penalties produce a biased estimator. Recent work invokes the sparsity assumptions to effectively remove the bias from a sparse estimator such as the lasso. These distributions can be used to give confidence intervals on edges in GGMs, and by extension their differences. However, in the case of comparing GGMs, these estimators do not make use of any assumed joint structure among the GGMs. Inspired by priors from brain functional connectivity we derive the distribution of parameter differences under a joint penalty when parameters are known to be sparse in the difference. This leads us to introduce the debiased multi-task fused lasso, whose distribution can be characterized in an efficient manner. We then show how the debiased lasso and multi-task fused lasso can be used to obtain confidence intervals on edge differences in GGMs. We validate the techniques proposed on a set of synthetic examples as well as neuro-imaging dataset created for the study of autism. "
800512887143821312,2016-11-21 01:35:19,https://t.co/whQ3fASSM3,Information-theoretic limits of Bayesian network structure learning. (arXiv:1601.07460v3 [cs.LG] UPDATED) https://t.co/whQ3fASSM3,1,13," Abstract: In this paper, we study the information-theoretic limits of learning the structure of Bayesian networks (BNs), on discrete as well as continuous random variables, from a finite amount of data. We show that the minimum number of samples required by any procedure to recover the correct structure grows as $\Omega(m)$ and $\Omega(k \log m + (k^2/m))$ for non-sparse and sparse BNs respectively, where $m$ is the number of variables and $k$ is the maximum number of parents per node. We provide a simple recipe, based on an extension of the Fano's inequality, to obtain information-theoretic limits of structure recovery for any exponential family BN. We instantiate our result for specific conditional distributions in the exponential family to characterize the fundamental limits of learning various commonly used BNs, such as Conditional Probability Table (CPT) based networks, Gaussian BNs, Noisy-OR networks, and Logistic regression (LR) networks. For each of the aforementioned networks, we identify important parameters of the conditional distributions that affect the complexity of learning such models. We also show that an existing procedure called SparsityBoost, for learning binary CPT networks is not information-theoretically optimal in the number of variables. En route to obtaining our main results, we obtain tight bounds on the number of sparse and non-sparse essential-DAGs. Finally, as a byproduct, we recover the information-theoretic limits of sparse variable selection for logistic regression. "
800512885726314496,2016-11-21 01:35:19,https://t.co/7BFUbhZXul,PAC-Bayesian Theorems for Multiview Learning. (arXiv:1606.07240v2 [stat.ML] UPDATED) https://t.co/7BFUbhZXul,0,3," Abstract: We tackle the issue of multiview learning which aims to take advantages of multiple represen-tations/views of the data. In this context, many machine learning algorithms exist. However, the majority of the theoretical studies focus on learning with exactly two representations. In this paper, we propose a general PAC-Bayesian theory for multiview learning with more than two views. We focus our study to binary classification models that take the form of a majority vote. We derive PAC-Bayesian generalization bounds allowing to consider different relations between empirical and true risks by taking into account a notion of diversity of the voters and views, and that can be naturally extended to semi-supervised learning. "
800512883146784768,2016-11-21 01:35:18,https://t.co/YWMki9VF5r,One Class Splitting Criteria for Random Forests. (arXiv:1611.01971v2 [stat.ML] UPDATED) https://t.co/YWMki9VF5r,1,5," Abstract: Random Forests (RFs) are strong machine learning tools for classification and regression. However, they remain supervised algorithms, and no extension of RFs to the one-class setting has been proposed, except for techniques based on second-class sampling. This work fills this gap by proposing a natural methodology to extend standard splitting criteria to the one-class setting, structurally generalizing RFs to one-class classification. An extensive benchmark of seven state-of-the-art anomaly detection algorithms is also presented. This empirically demonstrates the relevance of our approach. "
799430254313947136,2016-11-18 01:53:19,https://t.co/9xkrCpe2Ly,Embedding Projector: Interactive Visualization and Interpretation of Embeddings. (arXiv:1611.05469v1 [stat.ML]) https://t.co/9xkrCpe2Ly,4,19," Abstract: Embeddings are ubiquitous in machine learning, appearing in recommender systems, NLP, and many other applications. Researchers and developers often need to explore the properties of a specific embedding, and one way to analyze embeddings is to visualize them. We present the Embedding Projector, a tool for interactive visualization and interpretation of embeddings. "
799430252325961729,2016-11-18 01:53:19,https://t.co/7Ga5H11Ead,Algebraic multigrid support vector machines. (arXiv:1611.05487v1 [stat.ML]) https://t.co/7Ga5H11Ead,0,5," Abstract: The support vector machine is a flexible optimization-based technique widely used for classification problems. In practice, its training part becomes computationally expensive on large-scale data sets because of such reasons as the complexity and number of iterations in parameter fitting methods, underlying optimization solvers, and nonlinearity of kernels. We introduce a fast multilevel framework for solving support vector machine models that is inspired by the algebraic multigrid. Significant improvement in the running has been achieved without any loss in the quality. The proposed technique is highly beneficial on imbalanced sets. We demonstrate computational results on publicly available and industrial data sets. "
799430250757287944,2016-11-18 01:53:19,https://t.co/Gwgm2yv6JK,Automatic Node Selection for Deep Neural Networks using Group Lasso Regularization. (arXiv:1611.05527v1 [cs.CL]) https://t.co/Gwgm2yv6JK,5,19," Abstract: We examine the effect of the Group Lasso (gLasso) regularizer in selecting the salient nodes of Deep Neural Network (DNN) hidden layers by applying a DNN-HMM hybrid speech recognizer to TED Talks speech data. We test two types of gLasso regularization, one for outgoing weight vectors and another for incoming weight vectors, as well as two sizes of DNNs: 2048 hidden layer nodes and 4096 nodes. Furthermore, we compare gLasso and L2 regularizers. Our experiment results demonstrate that our DNN training, in which the gLasso regularizer was embedded, successfully selected the hidden layer nodes that are necessary and sufficient for achieving high classification power. "
799430249310195712,2016-11-18 01:53:18,https://t.co/9ZAZUXJU3H,Stochastic Gradient Descent in Continuous Time. (arXiv:1611.05545v1 [math.PR]) https://t.co/9ZAZUXJU3H,3,18," Abstract: We consider stochastic gradient descent for continuous-time models. Traditional approaches for the statistical estimation of continuous-time models, such as batch optimization, can be impractical for large datasets where observations occur over a long period of time. Stochastic gradient descent provides a computationally efficient method for such statistical learning problems. The stochastic gradient descent algorithm performs an online parameter update in continuous time, with the parameter updates $\theta_t$ satisfying a stochastic differential equation. We prove that $\lim_{t \rightarrow \infty} \nabla \bar g(\theta_t) = 0$ where $\bar g$ is a natural objective function for the estimation of the continuous-time dynamics. The convergence proof leverages ergodicity by using an appropriate Poisson equation to help describe the evolution of the parameters for large times. Numerical analysis of the stochastic gradient descent algorithm is presented for several examples, including the Ornstein-Uhlenbeck process, Burger's stochastic partial differential equation, and reinforcement learning. "
799430246223278080,2016-11-18 01:53:18,https://t.co/VdCEyeYeBq,Boosting Variational Inference. (arXiv:1611.05559v1 [stat.ML]) https://t.co/VdCEyeYeBq,4,15," Abstract: Modern Bayesian inference typically requires some form of posterior approximation, and mean-field variational inference (MFVI) is an increasingly popular choice due to its speed. But MFVI can be inaccurate in various aspects, including an inability to capture multimodality in the posterior and underestimation of the posterior covariance. These issues arise since MFVI considers approximations to the posterior only in a family of factorized distributions. We instead consider a much more flexible approximating family consisting of all possible finite mixtures of a parametric base distribution (e.g., Gaussian). In order to efficiently find a high-quality posterior approximation within this family, we borrow ideas from gradient boosting and propose boosting variational inference (BVI). BVI iteratively improves the current approximation by mixing it with a new component from the base distribution family. We develop practical algorithms for BVI and demonstrate their performance on both real and simulated data. "
799430244725878784,2016-11-18 01:53:17,https://t.co/rDrHK34V8X,"GENESIM: genetic extraction of a single, interpretable model. (arXiv:1611.05722v1 [stat.ML]) https://t.co/rDrHK34V8X",0,3," Abstract: Models obtained by decision tree induction techniques excel in being interpretable.However, they can be prone to overfitting, which results in a low predictive performance. Ensemble techniques are able to achieve a higher accuracy. However, this comes at a cost of losing interpretability of the resulting model. This makes ensemble techniques impractical in applications where decision support, instead of decision making, is crucial. To bridge this gap, we present the GENESIM algorithm that transforms an ensemble of decision trees to a single decision tree with an enhanced predictive performance by using a genetic algorithm. We compared GENESIM to prevalent decision tree induction and ensemble techniques using twelve publicly available data sets. The results show that GENESIM achieves a better predictive performance on most of these data sets than decision tree induction techniques and a predictive performance in the same order of magnitude as the ensemble techniques. Moreover, the resulting model of GENESIM has a very low complexity, making it very interpretable, in contrast to ensemble techniques. "
799430243006062593,2016-11-18 01:53:17,https://t.co/cjPlXYect6,Unimodal Thompson Sampling for Graph-Structured Arms. (arXiv:1611.05724v1 [cs.LG]) https://t.co/cjPlXYect6,0,3," Abstract: We study, to the best of our knowledge, the first Bayesian algorithm for unimodal Multi-Armed Bandit (MAB) problems with graph structure. In this setting, each arm corresponds to a node of a graph and each edge provides a relationship, unknown to the learner, between two nodes in terms of expected reward. Furthermore, for any node of the graph there is a path leading to the unique node providing the maximum expected reward, along which the expected reward is monotonically increasing. Previous results on this setting describe the behavior of frequentist MAB algorithms. In our paper, we design a Thompson Sampling-based algorithm whose asymptotic pseudo-regret matches the lower bound for the considered setting. We show that -as it happens in a wide number of scenarios- Bayesian MAB algorithms dramatically outperform frequentist ones. In particular, we provide a thorough experimental evaluation of the performance of our and state-of-the-art algorithms as the properties of the graph vary. "
799430241399820288,2016-11-18 01:53:16,https://t.co/s3ICFDX9CH,A Multi-Modal Graph-Based Semi-Supervised Pipeline for Predicting Cancer Survival. (arXiv:1611.05751v1 [cs.LG]) https://t.co/s3ICFDX9CH,0,4," Abstract: Cancer survival prediction is an active area of research that can help prevent unnecessary therapies and improve patient's quality of life. Gene expression profiling is being widely used in cancer studies to discover informative biomarkers that aid predict different clinical endpoint prediction. We use multiple modalities of data derived from RNA deep-sequencing (RNA-seq) to predict survival of cancer patients. Despite the wealth of information available in expression profiles of cancer tumors, fulfilling the aforementioned objective remains a big challenge, for the most part, due to the paucity of data samples compared to the high dimension of the expression profiles. As such, analysis of transcriptomic data modalities calls for state-of-the-art big-data analytics techniques that can maximally use all the available data to discover the relevant information hidden within a significant amount of noise. In this paper, we propose a pipeline that predicts cancer patients' survival by exploiting the structure of the input (manifold learning) and by leveraging the unlabeled samples using Laplacian support vector machines, a graph-based semi supervised learning (GSSL) paradigm. We show that under certain circumstances, no single modality per se will result in the best accuracy and by fusing different models together via a stacked generalization strategy, we may boost the accuracy synergistically. We apply our approach to two cancer datasets and present promising results. We maintain that a similar pipeline can be used for predictive tasks where labeled samples are expensive to acquire. "
799430239541751809,2016-11-18 01:53:16,https://t.co/njkawx6HGl,Learning to reinforcement learn. (arXiv:1611.05763v1 [cs.LG]) https://t.co/njkawx6HGl,7,34," Abstract: In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience. "
799430238048579585,2016-11-18 01:53:16,https://t.co/TAbeBtVVBq,Towards the Modeling of Behavioral Trajectories of Users in Online Social Media. (arXiv:1611.05778v1 [cs.CY]) https://t.co/TAbeBtVVBq,0,5," Abstract: In this paper, we introduce a methodology that allows to model behavioral trajectories of users in online social media. First, we illustrate how to leverage the probabilistic framework provided by Hidden Markov Models (HMMs) to represent users by embedding the temporal sequences of actions they performed online. We then derive a model-based distance between trained HMMs, and we use spectral clustering to find homogeneous clusters of users showing similar behavioral trajectories. To provide platform-agnostic results, we apply the proposed approach to two different online social media --- i.e. Facebook and YouTube. We conclude discussing merits and limitations of our approach as well as future and promising research directions. "
799430236081270784,2016-11-18 01:53:15,https://t.co/4iYML7gLDm,Gap Safe screening rules for sparsity enforcing penalties. (arXiv:1611.05780v1 [stat.ML]) https://t.co/4iYML7gLDm,0,2," Abstract: In high dimensional regression context, sparsity enforcing penalties have proved useful to regularize the data-fitting term. A recently introduced technique called \emph{screening rules}, leverage the expected sparsity of the solutions by ignoring some variables in the optimization, hence leading to solver speed-ups. When the procedure is guaranteed not to discard features wrongly the rules are said to be \emph{safe}. We propose a unifying framework that can cope with generalized linear models regularized with standard sparsity enforcing penalties such as $\ell_1$ or $\ell_1/\ell_2$ norms. Our technique allows to discard safely more variables than previously considered safe rules, particularly for low regularization parameters. Our proposed Gap Safe rules (so called because they rely on duality gap computation) can cope with any iterative solver but is particularly well suited to block coordinate descent for many standard learning tasks: Lasso, Sparse-Group Lasso, multi-task Lasso, binary and multinomial logistic regression, etc. For all such tasks and on all tested datasets, we report significant speed-ups compared to previously proposed safe rules. "
799430234542116864,2016-11-18 01:53:15,https://t.co/r38T2zGxEZ,Nothing Else Matters: Model-Agnostic Explanations By Identifying Prediction Invariance. (arXiv:1611.05817v1 [stat.… https://t.co/r38T2zGxEZ,1,4," Abstract: At the core of interpretable machine learning is the question of whether humans are able to make accurate predictions about a model's behavior. Assumed in this question are three properties of the interpretable output: coverage, precision, and effort. Coverage refers to how often humans think they can predict the model's behavior, precision to how accurate humans are in those predictions, and effort is either the up-front effort required in interpreting the model, or the effort required to make predictions about a model's behavior. In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that produces high-precision rule-based explanations for which the coverage boundaries are very clear. We compare aLIME to linear LIME with simulated experiments, and demonstrate the flexibility of aLIME with qualitative examples from a variety of domains and tasks. "
799430233116045312,2016-11-18 01:53:14,https://t.co/ho0JvGOeOz,Implementing a Bayes Filter in a Neural Circuit: The Case of Unknown Stimulus Dynamics. (arXiv:1512.07839v3 [cs.LG… https://t.co/ho0JvGOeOz,1,7," Abstract: In order to interact intelligently with objects in the world, animals must first transform neural population responses into estimates of the dynamic, unknown stimuli which caused them. The Bayesian solution to this problem is known as a Bayes filter, which applies Bayes' rule to combine population responses with the predictions of an internal model. In this paper we present a method for learning to approximate a Bayes filter when the stimulus dynamics are unknown. To do this we use the inferential properties of probabilistic population codes to compute Bayes' rule, and train a neural network to compute approximate predictions by the method of maximum likelihood. In particular, we perform stochastic gradient descent on the negative log-likelihood with a novel approximation of the gradient. We demonstrate our methods on a finite-state, a linear, and a nonlinear filtering problem, and show how the hidden layer of the neural network develops tuning curves which are consistent with findings in experimental neuroscience. "
799430231543005187,2016-11-18 01:53:14,https://t.co/tGEw8h3rw9,Fast Methods for Recovering Sparse Parameters in Linear Low Rank Models. (arXiv:1606.08009v2 [cs.LG] UPDATED) https://t.co/tGEw8h3rw9,0,6," Abstract: In this paper, we investigate the recovery of a sparse weight vector (parameters vector) from a set of noisy linear combinations. However, only partial information about the matrix representing the linear combinations is available. Assuming a low-rank structure for the matrix, one natural solution would be to first apply a matrix completion on the data, and then to solve the resulting compressed sensing problem. In big data applications such as massive MIMO and medical data, the matrix completion step imposes a huge computational burden. Here, we propose to reduce the computational cost of the completion task by ignoring the columns corresponding to zero elements in the sparse vector. To this end, we employ a technique to initially approximate the support of the sparse vector. We further propose to unify the partial matrix completion and sparse vector recovery into an augmented four-step problem. Simulation results reveal that the augmented approach achieves the best performance, while both proposed methods outperform the natural two-step technique with substantially less computational requirements. "
799430229936644096,2016-11-18 01:53:14,https://t.co/aeuVhFjct3,Learning in Quantum Control: High-Dimensional Global Optimization for Noisy Quantum Dynamics. (arXiv:1607.03428v2 … https://t.co/aeuVhFjct3,0,6," Abstract: Quantum control is valuable for various quantum technologies such as high-fidelity gates for universal quantum computing, adaptive quantum-enhanced metrology, and ultra-cold atom manipulation. Although supervised machine learning and reinforcement learning are widely used for optimizing control parameters in classical systems, quantum control for parameter optimization is mainly pursued via gradient-based greedy algorithms. Although the quantum fitness landscape is often compatible with greedy algorithms, sometimes greedy algorithms yield poor results, especially for large-dimensional quantum systems. We employ differential evolution algorithms to circumvent the stagnation problem of non-convex optimization. We improve quantum control fidelity for noisy system by averaging over the objective function. To reduce computational cost, we introduce heuristics for early termination of runs and for adaptive selection of search subspaces. Our implementation is massively parallel and vectorized to reduce run time even further. We demonstrate our methods with two examples, namely quantum phase estimation and quantum gate design, for which we achieve superior fidelity and scalability than obtained using greedy algorithms. "
799430227344687104,2016-11-18 01:53:13,https://t.co/oVhU90TaMV,Inherent Trade-Offs in the Fair Determination of Risk Scores. (arXiv:1609.05807v2 [cs.LG] UPDATED) https://t.co/oVhU90TaMV,0,2," Abstract: Recent discussion in the public sphere about algorithmic classification has involved tension between competing notions of what it means for a probabilistic classification to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously. Moreover, even satisfying all three conditions approximately requires that the data lie in an approximate version of one of the constrained special cases identified by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them. "
799430225163522048,2016-11-18 01:53:13,https://t.co/bJdWyjIk0l,Sparsity-driven weighted ensemble classifier. (arXiv:1610.00270v2 [stat.ML] UPDATED) https://t.co/bJdWyjIk0l,0,6," Abstract: In this letter, a novel weighted ensemble classifier is proposed that improves classification accuracy and minimizes the number of classifiers. Ensemble weight finding problem is modeled as a cost function with following terms: (a) a data fidelity term aiming to decrease misclassification rate, (b) a sparsity term aiming to decrease the number of classifiers, and (c) a non-negativity constraint on the weights of the classifiers. The proposed cost function is a non-convex and hard to solve; thus, convex relaxation techniques and novel approximations are employed to obtain a numerically efficient solution. The proposed method achieves better or similar performance compared to state-of-the art classifier ensemble methods, while using lower number of classifiers. "
799430223678861312,2016-11-18 01:53:12,https://t.co/4SwyaN9538,Tensorial Mixture Models. (arXiv:1610.04167v2 [cs.LG] UPDATED) https://t.co/4SwyaN9538,3,9," Abstract: We introduce a generative model, we call Tensorial Mixture Models (TMMs) based on mixtures of basic component distributions over local structures (e.g. patches in an image) where the dependencies between the local-structures are represented by a ""priors tensor"" holding the prior probabilities of assigning a component distribution to each local-structure. In their general form, TMMs are intractable as the prior tensor is typically of exponential size. However, when the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the TMM into a Convolutional Arithmetic Circuit (ConvAC). A ConvAC corresponds to a shallow (single hidden layer) network when the priors tensor is decomposed by a CP (sum of rank-1) approach and corresponds to a deep network when the decomposition follows the Hierarchical Tucker (HT) model. The ConvAC representation of a TMM possesses several attractive properties. First, the inference is tractable and is implemented by a forward pass through a deep network. Second, the architectural design of the model follows the deep networks community design, i.e., the structure of TMMs is determined by just two easily understood factors: size of pooling windows and number of channels. Finally, we demonstrate the effectiveness of our model when tackling the problem of classification with missing data, leveraging TMMs unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution. "
799430221996789760,2016-11-18 01:53:12,https://t.co/CdiJMnXwU9,Gated End-to-End Memory Networks. (arXiv:1610.04211v2 [cs.CL] UPDATED) https://t.co/CdiJMnXwU9,0,8," Abstract: Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks, MemN2N, have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architecture, GMemN2N. From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art. "
799430220621246464,2016-11-18 01:53:11,https://t.co/gL2ONvIkht,Universal adversarial perturbations. (arXiv:1610.08401v2 [cs.CV] UPDATED) https://t.co/gL2ONvIkht,2,10," Abstract: Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images. "
799067441984892928,2016-11-17 01:51:38,https://t.co/dQ4JN4uimp,Localized Coulomb Descriptors for the Gaussian Approximation Potential. (arXiv:1611.05126v1 [stat.ML]) https://t.co/dQ4JN4uimp,0,3," Abstract: We introduce a novel class of localized atomic environment representation functions, based upon the global Coulomb matrix, which have dimensionality either quadratic or linear in the number of atoms in the local atomic environment. By combining these functions with the Gaussian approximation potential approach, we present LC-GAP, a new system for generating atomic potentials through machine learning (ML). Tests on the QM7, QM7b and GDB9 biomolecular datasets demonstrate that potentials created with LC-GAP can successfully predict atomization energies for molecules larger than those used for training to chemical accuracy, and can (in the case of QM7b) also be used to predict a range of other atomic properties with accuracy in line with the recent literature. "
799067440172896256,2016-11-17 01:51:38,https://t.co/wbrc92qVWJ,Machine Learning Approach for Skill Evaluation in Robotic-Assisted Surgery. (arXiv:1611.05136v1 [cs.AI]) https://t.co/wbrc92qVWJ,0,4," Abstract: Evaluating surgeon skill has predominantly been a subjective task. Development of objective methods for surgical skill assessment are of increased interest. Recently, with technological advances such as robotic-assisted minimally invasive surgery (RMIS), new opportunities for objective and automated assessment frameworks have arisen. In this paper, we applied machine learning methods to automatically evaluate performance of the surgeon in RMIS. Six important movement features were used in the evaluation including completion time, path length, depth perception, speed, smoothness and curvature. Different classification methods applied to discriminate expert and novice surgeons. We test our method on real surgical data for suturing task and compare the classification result with the ground truth data (obtained by manual labeling). The experimental results show that the proposed framework can classify surgical skill level with relatively high accuracy of 85.7%. This study demonstrates the ability of machine learning methods to automatically classify expert and novice surgeons using movement features for different RMIS tasks. Due to the simplicity and generalizability of the introduced classification method, it is easy to implement in existing trainers. "
799067438545518597,2016-11-17 01:51:37,https://t.co/PzZ8c0qCvQ,A Semi-Markov Switching Linear Gaussian Model for Censored Physiological Data. (arXiv:1611.05146v1 [cs.LG]) https://t.co/PzZ8c0qCvQ,1,2," Abstract: Critically ill patients in regular wards are vulnerable to unanticipated clinical dete- rioration which requires timely transfer to the intensive care unit (ICU). To allow for risk scoring and patient monitoring in such a setting, we develop a novel Semi- Markov Switching Linear Gaussian Model (SSLGM) for the inpatients' physiol- ogy. The model captures the patients' latent clinical states and their corresponding observable lab tests and vital signs. We present an efficient unsupervised learn- ing algorithm that capitalizes on the informatively censored data in the electronic health records (EHR) to learn the parameters of the SSLGM; the learned model is then used to assess the new inpatients' risk for clinical deterioration in an online fashion, allowing for timely ICU admission. Experiments conducted on a het- erogeneous cohort of 6,094 patients admitted to a large academic medical center show that the proposed model significantly outperforms the currently deployed risk scores such as Rothman index, MEWS, SOFA and APACHE. "
799067436855214083,2016-11-17 01:51:37,https://t.co/DkQZKhHWw2,Net-Trim: A Layer-wise Convex Pruning of Deep Neural Networks. (arXiv:1611.05162v1 [cs.LG]) https://t.co/DkQZKhHWw2,1,4," Abstract: Model reduction is a highly desirable process for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models, overfitting and model redundancy negatively affects the prediction accuracy and model variance. Net-Trim is a layer-wise convex framework to prune (sparsify) deep neural networks. The method is applicable to neural networks operating with the rectified linear unit (ReLU) as the nonlinear activation. The basic idea is to retrain the network layer by layer keeping the layer inputs and outputs close to the originally trained model, while seeking a sparse transform matrix. We present both the parallel and cascade versions of the algorithm. While the former enjoys computational distributability, the latter is capable of achieving simpler models. In both cases, we mathematically show a consistency between the retrained model and the initial trained network. We also derive the general sufficient conditions for the recovery of a sparse transform matrix. In the case of standard Gaussian training samples of dimension $N$ being fed to a layer, and $s$ being the maximum number of nonzero terms across all columns of the transform matrix, we show that $\mathcal{O}(s\log N)$ samples are enough to accurately learn the layer model. "
799067434506403843,2016-11-17 01:51:36,https://t.co/uXYjFzW6tn,Graph Learning from Data under Structural and Laplacian Constraints. (arXiv:1611.05181v1 [cs.LG]) https://t.co/uXYjFzW6tn,0,9," Abstract: Graphs are fundamental mathematical structures used in various fields to represent data, signals and processes. In this paper, we propose a novel framework for learning/estimating graphs from data. The proposed framework includes (i) formulation of various graph learning problems, (ii) their probabilistic interpretations and (iii) efficient algorithms to solve them. We specifically focus on graph learning problems where the goal is to estimate a graph Laplacian matrix from some observed data under given structural constraints (e.g., graph connectivity and sparsity). Our experimental results demonstrate that the proposed algorithms outperform the current state-of-the-art methods in terms of graph learning performance. "
799067432983871496,2016-11-17 01:51:36,https://t.co/LyqeJiF5W9,Deep Variational Inference Without Pixel-Wise Reconstruction. (arXiv:1611.05209v1 [stat.ML]) https://t.co/LyqeJiF5W9,0,9," Abstract: Variational autoencoders (VAEs), that are built upon deep neural networks have emerged as popular generative models in computer vision. Most of the work towards improving variational autoencoders has focused mainly on making the approximations to the posterior flexible and accurate, leading to tremendous progress. However, there have been limited efforts to replace pixel-wise reconstruction, which have known shortcomings. In this work, we use real-valued non-volume preserving transformations (real NVP) to exactly compute the conditional likelihood of the data given the latent distribution. We show that a simple VAE with this form of reconstruction is competitive with complicated VAE structures, on image modeling tasks. As part of our model, we develop powerful conditional coupling layers that enable real NVP to learn with fewer intermediate layers. "
799067431444561922,2016-11-17 01:51:36,https://t.co/o4bZ4iliOs,Neural Style Representations and the Large-Scale Classification of Artistic Style. (arXiv:1611.05368v1 [cs.CV]) https://t.co/o4bZ4iliOs,2,5," Abstract: The artistic style of a painting is a subtle aesthetic judgment used by art historians for grouping and classifying artwork. The recently introduced `neural-style' algorithm substantially succeeds in merging the perceived artistic style of one image or set of images with the perceived content of another. In light of this and other recent developments in image analysis via convolutional neural networks, we investigate the effectiveness of a `neural-style' representation for classifying the artistic style of paintings. "
799067430073016324,2016-11-17 01:51:35,https://t.co/CC34YxBGXC,Spectral Convolution Networks. (arXiv:1611.05378v1 [cs.LG]) https://t.co/CC34YxBGXC,2,9," Abstract: Previous research has shown that computation of convolution in the frequency domain provides a significant speedup versus traditional convolution network implementations. However, this performance increase comes at the expense of repeatedly computing the transform and its inverse in order to apply other network operations such as activation, pooling, and dropout. We show, mathematically, how convolution and activation can both be implemented in the frequency domain using either the Fourier or Laplace transformation. The main contributions are a description of spectral activation under the Fourier transform and a further description of an efficient algorithm for computing both convolution and activation under the Laplace transform. By computing both the convolution and activation functions in the frequency domain, we can reduce the number of transforms required, as well as reducing overall complexity. Our description of a spectral activation function, together with existing spectral analogs of other network functions may then be used to compose a fully spectral implementation of a convolution network. "
799067428563075072,2016-11-17 01:51:35,https://t.co/Ul9IIbLTMo,ZipML: An End-to-end Bitwise Framework for Dense Generalized Linear Models. (arXiv:1611.05402v1 [cs.LG]) https://t.co/Ul9IIbLTMo,0,3," Abstract: We present ZipML, the first framework for training dense generalized linear models using end-to-end low-precision representation--in ZipML, all movements of data, including those for input samples, model, and gradients, are represented using as little as two bits per component. Within our framework, we have successfully compressed, separately, the input data by 16x, gradient by 16x, and model by 16x while still getting the same training result. Even for the most challenging datasets, we find that robust convergence can be ensured using only an end-to-end 8-bit representation or a 6-bit representation if only samples are quantized. Our work builds on previous research on using low-precision representations for gradient and model in the context of stochastic gradient descent. Our main technical contribution is a new set of techniques which allow the training samples to be processed with low precision, without affecting the convergence of the algorithm. In turn, this leads to a system where all data items move in a quantized, low precision format. In particular, we first establish that randomized rounding, while sufficient when quantizing the model and the gradients, is biased when quantizing samples, and thus leads to a different training result. We propose two new data representations which converge to the same solution as in the original data representation both in theory and empirically and require as little as 2-bits per component. As a result, if the original data is stored as 32-bit floats, we decrease the bandwidth footprint for each training iteration by up to 16x. Our results hold for models such as linear regression and least squares SVM. ZipML raises interesting theoretical questions related to the robustness of SGD to approximate data, model, and gradient representations. We conclude this working paper by a description of ongoing work extending these preliminary results. "
799067425299898369,2016-11-17 01:51:34,https://t.co/4TkQDvqVkI,A Semidefinite Program for Structured Blockmodels. (arXiv:1611.05407v1 [math.ST]) https://t.co/4TkQDvqVkI,0,4," Abstract: Semidefinite programs have recently been developed for the problem of community detection, which may be viewed as a special case of the stochastic blockmodel. Here, we develop a semidefinite program that can be tailored to other instances of the blockmodel, such as non-assortative networks and overlapping communities. We establish label recovery in sparse settings, with conditions that are analogous to recent results for community detection. In settings where the data is not generated by a blockmodel, we give an oracle inequality that bounds excess risk relative to the best blockmodel approximation. Simulations are presented for community detection, for overlapping communities, and for latent space models. "
799067423878037510,2016-11-17 01:51:34,https://t.co/RpDCytakBh,ProjE: Embedding Projection for Knowledge Graph Completion. (arXiv:1611.05425v1 [cs.AI]) https://t.co/RpDCytakBh,0,4," Abstract: With the large volume of new information created every day, determining the validity of information in a knowledge graph and filling in its missing parts are crucial tasks for many researchers and practitioners. To address this challenge, a number of knowledge graph completion methods have been developed using low-dimensional graph embeddings. Although researchers continue to improve these models using an increasingly complex feature space, we show that simple changes in the architecture of the underlying model can outperform state-of-the-art models without the need for complex feature engineering. In this work, we present a shared variable neural network model called ProjE that fills-in missing information in a knowledge graph by learning joint embeddings of the knowledge graph's entities and edges, and through subtle, but important, changes to the standard loss function. In doing so, ProjE has a parameter size that is smaller than 11 out of 15 existing methods while performing $37\%$ better than the current-best method on standard datasets. We also show, via a new fact checking task, that ProjE is capable of accurately determining the veracity of many declarative statements. "
799067421789274118,2016-11-17 01:51:33,https://t.co/llHc0OFjgH,ROS Regression: Integrating Regularization and Optimal Scaling Regression. (arXiv:1611.05433v1 [stat.ML]) https://t.co/llHc0OFjgH,0,2," Abstract: In this paper we combine two important extensions of ordinary least squares regression: regularization and optimal scaling. Optimal scaling (sometimes also called optimal scoring) has originally been developed for categorical data, and the process finds quantifications for the categories that are optimal for the regression model in the sense that they maximize the multiple correlation. Although the optimal scaling method was developed initially for variables with a limited number of categories, optimal transformations of continuous variables are a special case. We will consider a variety of transformation types; typically we use step functions for categorical variables, and smooth (spline) functions for continuous variables. Both types of functions can be restricted to be monotonic, preserving the ordinal information in the data. In addition to optimal scaling, three regularization methods will be considered: Ridge regression, the Lasso, and the Elastic Net. The resulting method will be called ROS Regression (Regularized Optimal Scaling Regression. We will show that the basic OS algorithm provides straightforward and efficient estimation of the regularized regression coefficients, automatically gives the Group Lasso and Blockwise Sparse Regression, and extends them with monotonicity properties. We will show that Optimal Scaling linearizes nonlinear relationships between predictors and outcome, and improves upon the condition of the predictor correlation matrix, increasing (on average) the conditional independence of the predictors. Alternative options for regularization of either regression coefficients or category quantifications are mentioned. Extended examples are provided. Keywords: Categorical Data, Optimal Scaling, Conditional Independence, Step Functions, Splines, Monotonic Transformations, Regularization, Lasso, Elastic Net, Group Lasso, Blockwise Sparse Regression. "
799067419104919555,2016-11-17 01:51:33,https://t.co/lGSNkcM80v,Asynchronous Decentralized 20 Questions for Adaptive Search. (arXiv:1511.03144v2 [cs.MA] UPDATED) https://t.co/lGSNkcM80v,0,2," Abstract: This paper considers the problem of adaptively searching for an unknown target using multiple agents connected through a time-varying network topology. Agents are equipped with sensors capable of fast information processing, and we propose a decentralized collaborative algorithm for controlling their search given noisy observations. Specifically, we propose decentralized extensions of the adaptive query-based search strategy that combines elements from the 20 questions approach and social learning. Under standard assumptions on the time-varying network dynamics, we prove convergence to correct consensus on the value of the parameter as the number of iterations go to infinity. The convergence analysis takes a novel approach using martingale-based techniques combined with spectral graph theory. Our results establish that stability and consistency can be maintained even with one-way updating and randomized pairwise averaging, thus providing a scalable low complexity method with performance guarantees. We illustrate the effectiveness of our algorithm for random network topologies. "
799067416164712450,2016-11-17 01:51:32,https://t.co/NFbMrFs7Ga,A Proximal Stochastic Quasi-Newton Algorithm. (arXiv:1602.00223v2 [cs.LG] UPDATED) https://t.co/NFbMrFs7Ga,0,4," Abstract: In this paper, we discuss the problem of minimizing the sum of two convex functions: a smooth function plus a non-smooth function. Further, the smooth part can be expressed by the average of a large number of smooth component functions, and the non-smooth part is equipped with a simple proximal mapping. We propose a proximal stochastic second-order method, which is efficient and scalable. It incorporates the Hessian in the smooth part of the function and exploits multistage scheme to reduce the variance of the stochastic gradient. We prove that our method can achieve linear rate of convergence. "
799067412914208768,2016-11-17 01:51:31,https://t.co/X5T8C0ecCT,High Dimensional Inference with Random Maximum A-Posteriori Perturbations. (arXiv:1602.03571v2 [cs.LG] UPDATED) https://t.co/X5T8C0ecCT,3,7," Abstract: This paper presents a new approach, called perturb-max, for high-dimensional statistical inference that is based on applying random perturbations followed by optimization. This framework injects randomness to maximum a-posteriori (MAP) predictors by randomly perturbing the potential function for the input. A classic result from extreme value statistics asserts that perturb-max operations generate unbiased samples from the Gibbs distribution using high-dimensional perturbations. Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive. However, when the perturbations are of low dimension, sampling the perturb-max prediction is as efficient as MAP optimization. This paper shows that the expected value of perturb-max inference with low dimensional perturbations can be used sequentially to generate unbiased samples from the Gibbs distribution. Furthermore the expected value of the maximal perturbations is a natural bound on the entropy of such perturb-max models. A measure concentration result for perturb-max values shows that the deviation of their sampled average from its expectation decays exponentially in the number of samples, allowing effective approximation of the expectation. "
799067408967286784,2016-11-17 01:51:30,https://t.co/5ULitOxokk,Harnessing Deep Neural Networks with Logic Rules. (arXiv:1603.06318v4 [cs.LG] UPDATED) https://t.co/5ULitOxokk,0,6," Abstract: Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems. "
799062537933307904,2016-11-17 01:32:09,https://t.co/bBJ6KgQ2z5,Iterative Hard Thresholding for Model Selection in Genome-Wide Association Studies. (arXiv:1608.01398v2 [stat.ML] … https://t.co/bBJ6KgQ2z5,0,6," Abstract: A genome-wide association study (GWAS) correlates marker variation with trait variation in a sample of individuals. Each study subject is genotyped at a multitude of SNPs (single nucleotide polymorphisms) spanning the genome. Here we assume that subjects are unrelated and collected at random and that trait values are normally distributed or transformed to normality. Over the past decade, researchers have been remarkably successful in applying GWAS analysis to hundreds of traits. The massive amount of data produced in these studies present unique computational challenges. Penalized regression with LASSO or MCP penalties is capable of selecting a handful of associated SNPs from millions of potential SNPs. Unfortunately, model selection can be corrupted by false positives and false negatives, obscuring the genetic underpinning of a trait. This paper introduces the iterative hard thresholding (IHT) algorithm to the GWAS analysis of continuous traits. Our parallel implementation of IHT accommodates SNP genotype compression and exploits multiple CPU cores and graphics processing units (GPUs). This allows statistical geneticists to leverage commodity desktop computers in GWAS analysis and to avoid supercomputing. We evaluate IHT performance on both simulated and real GWAS data and conclude that it reduces false positive and false negative rates while remaining competitive in computational time with penalized regression. Source code is freely available at this https URL "
799062535592886273,2016-11-17 01:32:09,https://t.co/yGvNYKFUhv,Chi-squared Amplification: Identifying Hidden Hubs. (arXiv:1608.03643v2 [cs.LG] UPDATED) https://t.co/yGvNYKFUhv,1,2," Abstract: We consider the following general hidden hubs model: an $n \times n$ random matrix $A$ with a subset $S$ of $k$ special rows (hubs): entries in rows outside $S$ are generated from the probability distribution $p_0 \sim N(0,\sigma_0^2)$; for each row in $S$, some $k$ of its entries are generated from $p_1 \sim N(0,\sigma_1^2)$, $\sigma_1>\sigma_0$, and the rest of the entries from $p_0$. The problem is to identify the high-degree hubs efficiently. This model includes and significantly generalizes the planted Gaussian Submatrix Model, where the special entries are all in a $k \times k$ submatrix. There are two well-known barriers: if $k\geq c\sqrt{n\ln n}$, just the row sums are sufficient to find $S$ in the general model. For the submatrix problem, this can be improved by a $\sqrt{\ln n}$ factor to $k \ge c\sqrt{n}$ by spectral methods or combinatorial methods. In the variant with $p_0=\pm 1$ (with probability $1/2$ each) and $p_1\equiv 1$, neither barrier has been broken. We give a polynomial-time algorithm to identify all the hidden hubs with high probability for $k \ge n^{0.5-\delta}$ for some $\delta >0$, when $\sigma_1^2>2\sigma_0^2$. The algorithm extends to the setting where planted entries might have different variances each at least as large as $\sigma_1^2$. We also show a nearly matching lower bound: for $\sigma_1^2 \le 2\sigma_0^2$, there is no polynomial-time Statistical Query algorithm for distinguishing between a matrix whose entries are all from $N(0,\sigma_0^2)$ and a matrix with $k=n^{0.5-\delta}$ hidden hubs for any $\delta >0$. The lower bound as well as the algorithm are related to whether the chi-squared distance of the two distributions diverges. At the critical value $\sigma_1^2=2\sigma_0^2$, we show that the general hidden hubs problem can be solved for $k\geq c\sqrt n(\ln n)^{1/4}$, improving on the naive row sum-based method. "
799062533671940096,2016-11-17 01:32:08,https://t.co/Td0zSt8qcK,Classifier comparison using precision. (arXiv:1609.09471v2 [cs.LG] UPDATED) https://t.co/Td0zSt8qcK,0,6," Abstract: New proposed models are often compared to state-of-the-art using statistical significance testing. Literature is scarce for classifier comparison using metrics other than accuracy. We present a survey of statistical methods that can be used for classifier comparison using precision, accounting for inter-precision correlation arising from use of same dataset. Comparisons are made using per-class precision and methods presented to test global null hypothesis of an overall model comparison. Comparisons are extended to multiple multi-class classifiers and to models using cross validation or its variants. Partial Bayesian update to precision is introduced when population prevalence of a class is known. Applications to compare deep architectures are studied. "
799062532497543168,2016-11-17 01:32:08,https://t.co/TAGckYIOsl,Theoretical Analysis of Domain Adaptation with Optimal Transport. (arXiv:1610.04420v2 [stat.ML] UPDATED) https://t.co/TAGckYIOsl,0,5," Abstract: Domain adaptation (DA) is an important and emerging field of machine learning that tackles the problem occurring when the distributions of training (source domain) and test (target domain) data are similar but different. Current theoretical results show that the efficiency of DA algorithms depends on their capacity of minimizing the divergence between source and target probability distributions. In this paper, we provide a theoretical study on the advantages that concepts borrowed from optimal transportation theory can bring to DA. In particular, we show that the Wasserstein metric can be used as a divergence measure between distributions to obtain generalization guarantees for three different learning settings: (i) classic DA with unsupervised target data (ii) DA combining source and target labeled data, (iii) multiple source DA. Based on the obtained results, we provide some insights showing when this analysis can be tighter than other existing frameworks. We also show that in the context of multiple source DA, the problem of estimating of the best joint hypothesis between source and target labeling functions can be reformulated using a Wasserstein distance-based loss function. We think that these results open the door to novel ideas and directions for DA. "
799062531008512003,2016-11-17 01:32:07,https://t.co/iloIHXwZ67,Learning to Reason With Adaptive Computation. (arXiv:1610.07647v2 [cs.CL] CROSS LISTED) https://t.co/iloIHXwZ67,0,7," Abstract: Multi-hop inference is necessary for machine learning systems to successfully solve tasks such as Recognising Textual Entailment and Machine Reading. In this work, we demonstrate the effectiveness of adaptive computation for learning the number of inference steps required for examples of different complexity and that learning the correct number of inference steps is difficult. We introduce the first model involving Adaptive Computation Time which provides a small performance benefit on top of a similar model without an adaptive component as well as enabling considerable insight into the reasoning process of the model. "
798701214087184384,2016-11-16 01:36:23,https://t.co/J5o9hB9qwu,Errors-in-variables models with dependent measurements. (arXiv:1611.04701v1 [stat.ML]) https://t.co/J5o9hB9qwu,1,5," Abstract: Suppose that we observe $y \in \mathbb{R}^n$ and $X \in \mathbb{R}^{n \times m}$ in the following errors-in-variables model: \begin{eqnarray*} y & = & X_0 \beta^* + \epsilon X & = & X_0 + W \end{eqnarray*} where $X_0$ is a $n \times m$ design matrix with independent subgaussian row vectors, $\epsilon \in \R^n$ is a noise vector and $W$ is a mean zero $n \times m$ random noise matrix with independent subgaussian column vectors, independent of $X_0$ and $\epsilon$. This model is significantly different from those analyzed in the literature in the sense that we allow the measurement error for each covariate to be a dependent vector across its $n$ observations. Such error structures appear in the science literature when modeling the trial-to-trial fluctuations in response strength shared across a set of neurons. We establish consistency in estimating $\beta^*$ and obtain the rates of convergence in the $\ell_q$ norm, where $q = 1, 2$ for the Lasso-type estimator, and for $q \in [1, 2]$ for a Dantzig-type conic programming estimator. We show error bounds which approach that of the regular Lasso and the Dantzig selector in case the errors in $W$ are tending to 0. We analyze the convergence rates of the gradient descent methods for solving the nonconvex programs and show that the composite gradient descent algorithm is guaranteed to converge at a geometric rate to a neighborhood of the global minimizers: the size of the neighborhood is bounded by the statistical error in the $\ell_2$ norm. Our analysis reveals interesting connections between compuational and statistical efficiency and the concentration of measure phenomenon in random matrix theory. We provide simulation evidence illuminating the theoretical predictions. "
798701212703059970,2016-11-16 01:36:22,https://t.co/EhvgRe0QqI,Recoverability of Joint Distribution from Missing Data. (arXiv:1611.04709v1 [stat.ML]) https://t.co/EhvgRe0QqI,0,5," Abstract: A probabilistic query may not be estimable from observed data corrupted by missing values if the data are not missing at random (MAR). It is therefore of theoretical interest and practical importance to determine in principle whether a probabilistic query is estimable from missing data or not when the data are not MAR. We present an algorithm that systematically determines whether the joint probability is estimable from observed data with missing values, assuming that the data-generation model is represented as a Bayesian network containing unobserved latent variables that not only encodes the dependencies among the variables but also explicitly portrays the mechanisms responsible for the missingness process. The result significantly advances the existing work. "
798701211310587904,2016-11-16 01:36:22,https://t.co/guSiaG6Bzp,Improved Particle Filters for Vehicle Localisation. (arXiv:1611.04790v1 [stat.ML]) https://t.co/guSiaG6Bzp,0,5," Abstract: The ability to track a moving vehicle is of crucial importance in numerous applications. The task has often been approached by the importance sampling technique of particle filters due to its ability to model non-linear and non-Gaussian dynamics, of which a vehicle travelling on a road network is a good example. Particle filters perform poorly when observations are highly informative. In this paper, we address this problem by proposing particle filters that sample around the most recent observation. The proposal leads to an order of magnitude improvement in accuracy and efficiency over conventional particle filters, especially when observations are infrequent but low-noise. "
798701209926467585,2016-11-16 01:36:22,https://t.co/1qQPtRJNOm,Multilinear Low-Rank Tensors on Graphs &amp; Applications. (arXiv:1611.04835v1 [cs.CV]) https://t.co/1qQPtRJNOm,0,5," Abstract: We propose a new framework for the analysis of low-rank tensors which lies at the intersection of spectral graph theory and signal processing. As a first step, we present a new graph based low-rank decomposition which approximates the classical low-rank SVD for matrices and multi-linear SVD for tensors. Then, building on this novel decomposition we construct a general class of convex optimization problems for approximately solving low-rank tensor inverse problems, such as tensor Robust PCA. The whole framework is named as 'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis shows: 1) MLRTG stands on the notion of approximate stationarity of multi-dimensional signals on graphs and 2) the approximation error depends on the eigen gaps of the graphs. We demonstrate applications for a wide variety of 4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance videos and hyperspectral images. Generalization of the tensor concepts to non-euclidean domain, orders of magnitude speed-up, low-memory requirement and significantly enhanced performance at low SNR are the key aspects of our framework. "
798701208701665280,2016-11-16 01:36:21,https://t.co/nfSLE4Prld,Unsupervised Learning with Truncated Gaussian Graphical Models. (arXiv:1611.04920v1 [stat.ML]) https://t.co/nfSLE4Prld,0,5," Abstract: Gaussian graphical models (GGMs) are widely used for statistical modeling, because of ease of inference and the ubiquitous use of the normal distribution in practical approximations. However, they are also known for their limited modeling abilities, due to the Gaussian assumption. In this paper, we introduce a novel variant of GGMs, which relaxes the Gaussian restriction and yet admits efficient inference. Specifically, we impose a bipartite structure on the GGM and govern the hidden variables by truncated normal distributions. The nonlinearity of the model is revealed by its connection to rectified linear unit (ReLU) neural networks. Meanwhile, thanks to the bipartite structure and appealing properties of truncated normals, we are able to train the models efficiently using contrastive divergence. We consider three output constructs, accounting for real-valued, binary and count data. We further extend the model to deep constructions and show that deep models can be used for unsupervised pre-training of rectifier neural networks. Extensive experimental results are provided to validate the proposed models and demonstrate their superiority over competing models. "
798701207007195136,2016-11-16 01:36:21,https://t.co/UXzViWFLbS,Iterative Orthogonal Feature Projection for Diagnosing Bias in Black-Box Models. (arXiv:1611.04967v1 [cs.LG]) https://t.co/UXzViWFLbS,4,3," Abstract: Predictive models are increasingly deployed for the purpose of determining access to services such as credit, insurance, and employment. Despite potential gains in productivity and efficiency, several potential problems have yet to be addressed, particularly the potential for unintentional discrimination. We present an iterative procedure, based on orthogonal projection of input attributes, for enabling interpretability of black-box predictive models. Through our iterative procedure, one can quantify the relative dependence of a black-box model on its input attributes.The relative significance of the inputs to a predictive model can then be used to assess the fairness (or discriminatory extent) of such a model. "
798701205472079876,2016-11-16 01:36:21,https://t.co/Klz5BWBfEz,Oracle Complexity of Second-Order Methods for Finite-Sum Problems. (arXiv:1611.04982v1 [math.OC]) https://t.co/Klz5BWBfEz,0,2," Abstract: Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in \emph{second-order} methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer -- perhaps surprisingly -- is negative, at least in terms of worst-case guarantees. However, we also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result. "
798701204205436928,2016-11-16 01:36:20,https://t.co/PJbWNuQf87,Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm. (arXiv:1611.05010v1 [stat.ML]) https://t.co/PJbWNuQf87,0,3," Abstract: In topic modeling, many algorithms that guarantee identifiability of the topics have been developed under the premise that there exist anchor words -- i.e., words that only appear (with positive probability) in one topic. Follow-up work has resorted to three or higher-order statistics of the data corpus to relax the anchor word assumption. Reliable estimates of higher-order statistics are hard to obtain, however, and the identification of topics under those models hinges on uncorrelatedness of the topics, which can be unrealistic. This paper revisits topic modeling based on second-order moments, and proposes an anchor-free topic mining framework. The proposed approach guarantees the identification of the topics under a much milder condition compared to the anchor-word assumption, thereby exhibiting much better robustness in practice. The associated algorithm only involves one eigen-decomposition and a few small linear programs. This makes it easy to implement and scale up to very large problem instances. Experiments using the TDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free approach exhibits very favorable performance (measured using coherence, similarity count, and clustering accuracy metrics) compared to the prior art. "
798701201617469444,2016-11-16 01:36:20,https://t.co/pGVaserLwP,Spectrum Estimation from Samples. (arXiv:1602.00061v3 [cs.LG] UPDATED) https://t.co/pGVaserLwP,0,2," Abstract: We consider the problem of approximating the set of eigenvalues of the covariance matrix of a multivariate distribution (equivalently, the problem of approximating the ""population spectrum""), given access to samples drawn from the distribution. The eigenvalues of the covariance of a distribution contain basic information about the distribution, including the presence or lack of structure in the distribution, the effective dimensionality of the distribution, and the applicability of higher-level machine learning and multivariate statistical tools. We consider this fundamental recovery problem in the regime where the number of samples is comparable, or even sublinear in the dimensionality of the distribution in question. First, we propose a theoretically optimal and computationally efficient algorithm for recovering the moments of the eigenvalues of the population covariance matrix. We then leverage this accurate moment recovery, via a Wasserstein distance argument, to show that the vector of eigenvalues can be accurately recovered. We provide finite--sample bounds on the expected error of the recovered eigenvalues, which imply that our estimator is asymptotically consistent as the dimensionality of the distribution and sample size tend towards infinity, even in the sublinear sample regime where the ratio of the sample size to the dimensionality tends to zero. In addition to our theoretical results, we show that our approach performs well in practice for a broad range of distributions and sample sizes. "
798701199918792704,2016-11-16 01:36:19,https://t.co/fhkQnfLa9Y,Multi-Information Source Optimization. (arXiv:1603.00389v2 [stat.ML] UPDATED) https://t.co/fhkQnfLa9Y,0,3," Abstract: We consider Bayesian optimization of an expensive-to-evaluate black-box objective function, where we also have access to cheaper approximations of the objective. In general, such approximations arise in applications such as reinforcement learning, engineering, and the natural sciences, and are subject to an inherent, unknown bias. This model discrepancy is caused by an inadequate internal model that deviates from reality and can vary over the domain, making the utilization of these approximations a non-trivial task. We present a novel algorithm that provides a rigorous mathematical treatment of the uncertainties arising from model discrepancies and noisy observations. Its optimization decisions rely on a value of information analysis that extends the Knowledge Gradient factor to the setting of multiple information sources that vary in cost: each sampling decision maximizes the predicted benefit per unit cost. We conduct an experimental evaluation that demonstrates that the method consistently outperforms other state-of-the-art techniques: it finds designs of considerably higher objective value and additionally inflicts less cost in the exploration process. "
798701198559866882,2016-11-16 01:36:19,https://t.co/UhZdNfnFf5,Extending Detection with Forensic Information. (arXiv:1603.09638v3 [cs.CR] UPDATED) https://t.co/UhZdNfnFf5,0,3," Abstract: For over a quarter century, security-relevant detection has been driven by models learned from input features collected from real or simulated environments. An artifact (e.g., network event, potential malware sample, suspicious email) is deemed malicious or non-malicious based on its similarity to the learned model at run-time. However, the training of the models has been historically limited to only those features available at run time. In this paper, we consider an alternate model construction approach that trains models using forensic ""privileged"" information--features available at training time but not at runtime--to improve the accuracy and resilience of detection systems. In particular, we adapt and extend recent advances in knowledge transfer, model influence, and distillation to enable the use of forensic data in a range of security domains. Our empirical study shows that privileged information increases detection precision and recall over a system with no privileged information: we observe up to 7.7% relative decrease in detection error for fast-flux bot detection, 8.6% for malware traffic detection, 7.3% for malware classification, and 16.9% for face recognition. We explore the limitations and applications of different privileged information techniques in detection systems. Such techniques open the door to systems that can integrate forensic data directly into detection models, and therein provide a means to fully exploit the information available about past security-relevant events. "
798701196903116804,2016-11-16 01:36:19,https://t.co/lr9FBc1Xsj,Density estimation using Real NVP. (arXiv:1605.08803v2 [cs.LG] UPDATED) https://t.co/lr9FBc1Xsj,2,11," Abstract: Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations. "
798701195648991232,2016-11-16 01:36:18,https://t.co/AJlUlMCzPQ,Safe Exploration in Finite Markov Decision Processes with Gaussian Processes. (arXiv:1606.04753v2 [cs.LG] UPDATED) https://t.co/AJlUlMCzPQ,0,3," Abstract: In classical reinforcement learning, when exploring an environment, agents accept arbitrary short term loss for long term gain. This is infeasible for safety critical applications, such as robotics, where even a single unsafe action may cause system failure. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an, a priori unknown, safety constraint that depends on states and actions. We aim to explore the MDP under this constraint, assuming that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm for this task and prove that it is able to completely explore the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover. "
798701193388232704,2016-11-16 01:36:18,https://t.co/tQ4E0WvTCl,Practical Secure Aggregation for Federated Learning on User-Held Data. (arXiv:1611.04482v1 [cs.CR] CROSS LISTED) https://t.co/tQ4E0WvTCl,1,3," Abstract: Secure Aggregation protocols allow a collection of mutually distrust parties, each holding a private value, to collaboratively compute the sum of those values without revealing the values themselves. We consider training a deep neural network in the Federated Learning model, using distributed stochastic gradient descent across user-held training data on mobile devices, wherein Secure Aggregation protects each user's model gradient. We design a novel, communication-efficient Secure Aggregation protocol for high-dimensional data that tolerates up to 1/3 users failing to complete the protocol. For 16-bit input values, our protocol offers 1.73x communication expansion for $2^{10}$ users and $2^{20}$-dimensional vectors, and 1.98x expansion for $2^{14}$ users and $2^{24}$ dimensional vectors. "
798337595507933184,2016-11-15 01:31:29,https://t.co/gWDzKXJdpR,Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM. (arXiv:1611.03879v1 [stat.ML]) https://t.co/gWDzKXJdpR,1,17," Abstract: Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is used as the building block in energy-based deep generative models. Due to numerical stability and quantifiability of the likelihood, RBM is commonly used with Bernoulli units. Here, we consider an alternative member of exponential family RBM with leaky rectified linear units -- called leaky RBM. We first study the joint and marginal distributions of leaky RBM under different leakiness, which provides us important insights by connecting the leaky RBM model and truncated Gaussian distributions. The connection leads us to a simple yet efficient method for sampling from this model, where the basic idea is to anneal the leakiness rather than the energy; -- i.e., start from a fully Gaussian/Linear unit and gradually decrease the leakiness over iterations. This serves as an alternative to the annealing of the temperature parameter and enables numerical estimation of the likelihood that are more efficient and more accurate than the commonly used annealed importance sampling (AIS). We further demonstrate that the proposed sampling algorithm enjoys faster mixing property than contrastive divergence algorithm, which benefits the training without any additional computational cost. "
798337593939107840,2016-11-15 01:31:29,https://t.co/q8YKl5Sz4Y,Reinforcement Learning of Contextual MDPs using Spectral Methods. (arXiv:1611.03907v1 [cs.AI]) https://t.co/q8YKl5Sz4Y,2,10," Abstract: We propose a new reinforcement learning (RL) algorithm for contextual Markov decision processes (CMDP) using spectral methods. CMDPs are structured MDPs where the dynamics and rewards depend on a smaller number of hidden states or contexts. If the mapping between the hidden and observed states is known a priori, then standard RL algorithms such as UCRL are guaranteed to attain low regret. Is it possible to achieve regret of the same order even when the mapping is unknown? We provide an affirmative answer in this paper. We exploit spectral methods to learn the mapping from hidden to observed states with guaranteed confidence bounds, and incorporate it into the UCRL-based framework to obtain order-optimal regret. "
798337591544254465,2016-11-15 01:31:28,https://t.co/By1vyMcNfW,"Kernel regression, minimax rates and effective dimensionality: beyond the regular case. (arXiv:1611.03979v1 [stat.… https://t.co/By1vyMcNfW",0,4," Abstract: We investigate if kernel regularization methods can achieve minimax convergence rates over a source condition regularity assumption for the target function. These questions have been considered in past literature, but only under specific assumptions about the decay, typically polynomial, of the spectrum of the the kernel mapping covariance operator. In the perspective of distribution-free results, we investigate this issue under much weaker assumption on the eigenvalue decay, allowing for more complex behavior that can reflect different structure of the data at different scales. "
798337589816107008,2016-11-15 01:31:28,https://t.co/GPLw7ql5Rr,Dual Teaching: A Practical Semi-supervised Wrapper Method. (arXiv:1611.03981v1 [cs.LG]) https://t.co/GPLw7ql5Rr,1,7," Abstract: Semi-supervised wrapper methods are concerned with building effective supervised classifiers from partially labeled data. Though previous works have succeeded in some fields, it is still difficult to apply semi-supervised wrapper methods to practice because the assumptions those methods rely on tend to be unrealistic in practice. For practical use, this paper proposes a novel semi-supervised wrapper method, Dual Teaching, whose assumptions are easy to set up. Dual Teaching adopts two external classifiers to estimate the false positives and false negatives of the base learner. Only if the recall of every external classifier is greater than zero and the sum of the precision is greater than one, Dual Teaching will train a base learner from partially labeled data as effectively as the fully-labeled-data-trained classifier. The effectiveness of Dual Teaching is proved in both theory and practice. "
798337588146962434,2016-11-15 01:31:28,https://t.co/sjXqR4gi1t,Riemannian Tensor Completion with Side Information. (arXiv:1611.03993v1 [stat.ML]) https://t.co/sjXqR4gi1t,1,6," Abstract: Riemannian optimization methods have shown to be both fast and accurate in recovering a large-scale tensor from its incomplete observation. However, in almost all recent Riemannian tensor completion methods, only low rank constraint is considered. Another important fact, side information or features, remains far from exploiting within the Riemannian optimization framework. In this paper, we explicitly incorporate the side information into a Riemannian minimization model. Specifically, a feature-embedded objective is designed to substantially reduce the sample complexity. For such a Riemannian optimization, a particular metric can be constructed based on the curvature of the objective, which leads to a novel Riemannian conjugate gradient descent solver. Numerical experiments suggest that our solver is more efficient than the state-of-the-art when a highly accurate solution is required. "
798337586636918785,2016-11-15 01:31:27,https://t.co/TaRQp86oOe,Entropic Causal Inference. (arXiv:1611.04035v1 [cs.AI]) https://t.co/TaRQp86oOe,0,5," Abstract: We consider the problem of identifying the causal direction between two discrete random variables using observational data. Unlike previous work, we keep the most general functional model but make an assumption on the unobserved exogenous variable: Inspired by Occam's razor, we assume that the exogenous variable is simple in the true causal direction. We quantify simplicity using R\'enyi entropy. Our main result is that, under natural assumptions, if the exogenous variable has low $H_0$ entropy (cardinality) in the true direction, it must have high $H_0$ entropy in the wrong direction. We establish several algorithmic hardness results about estimating the minimum entropy exogenous variable. We show that the problem of finding the exogenous variable with minimum entropy is equivalent to the problem of finding minimum joint entropy given $n$ marginal distributions, also known as minimum entropy coupling problem. We propose an efficient greedy algorithm for the minimum entropy coupling problem, that for $n=2$ provably finds a local optimum. This gives a greedy algorithm for finding the exogenous variable with minimum $H_1$ (Shannon Entropy). Our greedy entropy-based causal inference algorithm has similar performance to the state of the art additive noise models in real datasets. One advantage of our approach is that we make no use of the values of random variables but only their distributions. Our method can therefore be used for causal inference for both ordinal and also categorical data, unlike additive noise models. "
798337585084911616,2016-11-15 01:31:27,https://t.co/NAdPTO2nOj,GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution. (arXiv:1611.04051v1 [stat.ML]) https://t.co/NAdPTO2nOj,2,22," Abstract: Generative Adversarial Networks (GAN) have limitations when the goal is to generate sequences of discrete elements. The reason for this is that samples from a distribution on discrete objects such as the multinomial are not differentiable with respect to the distribution parameters. This problem can be avoided by using the Gumbel-softmax distribution, which is a continuous approximation to a multinomial distribution parameterized in terms of the softmax function. In this work, we evaluate the performance of GANs based on recurrent neural networks with Gumbel-softmax output distributions in the task of generating sequences of discrete elements. "
798337583453536256,2016-11-15 01:31:26,https://t.co/OaV57uXmiv,Error Metrics for Learning Reliable Manifolds from Streaming Data. (arXiv:1611.04067v1 [stat.ML]) https://t.co/OaV57uXmiv,0,2," Abstract: Spectral dimensionality reduction is frequently used to identify low-dimensional structure in high-dimensional data. However, learning manifolds, especially from the streaming data, is computationally and memory expensive. In this paper, we argue that a stable manifold can be learned using only a fraction of the stream, and the remaining stream can be mapped to the manifold in a significantly less costly manner. Identifying the transition point at which the manifold is stable is the key step. We present error metrics that allow us to identify the transition point for a given stream by quantitatively assessing the quality of a manifold learned using Isomap. We further propose an efficient mapping algorithm, called S-Isomap, that can be used to map new samples onto the stable manifold. We describe experiments on a variety of data sets that show that the proposed approach is computationally efficient without sacrificing accuracy. "
798337581763219456,2016-11-15 01:31:26,https://t.co/nDjki8IGKp,Low-rank and Adaptive Sparse Signal (LASSI) Models for Highly Accelerated Dynamic Imaging. (arXiv:1611.04069v1 [st… https://t.co/nDjki8IGKp,0,2," Abstract: Sparsity-based approaches have been popular in many applications in image processing and imaging. Compressed sensing exploits the sparsity of images in a transform domain or dictionary to improve image recovery from undersampled measurements. In the context of inverse problems in dynamic imaging, recent research has demonstrated the promise of sparsity and low-rank techniques. For example, the patches of the underlying data are modeled as sparse in an adaptive dictionary domain, and the resulting image and dictionary estimation from undersampled measurements is called dictionary-blind compressed sensing, or the dynamic image sequence is modeled as a sum of low-rank and sparse (in some transform domain) components (L+S model) that are estimated from limited measurements. In this work, we investigate a data-adaptive extension of the L+S model, dubbed LASSI, where the temporal image sequence is decomposed into a low-rank component and a component whose spatiotemporal (3D) patches are sparse in some adaptive dictionary domain. We investigate various formulations and efficient methods for jointly estimating the underlying dynamic signal components and the spatiotemporal dictionary from limited measurements. We also obtain efficient sparsity penalized dictionary-blind compressed sensing methods as special cases of our LASSI algorithms. Our numerical experiments demonstrate the promising performance of LASSI schemes for dynamic magnetic resonance image reconstruction from limited k-t space data compared to recent methods such as k-t SLR and L+S, and compared to the proposed dictionary-blind compressed sensing method. "
798337580316164097,2016-11-15 01:31:26,https://t.co/CFc3YxAAjZ,Accelerated Variance Reduced Block Coordinate Descent. (arXiv:1611.04149v1 [stat.ML]) https://t.co/CFc3YxAAjZ,0,5," Abstract: Algorithms with fast convergence, small number of data access, and low per-iteration complexity are particularly favorable in the big data era, due to the demand for obtaining \emph{highly accurate solutions} to problems with \emph{a large number of samples} in \emph{ultra-high} dimensional space. Existing algorithms lack at least one of these qualities, and thus are inefficient in handling such big data challenge. In this paper, we propose a method enjoying all these merits with an accelerated convergence rate $O(\frac{1}{k^2})$. Empirical studies on large scale datasets with more than one million features are conducted to show the effectiveness of our methods in practice. "
798337578789457920,2016-11-15 01:31:25,https://t.co/E73gg1TD7r,Realistic risk-mitigating recommendations via inverse classification. (arXiv:1611.04199v1 [cs.LG]) https://t.co/E73gg1TD7r,1,4," Abstract: Inverse classification, the process of making meaningful perturbations to a test point such that it is more likely to have a desired classification, has previously been addressed using data from a single static point in time. Such an approach yields inflated probability estimates, stemming from an implicitly made assumption that recommendations are implemented instantaneously. We propose using longitudinal data to alleviate such issues in two ways. First, we use past outcome probabilities as features in the present. Use of such past probabilities ties historical behavior to the present, allowing for more information to be taken into account when making initial probability estimates and subsequently performing inverse classification. Secondly, following inverse classification application, optimized instances' unchangeable features (e.g.,~age) are updated using values from the next longitudinal time period. Optimized test instance probabilities are then reassessed. Updating the unchangeable features in this manner reflects the notion that improvements in outcome likelihood, which result from following the inverse classification recommendations, do not materialize instantaneously. As our experiments demonstrate, more realistic estimates of probability can be obtained by factoring in such considerations. "
798337576792977409,2016-11-15 01:31:25,https://t.co/StJo2fN72C,Joint mean and covariance estimation with unreplicated matrix-variate data. (arXiv:1611.04208v1 [stat.ML]) https://t.co/StJo2fN72C,0,5," Abstract: It has been proposed that complex populations, such as those that arise in genomics studies, may exhibit dependencies among the observations as well as among the variables. This gives rise to the challenging problem of analyzing unreplicated high-dimensional data with unknown mean and dependence structures. Matrix-variate approaches that impose various forms of (inverse) covariance sparsity allow flexible dependence structures to be estimated, but cannot directly be applied when the mean and dependences are estimated jointly. We present a practical method utilizing penalized (inverse) covariance estimation and generalized least squares to address this challenge. The advantages of our approaches are: (i) dependence graphs and covariance structures can be estimated in the presence of unknown mean structure, (ii) the mean structure becomes more efficiently estimated when accounting for the dependence structure(s); and (iii) inferences about the mean parameters become correctly calibrated. We establish consistency and obtain rates of convergence for estimating the mean parameters and covariance matrices. We use simulation studies and analysis of genomic data from a twin study of ulcerative colitis to illustrate the statistical convergence and the performance of the procedures in practical settings. Furthermore, several lines of evidence show that the test statistics for differential gene expression produced by our method are correctly calibrated. "
798337575421407232,2016-11-15 01:31:25,https://t.co/L9Y6nxeQXC,Preference Completion from Partial Rankings. (arXiv:1611.04218v1 [stat.ML]) https://t.co/L9Y6nxeQXC,0,4," Abstract: We propose a novel and efficient algorithm for the collaborative preference completion problem, which involves jointly estimating individualized rankings for a set of entities over a shared set of items, based on a limited number of observed affinity values. Our approach exploits the observation that while preferences are often recorded as numerical scores, the predictive quantity of interest is the underlying rankings. Thus, attempts to closely match the recorded scores may lead to overfitting and impair generalization performance. Instead, we propose an estimator that directly fits the underlying preference order, combined with nuclear norm constraints to encourage low--rank parameters. Besides (approximate) correctness of the ranking order, the proposed estimator makes no generative assumption on the numerical scores of the observations. One consequence is that the proposed estimator can fit any consistent partial ranking over a subset of the items represented as a directed acyclic graph (DAG), generalizing standard techniques that can only fit preference scores. Despite this generality, for supervision representing total or blockwise total orders, the computational complexity of our algorithm is within a $\log$ factor of the standard algorithms for nuclear norm regularization based estimates for matrix completion. We further show promising empirical results for a novel and challenging application of collaboratively ranking of the associations between brain--regions and cognitive neuroscience terms. "
798337573995315200,2016-11-15 01:31:24,https://t.co/HhnkeT6Llq,Identity Matters in Deep Learning. (arXiv:1611.04231v1 [cs.LG]) https://t.co/HhnkeT6Llq,1,9," Abstract: An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as \emph{batch normalization}, but was also key to the immense success of \emph{residual networks}. In this work, we put the principle of \emph{identity parameterization} on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for linear feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size. Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks. "
798337572699275265,2016-11-15 01:31:24,https://t.co/B3CXDo8uw8,Statistical mechanics of the inverse Ising problem and the optimal objective function. (arXiv:1611.04281v1 [cond-m… https://t.co/B3CXDo8uw8,1,8," Abstract: The inverse Ising problem seeks to reconstruct the parameters of an Ising Hamiltonian on the basis of spin configurations sampled from the Boltzmann measure. Recently, strategies to solve the inverse Ising problem based on convex optimisation have proven to be very successful. These approaches maximize particular objective functions with respect to the model parameters. Examples are the pseudolikelihood method and interaction screening. In this paper, we establish a link between approaches to the inverse Ising problem based on convex optimisation and the statistical physics of disordered systems. We characterize the performance of an arbitrary objective function and calculate the objective function which optimally reconstructs the model parameters. We evaluate the optimal objective function within a replica-symmetric ansatz and compare the results of the optimal objective function with other reconstruction methods. Apart from giving a theoretical underpinning to solving the inverse Ising problem by convex optimisation, the optimal method outperforms state-of-the art methods, in some regimes by a significant margin. "
798337571185160193,2016-11-15 01:31:24,https://t.co/DvhYrvfOtQ,Generate Models and Model Criticism via Optimized Maximum Mean Discrepancy. (arXiv:1611.04488v1 [stat.ML]) https://t.co/DvhYrvfOtQ,0,3," Abstract: We propose a method to optimize the representation and distinguishability of samples from two probability distributions, by maximizing the estimated power of a statistical test based on the maximum mean discrepancy (MMD). This optimized MMD is applied to the setting of unsupervised learning by generative adversarial networks (GAN), in which a model attempts to generate realistic samples, and a discriminator attempts to tell these apart from data samples. In this context, the MMD may be used in two roles: first, as a discriminator, either directly on the samples, or on features of the samples. Second, the MMD can be used to evaluate the performance of a generative model, by testing the model's samples against a reference data set. In the latter role, the optimized MMD is particularly helpful, as it gives an interpretable indication of how the model and data distributions differ, even in cases where individual model samples are not easily distinguished either by eye or by classifier. "
798337569423388672,2016-11-15 01:31:23,https://t.co/yf3wUq5fOy,Post Training in Deep Learning with Last Kernel. (arXiv:1611.04499v1 [stat.ML]) https://t.co/yf3wUq5fOy,0,5," Abstract: One of the main challenges of deep learning methods is the choice of an appropriate training strategy. In particular, additional steps, such as unsupervised pre-training, have been shown to greatly improve the performances of deep structures. In this paper, we introduce a new training step,the post-training, which takes place after the training and where only specific layers are trained. In particular, we focus on the particular case -- named Last Kernel -- where only the last layer is trained. This step aims to find the optimal use of data representation learned during the other phases of the training. We show that Last Kernel can be effortlessly added to most learning strategies, is computationally inexpensive, does not cause overfitting and often produces significant improvement. Additionally, we show that with commonly used losses and activation functions, Last Kernel solves a convex closed optimization problems, offering rapid convergence -- or even closed-form solutions. "
798337567167041536,2016-11-15 01:31:23,https://t.co/U4M7C5mEI5,Deep Learning with Sets and Point Clouds. (arXiv:1611.04500v1 [stat.ML]) https://t.co/U4M7C5mEI5,1,7," Abstract: We study a simple notion of structural invariance that readily suggests a parameter-sharing scheme in deep neural networks. In particular, we define structure as a collection of relations, and derive graph convolution and recurrent neural networks as special cases. We study composition of basic structures in defining models that are invariant to more complex ""product"" structures such as graph of graphs, sets of images or sequence of sets. For demonstration, our experimental results are focused on the setting where the discrete structure of interest is a set. We present results on several novel and non-trivial problems on sets, including semi-supervised learning using clustering information, point-cloud classification and set outlier detection. "
798337565766152192,2016-11-15 01:31:22,https://t.co/BMzhnNsZAN,Benchmarking Quantum Hardware for Training of Fully Visible Boltzmann Machines. (arXiv:1611.04528v1 [quant-ph]) https://t.co/BMzhnNsZAN,0,6," Abstract: Quantum annealing (QA) is a hardware-based heuristic optimization and sampling method applicable to discrete undirected graphical models. While similar to simulated annealing, QA relies on quantum, rather than thermal, effects to explore complex search spaces. For many classes of problems, QA is known to offer computational advantages over simulated annealing. Here we report on the ability of recent QA hardware to accelerate training of fully visible Boltzmann machines. We characterize the sampling distribution of QA hardware, and show that in many cases, the quantum distributions differ significantly from classical Boltzmann distributions. In spite of this difference, training (which seeks to match data and model statistics) using standard classical gradient updates is still effective. We investigate the use of QA for seeding Markov chains as an alternative to contrastive divergence (CD) and persistent contrastive divergence (PCD). Using $k=50$ Gibbs steps, we show that for problems with high-energy barriers between modes, QA-based seeds can improve upon chains with CD and PCD initializations. For these hard problems, QA gradient estimates are more accurate, and allow for faster learning. Furthermore, and interestingly, even the case of raw QA samples (that is, $k=0$) achieved similar improvements. We argue that this relates to the fact that we are training a quantum rather than classical Boltzmann distribution in this case. The learned parameters give rise to hardware QA distributions closely approximating classical Boltzmann distributions that are hard to train with CD/PCD. "
798337564369240064,2016-11-15 01:31:22,https://t.co/Kfvay9SAf3,Splitting matters: how monotone transformation of predictor variables may improve the predictions of decision tree… https://t.co/Kfvay9SAf3,0,3," Abstract: It is widely believed that the prediction accuracy of decision tree models is invariant under any strictly monotone transformation of the individual predictor variables. However, this statement may be false when predicting new observations with values that were not seen in the training-set and are close to the location of the split point of a tree rule. The sensitivity of the prediction error to the split point interpolation is high when the split point of the tree is estimated based on very few observations, reaching 9% misclassification error when only 10 observations are used for constructing a split, and shrinking to 1% when relying on 100 observations. This study compares the performance of alternative methods for split point interpolation and concludes that the best choice is taking the mid-point between the two closest points to the split point of the tree. Furthermore, if the (continuous) distribution of the predictor variable is known, then using its probability integral for transforming the variable (""quantile transformation"") will reduce the model's interpolation error by up to about a half on average. Accordingly, this study provides guidelines for both developers and users of decision tree models (including bagging and random forest). "
798337562758828032,2016-11-15 01:31:22,https://t.co/1SfT3mvAsN,On the Complexity of Best Arm Identification in Multi-Armed Bandit Models. (arXiv:1407.4443v2 [stat.ML] UPDATED) https://t.co/1SfT3mvAsN,1,4," Abstract: The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the m best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when m is larger than 1 under general assumptions. In the specific case of two armed-bandits, we derive refined lower bounds in both the fixed-confidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed-budget setting may be smaller than the complexity of the fixed-confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest : a deviation lemma for self-normalized sums (Lemma 19) and a novel change of measure inequality for bandit models (Lemma 1). "
798337560992829440,2016-11-15 01:31:21,https://t.co/omGtSb3ZJ2,Optimal Transport vs. Fisher-Rao distance between Copulas for Clustering Multivariate Time Series. (arXiv:1604.086… https://t.co/omGtSb3ZJ2,0,8," Abstract: We present a methodology for clustering N objects which are described by multivariate time series, i.e. several sequences of real-valued random variables. This clustering methodology leverages copulas which are distributions encoding the dependence structure between several random variables. To take fully into account the dependence information while clustering, we need a distance between copulas. In this work, we compare renowned distances between distributions: the Fisher-Rao geodesic distance, related divergences and optimal transport, and discuss their advantages and disadvantages. Applications of such methodology can be found in the clustering of financial assets. A tutorial, experiments and implementation for reproducible research can be found at www.datagrapple.com/Tech. "
798337559558582272,2016-11-15 01:31:21,https://t.co/b0MCDHNGvx,Interaction Screening: Efficient and Sample-Optimal Learning of Ising Models. (arXiv:1605.07252v2 [cs.LG] UPDATED) https://t.co/b0MCDHNGvx,0,3," Abstract: We consider the problem of learning the underlying graph of an unknown Ising model on p spins from a collection of i.i.d. samples generated from the model. We suggest a new estimator that is computationally efficient and requires a number of samples that is near-optimal with respect to previously established information-theoretic lower-bound. Our statistical estimator has a physical interpretation in terms of ""interaction screening"". The estimator is consistent and is efficiently implemented using convex optimization. We prove that with appropriate regularization, the estimator recovers the underlying graph using a number of samples that is logarithmic in the system size p and exponential in the maximum coupling-intensity and maximum node-degree. "
798337557469757440,2016-11-15 01:31:20,https://t.co/mvghLQjm6e,Sequential Neural Models with Stochastic Layers. (arXiv:1605.07571v2 [stat.ML] UPDATED) https://t.co/mvghLQjm6e,0,7," Abstract: How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling. "
798337555192127488,2016-11-15 01:31:20,https://t.co/ogJkmZrer3,Spectral Methods for Correlated Topic Models. (arXiv:1605.09080v5 [cs.LG] UPDATED) https://t.co/ogJkmZrer3,0,5," Abstract: In this paper, we propose guaranteed spectral methods for learning a broad range of topic models, which generalize the popular Latent Dirichlet Allocation (LDA). We overcome the limitation of LDA to incorporate arbitrary topic correlations, by assuming that the hidden topic proportions are drawn from a flexible class of Normalized Infinitely Divisible (NID) distributions. NID distributions are generated through the process of normalizing a family of independent Infinitely Divisible (ID) random variables. The Dirichlet distribution is a special case obtained by normalizing a set of Gamma random variables. We prove that this flexible topic model class can be learned via spectral methods using only moments up to the third order, with (low order) polynomial sample and computational complexity. The proof is based on a key new technique derived here that allows us to diagonalize the moments of the NID distribution through an efficient procedure that requires evaluating only univariate integrals, despite the fact that we are handling high dimensional multivariate moments. In order to assess the performance of our proposed Latent NID topic model, we use two real datasets of articles collected from New York Times and Pubmed. Our experiments yield improved perplexity on both datasets compared with the baseline. "
798337552814055427,2016-11-15 01:31:19,https://t.co/EsFJuGt8v5,Ancestral Causal Inference. (arXiv:1606.07035v2 [cs.LG] UPDATED) https://t.co/EsFJuGt8v5,0,8," Abstract: Constraint-based causal discovery from limited data is a notoriously difficult challenge due to the many borderline independence test decisions. Several approaches to improve the reliability of the predictions by exploiting redundancy in the independence information have been proposed recently. Though promising, existing approaches can still be greatly improved in terms of accuracy and scalability. We present a novel method that reduces the combinatorial explosion of the search space by using a more coarse-grained representation of causal information, drastically reducing computation time. Additionally, we propose a method to score causal predictions based on their confidence. Crucially, our implementation also allows one to easily combine observational and interventional data and to incorporate various types of available background knowledge. We prove soundness and asymptotic consistency of our method and demonstrate that it can outperform the state-of-the-art on synthetic data, achieving a speedup of several orders of magnitude. We illustrate its practical feasibility by applying it on a challenging protein data set. "
798337551526404096,2016-11-15 01:31:19,https://t.co/wQIW3N3y6I,Learning Unitary Operators with Help From u(n). (arXiv:1607.04903v2 [stat.ML] UPDATED) https://t.co/wQIW3N3y6I,0,5," Abstract: A major challenge in the training of recurrent neural networks is the so-called vanishing or exploding gradient problem. The use of a norm-preserving transition operator can address this issue, but parametrization is challenging. In this work we focus on unitary operators and describe a parametrization using the Lie algebra $\mathfrak{u}(n)$ associated with the Lie group $U(n)$ of $n \times n$ unitary matrices. The exponential map provides a correspondence between these spaces, and allows us to define a unitary matrix using $n^2$ real coefficients relative to a basis of the Lie algebra. The parametrization is closed under additive updates of these coefficients, and thus provides a simple space in which to do gradient descent. We demonstrate the effectiveness of this parametrization on the problem of learning arbitrary unitary operators, comparing to several baselines and outperforming a recently-proposed lower-dimensional parametrization. This suggests a route to generalising a recently-proposed unitary recurrent neural network to arbitrary unitary matrices, solving a problem the well-known long short-term memory network was invented to address, but with a simplified and elegant network architecture. "
798337550029062144,2016-11-15 01:31:18,https://t.co/CJZ7wo55ON,A nonparametric sequential test for online randomized experiments. (arXiv:1610.02490v3 [stat.ML] UPDATED) https://t.co/CJZ7wo55ON,0,5," Abstract: We propose a nonparametric sequential test that aims to address two practical problems pertinent to online randomized experiments: (i) how to do a hypothesis test for complex metrics; (ii) how to prevent type $1$ error inflation under continuous monitoring. The proposed test does not require knowledge of the underlying probability distribution generating the data. We use the bootstrap to estimate the likelihood for blocks of data followed by mixture sequential probability ratio test. We validate this procedure on data from a major online e-commerce website. We show that the proposed test controls type $1$ error at any time, has good power, is robust to misspecification in the distribution generating the data, and allows quick inference in online randomized experiments. "
798337547999019008,2016-11-15 01:31:18,https://t.co/UdyBE67V04,Parallelizing Spectral Algorithms for Kernel Learning. (arXiv:1610.07487v2 [math.ST] UPDATED) https://t.co/UdyBE67V04,1,9," Abstract: We consider a distributed learning approach in supervised learning for a large class of spectral regularization methods in an RKHS framework. The data set of size n is partitioned into $m=O(n^\alpha)$, $\alpha \leq \frac{1}{2}$, disjoint subsets. On each subset, some spectral regularization method (belonging to a large class, including in particular Kernel Ridge Regression, $L^2$-boosting and spectral cut-off) is applied. The regression function $f$ is then estimated via simple averaging, leading to a substantial reduction in computation time. We show that minimax optimal rates of convergence are preserved if m grows sufficiently slowly (corresponding to an upper bound for $\alpha$) as $n \to \infty$, depending on the smoothness assumptions on $f$ and the intrinsic dimensionality. In spirit, our approach is classical. "
797975904844677120,2016-11-14 01:34:16,https://t.co/LYoW4YPWSp,Estimating Dynamic Treatment Regimes in Mobile Health Using V-learning. (arXiv:1611.03531v1 [stat.ML]) https://t.co/LYoW4YPWSp,1,7," Abstract: The vision for precision medicine is to use individual patient characteristics to inform a personalized treatment plan that leads to the best healthcare possible for each patient. Mobile technologies have an important role to play in this vision as they offer a means to monitor a patient's health status in real-time and subsequently to deliver interventions if, when, and in the dose that they are needed. Dynamic treatment regimes formalize individualized treatment plans as sequences of decision rules, one per stage of clinical intervention, that map current patient information to a recommended treatment. However, existing methods for estimating optimal dynamic treatment regimes are designed for a small number of fixed decision points occurring on a coarse time-scale. We propose a new reinforcement learning method for estimating an optimal treatment regime that is applicable to data collected using mobile technologies in an outpatient setting. The proposed method accommodates an indefinite time horizon and minute-by-minute decision making that are common in mobile health applications. We show the proposed estimators are consistent and asymptotically normal under mild conditions. The proposed methods are applied to estimate an optimal dynamic treatment regime for controlling blood glucose levels in patients with type 1 diabetes. "
797975902751784960,2016-11-14 01:34:15,https://t.co/fZP3EKl0aA,Simple and Efficient Parallelization for Probabilistic Temporal Tensor Factorization. (arXiv:1611.03578v1 [stat.ML… https://t.co/fZP3EKl0aA,1,6," Abstract: Probabilistic Temporal Tensor Factorization (PTTF) is an effective algorithm to model the temporal tensor data. It leverages a time constraint to capture the evolving properties of tensor data. Nowadays the exploding dataset demands a large scale PTTF analysis, and a parallel solution is critical to accommodate the trend. Whereas, the parallelization of PTTF still remains unexplored. In this paper, we propose a simple yet efficient Parallel Probabilistic Temporal Tensor Factorization, referred to as P$^2$T$^2$F, to provide a scalable PTTF solution. P$^2$T$^2$F is fundamentally disparate from existing parallel tensor factorizations by considering the probabilistic decomposition and the temporal effects of tensor data. It adopts a new tensor data split strategy to subdivide a large tensor into independent sub-tensors, the computation of which is inherently parallel. We train P$^2$T$^2$F with an efficient algorithm of stochastic Alternating Direction Method of Multipliers, and show that the convergence is guaranteed. Experiments on several real-word tensor datasets demonstrate that P$^2$T$^2$F is a highly effective and efficiently scalable algorithm dedicated for large scale probabilistic temporal tensor analysis. "
797975900503453696,2016-11-14 01:34:15,https://t.co/V85XPMjvnW,Tricks from Deep Learning. (arXiv:1611.03777v1 [cs.LG]) https://t.co/V85XPMjvnW,8,23," Abstract: The deep learning community has devised a diverse set of methods to make gradient optimization, using large datasets, of large and highly complex models with deeply cascaded nonlinearities, practical. Taken as a whole, these methods constitute a breakthrough, allowing computational structures which are quite wide, very deep, and with an enormous number and variety of free parameters to be effectively optimized. The result now dominates much of practical machine learning, with applications in machine translation, computer vision, and speech recognition. Many of these methods, viewed through the lens of algorithmic differentiation (AD), can be seen as either addressing issues with the gradient itself, or finding ways of achieving increased efficiency using tricks that are AD-related, but not provided by current AD systems. The goal of this paper is to explain not just those methods of most relevance to AD, but also the technical constraints and mindset which led to their discovery. After explaining this context, we present a ""laundry list"" of methods developed by the deep learning community. Two of these are discussed in further mathematical detail: a way to dramatically reduce the size of the tape when performing reverse-mode AD on a (theoretically) time-reversible process like an ODE integrator; and a new mathematical insight that allows for the implementation of a stochastic Newton's method. "
797975898460921857,2016-11-14 01:34:14,https://t.co/QgdaxdXagT,Understanding the 2016 US Presidential Election using ecological inference and distribution regression with census… https://t.co/QgdaxdXagT,6,12," Abstract: We combine fine-grained spatially referenced census data with the vote outcomes from the 2016 US presidential election. Using this dataset, we perform ecological inference using distribution regression (Flaxman et al, KDD 2015) with a multinomial-logit regression so as to model the vote outcome Trump, Clinton, Other / Didn't vote as a function of demographic and socioeconomic features. Ecological inference allows us to estimate ""exit poll"" style results like what was Trump's support among white women, but for entirely novel categories. We also perform exploratory data analysis to understand which census variables are predictive of voting for Trump, voting for Clinton, or not voting for either. All of our methods are implemented in python and R and are available online for replication. "
797975896967839749,2016-11-14 01:34:14,https://t.co/oaIYZZROwU,Recovery Guarantee of Non-negative Matrix Factorization via Alternating Updates. (arXiv:1611.03819v1 [cs.LG]) https://t.co/oaIYZZROwU,0,3," Abstract: Non-negative matrix factorization is a popular tool for decomposing data into feature and weight matrices under non-negativity constraints. It enjoys practical success but is poorly understood theoretically. This paper proposes an algorithm that alternates between decoding the weights and updating the features, and shows that assuming a generative model of the data, it provably recovers the ground-truth under fairly mild conditions. In particular, its only essential requirement on features is linear independence. Furthermore, the algorithm uses ReLU to exploit the non-negativity for decoding the weights, and thus can tolerate adversarial noise that can potentially be as large as the signal, and can tolerate unbiased noise much larger than the signal. The analysis relies on a carefully designed coupling between two potential functions, which we believe is of independent interest. "
797975895059337216,2016-11-14 01:34:13,https://t.co/VzV6ukapoJ,Learning to Learn for Global Optimization of Black Box Functions. (arXiv:1611.03824v1 [stat.ML]) https://t.co/VzV6ukapoJ,2,9," Abstract: We present a learning to learn approach for training recurrent neural networks to perform black-box global optimization. In the meta-learning phase we use a large set of smooth target functions to learn a recurrent neural network (RNN) optimizer, which is either a long-short term memory network or a differentiable neural computer. After learning, the RNN can be applied to learn policies in reinforcement learning, as well as other black-box learning tasks, including continuous correlated bandits and experimental design. We compare this approach to Bayesian optimization, with emphasis on the issues of computation speed, horizon length, and exploration-exploitation trade-offs. "
797975893247279104,2016-11-14 01:34:13,https://t.co/8CwAenjSCR,A Bayesian Network Model for Interesting Itemsets. (arXiv:1510.04130v2 [stat.ML] UPDATED) https://t.co/8CwAenjSCR,0,8," Abstract: Mining itemsets that are the most interesting under a statistical model of the underlying data is a commonly used and well-studied technique for exploratory data analysis, with the most recent interestingness models exhibiting state of the art performance. Continuing this highly promising line of work, we propose the first, to the best of our knowledge, generative model over itemsets, in the form of a Bayesian network, and an associated novel measure of interestingness. Our model is able to efficiently infer interesting itemsets directly from the transaction database using structural EM, in which the E-step employs the greedy approximation to weighted set cover. Our approach is theoretically simple, straightforward to implement, trivially parallelizable and retrieves itemsets whose quality is comparable to, if not better than, existing state of the art algorithms as we demonstrate on several real-world datasets. "
797975890663612416,2016-11-14 01:34:12,https://t.co/Pr4aIWZdnh,A Subsequence Interleaving Model for Sequential Pattern Mining. (arXiv:1602.05012v2 [stat.ML] UPDATED) https://t.co/Pr4aIWZdnh,0,3," Abstract: Recent sequential pattern mining methods have used the minimum description length (MDL) principle to define an encoding scheme which describes an algorithm for mining the most compressing patterns in a database. We present a novel subsequence interleaving model based on a probabilistic model of the sequence database, which allows us to search for the most compressing set of patterns without designing a specific encoding scheme. Our proposed algorithm is able to efficiently mine the most relevant sequential patterns and rank them using an associated measure of interestingness. The efficient inference in our model is a direct result of our use of a structural expectation-maximization framework, in which the expectation-step takes the form of a submodular optimization problem subject to a coverage constraint. We show on both synthetic and real world datasets that our model mines a set of sequential patterns with low spuriousness and redundancy, high interpretability and usefulness in real-world applications. Furthermore, we demonstrate that the quality of the patterns from our approach is comparable to, if not better than, existing state of the art sequential pattern mining algorithms. "
797975888411303940,2016-11-14 01:34:12,https://t.co/heHgOyHK9i,An Information Criterion for Inferring Coupling in Distributed Dynamical Systems. (arXiv:1605.06931v3 [cs.LG] UPDA… https://t.co/heHgOyHK9i,0,4," Abstract: The behaviour of many real-world phenomena can be modelled by nonlinear dynamical systems whereby a latent system state is observed through a filter. We are interested in interacting subsystems of this form, which we model by a set of coupled maps as a synchronous update graph dynamical systems. Specifically, we study the structure learning problem for spatially distributed dynamical systems coupled via a directed acyclic graph. Unlike established structure learning procedures that find locally maximum posterior probabilities of a network structure containing latent variables, our work exploits the properties of dynamical systems to compute globally optimal approximations of these distributions. We arrive at this result by the use of time delay embedding theorems. Taking an information-theoretic perspective, we show that the log-likelihood has an intuitive interpretation in terms of information transfer. "
797975886557487104,2016-11-14 01:34:11,https://t.co/zIabsQLnm3,Modeling Missing Data in Clinical Time Series with RNNs. (arXiv:1606.04130v5 [cs.LG] UPDATED) https://t.co/zIabsQLnm3,2,6," Abstract: We demonstrate a simple strategy to cope with missing data in sequential inputs, addressing the task of multilabel classification of diagnoses given clinical time series. Collected from the pediatric intensive care unit (PICU) at Children's Hospital Los Angeles, our data consists of multivariate time series of observations. The measurements are irregularly spaced, leading to missingness patterns in temporally discretized sequences. While these artifacts are typically handled by imputation, we achieve superior predictive performance by treating the artifacts as features. Unlike linear models, recurrent neural networks can realize this improvement using only simple binary indicators of missingness. For linear models, we show an alternative strategy to capture this signal. Training models on missingness patterns only, we show that for some diseases, what tests are run can be as predictive as the results themselves. "
797975884128882688,2016-11-14 01:34:11,https://t.co/ivZ3WAD699,A General Characterization of the Statistical Query Complexity. (arXiv:1608.02198v2 [cs.LG] UPDATED) https://t.co/ivZ3WAD699,0,2," Abstract: Statistical query (SQ) algorithms are algorithms that have access to an {\em SQ oracle} for the input distribution $D$ instead of i.i.d.~ samples from $D$. Given a query function $\phi:X \rightarrow [-1,1]$, the oracle returns an estimate of ${\bf E}_{ x\sim D}[\phi(x)]$ within some tolerance $\tau_\phi$ that roughly corresponds to the number of samples. In this work we demonstrate that the complexity of solving general problems over distributions using SQ algorithms can be captured by a relatively simple notion of statistical dimension that we introduce. SQ algorithms capture a broad spectrum of algorithmic approaches used in theory and practice, most notably, convex optimization techniques. Hence our statistical dimension allows to investigate the power of a variety of algorithmic approaches by analyzing a single linear-algebraic parameter. Such characterizations were investigated over the past 20 years in learning theory but prior characterizations are restricted to the much simpler setting of classification problems relative to a fixed distribution on the domain (Blum et al., 1994; Bshouty and Feldman, 2002; Yang, 2001; Simon, 2007; Feldman, 2012; Szorenyi, 2009). Our characterization is also the first to precisely characterize the necessary tolerance of queries. We give applications of our techniques to two open problems in learning theory and to algorithms that are subject to memory and communication constraints. "
797975882249945092,2016-11-14 01:34:10,https://t.co/zsMXxgTEiS,Understanding Deep Neural Networks with Rectified Linear Units. (arXiv:1611.01491v2 [cs.LG] UPDATED) https://t.co/zsMXxgTEiS,2,14," Abstract: In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give the first-ever polynomial time (in the size of data) algorithm to train a ReLU DNN with one hidden layer to {\em global optimality}. This follows from our complete characterization of the ReLU DNN function class whereby we show that a $\R^n \to \R$ function is representable by a ReLU DNN {\em if and only if} it is a continuous piecewise linear function. The main tool used to prove this characterization is an elegant result from tropical geometry. Further, for the $n=1$ case, we show that a single hidden layer suffices to express all piecewise linear functions, and we give tight bounds for the size of such a ReLU DNN. We follow up with gap results showing that there is a smoothly parameterized family of $\R\to \R$ ""hard"" functions that lead to an exponential blow-up in size, if the number of layers is decreased by a small amount. An example consequence of our gap theorem is that for every natural number $N$, there exists a function representable by a ReLU DNN with depth $N^2+1$ and total size $N^3$, such that any ReLU DNN with depth at most $N + 1$ will require at least $\frac12N^{N+1}-1$ total nodes. Finally, we construct a family of $\R^n\to \R$ functions for $n\geq 2$ (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension $n$ at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on $n$ has not been achieved by previous families of ""hard"" functions in the neural nets literature. This construction utilizes the theory of zonotopes from polyhedral theory. "
797975879053778944,2016-11-14 01:34:09,https://t.co/bDiiuvOf9x,Accelerating the BSM interpretation of LHC data with machine learning. (arXiv:1611.02704v1 [hep-ph] CROSS LISTED) https://t.co/bDiiuvOf9x,0,5," Abstract: The interpretation of Large Hadron Collider (LHC) data in the framework of Beyond the Standard Model (BSM) theories is hampered by the need to run computationally expensive event generators and detector simulators. Performing statistically convergent scans of high-dimensional BSM theories is consequently challenging, and in practice unfeasible for very high-dimensional BSM theories. We present here a new machine learning method that accelerates the interpretation of LHC data, by learning the relationship between BSM theory parameters and data. As a proof-of-concept, we demonstrate that this technique accurately predicts natural SUSY signal events in two signal regions at the High Luminosity LHC, up to four orders of magnitude faster than standard techniques. The new approach makes it possible to rapidly and accurately reconstruct the theory parameters of complex BSM theories, should an excess in the data be discovered at the LHC. "
796888570673950721,2016-11-11 01:33:35,https://t.co/JSF5gBK6Oa,Diversity Leads to Generalization in Neural Networks. (arXiv:1611.03131v1 [cs.LG]) https://t.co/JSF5gBK6Oa,2,18," Abstract: Neural networks are a powerful class of functions that can be trained with simple gradient descent to achieve state-of-the-art performance on a variety of applications. Despite their practical success, there is a paucity of results that provide theoretical guarantees on why they are so effective. Lying in the center of the problem is the difficulty of analyzing the non-convex objective function with potentially numerous local minima and saddle points. Can neural networks corresponding to the stationary points of the objective function learn the true labeling function? If yes, what are the key factors contributing to such generalization ability? In this paper, we provide answers to these questions by analyzing one-hidden-layer neural networks with ReLU activation, and show that despite the non-convexity, neural networks with diverse units can learn the true function. We bypass the non-convexity issue by directly analyzing the first order condition, and show that the loss is bounded if the smallest singular value of the ""extended feature matrix"" is large enough. We make novel use of techniques from kernel methods and geometric discrepancy, and identify a new relation linking the smallest singular value to the spectrum of a kernel function associated with the activation function and to the diversity of the units. Our results also suggest a novel regularization function to promote unit diversity for potentially better generalization ability. "
796888566995582977,2016-11-11 01:33:34,https://t.co/SeLozQwTMh,Mahalanobis Distance for Class Averaging of Cryo-EM Images. (arXiv:1611.03193v1 [stat.AP]) https://t.co/SeLozQwTMh,0,4," Abstract: Single particle reconstruction (SPR) from cryo-electron microscopy (EM) is a technique in which the 3D structure of a molecule needs to be determined from its contrast transfer function (CTF) affected, noisy 2D projection images taken at unknown viewing directions. One of the main challenges in cryo-EM is the typically low signal to noise ratio (SNR) of the acquired images. 2D classification of images, followed by class averaging, improves the SNR of the resulting averages, and is used for selecting particles from micrographs and for inspecting the particle images. We introduce a new affinity measure, akin to the Mahalanobis distance, to compare cryo-EM images belonging to different defocus groups. The new similarity measure is employed to detect similar images, thereby leading to an improved algorithm for class averaging. We evaluate the performance of the proposed class averaging procedure on synthetic datasets, obtaining state of the art classification. "
796888562935484417,2016-11-11 01:33:33,https://t.co/VLco3HDeUa,Low Data Drug Discovery with One-shot Learning. (arXiv:1611.03199v1 [cs.LG]) https://t.co/VLco3HDeUa,2,10," Abstract: Recent advances in machine learning have made significant contributions to drug discovery. Deep neural networks in particular have been demonstrated to provide significant boosts in predictive power when inferring the properties and activities of small-molecule compounds. However, the applicability of these techniques has been limited by the requirement for large amounts of training data. In this work, we demonstrate how one-shot learning can be used to significantly lower the amounts of data required to make meaningful predictions in drug discovery applications. We introduce a new architecture, the residual LSTM embedding, that, when combined with graph convolutional neural networks, significantly improves the ability to learn meaningful distance metrics over small-molecules. We open source all models introduced in this work as part of DeepChem, an open-source framework for deep-learning in drug discovery. "
796888560066592768,2016-11-11 01:33:32,https://t.co/0SESn3vahQ,Feature Selection with the R Package MXM: Discovering Statistically-Equivalent Feature Subsets. (arXiv:1611.03227v… https://t.co/0SESn3vahQ,3,9," Abstract: The statistically equivalent signature (SES) algorithm is a method for feature selection inspired by the principles of constrained-based learning of Bayesian Networks. Most of the currently available feature-selection methods return only a single subset of features, supposedly the one with the highest predictive power. We argue that in several domains multiple subsets can achieve close to maximal predictive accuracy, and that arbitrarily providing only one has several drawbacks. The SES method attempts to identify multiple, predictive feature subsets whose performances are statistically equivalent. Under that respect SES subsumes and extends previous feature selection algorithms, like the max-min parent children algorithm. SES is implemented in an homonym function included in the R package MXM, standing for mens ex machina, meaning 'mind from the machine' in Latin. The MXM implementation of SES handles several data-analysis tasks, namely classification, regression and survival analysis. In this paper we present the SES algorithm, its implementation, and provide examples of use of the SES function in R. Furthermore, we analyze three publicly available data sets to illustrate the equivalence of the signatures retrieved by SES and to contrast SES against the state-of-the-art feature selection method LASSO. Our results provide initial evidence that the two methods perform comparably well in terms of predictive accuracy and that multiple, equally predictive signatures are actually present in real world data. "
796888556971196417,2016-11-11 01:33:32,https://t.co/WUcO3Q7kMi,Policy Search with High-Dimensional Context Variables. (arXiv:1611.03231v1 [stat.ML]) https://t.co/WUcO3Q7kMi,1,3," Abstract: Direct contextual policy search methods learn to improve policy parameters and simultaneously generalize these parameters to different context or task variables. However, learning from high-dimensional context variables, such as camera images, is still a prominent problem in many real-world tasks. A naive application of unsupervised dimensionality reduction methods to the context variables, such as principal component analysis, is insufficient as task-relevant input may be ignored. In this paper, we propose a contextual policy search method in the model-based relative entropy stochastic search framework with integrated dimensionality reduction. We learn a model of the reward that is locally quadratic in both the policy parameters and the context variables. Furthermore, we perform supervised linear dimensionality reduction on the context variables by nuclear norm regularization. The experimental results show that the proposed method outperforms naive dimensionality reduction via principal component analysis and a state-of-the-art contextual policy search method. "
796888554022563840,2016-11-11 01:33:31,https://t.co/uzPvhEP8Ro,Distributed Estimation and Learning over Heterogeneous Networks. (arXiv:1611.03328v1 [stat.AP]) https://t.co/uzPvhEP8Ro,0,3," Abstract: We consider several estimation and learning problems that networked agents face when making decisions given their uncertainty about an unknown variable. Our methods are designed to efficiently deal with heterogeneity in both size and quality of the observed data, as well as heterogeneity over time (intermittence). The goal of the studied aggregation schemes is to efficiently combine the observed data that is spread over time and across several network nodes, accounting for all the network heterogeneities. Moreover, we require no form of coordination beyond the local neighborhood of every network agent or sensor node. The three problems that we consider are (i) maximum likelihood estimation of the unknown given initial data sets, (ii) learning the true model parameter from streams of data that the agents receive intermittently over time, and (iii) minimum variance estimation of a complete sufficient statistic from several data points that the networked agents collect over time. In each case we rely on an aggregation scheme to combine the observations of all agents; moreover, when the agents receive streams of data over time, we modify the update rules to accommodate the most recent observations. In every case, we demonstrate the efficiency of our algorithms by proving convergence to the globally efficient estimators given the observations of all agents. We supplement these results by investigating the rate of convergence and providing finite-time performance guarantees. "
796888550247698432,2016-11-11 01:33:30,https://t.co/G7jZFpcmvq,Disentangling factors of variation in deep representations using adversarial training. (arXiv:1611.03383v1 [cs.LG]) https://t.co/G7jZFpcmvq,2,19," Abstract: We introduce a conditional generative model for learning to disentangle the hidden factors of variation within a set of labeled observations, and separate them into complementary codes. One code summarizes the specified factors of variation associated with the labels. The other summarizes the remaining unspecified variability. During training, the only available source of supervision comes from our ability to distinguish among different observations belonging to the same class. Examples of such observations include images of a set of labeled objects captured at different viewpoints, or recordings of set of speakers dictating multiple phrases. In both instances, the intra-class diversity is the source of the unspecified factors of variation: each object is observed at multiple viewpoints, and each speaker dictates multiple phrases. Learning to disentangle the specified factors from the unspecified ones becomes easier when strong supervision is possible. Suppose that during training, we have access to pairs of images, where each pair shows two different objects captured from the same viewpoint. This source of alignment allows us to solve our task using existing methods. However, labels for the unspecified factors are usually unavailable in realistic scenarios where data acquisition is not strictly controlled. We address the problem of disentanglement in this more general setting by combining deep convolutional autoencoders with a form of adversarial training. Both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies. Experimental results on synthetic and real datasets show that the proposed method is capable of generalizing to unseen classes and intra-class variabilities. "
796888546497925121,2016-11-11 01:33:29,https://t.co/Vm6cyYVx35,Learning an Astronomical Catalog of the Visible Universe through Scalable Bayesian Inference. (arXiv:1611.03404v1 … https://t.co/Vm6cyYVx35,0,5," Abstract: Celeste is a procedure for inferring astronomical catalogs that attains state-of-the-art scientific results. To date, Celeste has been scaled to at most hundreds of megabytes of astronomical images: Bayesian posterior inference is notoriously demanding computationally. In this paper, we report on a scalable, parallel version of Celeste, suitable for learning catalogs from modern large-scale astronomical datasets. Our algorithmic innovations include a fast numerical optimization routine for Bayesian posterior inference and a statistically efficient scheme for decomposing astronomical optimization problems into subproblems. Our scalable implementation is written entirely in Julia, a new high-level dynamic programming language designed for scientific and numerical computing. We use Julia's high-level constructs for shared and distributed memory parallelism, and demonstrate effective load balancing and efficient scaling on up to 8192 Xeon cores on the NERSC Cori supercomputer. "
796888542551142400,2016-11-11 01:33:28,https://t.co/UOv637FZ0t,Why is it Difficult to Detect Sudden and Unexpected Epidemic Outbreaks in Twitter?. (arXiv:1611.03426v1 [cs.CY]) https://t.co/UOv637FZ0t,2,4," Abstract: Social media services such as Twitter are a valuable source of information for decision support systems. Many studies have shown that this also holds for the medical domain, where Twitter is considered a viable tool for public health officials to sift through relevant information for the early detection, management, and control of epidemic outbreaks. This is possible due to the inherent capability of social media services to transmit information faster than traditional channels. However, the majority of current studies have limited their scope to the detection of common and seasonal health recurring events (e.g., Influenza-like Illness), partially due to the noisy nature of Twitter data, which makes outbreak detection and management very challenging. Within the European project M-Eco, we developed a Twitter-based Epidemic Intelligence (EI) system, which is designed to also handle a more general class of unexpected and aperiodic outbreaks. In particular, we faced three main research challenges in this endeavor: 1) dynamic classification to manage terminology evolution of Twitter messages, 2) alert generation to produce reliable outbreak alerts analyzing the (noisy) tweet time series, and 3) ranking and recommendation to support domain experts for better assessment of the generated alerts. In this paper, we empirically evaluate our proposed approach to these challenges using real-world outbreak datasets and a large collection of tweets. We validate our solution with domain experts, describe our experiences, and give a more realistic view on the benefits and issues of analyzing social media for public health. "
796888538046468096,2016-11-11 01:33:27,https://t.co/IakxEddfBH,Multi-Task Multiple Kernel Relationship Learning. (arXiv:1611.03427v1 [stat.ML]) https://t.co/IakxEddfBH,0,5," Abstract: This paper presents a novel multitask multiple-kernel learning framework that efficiently learns the kernel weights leveraging the relationship across multiple tasks. The idea is to automatically infer this task relationship in the \textit{RKHS} space corresponding to the given base kernels. The problem is formulated as a regularization-based approach called \textit{Multi-Task Multiple Kernel Relationship Learning} (\textit{MK-MTRL}), which models the task relationship matrix from the weights learned from latent feature spaces of task-specific base kernels. Unlike in previous work, the proposed formulation allows one to incorporate prior knowledge for simultaneously learning several related task. We propose an alternating minimization algorithm to learn the model parameters, kernel weights and task relationship matrix. In order to tackle large-scale problems, we further propose a two-stage \textit{MK-MTRL} online learning algorithm and show that it significantly reduces the computational time, and also achieves performance comparable to that of the joint learning framework. Experimental results on benchmark datasets show that the proposed formulations outperform several state-of-the-art multi-task learning methods. "
796888534141599745,2016-11-11 01:33:26,https://t.co/eFt09fkmQx,Importance Sampling with Unequal Support. (arXiv:1611.03451v1 [cs.LG]) https://t.co/eFt09fkmQx,1,6," Abstract: Importance sampling is often used in machine learning when training and testing data come from different distributions. In this paper we propose a new variant of importance sampling that can reduce the variance of importance sampling-based estimates by orders of magnitude when the supports of the training and testing distributions differ. After motivating and presenting our new importance sampling estimator, we provide a detailed theoretical analysis that characterizes both its bias and variance relative to the ordinary importance sampling estimator (in various settings, which include cases where ordinary importance sampling is biased, while our new estimator is not, and vice versa). We conclude with an example of how our new importance sampling estimator can be used to improve estimates of how well a new treatment policy for diabetes will work for an individual, using only data from when the individual used a previous treatment policy. "
796888530953863169,2016-11-11 01:33:25,https://t.co/iddnIPPdO8,Exponential Machines. (arXiv:1605.03795v2 [stat.ML] UPDATED) https://t.co/iddnIPPdO8,0,15," Abstract: Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K. "
796888527728480261,2016-11-11 01:33:25,https://t.co/mOuVTSSEpT,Sequence-to-Sequence Learning as Beam-Search Optimization. (arXiv:1606.02960v2 [cs.CL] UPDATED) https://t.co/mOuVTSSEpT,0,10," Abstract: Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation. "
796888524398219265,2016-11-11 01:33:24,https://t.co/Y6bDXeexhK,Combinatorial Determinantal Point Processes. (arXiv:1608.00554v2 [cs.DS] UPDATED) https://t.co/Y6bDXeexhK,0,3," Abstract: Determinantal Point Processes (DPPs) are probabilistic models of repulsion that originate in quantum physics and random matrix theory and have been of recent interest in computer science. DPPs define distributions over subsets of a given ground set, and exhibit interesting properties such as negative correlation. When applied to kernel methods in machine learning, DPPs give rise to an efficient algorithm to select a small, diverse sample of the given data. Kulesza and Taskar [KT12] posed a natural open question: Can we sample from DPPs when there are additional constraints on the allowable subsets? In this paper, we study the complexity of sampling from DPPs over combinatorially-constrained families of subsets and present several connections and applications. We start by showing that it is at least as hard to sample from combinatorial DPPs as it is to compute the mixed discriminant of a tuple of positive semidefinite matrices. Subsequently, we give a polynomial time algorithm for sampling from combinatorial DPPs with a constant number of linear constraints; thus, we make significant progress towards answering the question of [KT12]. These results can lead to several non-trivial applications; e.g., we show how to derandomize a result of Nikolov and Singh [NS16] for maximizing subdeterminants under (a constant number of) partition constraints and -- motivated by making the [MSS13] proof for the Kadison-Singer problem algorithmic -- give an algorithm to compute the higher-order coefficients of mixed characteristic polynomials. "
796888521265020928,2016-11-11 01:33:23,https://t.co/IVl3VycEP5,Neural Photo Editing with Introspective Adversarial Networks. (arXiv:1609.07093v2 [cs.LG] UPDATED) https://t.co/IVl3VycEP5,0,8," Abstract: We present the Neural Photo Editor, an interface for exploring the latent space of generative image models and making large, semantically coherent changes to existing images. Our interface is powered by the Introspective Adversarial Network, a hybridization of the Generative Adversarial Network and the Variational Autoencoder designed for use in the editor. Our model makes use of a novel computational block based on dilated convolutions, and Orthogonal Regularization, a novel weight regularization method. We validate our model on CelebA, SVHN, and ImageNet, and produce samples and reconstructions with high visual fidelity. "
796888518207401984,2016-11-11 01:33:22,https://t.co/erC5dqtong,Socratic Learning: Empowering the Generative Model. (arXiv:1610.08123v2 [cs.LG] CROSS LISTED) https://t.co/erC5dqtong,0,3," Abstract: Modern machine learning techniques, such as deep learning, often use discriminative models that require large amounts of labeled data. An alternative approach is to use a generative model, which leverages heuristics from domain experts to train on unlabeled data. Domain experts often prefer to use generative models because they ""tell a story"" about their data. Unfortunately, generative models are typically less accurate than discriminative models. Several recent approaches combine both types of model to exploit their strengths. In this setting, a misspecified generative model can hurt the performance of subsequent discriminative training. To address this issue, we propose a framework called Socratic learning that automatically uses information from the discriminative model to correct generative model misspecification. Furthermore, this process provides users with interpretable feedback about how to improve their generative model. We evaluate Socratic learning on real-world relation extraction tasks and observe an immediate improvement in classification accuracy that could otherwise require several weeks of effort by domain experts. "
796888512620597248,2016-11-11 01:33:21,https://t.co/ERy1uw678D,Predicting User Roles in Social Networks using Transfer Learning with Feature Transformation. (arXiv:1611.02941v1 … https://t.co/ERy1uw678D,2,11," Abstract: How can we recognise social roles of people, given a completely unlabelled social network? We present a transfer learning approach to network role classification based on feature transformations from each network's local feature distribution to a global feature space. Experiments are carried out on real-world datasets. (See manuscript for the full abstract.) "
796527371725766656,2016-11-10 01:38:18,https://t.co/xlMKtestAs,Variational Lossy Autoencoder. (arXiv:1611.02731v1 [cs.LG]) https://t.co/xlMKtestAs,3,17," Abstract: Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the code only ""autoencodes"" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution $p(z)$ and decoding distribution $p(x|z)$, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks. "
796527369607647232,2016-11-10 01:38:18,https://t.co/wJ92UcQQ9t,Recursive Decomposition for Nonconvex Optimization. (arXiv:1611.02755v1 [cs.AI]) https://t.co/wJ92UcQQ9t,0,11," Abstract: Continuous optimization is an important problem in many areas of AI, including vision, robotics, probabilistic inference, and machine learning. Unfortunately, most real-world optimization problems are nonconvex, causing standard convex techniques to find only local optima, even with extensions like random restarts and simulated annealing. We observe that, in many cases, the local modes of the objective function have combinatorial structure, and thus ideas from combinatorial optimization can be brought to bear. Based on this, we propose a problem-decomposition approach to nonconvex optimization. Similarly to DPLL-style SAT solvers and recursive conditioning in probabilistic inference, our algorithm, RDIS, recursively sets variables so as to simplify and decompose the objective function into approximately independent sub-functions, until the remaining functions are simple enough to be optimized by standard techniques like gradient descent. The variables to set are chosen by graph partitioning, ensuring decomposition whenever possible. We show analytically that RDIS can solve a broad class of nonconvex optimization problems exponentially faster than gradient descent with random restarts. Experimentally, RDIS outperforms standard techniques on problems like structure from motion and protein folding. "
796527368026394625,2016-11-10 01:38:17,https://t.co/TZOMAjlCPd,RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning. (arXiv:1611.02779v1 [cs.AI]) https://t.co/TZOMAjlCPd,1,16," Abstract: Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a ""fast"" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (""slow"") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the ""fast"" RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems. "
796527366357000192,2016-11-10 01:38:17,https://t.co/BPiHgNTGZY,Gaussian process regression can turn non-uniform and undersampled diffusion MRI data into diffusion spectrum imagi… https://t.co/BPiHgNTGZY,0,3," Abstract: We propose to use Gaussian process regression to accurately estimate the diffusion MRI signal at arbitrary locations in q-space. By estimating the signal on a grid, we can do synthetic diffusion spectrum imaging: reconstructing the ensemble averaged propagator (EAP) by an inverse Fourier transform. We also propose an alternative reconstruction method guaranteeing a nonnegative EAP that integrates to unity. The reconstruction is validated on data simulated from two Gaussians at various crossing angles. Moreover, we demonstrate on non-uniformly sampled in vivo data that the method is far superior to linear interpolation, and allows a drastic undersampling of the data with only a minor loss of accuracy. We envision the method as a potential replacement for standard diffusion spectrum imaging, in particular when acquistion time is limited. "
796527364553449474,2016-11-10 01:38:17,https://t.co/NSGkp1HKoZ,Node Embedding via Word Embedding for Network Community Discovery. (arXiv:1611.03028v1 [cs.SI]) https://t.co/NSGkp1HKoZ,1,6," Abstract: Neural node embeddings have recently emerged as a powerful representation for supervised learning tasks involving graph-structured data. We leverage this recent advance to develop a novel algorithm for unsupervised community discovery in graphs. Through extensive experimental studies on simulated and real-world data, we demonstrate that the proposed approach consistently improves over the current state-of-the-art. Specifically, our approach empirically attains the information-theoretic limits for community recovery under the benchmark Stochastic Block Models for graph generation and exhibits better stability and accuracy over both Spectral Clustering and Acyclic Belief Propagation in the community recovery limits. "
796527362842259457,2016-11-10 01:38:16,https://t.co/ywYzu1IqLL,On the Diffusion Geometry of Graph Laplacians and Applications. (arXiv:1611.03033v1 [math.SP]) https://t.co/ywYzu1IqLL,2,8," Abstract: We study directed, weighted graphs $G=(V,E)$ and consider the (not necessarily symmetric) averaging operator $$ (\mathcal{L}u)(i) = -\sum_{j \sim_{} i}{p_{ij} (u(j) - u(i))},$$ where $p_{ij}$ are normalized edge weights. Given a vertex $i \in V$, we define the diffusion distance to a set $B \subset V$ as the smallest number of steps $d_{B}(i) \in \mathbb{N}$ required for half of all random walks started in $i$ and moving randomly with respect to the weights $p_{ij}$ to visit $B$ within $d_{B}(i)$ steps. Our main result is that the eigenfunctions interact nicely with this notion of distance. In particular, if $u$ satisfies $\mathcal{L}u = \lambda u$ on $V$ and $$ B = \left\{ i \in V: - \varepsilon \leq u(i) \leq \varepsilon \right\} \neq \emptyset,$$ then, for all $i \in V$, $$ d_{B}(i) \log{\left( \frac{1}{|1-\lambda|} \right) } \geq \log{\left( \frac{ |u(i)| }{\|u\|_{L^{\infty}}} \right)} - \log{\left(\frac{1}{2} + \varepsilon\right)}.$$ $d_B(i)$ is a remarkably good approximation of $|u|$ in the sense of having very high correlation. The result implies that the classical one-dimensional spectral embedding preserves particular aspects of geometry in the presence of clustered data. We also give a continuous variant of the result which has a connection to the hot spots conjecture. "
796527361042874368,2016-11-10 01:38:16,https://t.co/vFPDfT1YP6,Group Regularized Estimation under Structural Hierarchy. (arXiv:1411.4691v3 [math.ST] UPDATED) https://t.co/vFPDfT1YP6,0,1," Abstract: Variable selection for models including interactions between explanatory variables often needs to obey certain hierarchical constraints. The weak or strong structural hierarchy requires that the existence of an interaction term implies at least one or both associated main effects to be present in the model. Lately, this problem has attracted a lot of attention, but existing computational algorithms converge slow even with a moderate number of predictors. Moreover, in contrast to the rich literature on ordinary variable selection, there is a lack of statistical theory to show reasonably low error rates of hierarchical variable selection. This work investigates a new class of estimators that make use of multiple group penalties to capture structural parsimony. We give the minimax lower bounds for strong and weak hierarchical variable selection and show that the proposed estimators enjoy sharp rate oracle inequalities. A general-purpose algorithm is developed with guaranteed convergence and global optimality. Simulations and real data experiments demonstrate the efficiency and efficacy of the proposed approach. "
796527359037935617,2016-11-10 01:38:15,https://t.co/9eefDtpa80,Correlated Random Measures. (arXiv:1507.00720v3 [stat.ML] UPDATED) https://t.co/9eefDtpa80,0,3," Abstract: We develop correlated random measures, random measures where the atom weights can exhibit a flexible pattern of dependence, and use them to develop powerful hierarchical Bayesian nonparametric models. Hierarchical Bayesian nonparametric models are usually built from completely random measures, a Poisson-process based construction in which the atom weights are independent. Completely random measures imply strong independence assumptions in the corresponding hierarchical model, and these assumptions are often misplaced in real-world settings. Correlated random measures address this limitation. They model correlation within the measure by using a Gaussian process in concert with the Poisson process. With correlated random measures, for example, we can develop a latent feature model for which we can infer both the properties of the latent features and their dependency pattern. We develop several other examples as well. We study a correlated random measure model of pairwise count data. We derive an efficient variational inference algorithm and show improved predictive performance on large data sets of documents, web clicks, and electronic health records. "
796527357368692736,2016-11-10 01:38:15,https://t.co/147TkFIhYp,Nonconvex Sparse Learning via Stochastic Optimization with Progressive Variance Reduction. (arXiv:1605.02711v2 [cs… https://t.co/147TkFIhYp,1,8," Abstract: We propose a stochastic variance reduced optimization algorithm for solving sparse learning problems with cardinality constraints. Sufficient conditions are provided, under which the proposed algorithm enjoys strong linear convergence guarantees and optimal estimation accuracy in high dimensions. We further extend the proposed algorithm to an asynchronous parallel variant with a near linear speedup. Numerical experiments demonstrate the efficiency of our algorithm in terms of both parameter estimation and computational performance. "
796527355825098752,2016-11-10 01:38:15,https://t.co/4FOMnBnK66,MCMC assisted by Belief Propagaion. (arXiv:1605.09042v4 [stat.ML] UPDATED) https://t.co/4FOMnBnK66,1,8," Abstract: Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most popular algorithms for computational inference in Graphical Models (GM). In principle, MCMC is an exact probabilistic method which, however, often suffers from exponentially slow mixing. In contrast, BP is a deterministic method, which is typically fast, empirically very successful, however in general lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC algorithms correcting the approximation error of BP, i.e., we provide a way to compensate for BP errors via a consecutive BP-aware MCMC. Our framework is based on the Loop Calculus (LC) approach which allows to express the BP error as a sum of weighted generalized loops. Although the full series is computationally intractable, it is known that a truncated series, summing up all 2-regular loops, is computable in polynomial-time for planar pair-wise binary GMs and it also provides a highly accurate approximation empirically. Motivated by this, we first propose a polynomial-time approximation MCMC scheme for the truncated series of general (non-planar) pair-wise binary models. Our main idea here is to use the Worm algorithm, known to provide fast mixing in other (related) problems, and then design an appropriate rejection scheme to sample 2-regular loops. Furthermore, we also design an efficient rejection-free MCMC scheme for approximating the full series. The main novelty underlying our design is in utilizing the concept of cycle basis, which provides an efficient decomposition of the generalized loops. In essence, the proposed MCMC schemes run on transformed GM built upon the non-trivial BP solution, and our experiments show that this synthesis of BP and MCMC outperforms both direct MCMC and bare BP schemes. "
796527354235551744,2016-11-10 01:38:14,https://t.co/SLW1d2W2jG,Generative Adversarial Nets from a Density Ratio Estimation Perspective. (arXiv:1610.02920v2 [stat.ML] UPDATED) https://t.co/SLW1d2W2jG,1,10," Abstract: Generative adversarial networks (GANs) are successful deep generative models. GANs are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful. "
796527352406798336,2016-11-10 01:38:14,https://t.co/M3ntFjsL51,Big Batch SGD: Automated Inference using Adaptive Batch Sizes. (arXiv:1610.05792v2 [cs.LG] UPDATED) https://t.co/M3ntFjsL51,0,7," Abstract: Classical stochastic gradient methods for optimization rely on noisy gradient approximations that become progressively less accurate as iterates approach a solution. The large noise and small signal in the resulting gradients makes it difficult to use them for adaptive stepsize selection and automatic stopping. We propose alternative ""big batch"" SGD schemes that adaptively grow the batch size over time to maintain a nearly constant signal-to-noise ratio in the gradient approximation. The resulting methods have similar convergence rates to classical SGD methods without requiring convexity of the objective function. The high fidelity gradients enable automated learning rate selection and do not require stepsize decay. For this reason, big batch methods are easily automated and can run with little or no user oversight. "
796527350888431616,2016-11-10 01:38:13,https://t.co/5dLQPTtzCp,Learning Bound for Parameter Transfer Learning. (arXiv:1610.08696v2 [stat.ML] UPDATED) https://t.co/5dLQPTtzCp,0,6," Abstract: We consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then, we introduce the notion of the local stability and parameter transfer learnability of parametric feature mapping,and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in self-taught learning. Although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance, their theoretical analysis has not been studied. In this paper, we also provide the first theoretical learning bound for self-taught learning. "
796163731474513922,2016-11-09 01:33:20,https://t.co/a2BH0ESDcT,Normalizing Flows on Riemannian Manifolds. (arXiv:1611.02304v1 [stat.ML]) https://t.co/a2BH0ESDcT,2,19," Abstract: We consider the problem of density estimation on Riemannian manifolds. Density estimation on manifolds has many applications in fluid-mechanics, optics and plasma physics and it appears often when dealing with angular variables (such as used in protein folding, robot limbs, gene-expression) and in general directional statistics. In spite of the multitude of algorithms available for density estimation in the Euclidean spaces $\mathbf{R}^n$ that scale to large n (e.g. normalizing flows, kernel methods and variational approximations), most of these methods are not immediately suitable for density estimation in more general Riemannian manifolds. We revisit techniques related to homeomorphisms from differential geometry for projecting densities to sub-manifolds and use it to generalize the idea of normalizing flows to more general Riemannian manifolds. The resulting algorithm is scalable, simple to implement and suitable for use with automatic differentiation. We demonstrate concrete examples of this method on the n-sphere $\mathbf{S}^n$. "
796163729687793665,2016-11-09 01:33:19,https://t.co/KiTY2EyYXL,Learning Influence Functions from Incomplete Observations. (arXiv:1611.02305v1 [cs.SI]) https://t.co/KiTY2EyYXL,1,3," Abstract: We study the problem of learning influence functions under incomplete observations of node activations. Incomplete observations are a major concern as most (online and real-world) social networks are not fully observable. We establish both proper and improper PAC learnability of influence functions under randomly missing observations. Proper PAC learnability under the Discrete-Time Linear Threshold (DLT) and Discrete-Time Independent Cascade (DIC) models is established by reducing incomplete observations to complete observations in a modified graph. Our improper PAC learnability result applies for the DLT and DIC models as well as the Continuous-Time Independent Cascade (CIC) model. It is based on a parametrization in terms of reachability features, and also gives rise to an efficient and practical heuristic. Experiments on synthetic and real-world datasets demonstrate the ability of our method to compensate even for a fairly large fraction of missing observations. "
796163727875862529,2016-11-09 01:33:19,https://t.co/s149BUSU6V,Adversarial Ladder Networks. (arXiv:1611.02320v1 [cs.NE]) https://t.co/s149BUSU6V,2,10," Abstract: The use of unsupervised data in addition to supervised data in training discriminative neural networks has improved the performance of this clas- sification scheme. However, the best results were achieved with a training process that is divided in two parts: first an unsupervised pre-training step is done for initializing the weights of the network and after these weights are refined with the use of supervised data. On the other hand adversarial noise has improved the results of clas- sical supervised learning. Recently, a new neural network topology called Ladder Network, where the key idea is based in some properties of hierar- chichal latent variable models, has been proposed as a technique to train a neural network using supervised and unsupervised data at the same time with what is called semi-supervised learning. This technique has reached state of the art classification. In this work we add adversarial noise to the ladder network and get state of the art classification, with several important conclusions on how adversarial noise can help in addition with new possible lines of investi- gation. We also propose an alternative to add adversarial noise to unsu- pervised data. "
796163725652856833,2016-11-09 01:33:18,https://t.co/AhlWcKSpYG,Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks. (arXiv:1611.02345v1 [cs.LG]) https://t.co/AhlWcKSpYG,1,11," Abstract: Modern convolutional networks, incorporating rectifiers and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. Nevertheless, methods from convex optimization such as gradient descent and Adam are widely used as building blocks for deep learning algorithms. This paper provides the first convergence guarantee applicable to modern convnets. The guarantee matches a lower bound for convex nonsmooth functions. The key technical tool is the neural Taylor approximation -- a straightforward application of Taylor expansions to neural networks -- and the associated Taylor loss. Experiments on a range of optimizers, layers, and tasks provide evidence that the analysis accurately captures the dynamics of neural optimization. The second half of the paper applies the Taylor approximation to isolate the main difficulty in training rectifier nets: that gradients are shattered. We investigate the hypothesis that, by exploring the space of activation configurations more thoroughly, adaptive optimizers such as RMSProp and Adam are able to converge to better solutions. "
796163723870240768,2016-11-09 01:33:18,https://t.co/6BTwl87sM8,An Online Prediction Framework for Non-Stationary Time Series. (arXiv:1611.02365v1 [stat.ML]) https://t.co/6BTwl87sM8,0,4," Abstract: We extend an online time series prediction algorithm for ARMA processes to describe a framework for time series prediction that can efficiently handle non-stationarities that exist in many real time series. We show that appropriate transformations to such time series can lead to theoretical and empirical gains. To account for the phenomenon of cointegration in the multivariate case, we present a novel algorithm EC-VARMA-OGD that estimates both the auto-regressive and the cointegrating parameters. Relaxing the assumptions for the analysis, we prove a sub-linear regret bound for all the methods described. We note that the theoretical guarantees do not provide a complete picture, thus we provide a data-dependent analysis of the follow-the-leader algorithm for least squares loss that explains the success of using non-stationary transformations. We support all of our results with experiments on simulated and real data. "
796163722033172480,2016-11-09 01:33:17,https://t.co/g92n8SMjJx,Combining observational and experimental data to find heterogeneous treatment effects. (arXiv:1611.02385v1 [cs.AI]) https://t.co/g92n8SMjJx,0,1," Abstract: Every design choice will have different effects on different units. However traditional A/B tests are often underpowered to identify these heterogeneous effects. This is especially true when the set of unit-level attributes is high-dimensional and our priors are weak about which particular covariates are important. However, there are often observational data sets available that are orders of magnitude larger. We propose a method to combine these two data sources to estimate heterogeneous treatment effects. First, we use observational time series data to estimate a mapping from covariates to unit-level effects. These estimates are likely biased but under some conditions the bias preserves unit-level relative rank orderings. If these conditions hold, we only need sufficient experimental data to identify a monotonic, one-dimensional transformation from observationally predicted treatment effects to real treatment effects. This reduces power demands greatly and makes the detection of heterogeneous effects much easier. As an application, we show how our method can be used to improve Facebook page recommendations. "
796163718946181120,2016-11-09 01:33:17,https://t.co/NemmLDHIcV,Learning Dynamic Programming with Split-Merge Networks. (arXiv:1611.02401v1 [cs.LG]) https://t.co/NemmLDHIcV,0,4," Abstract: We consider the learning of algorithmic tasks by mere observation of input-output pairs. Rather than studying this as a black-box discrete regression problem with no assumption whatsoever on the input-output mapping, we concentrate on tasks that are amenable to the principle of \emph{divide and conquer}, and study what are its implications in terms of learning. This principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to \emph{split} a given input into two disjoint sets, and how to \emph{merge} two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner. As a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls. "
796163716786044928,2016-11-09 01:33:16,https://t.co/9dMnmRQpcZ,A Bayesian optimization approach to find Nash equilibria. (arXiv:1611.02440v1 [stat.ML]) https://t.co/9dMnmRQpcZ,1,4," Abstract: Game theory finds nowadays a broad range of applications in engineering and machine learning. However, in a derivative-free, expensive black-box context, very few algorithmic solutions are available to find game equilibria. Here, we propose a novel Gaussian-process based approach for solving games in this context. We follow a classical Bayesian optimization framework, with sequential sampling decisions based on acquisition functions. Two strategies are proposed, based either on the probability of achieving equilibrium or on the Stepwise Uncertainty Reduction paradigm. Practical and numerical aspects are discussed in order to enhance the scalability and reduce computation time. Our approach is evaluated on several synthetic game problems with varying number of players and decision space dimensions. We show that equilibria can be found reliably for a fraction of the cost (in terms of black-box evaluations) compared to classical, derivative-based algorithms. "
796163715053875200,2016-11-09 01:33:16,https://t.co/X2I0Yur9k2,Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders. (arXiv:1611.02648v1 [cs.LG]) https://t.co/X2I0Yur9k2,2,18," Abstract: We study a variant of the variational autoencoder model with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the standard variational approach in these models is unsuited for unsupervised clustering, and mitigate this problem by leveraging a principled information-theoretic regularisation term known as consistency violation. Adding this term to the standard variational optimisation objective yields networks with both meaningful internal representations and well-defined clusters. We demonstrate the performance of this scheme on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving higher performance on unsupervised clustering classification than previous approaches. "
796163713300623361,2016-11-09 01:33:15,https://t.co/rVJbX8d0PP,Dependence Measure for non-additive model. (arXiv:1310.1562v4 [stat.ML] UPDATED) https://t.co/rVJbX8d0PP,0,1," Abstract: We proposed a new statistical dependency measure called Copula Dependency Coefficient(CDC) for two sets of variables based on copula. It is robust to outliers, easy to implement, powerful and appropriate to high-dimensional variables. These properties are important in many applications. Experimental results show that CDC can detect the dependence between variables in both additive and non-additive models. "
796163711648104448,2016-11-09 01:33:15,https://t.co/sfOYQNEdH1,On the Prediction Performance of the Lasso. (arXiv:1402.1700v2 [math.ST] UPDATED) https://t.co/sfOYQNEdH1,0,3," Abstract: Although the Lasso has been extensively studied, the relationship between its prediction performance and the correlations of the covariates is not fully understood. In this paper, we give new insights into this relationship in the context of multiple linear regression. We show, in particular, that the incorporation of a simple correlation measure into the tuning parameter can lead to a nearly optimal prediction performance of the Lasso even for highly correlated covariates. However, we also reveal that for moderately correlated covariates, the prediction performance of the Lasso can be mediocre irrespective of the choice of the tuning parameter. We finally show that our results also lead to near-optimal rates for the least-squares estimator with total variation penalty. "
796163709794127872,2016-11-09 01:33:15,https://t.co/fDIelxEgRB,SGD with Variance Reduction beyond Empirical Risk Minimization. (arXiv:1510.04822v3 [stat.ML] UPDATED) https://t.co/fDIelxEgRB,0,5," Abstract: We introduce a doubly stochastic proximal gradient algorithm for optimizing a finite average of smooth convex functions, whose gradients depend on numerically expensive expectations. Our main motivation is the acceleration of the optimization of the regularized Cox partial-likelihood (the core model used in survival analysis), but our algorithm can be used in different settings as well. The proposed algorithm is doubly stochastic in the sense that gradient steps are done using stochastic gradient descent (SGD) with variance reduction, where the inner expectations are approximated by a Monte-Carlo Markov-Chain (MCMC) algorithm. We derive conditions on the MCMC number of iterations guaranteeing convergence, and obtain a linear rate of convergence under strong convexity and a sublinear rate without this assumption. We illustrate the fact that our algorithm improves the state-of-the-art solver for regularized Cox partial-likelihood on several datasets from survival analysis. "
796163706946256896,2016-11-09 01:33:14,https://t.co/HFIeT9vUNn,Wavelet Scattering Regression of Quantum Chemical Energies. (arXiv:1605.04654v2 [math.CA] UPDATED) https://t.co/HFIeT9vUNn,0,4," Abstract: We introduce multiscale invariant dictionaries to estimate quantum chemical energies of organic molecules, from training databases. Molecular energies are invariant to isometric atomic displacements, and are Lipschitz continuous to molecular deformations. Similarly to density functional theory (DFT), the molecule is represented by an electronic density function. A multiscale invariant dictionary is calculated with wavelet scattering invariants. It cascades a first wavelet transform which separates scales, with a second wavelet transform which computes interactions across scales. Sparse scattering regressions give state of the art results over two databases of organic planar molecules. On these databases, the regression error is of the order of the error produced by DFT codes, but at a fraction of the computational cost. "
796163704802906113,2016-11-09 01:33:13,https://t.co/t2Jx21RH8m,NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and Stochastic Optimization. (arXiv:1605.07747v2 [… https://t.co/t2Jx21RH8m,0,3," Abstract: We study a stochastic and distributed algorithm for nonconvex problems whose objective consists of a sum of $N$ nonconvex $L_i/N$-smooth functions, plus a nonsmooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT) algorithm splits the problem into $N$ subproblems, and utilizes an augmented Lagrangian based primal-dual scheme to solve it in a distributed and stochastic manner. With a special non-uniform sampling, a version of NESTT achieves $\epsilon$-stationary solution using $\mathcal{O}((\sum_{i=1}^N\sqrt{L_i/N})^2/\epsilon)$ gradient evaluations, which can be up to $\mathcal{O}(N)$ times better than the (proximal) gradient descent methods. It also achieves Q-linear convergence rate for nonconvex $\ell_1$ penalized quadratic problems with polyhedral constraints. Further, we reveal a fundamental connection between primal-dual based methods and a few primal only methods such as IAG/SAG/SAGA. "
796163702990970880,2016-11-09 01:33:13,https://t.co/aJtSEu1vl0,Safe and Efficient Off-Policy Reinforcement Learning. (arXiv:1606.02647v2 [cs.LG] UPDATED) https://t.co/aJtSEu1vl0,0,5," Abstract: In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace($\lambda$), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of ""off-policyness""; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to $Q^*$ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q($\lambda$), which was an open problem since 1989. We illustrate the benefits of Retrace($\lambda$) on a standard suite of Atari 2600 games. "
796163700906455040,2016-11-09 01:33:12,https://t.co/3jcVhvTdo4,On the prediction loss of the lasso in the partially labeled setting. (arXiv:1606.06179v2 [math.ST] UPDATED) https://t.co/3jcVhvTdo4,0,3," Abstract: In this paper we revisit the risk bounds of the lasso estimator in the context of transductive and semi-supervised learning. In other terms, the setting under consideration is that of regression with random design under partial labeling. The main goal is to obtain user-friendly bounds on the off-sample prediction risk. To this end, the simple setting of bounded response variable and bounded (high-dimensional) covariates is considered. We propose some new adaptations of the lasso to these settings and establish oracle inequalities both in expectation and in deviation. These results provide non-asymptotic upper bounds on the risk that highlight the interplay between the bias due to the mis-specification of the linear model, the bias due to the approximate sparsity and the variance. They also demonstrate that the presence of a large number of unlabeled features may have significant positive impact in the situations where the restricted eigenvalue of the design matrix vanishes or is very small. "
796163698905718785,2016-11-09 01:33:12,https://t.co/VFJETOclb4,"Graphons, mergeons, and so on!. (arXiv:1607.01718v3 [stat.ML] UPDATED) https://t.co/VFJETOclb4",1,3," Abstract: In this work we develop a theory of hierarchical clustering for graphs. Our modeling assumption is that graphs are sampled from a graphon, which is a powerful and general model for generating graphs and analyzing large networks. Graphons are a far richer class of graph models than stochastic blockmodels, the primary setting for recent progress in the statistical theory of graph clustering. We define what it means for an algorithm to produce the ""correct"" clustering, give sufficient conditions in which a method is statistically consistent, and provide an explicit algorithm satisfying these properties. "
796163695894233088,2016-11-09 01:33:11,https://t.co/PLneHivFvY,Revisiting Role Discovery in Networks: From Node to Edge Roles. (arXiv:1610.00844v2 [stat.ML] UPDATED) https://t.co/PLneHivFvY,0,2," Abstract: Previous work in network analysis has focused on modeling the mixed-memberships of node roles in the graph, but not the roles of edges. We introduce the edge role discovery problem and present a generalizable framework for learning and extracting edge roles from arbitrary graphs automatically. Furthermore, while existing node-centric role models have mainly focused on simple degree and egonet features, this work also explores graphlet features for role discovery. In addition, we also develop an approach for automatically learning and extracting important and useful edge features from an arbitrary graph. The experimental results demonstrate the utility of edge roles for network analysis tasks on a variety of graphs from various problem domains. "
795802729411125248,2016-11-08 01:38:50,https://t.co/X418JeZF4l,Algorithms for Fitting the Constrained Lasso. (arXiv:1611.01511v1 [stat.ML]) https://t.co/X418JeZF4l,0,1," Abstract: We compare alternative computing strategies for solving the constrained lasso problem. As its name suggests, the constrained lasso extends the widely-used lasso to handle linear constraints, which allow the user to incorporate prior information into the model. In addition to quadratic programming, we employ the alternating direction method of multipliers (ADMM) and also derive an efficient solution path algorithm. Through both simulations and real data examples, we compare the different algorithms and provide practical recommendations in terms of efficiency and accuracy for various sizes of data. We also show that, for an arbitrary penalty matrix, the generalized lasso can be transformed to a constrained lasso, while the converse is not true. Thus, our methods can also be used for estimating a generalized lasso, which has wide-ranging applications. Code for implementing the algorithms is freely available in the Matlab toolbox SparseReg. "
795802727053979648,2016-11-08 01:38:50,https://t.co/alEyTDq9Tp,Topology and Geometry of Deep Rectified Network Optimization Landscapes. (arXiv:1611.01540v1 [stat.ML]) https://t.co/alEyTDq9Tp,3,10," Abstract: The loss surface of deep neural networks has recently attracted interest in the optimization and machine learning communities as a prime example of high-dimensional non-convex problem. Some insights were recently gained using spin glass models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model. In this work, we do not make any such assumption and study conditions on the data distribution and model architecture that prevent the existence of bad local minima. Our theoretical work quantifies and formalizes two important \emph{folklore} facts: (i) the landscape of deep linear networks has a radically different topology from that of deep half-rectified ones, and (ii) that the energy landscape in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. Our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected, and we provide explicit bounds that reveal the aforementioned interplay. The conditioning of gradient descent is the next challenge we address. We study this question through the geometry of the level sets, and we introduce an algorithm to efficiently estimate the regularity of such sets on large-scale networks. Our empirical results show that these level sets remain connected throughout all the learning phase, suggesting a near convex behavior, but they become exponentially more curvy as the energy level decays, in accordance to what is observed in practice with very low curvature attractors. "
795802724231184384,2016-11-08 01:38:49,https://t.co/vq1KPtScRc,Classification with Ultrahigh-Dimensional Features. (arXiv:1611.01541v1 [stat.ML]) https://t.co/vq1KPtScRc,1,5," Abstract: Although much progress has been made in classification with high-dimensional features \citep{Fan_Fan:2008, JGuo:2010, CaiSun:2014, PRXu:2014}, classification with ultrahigh-dimensional features, wherein the features much outnumber the sample size, defies most existing work. This paper introduces a novel and computationally feasible multivariate screening and classification method for ultrahigh-dimensional data. Leveraging inter-feature correlations, the proposed method enables detection of marginally weak and sparse signals and recovery of the true informative feature set, and achieves asymptotic optimal misclassification rates. We also show that the proposed procedure provides more powerful discovery boundaries compared to those in \citet{CaiSun:2014} and \citet{JJin:2009}. The performance of the proposed procedure is evaluated using simulation studies and demonstrated via classification of patients with different post-transplantation renal functional types. "
795802721588744193,2016-11-08 01:38:48,https://t.co/4foKRrvJB9,Class-prior Estimation for Learning from Positive and Unlabeled Data. (arXiv:1611.01586v1 [cs.LG]) https://t.co/4foKRrvJB9,1,2," Abstract: We consider the problem of estimating the class prior in an unlabeled dataset. Under the assumption that an additional labeled dataset is available, the class prior can be estimated by fitting a mixture of class-wise data distributions to the unlabeled data distribution. However, in practice, such an additional labeled dataset is often not available. In this paper, we show that, with additional samples coming only from the positive class, the class prior of the unlabeled dataset can be estimated correctly. Our key idea is to use properly penalized divergences for model fitting to cancel the error caused by the absence of negative samples. We further show that the use of the penalized $L_1$-distance gives a computationally efficient algorithm with an analytic solution. The consistency, stability, and estimation error are theoretically analyzed. Finally, we experimentally demonstrate the usefulness of the proposed method. "
795802718594072581,2016-11-08 01:38:48,https://t.co/iU06XKxtPn,Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening. (arXiv:1611.01606v1 [cs.LG… https://t.co/iU06XKxtPn,0,9," Abstract: We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time. We evaluate the performance of our approach on the 49 games of the challenging Arcade Learning Environment, and report significant improvements in both training time and accuracy. "
795802715196719104,2016-11-08 01:38:47,https://t.co/ef9g1d7sM7,PGQ: Combining policy gradient and Q-learning. (arXiv:1611.01626v1 [cs.LG]) https://t.co/ef9g1d7sM7,2,7," Abstract: Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as 'PGQ', for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQ. In particular, we tested PGQ on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning. "
795802709811228672,2016-11-08 01:38:45,https://t.co/pB8pvIZy7R,TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency. (arXiv:1611.01702v1 [cs.CL]) https://t.co/pB8pvIZy7R,0,12," Abstract: In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence - both semantic and syntactic - but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis and report a new state-of-the-art error rate on the IMDB movie review dataset that amounts to a $13.3\%$ improvement over the previous best result. Finally TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation. "
795802706678022144,2016-11-08 01:38:45,https://t.co/3mOq0cQGlF,"Detecting Dependencies in High-Dimensional, Sparse Databases Using Probabilistic Programming and Non-parametric Ba… https://t.co/3mOq0cQGlF",0,5," Abstract: Sparse databases with hundreds of variables are commonplace. In this setting, it is both statistically and computationally challenging to detect true predictive relationships between variables and also to suppress false positives. This paper proposes a new approach to dependency detection that combines probabilistic programming, information theory, and non-parametric Bayesian modeling. The key ideas are to (i) build an ensemble of joint probability models for the whole database via approximate posterior inference in CrossCat, a non-parametric factorial mixture; (ii) identify independencies by analyzing model structures; and (iii) report the distribution on conditional mutual information induced by posterior uncertainty over the ensemble of models. This paper presents experiments showing that the approach finds relationships that pairwise correlation misses, including context-specific independencies, on databases of mathematics exam scores and global indicators of macroeconomic development. "
795802703452585984,2016-11-08 01:38:44,https://t.co/FBtJjIMDc8,Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning. (arXiv:1611.01722… https://t.co/FBtJjIMDc8,0,1," Abstract: We propose a simple algorithm to train stochastic neural networks to draw samples from given target distributions for probabilistic inference. Our method is based on iteratively adjusting the neural network parameters so that the output changes along a Stein variational gradient that maximumly decreases the KL divergence with the target distribution. Our method works for any target distribution specified by their unnormalized density function, and can train any black-box architectures that are differentiable in terms of the parameters we want to adapt. As an application of our method, we propose an amortized MLE algorithm for training deep energy model, where a neural sampler is adaptively trained to approximate the likelihood function. Our method mimics an adversarial game between the deep energy model and the neural sampler, and obtains realistic-looking images competitive with the state-of-the-art results. "
795802700994805760,2016-11-08 01:38:43,https://t.co/RYC8XEwlbv,EM Algorithm and Stochastic Control in Economics. (arXiv:1611.01767v1 [q-fin.EC]) https://t.co/RYC8XEwlbv,0,3," Abstract: Generalising the idea of the classical EM algorithm that is widely used for computing maximum likelihood estimates, we propose an EM-Control (EM-C) algorithm for solving multi-period finite time horizon stochastic control problems. The new algorithm sequentially updates the control policies in each time period using Monte Carlo simulation in a forward-backward manner; in other words, the algorithm goes forward in simulation and backward in optimization in each iteration. Similar to the EM algorithm, the EM-C algorithm has the monotonicity of performance improvement in each iteration, leading to good convergence properties. We demonstrate the effectiveness of the algorithm by solving stochastic control problems in the monopoly pricing of perishable assets and in the study of real business cycle. "
795802698226470913,2016-11-08 01:38:43,https://t.co/2u0WTH3Tal,Learning to Perform Physics Experiments via Deep Reinforcement Learning. (arXiv:1611.01843v1 [stat.ML]) https://t.co/2u0WTH3Tal,0,2," Abstract: When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations. "
795802695974141957,2016-11-08 01:38:42,https://t.co/f3jJZfmWDs,Urban Distribution Grid Topology Estimation via Group Lasso. (arXiv:1611.01845v1 [stat.ML]) https://t.co/f3jJZfmWDs,0,2," Abstract: The growing penetration of distributed energy resources (DERs) in urban areas raises multiple reliability issues. The topology reconstruction is a critical step to ensure the robustness of distribution grid operation. However, the bus connectivity and network topology reconstruction are hard in distribution grids. The reasons are that 1) the branches are challenging and expensive to monitor due to underground setup; 2) the inappropriate assumption of radial topology in many studies that urban grids are mesh. To address these drawbacks, we propose a new data-driven approach to reconstruct distribution grid topology by utilizing the newly available smart meter data. Specifically, a graphical model is built to model the probabilistic relationships among different voltage measurements. With proof, the bus connectivity and topology estimation problems are formulated as a linear regression problem with least absolute shrinkage on grouped variables (Group Lasso) to deal with meshed network structures. Simulation results show highly accurate estimation in IEEE standard distribution test systems with and without loops using real smart meter data. "
795802693751152640,2016-11-08 01:38:42,https://t.co/3evxeF5rBt,An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax. (arXiv… https://t.co/3evxeF5rBt,1,6," Abstract: A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. From the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, allowing complete, overcomplete, or undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from image datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. "
795802690668429312,2016-11-08 01:38:41,https://t.co/XGmq3vtIT8,Joint Multimodal Learning with Deep Generative Models. (arXiv:1611.01891v1 [stat.ML]) https://t.co/XGmq3vtIT8,2,8," Abstract: We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally. "
795802687031873536,2016-11-08 01:38:40,https://t.co/UY8t6D65yT,Optimal rates for the regularized learning algorithms under general source condition. (arXiv:1611.01900v1 [stat.ML… https://t.co/UY8t6D65yT,0,2, Abstract: We consider the learning algorithms under general source condition with the polynomial decay of the eigenvalues of the integral operator in vector-valued function setting. We discuss the upper convergence rates of Tikhonov regularizer under general source condition corresponding to increasing monotone index function. The convergence issues are studied for general regularization schemes by using the concept of operator monotone index functions in minimax setting. Further we also address the minimum possible error for any learning algorithm. 
795802683307393024,2016-11-08 01:38:39,https://t.co/BNxeysXvsh,Deep Reinforcement Learning with Averaged Target DQN. (arXiv:1611.01929v1 [cs.AI]) https://t.co/BNxeysXvsh,1,8," Abstract: The commonly used Q-learning algorithm combined with function approximation induces systematic overestimations of state-action values. These systematic errors might cause instability, poor performance and sometimes divergence of learning. In this work, we present the Averaged Target DQN (ADQN) algorithm, an adaptation to the DQN class of algorithms which uses a weighted average over past learned networks to reduce generalization noise variance. As a consequence, this leads to reduced overestimations, more stable learning process and improved performance. Additionally, we analyze ADQN variance reduction along trajectories and demonstrate the performance of ADQN on a toy Gridworld problem, as well as on several of the Atari 2600 games from the Arcade Learning Environment. "
795802680178470912,2016-11-08 01:38:38,https://t.co/LJRk83HsEi,Linear Convergence of SVRG in Statistical Estimation. (arXiv:1611.01957v1 [stat.ML]) https://t.co/LJRk83HsEi,0,3," Abstract: SVRG and its variants are among the state of art optimization algorithms for the large scale machine learning problem. It is well known that SVRG converges linearly when the objective function is strongly convex. However this setup does not include several important formulations such as Lasso, group Lasso, logistic regression, among others. In this paper, we prove that, for a class of statistical M-estimators where {\em strong convexity does not hold}, SVRG can solve the formulation with {\em a linear convergence rate}. Our analysis makes use of {\em restricted strong convexity}, under which we show that SVRG converges linearly to the fundamental statistical precision of the model, i.e., the difference between true unknown parameter $\theta^*$ and the optimal solution $\hat{\theta}$ of the model. This improves previous convergence analysis on the non-strongly convex setup that achieves sub-linear convergence rate. "
795802676508430336,2016-11-08 01:38:38,https://t.co/YWMki9VF5r,One Class Splitting Criteria for Random Forests. (arXiv:1611.01971v1 [stat.ML]) https://t.co/YWMki9VF5r,3,3," Abstract: Random Forests (RFs) are strong machine learning tools for classification and regression. However, they remain supervised algorithms, and no extension of RFs to the one-class setting has been proposed, except for techniques based on second-class sampling. This work fills this gap by proposing a natural methodology to extend standard splitting criteria to the one-class setting, structurally generalizing RFs to one-class classification. An extensive benchmark of seven state-of-the-art anomaly detection algorithms is also presented. This empirically demonstrates the relevance of our approach. "
795802672918102017,2016-11-08 01:38:37,https://t.co/7z4iDUIjFc,Convergence Analysis of Distributed Inference with Vector-Valued Gaussian Belief Propagation. (arXiv:1611.02010v1 … https://t.co/7z4iDUIjFc,0,2," Abstract: In networks such as the smart grid, communication networks, and social networks, local measurements/observations are scattered over a wide geographical area. Centralized inference algorithm are based on gathering all the observations at a central processing unit. However, with data explosion and ever-increasing network sizes, centralized inference suffers from large communication overhead, heavy computation burden at the center, and susceptibility to central node failure. This paper considers inference over networks using factor graphs and a distributed inference algorithm based on Gaussian belief propagation. The distributed inference involves only local computation of the information matrix and of the mean vector and message passing between neighbors. We discover and show analytically that the message information matrix converges exponentially fast to a unique positive definite limit matrix for arbitrary positive semidefinite initialization. We provide the necessary and sufficient convergence condition for the belief mean vector to converge to the optimal centralized estimator. An easily verifiable sufficient convergence condition on the topology of a factor graph is further provided. "
795802669327810560,2016-11-08 01:38:36,https://t.co/acQRERdxQ6,Robust supervised learning under uncertainty in dataset shift. (arXiv:1611.02041v1 [stat.ML]) https://t.co/acQRERdxQ6,0,2," Abstract: When machine learning is deployed in the real world, its performance can be significantly undermined because test data may follow a different distribution from training data. To build a reliable machine learning system in such a scenario, we propose a supervised learning framework that is explicitly robust to the uncertainty of dataset shift. Our robust learning framework is flexible in modeling various dataset shift scenarios. It is also computationally efficient in that it acts as a robust wrapper around existing gradient-based supervised learning algorithms, while adding negligible computational overheads. We discuss practical considerations in robust supervised learning and show the effectiveness of our approach on both synthetic and benchmark datasets. "
795802666475679744,2016-11-08 01:38:35,https://t.co/zEN7oPVOc4,Reinforcement Learning Approach for Parallelization in Filters Aggregation Based Feature Selection Algorithms. (ar… https://t.co/zEN7oPVOc4,0,2," Abstract: One of the classical problems in machine learning and data mining is feature selection. A feature selection algorithm is expected to be quick, and at the same time it should show high performance. MeLiF algorithm effectively solves this problem using ensembles of ranking filters. This article describes two different ways to improve MeLiF algorithm performance with parallelization. Experiments show that proposed schemes significantly improves algorithm performance and increase feature selection quality. "
795802663057326080,2016-11-08 01:38:34,https://t.co/FFuFoyv6je,Reinforcement-based Simultaneous Algorithm and its Hyperparameters Selection. (arXiv:1611.02053v1 [cs.LG]) https://t.co/FFuFoyv6je,0,4," Abstract: Many algorithms for data analysis exist, especially for classification problems. To solve a data analysis problem, a proper algorithm should be chosen, and also its hyperparameters should be selected. In this paper, we present a new method for the simultaneous selection of an algorithm and its hyperparameters. In order to do so, we reduced this problem to the multi-armed bandit problem. We consider an algorithm as an arm and algorithm hyperparameters search during a fixed time as the corresponding arm play. We also suggest a problem-specific reward function. We performed the experiments on 10 real datasets and compare the suggested method with the existing one implemented in Auto-WEKA. The results show that our method is significantly better in most of the cases and never worse than the Auto-WEKA. "
795802659374690304,2016-11-08 01:38:33,https://t.co/Wgpd57M8HI,Distributed Coordinate Descent for Generalized Linear Models with Regularization. (arXiv:1611.02101v1 [stat.ML]) https://t.co/Wgpd57M8HI,1,5," Abstract: Generalized linear model with $L_1$ and $L_2$ regularization is a widely used technique for solving classification, class probability estimation and regression problems. With the numbers of both features and examples growing rapidly in the fields like text mining and clickstream data analysis parallelization and the use of cluster architectures becomes important. We present a novel algorithm for fitting regularized generalized linear models in the distributed environment. The algorithm splits data between nodes by features, uses coordinate descent on each node and line search to merge results globally. Convergence proof is provided. A modifications of the algorithm addresses slow node problem. For an important particular case of logistic regression we empirically compare our program with several state-of-the art approaches that rely on different algorithmic and data spitting methods. Experiments demonstrate that our approach is scalable and superior when training on large and sparse datasets. "
795802655599763456,2016-11-08 01:38:33,https://t.co/T28BOAhJdo,Unrolled Generative Adversarial Networks. (arXiv:1611.02163v1 [cs.LG]) https://t.co/T28BOAhJdo,0,7," Abstract: We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator. "
795802651640348672,2016-11-08 01:38:32,https://t.co/3fg4sFoggR,Using Social Dynamics to Make Individual Predictions: Variational Inference with a Stochastic Kinetic Model. (arXi… https://t.co/3fg4sFoggR,0,2," Abstract: Social dynamics is concerned primarily with interactions among individuals and the resulting group behaviors, modeling the temporal evolution of social systems via the interactions of individuals within these systems. In particular, the availability of large-scale data from social networks and sensor networks offers an unprecedented opportunity to predict state-changing events at the individual level. Examples of such events include disease transmission, opinion transition in elections, and rumor propagation. Unlike previous research focusing on the collective effects of social systems, this study makes efficient inferences at the individual level. In order to cope with dynamic interactions among a large number of individuals, we introduce the stochastic kinetic model to capture adaptive transition probabilities and propose an efficient variational inference algorithm the complexity of which grows linearly --- rather than exponentially --- with the number of individuals. To validate this method, we have performed epidemic-dynamics experiments on wireless sensor network data collected from more than ten thousand people over three years. The proposed algorithm was used to track disease transmission and predict the probability of infection for each individual. Our results demonstrate that this method is more efficient than sampling while nonetheless achieving high accuracy. "
795802648809263104,2016-11-08 01:38:31,https://t.co/5ginyY3aOr,Minimax-optimal semi-supervised regression on unknown manifolds. (arXiv:1611.02221v1 [stat.ML]) https://t.co/5ginyY3aOr,1,3," Abstract: We consider the problem of semi-supervised regression when the predictor variables are drawn from an unknown manifold. A simple approach to this problem is to first use both the labeled and unlabeled data to estimate the manifold geodesic distance between pairs of points, and then apply a k nearest neighbor regressor based on these distance estimates. We prove that given sufficiently many unlabeled points, this simple method which we dub geodesic kNN regression, achieves the optimal finite-sample minimax bound on the mean squared error, as if the manifold were completely specified. Furthermore, we show how this approach can be efficiently implemented requiring only O(k N log N) operations to estimate the regression function at all N labeled and unlabeled points. We illustrate this approach on two datasets with a manifold structure: indoor localization using WiFi fingerprints and facial pose estimation. For both problems we obtain better results than the popular procedure based on Laplacian eigenvectors. "
795802644493336577,2016-11-08 01:38:30,https://t.co/6HwkRBs8Zp,Hierarchical compositional feature learning. (arXiv:1611.02252v1 [cs.LG]) https://t.co/6HwkRBs8Zp,0,5," Abstract: We introduce the hierarchical compositional network (HCN), a directed generative model able to discover and disentangle, without supervision, the building blocks of a set of binary images. The building blocks are binary features defined hierarchically as a composition of some of the features in the layer immediately below, arranged in a particular manner. At a high level, HCN is similar to a sigmoid belief network with pooling. Inference and learning in HCN are very challenging and existing variational approximations do not work satisfactorily. A main contribution of this work is to show that both can be addressed using max-product message passing (MPMP) with a particular schedule (no EM required). Also, using MPMP as an inference engine for HCN makes new tasks simple: adding supervision information, classifying images, or performing inpainting all correspond to clamping some variables of the model to their known values and running MPMP on the rest. When used for classification, fast inference with HCN has exactly the same functional form as a convolutional neural network (CNN) with linear activations and binary weights. However, HCN's features are qualitatively very different. "
795802641066557441,2016-11-08 01:38:29,https://t.co/3MduUGz0BP,Learning Time Series Detection Models from Temporally Imprecise Labels. (arXiv:1611.02258v1 [stat.ML]) https://t.co/3MduUGz0BP,0,2," Abstract: In this paper, we consider a new low-quality label learning problem: learning time series detection models from temporally imprecise labels. In this problem, the data consist of a set of input time series, and supervision is provided by a sequence of noisy time stamps corresponding to the occurrence of positive class events. Such temporally imprecise labels commonly occur in areas like mobile health research where human annotators are tasked with labeling the occurrence of very short duration events. We propose a general learning framework for this problem that can accommodate different base classifiers and noise models. We present results on real mobile health data showing that the proposed framework significantly outperforms a number of alternatives including assuming that the label time stamps are noise-free, transforming the problem into the multiple instance learning framework, and learning on labels that were manually re-aligned. "
795802637526507520,2016-11-08 01:38:28,https://t.co/NkYO8iyiyn,Gaussian Attention Model and Its Application to Knowledgebase Embedding and Question Answering. (arXiv:1611.02266v… https://t.co/NkYO8iyiyn,0,3," Abstract: We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to blurred attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledgebase into a continuous vector space and then train a model that performs question answering about the entities in the knowledgebase. The proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of thoughts in a natural way. On a dataset of soccer players who participated in the FIFA World Cup 2014, we demonstrate that our model can handle both path queries and conjunctive queries well. "
795802635416850434,2016-11-08 01:38:28,https://t.co/yYzHA7CGFZ,Optimal Binary Autoencoding with Pairwise Correlations. (arXiv:1611.02268v1 [cs.LG]) https://t.co/yYzHA7CGFZ,0,7," Abstract: We formulate learning of a binary autoencoder as a biconvex optimization problem which learns from the pairwise correlations between encoded and decoded bits. Among all possible algorithms that use this information, ours finds the autoencoder that reconstructs its inputs with worst-case optimal loss. The optimal decoder is a single layer of artificial neurons, emerging entirely from the minimax loss minimization, and with weights learned by convex optimization. All this is reflected in competitive experimental results, demonstrating that binary autoencoding can be done efficiently by conveying information in pairwise correlations in an optimal fashion. "
795438972298285061,2016-11-07 01:33:24,https://t.co/RfF8Z74rX5,PrivLogit: Efficient Privacy-preserving Logistic Regression by Tailoring Numerical Optimizers. (arXiv:1611.01170v1… https://t.co/RfF8Z74rX5,2,7," Abstract: Safeguarding privacy in machine learning is highly desirable, especially in collaborative studies across many organizations. Privacy-preserving distributed machine learning (based on cryptography) is popular to solve the problem. However, existing cryptographic protocols still incur excess computational overhead. Here, we make a novel observation that this is partially due to naive adoption of mainstream numerical optimization (e.g., Newton method) and failing to tailor for secure computing. This work presents a contrasting perspective: customizing numerical optimization specifically for secure settings. We propose a seemingly less-favorable optimization method that can in fact significantly accelerate privacy-preserving logistic regression. Leveraging this new method, we propose two new secure protocols for conducting logistic regression in a privacy-preserving and distributed manner. Extensive theoretical and empirical evaluations prove the competitive performance of our two secure proposals while without compromising accuracy or privacy: with speedup up to 2.3x and 8.1x, respectively, over state-of-the-art; and even faster as data scales up. Such drastic speedup is on top of and in addition to performance improvements from existing (and future) state-of-the-art cryptography. Our work provides a new way towards efficient and practical privacy-preserving logistic regression for large-scale studies which are common for modern science. "
795438969743933442,2016-11-07 01:33:23,https://t.co/FnzDmv1SRp,Adaptive Geometric Multiscale Approximations for Intrinsically Low-dimensional Data. (arXiv:1611.01179v1 [stat.ML]) https://t.co/FnzDmv1SRp,0,4," Abstract: We consider the problem of efficiently approximating and encoding high-dimensional data sampled from a probability distribution $\rho$ in $\mathbb{R}^D$, that is nearly supported on a $d$-dimensional set $\mathcal{M}$ - for example supported on a $d$-dimensional Riemannian manifold. Geometric Multi-Resolution Analysis (GMRA) provides a robust and computationally efficient procedure to construct low-dimensional geometric approximations of $\mathcal{M}$ at varying resolutions. We introduce a thresholding algorithm on the geometric wavelet coefficients, leading to what we call adaptive GMRA approximations. We show that these data-driven, empirical approximations perform well, when the threshold is chosen as a suitable universal function of the number of samples $n$, on a wide variety of measures $\rho$, that are allowed to exhibit different regularity at different scales and locations, thereby efficiently encoding data from more complex measures than those supported on manifolds. These approximations yield a data-driven dictionary, together with a fast transform mapping data to coefficients, and an inverse of such a map. The algorithms for both the dictionary construction and the transforms have complexity $C n \log n$ with the constant linear in $D$ and exponential in $d$. Our work therefore establishes adaptive GMRA as a fast dictionary learning algorithm with approximation guarantees. We include several numerical experiments on both synthetic and real data, confirming our theoretical results and demonstrating the effectiveness of adaptive GMRA. "
795438967650926593,2016-11-07 01:33:23,https://t.co/jUXxyZPw1E,Demystifying ResNet. (arXiv:1611.01186v1 [cs.NE]) https://t.co/jUXxyZPw1E,2,20," Abstract: We provide a theoretical explanation for the superb performance of ResNet via the study of deep linear networks and some nonlinear variants. We show that with or without nonlinearities, by adding shortcuts that have depth two, the condition number of the Hessian of the loss function at the zero initial point is depth-invariant, which makes training very deep models no more difficult than shallow ones. Shortcuts of higher depth result in an extremely flat (high-order) stationary point initially, from which the optimization algorithm is hard to escape. The 1-shortcut, however, is essentially equivalent to no shortcuts. Extensive experiments are provided accompanying our theoretical results. We show that initializing the network to small weights with 2-shortcuts achieves significantly better results than random Gaussian (Xavier) initialization, orthogonal initialization, and shortcuts of deeper depth, from various perspectives ranging from final loss, learning dynamics and stability, to the behavior of the Hessian along the learning process. "
795438965587378176,2016-11-07 01:33:22,https://t.co/ZBxfIDHo82,Combating Reinforcement Learning's Sisyphean Curse with Intrinsic Fear. (arXiv:1611.01211v1 [cs.LG]) https://t.co/ZBxfIDHo82,0,7," Abstract: To use deep reinforcement learning in the wild, we might hope for an agent that would never make catastrophic mistakes. At the very least, we could hope that an agent would eventually learn to avoid old mistakes. Unfortunately, even in simple environments, modern deep reinforcement learning techniques are doomed by a Sisyphean curse. Owing to the use of function approximation, these agents eventually forget experiences as they become exceedingly unlikely under a new policy. Consequently, for as long as they continue to train, state-aggregating agents may periodically relive catastrophic mistakes. We demonstrate unacceptable performance of deep Q-networks on two toy problems. We then introduce intrinsic fear, a method that mitigates these problems by avoiding dangerous states. "
795438963645435904,2016-11-07 01:33:22,https://t.co/6t73leHegi,Statistical Inverse Formulation of Optical Flow with Uncertainty Quantification. (arXiv:1611.01230v1 [stat.ML]) https://t.co/6t73leHegi,0,5," Abstract: Optical flow refers to the visual motion observed between two consecutive images. Since the degree of freedom is typically much larger than the constraints imposed by the image observations, the straightforward formulation of optical flow inference is an ill-posed problem. By setting some type of additional ""regularity"" constraints, classical approaches formulate a well-posed optical flow inference problem in the form of a parameterized set of variational equations. In this work we build a mathematical connection, focused on optical flow methods, between classical variational optical flow approaches and Bayesian statistical inversion. A classical optical flow solution is in fact identical to a maximum a posteriori estimator under the assumptions of linear model with additive independent Gaussian noise and a Gaussian prior distribution. Unlike classical approaches, the statistical inversion approach to optical flow estimation not only allows for ""point"" estimates, but also provides a distribution of solutions which can be used for ensemble estimation and in particular uncertainty quantification. "
795438961934143489,2016-11-07 01:33:21,https://t.co/jpxbYLpVlb,Deep Information Propagation. (arXiv:1611.01232v1 [stat.ML]) https://t.co/jpxbYLpVlb,15,46," Abstract: We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively. "
795438959669219332,2016-11-07 01:33:21,https://t.co/IQ4hqmYnTz,Adversarial Machine Learning at Scale. (arXiv:1611.01236v1 [cs.CV]) https://t.co/IQ4hqmYnTz,0,11," Abstract: Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process. "
795438957362368512,2016-11-07 01:33:20,https://t.co/HM7iUUZ3I3,Reparameterization trick for discrete variables. (arXiv:1611.01239v1 [stat.ML]) https://t.co/HM7iUUZ3I3,15,58," Abstract: Low-variance gradient estimation is crucial for learning directed graphical models parameterized by neural networks, where the reparameterization trick is widely used for those with continuous variables. While this technique gives low-variance gradient estimates, it has not been directly applicable to discrete variables, the sampling of which inherently requires discontinuous operations. We argue that the discontinuity can be bypassed by marginalizing out the variable of interest, which results in a new reparameterization trick for discrete variables. This reparameterization greatly reduces the variance, which is understood by regarding the method as an application of common random numbers to the estimation. The resulting estimator is theoretically guaranteed to have a variance not larger than that of the likelihood-ratio method with the optimal input-dependent baseline. We give empirical results for variational learning of sigmoid belief networks. "
795438954677936130,2016-11-07 01:33:19,https://t.co/X75GjrlDEr,Information Dropout: learning optimal representations through noise. (arXiv:1611.01353v1 [stat.ML]) https://t.co/X75GjrlDEr,9,51," Abstract: We introduce Information Dropout, a generalization of dropout that is motivated by the Information Bottleneck principle and highlights the way in which injecting noise in the activations can help in learning optimal representations of the data. Information Dropout is rooted in information theoretic principles, it includes as special cases several existing dropout methods, like Gaussian Dropout and Variational Dropout, and, unlike classical dropout, it can learn and build representations that are invariant to nuisances of the data, like occlusions and clutter. When the task is the reconstruction of the input, we show that the information dropout method yields a variational autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Our experiments validate the theoretical intuitions behind our method, and we find that information dropout achieves a comparable or better generalization performance than binary dropout, especially on smaller models, since it can automatically adapt the noise to the structure of the network, as well as to the test sample. "
795438951829999616,2016-11-07 01:33:19,https://t.co/VKPx2E8Rk7,Ways of Conditioning Generative Adversarial Networks. (arXiv:1611.01455v1 [cs.LG]) https://t.co/VKPx2E8Rk7,0,8," Abstract: The GANs are generative models whose random samples realistically reflect natural images. It also can generate samples with specific attributes by concatenating a condition vector into the input, yet research on this field is not well studied. We propose novel methods of conditioning generative adversarial networks (GANs) that achieve state-of-the-art results on MNIST and CIFAR-10. We mainly introduce two models: an information retrieving model that extracts conditional information from the samples, and a spatial bilinear pooling model that forms bilinear features derived from the spatial cross product of an image and a condition vector. These methods significantly enhance log-likelihood of test data under the conditional distributions compared to the methods of concatenation. "
795438949451894784,2016-11-07 01:33:18,https://t.co/wLfYuIdyD9,Learning heat diffusion graphs. (arXiv:1611.01456v1 [cs.LG]) https://t.co/wLfYuIdyD9,0,5," Abstract: Effective information analysis generally boils down to properly identifying the structure or geometry of the data, which is often represented by a graph. In some applications, this structure may be partly determined by design constraints or pre-determined sensing arrangements, like in road transportation networks for example. In general though, the data structure is not readily available and becomes pretty difficult to define. In particular, the global smoothness assumptions, that most of the existing works adopt, are often too general and unable to properly capture localized properties of data. In this paper, we go beyond this classical data model and rather propose to represent information as a sparse combination of localized functions that live on a data structure represented by a graph. Based on this model, we focus on the problem of inferring the connectivity that best explains the data samples at different vertices of a graph that is a priori unknown. We concentrate on the case where the observed data is actually the sum of heat diffusion processes, which is a quite common model for data on networks or other irregular structures. We cast a new graph learning problem and solve it with an efficient nonconvex optimization algorithm. Experiments on both synthetic and real world data finally illustrate the benefits of the proposed graph learning framework and confirm that the data structure can be efficiently learned from data observations only. We believe that our algorithm will help solving key questions in diverse application domains such as social and biological network analysis where it is crucial to unveil proper geometry for data understanding and inference. "
795438946922754048,2016-11-07 01:33:18,https://t.co/axQMXwLmCh,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling. (arXiv:1611.01462v1 [cs.LG]) https://t.co/axQMXwLmCh,0,6," Abstract: Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our LSTM model lowers the state of the art word-level perplexity on the Penn Treebank to 68.5. "
795438943902793728,2016-11-07 01:33:17,https://t.co/vZPyF9Xx5F,Why comparing survival curves between two prognostic subgroups may be misleading. (arXiv:1611.01480v1 [stat.ME]) https://t.co/vZPyF9Xx5F,0,1," Abstract: We consider the validation of prognostic diagnostic tests that predict two prognostic subgroups (high-risk vs low-risk) for a given disease or treatment. When comparing survival curves between two prognostic subgroups the possibility of misclassification arises, i.e. a patient predicted as high-risk might be de facto low-risk and vice versa. This is a fundamental difference from comparing survival curves between two populations (e.g. control vs treatment in RCT), where there is not an option of misclassification between members of populations. We show that there is a relationship between prognostic subgroups' survival estimates at a time point and positive and negative predictive values in the classification settings. Consequently, the prevalence needs to be taken into account when validating the survival of prognostic subgroups at a time point. Our findings question current methods of comparing survival curves between prognostic subgroups in the validation set because they do not take into account the survival rates of the population. "
795438941465968640,2016-11-07 01:33:16,https://t.co/zsMXxgTEiS,Understanding Deep Neural Networks with Rectified Linear Units. (arXiv:1611.01491v1 [cs.LG]) https://t.co/zsMXxgTEiS,1,37," Abstract: In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give the first-ever polynomial time (in the size of data) algorithm to train a ReLU DNN with one hidden layer to {\em global optimality}. This follows from our complete characterization of the ReLU DNN function class whereby we show that a $\R^n \to \R$ function is representable by a ReLU DNN {\em if and only if} it is a continuous piecewise linear function. The main tool used to prove this characterization is an elegant result from tropical geometry. Further, for the $n=1$ case, we show that a single hidden layer suffices to express all piecewise linear functions, and we give tight bounds for the size of such a ReLU DNN. We follow up with gap results showing that there is a smoothly parameterized family of $\R\to \R$ ""hard"" functions that lead to an exponential blow-up in size, if the number of layers is decreased by a small amount. An example consequence of our gap theorem is that for every natural number $N$, there exists a function representable by a ReLU DNN with depth $N^2+1$ and total size $N^3$, such that any ReLU DNN with depth at most $N + 1$ will require at least $\frac12N^{N+1}-1$ total nodes. Finally, we construct a family of $\R^n\to \R$ functions for $n\geq 2$ (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension $n$ at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on $n$ has not been achieved by previous families of ""hard"" functions in the neural nets literature. This construction utilizes the theory of zonotopes from polyhedral theory. "
795438938768965633,2016-11-07 01:33:16,https://t.co/2AIAkyQFuL,Estimating Causal Direction and Confounding of Two Discrete Variables. (arXiv:1611.01504v1 [stat.ML]) https://t.co/2AIAkyQFuL,0,2," Abstract: We propose a method to classify the causal relationship between two discrete variables given only the joint distribution of the variables, acknowledging that the method is subject to an inherent baseline error. We assume that the causal system is acyclicity, but we do allow for hidden common causes. Our algorithm presupposes that the probability distributions $P(C)$ of a cause $C$ is independent from the probability distribution $P(E\mid C)$ of the cause-effect mechanism. While our classifier is trained with a Bayesian assumption of flat hyperpriors, we do not make this assumption about our test data. This work connects to recent developments on the identifiability of causal models over continuous variables under the assumption of ""independent mechanisms"". Carefully-commented Python notebooks that reproduce all our experiments are available online at this http URL "
795438936285990912,2016-11-07 01:33:15,https://t.co/h8kag322VK,Contextual Semibandits via Supervised Learning Oracles. (arXiv:1502.05890v4 [cs.LG] UPDATED) https://t.co/h8kag322VK,0,3," Abstract: We study an online decision making problem where on each round a learner chooses a list of items based on some side information, receives a scalar feedback value for each individual item, and a reward that is linearly related to this feedback. These problems, known as contextual semibandits, arise in crowdsourcing, recommendation, and many other domains. This paper reduces contextual semibandits to supervised learning, allowing us to leverage powerful supervised learning methods in this partial-feedback setting. Our first reduction applies when the mapping from feedback to reward is known and leads to a computationally efficient algorithm with near-optimal regret. We show that this algorithm outperforms state-of-the-art approaches on real-world learning-to-rank datasets, demonstrating the advantage of oracle-based algorithms. Our second reduction applies to the previously unstudied setting when the linear mapping from feedback to reward is unknown. Our regret guarantees are superior to prior techniques that ignore the feedback. "
795438934381752320,2016-11-07 01:33:15,https://t.co/Tc97mEMyRh,Exact Inference Techniques for the Analysis of Bayesian Attack Graphs. (arXiv:1510.02427v2 [cs.CR] UPDATED) https://t.co/Tc97mEMyRh,0,1," Abstract: Attack graphs are a powerful tool for security risk assessment by analysing network vulnerabilities and the paths attackers can use to compromise network resources. The uncertainty about the attacker's behaviour makes Bayesian networks suitable to model attack graphs to perform static and dynamic analysis. Previous approaches have focused on the formalization of attack graphs into a Bayesian model rather than proposing mechanisms for their analysis. In this paper we propose to use efficient algorithms to make exact inference in Bayesian attack graphs, enabling the static and dynamic network risk assessments. To support the validity of our approach we have performed an extensive experimental evaluation on synthetic Bayesian attack graphs with different topologies, showing the computational advantages in terms of time and memory use of the proposed techniques when compared to existing approaches. "
795438930795646976,2016-11-07 01:33:14,https://t.co/9S6UatNBxm,Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters. (arXiv:1602.02151v2 [cs.LG] UPDATED) https://t.co/9S6UatNBxm,1,9," Abstract: The amount of data available in the world is growing faster than our ability to deal with it. However, if we take advantage of the internal \emph{structure}, data may become much smaller for machine learning purposes. In this paper we focus on one of the fundamental machine learning tasks, empirical risk minimization (ERM), and provide faster algorithms with the help from the clustering structure of the data. We introduce a simple notion of raw clustering that can be efficiently computed from the data, and propose two algorithms based on clustering information. Our accelerated algorithm ClusterACDM is built on a novel Haar transformation applied to the dual space of the ERM problem, and our variance-reduction based algorithm ClusterSVRG introduces a new gradient estimator using clustering. Our algorithms outperform their classical counterparts ACDM and SVRG respectively. "
795438927830286336,2016-11-07 01:33:13,https://t.co/DdsbDFICbC,Kernel-based Tests for Joint Independence. (arXiv:1603.00285v3 [math.ST] UPDATED) https://t.co/DdsbDFICbC,0,3," Abstract: We investigate the problem of testing whether $d$ random variables, which may or may not be continuous, are jointly (or mutually) independent. Our method builds on ideas of the two variable Hilbert-Schmidt independence criterion (HSIC) but allows for an arbitrary number of variables. We embed the $d$-dimensional joint distribution and the product of the marginals into a reproducing kernel Hilbert space and define the $d$-variable Hilbert-Schmidt independence criterion (dHSIC) as the squared distance between the embeddings. In the population case, the value of dHSIC is zero if and only if the $d$ variables are jointly independent, as long as the kernel is characteristic. Based on an empirical estimate of dHSIC, we define three different non-parametric hypothesis tests: a permutation test, a bootstrap test and a test based on a Gamma approximation. We prove that the permutation test achieves the significance level and that the bootstrap test achieves pointwise asymptotic significance level as well as pointwise asymptotic consistency (i.e., it is able to detect any type of fixed dependence in the large sample limit). The Gamma approximation does not come with these guarantees; however, it is computationally very fast and for small $d$, it performs well in practice. Finally, we apply the test to a problem in causal discovery. "
795438925854679040,2016-11-07 01:33:13,https://t.co/6q8nAfiLPS,Do Deep Convolutional Nets Really Need to be Deep and Convolutional?. (arXiv:1603.05691v3 [stat.ML] UPDATED) https://t.co/6q8nAfiLPS,9,30," Abstract: Yes, they do. This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained. Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution. Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher. "
795438923245912064,2016-11-07 01:33:12,https://t.co/IZWraOymST,Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks. (arXiv:1605.08346v2 [cs.IT] UPDATED) https://t.co/IZWraOymST,0,6," Abstract: Recurrent neural networks (RNNs) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time-varying data processing tasks. To understand the success and limitations of RNNs, it is critical that we advance our analysis of their fundamental memory properties. We focus on echo state networks (ESNs), which are RNNs with simple memoryless nodes and random connectivity. In most existing analyses, the short-term memory (STM) capacity results conclude that the ESN network size must scale linearly with the input size for unstructured inputs. The main contribution of this paper is to provide general results characterizing the STM capacity for linear ESNs with multidimensional input streams when the inputs have common low-dimensional structure: sparsity in a basis or significant statistical dependence between inputs. In both cases, we show that the number of nodes in the network must scale linearly with the information rate and poly-logarithmically with the ambient input dimension. The analysis relies on advanced applications of random matrix theory and results in explicit non-asymptotic bounds on the recovery error. Taken together, this analysis provides a significant step forward in our understanding of the STM properties in RNNs. "
795438919915544576,2016-11-07 01:33:11,https://t.co/BImJXOQkkM,Adversarial Feature Learning. (arXiv:1605.09782v4 [cs.LG] UPDATED) https://t.co/BImJXOQkkM,1,12," Abstract: The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to ""linearize semantics"" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning. "
795438915922653184,2016-11-07 01:33:10,https://t.co/qj4bcJTpKl,"First Efficient Convergence for Streaming k-PCA: a Global, Gap-Free, and Near-Optimal Rate. (arXiv:1607.07837v3 [m… https://t.co/qj4bcJTpKl",0,3," Abstract: We study streaming principal component analysis (PCA), that is to find the top $k$ eigenvectors of a $d\times d$ hidden matrix $\bf \Sigma$ with online vectors drawn from covariance matrix $\bf \Sigma$. We provide GLOBAL convergence for Oja's algorithm which is popularly used in practice but lacks theoretical understanding for $k>1$. We also provide a modified variant $\mathsf{Oja}^{++}$ that runs EVEN FASTER than Oja's. Our results match the information theoretic lower bound in terms of dependency on error, on eigengap, on rank $k$, and on dimension $d$, up to poly-log factors. In addition, our convergence rate can be made gap-free, that is proportional to the approximation error and independent of the eigengap. In contrast, for general rank $k$, before our work (1) it was open to design any algorithm with efficient global convergence rate; and (2) it was open to design any algorithm with (even local) gap-free convergence rate. "
795438912973971456,2016-11-07 01:33:10,https://t.co/4vFlLq6DFq,Learning in Implicit Generative Models. (arXiv:1610.03483v2 [stat.ML] UPDATED) https://t.co/4vFlLq6DFq,1,11," Abstract: Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination. "
795438911078170625,2016-11-07 01:33:09,https://t.co/dd1VmefDGW,Revisiting Classifier Two-Sample Tests. (arXiv:1610.06545v2 [stat.ML] UPDATED) https://t.co/dd1VmefDGW,0,1," Abstract: The goal of two-sample tests is to assess whether two samples, $S_P \sim P^n$ and $S_Q \sim Q^m$, are drawn from the same distribution. Perhaps intriguingly, one relatively unexplored method to build two-sample tests is the use of binary classifiers. In particular, construct a dataset by pairing the $n$ examples in $S_P$ with a positive label, and by pairing the $m$ examples in $S_Q$ with a negative label. If the null hypothesis ""$P = Q$"" is true, then the classification accuracy of a binary classifier on a held-out subset of this dataset should remain near chance-level. As we will show, such Classifier Two-Sample Tests (C2ST) learn a suitable representation of the data on the fly, return test statistics in interpretable units, have a simple null distribution, and their predictive uncertainty allow to interpret where $P$ and $Q$ differ. The goal of this paper is to establish the properties, performance, and uses of C2ST. First, we analyze their main theoretical properties. Second, we compare their performance against a variety of state-of-the-art alternatives. Third, we propose their use to evaluate the sample quality of generative models with intractable likelihoods, such as Generative Adversarial Networks (GANs). Fourth, we showcase the novel application of GANs together with C2ST for causal discovery. "
795438903444590598,2016-11-07 01:33:07,https://t.co/FdQOGALfmq,"C-mix: a high dimensional mixture model for censored durations, with applications to genetic data. (arXiv:1610.074… https://t.co/FdQOGALfmq",0,2," Abstract: We introduce a mixture model for censored durations (C-mix), and develop maximum likelihood inference for the joint estimation of the time distributions and latent regression parameters of the model. We consider a high-dimensional setting, with datasets containing a large number of biomedical covariates. We therefore penalize the negative log-likelihood by the Elastic-Net, which leads to a sparse parameterization of the model. Inference is achieved using an efficient Quasi-Newton Expectation Maximization (QNEM) algorithm, for which we provide convergence properties. We then propose a score by assessing the patients risk of early adverse event. The statistical performance of the method is examined on an extensive Monte Carlo simulation study, and finally illustrated on three genetic datasets with high-dimensional covariates. We show that our approach outperforms the state-of-the-art, namely both the CURE and Cox proportional hazards models for this task, both in terms of C-index and AUC(t). "
794337076883890180,2016-11-04 00:34:51,https://t.co/ZR9nvPkQxN,Cross-validation based Nonlinear Shrinkage. (arXiv:1611.00798v1 [stat.ML]) https://t.co/ZR9nvPkQxN,0,4," Abstract: Many machine learning algorithms require precise estimates of covariance matrices. The sample covariance matrix performs poorly in high-dimensional settings, which has stimulated the development of alternative methods, the majority based on factor models and shrinkage. Recent work of Ledoit and Wolf has extended the shrinkage framework to Nonlinear Shrinkage (NLS), a more powerful covariance estimator based on Random Matrix Theory. Our contribution shows that, contrary to claims in the literature, cross-validation based covariance matrix estimation (CVC) yields comparable performance at strongly reduced complexity and runtime. On two real world data sets, we show that the CVC estimator yields superior results than competing shrinkage and factor based methods. "
794337073767600130,2016-11-04 00:34:51,https://t.co/9EnUvRIlVF,Temporal Matrix Completion with Locally Linear Latent Factors for Medical Applications. (arXiv:1611.00800v1 [cs.LG… https://t.co/9EnUvRIlVF,0,1," Abstract: Regular medical records are useful for medical practitioners to analyze and monitor patient health status especially for those with chronic disease, but such records are usually incomplete due to unpunctuality and absence of patients. In order to resolve the missing data problem over time, tensor-based model is suggested for missing data imputation in recent papers because this approach makes use of low rank tensor assumption for highly correlated data. However, when the time intervals between records are long, the data correlation is not high along temporal direction and such assumption is not valid. To address this problem, we propose to decompose a matrix with missing data into its latent factors. Then, the locally linear constraint is imposed on these factors for matrix completion in this paper. By using a publicly available dataset and two medical datasets collected from hospital, experimental results show that the proposed algorithm achieves the best performance by comparing with the existing methods. "
794337072026976256,2016-11-04 00:34:50,https://t.co/TeqJs5FLp1,Gaussian Processes for Survival Analysis. (arXiv:1611.00817v1 [stat.ML]) https://t.co/TeqJs5FLp1,1,6," Abstract: We introduce a semi-parametric Bayesian model for survival analysis. The model is centred on a parametric baseline hazard, and uses a Gaussian process to model variations away from it nonparametrically, as well as dependence on covariates. As opposed to many other methods in survival analysis, our framework does not impose unnecessary constraints in the hazard rate or in the survival function. Furthermore, our model handles left, right and interval censoring mechanisms common in survival analysis. We propose a MCMC algorithm to perform inference and an approximation scheme based on random Fourier features to make computations faster. We report experimental results on synthetic and real data, showing that our model performs better than competing models such as Cox proportional hazards, ANOVA-DDP and random survival forests. "
794337069845901314,2016-11-04 00:34:50,https://t.co/QFxNvcqFQx,Initialization and Coordinate Optimization for Multi-way Matching. (arXiv:1611.00838v1 [stat.ML]) https://t.co/QFxNvcqFQx,0,2," Abstract: We consider the problem of consistently matching multiple sets of elements to each other, which is a common task in fields such as computer vision. To solve the underlying NP-hard objective, existing methods often relax or approximate it, but end up with unsatisfying empirical performances due to their inexact objectives. We propose a coordinate update algorithm that directly solves the exact objective. By using the pairwise alignment information to build an undirected graph and initializing the permutation matrices along the edges of its Maximum Spanning Tree, our algorithm successfully avoids bad local optima. Theoretically, with high probability our algorithm could guarantee to solve this problem optimally on data with reasonable noise. Empirically, our algorithm consistently and significantly outperforms existing methods on several benchmark tasks on real datasets. "
794337067840995329,2016-11-04 00:34:49,https://t.co/rKgAJ7sHjD,Tensor Decomposition via Variational Auto-Encoder. (arXiv:1611.00866v1 [stat.ML]) https://t.co/rKgAJ7sHjD,3,15," Abstract: Tensor decomposition is an important technique for capturing the high-order interactions among multiway data. Multi-linear tensor composition methods, such as the Tucker decomposition and the CANDECOMP/PARAFAC (CP), assume that the complex interactions among objects are multi-linear, and are thus insufficient to represent nonlinear relationships in data. Another assumption of these methods is that a predefined rank should be known. However, the rank of tensors is hard to estimate, especially for cases with missing values. To address these issues, we design a Bayesian generative model for tensor decomposition. Different from the traditional Bayesian methods, the high-order interactions of tensor entries are modeled with variational auto-encoder. The proposed model takes advantages of Neural Networks and nonparametric Bayesian models, by replacing the multi-linear product in traditional Bayesian tensor decomposition with a complex nonlinear function (via Neural Networks) whose parameters can be learned from data. Experimental results on synthetic data and real-world chemometrics tensor data have demonstrated that our new model can achieve significantly higher prediction performance than the state-of-the-art tensor decomposition approaches. "
794337065739677696,2016-11-04 00:34:49,https://t.co/6tpFI87XAX,Fast Eigenspace Approximation using Random Signals. (arXiv:1611.00938v1 [cs.DS]) https://t.co/6tpFI87XAX,0,2," Abstract: We focus in this work on the estimation of the first $k$ eigenvectors of any graph Laplacian using filtering of Gaussian random signals. We prove that we only need $k$ such signals to be able to exactly recover as many of the smallest eigenvectors, regardless of the number of nodes in the graph. In addition, we address key issues in implementing the theoretical concepts in practice using accurate approximated methods. We also propose fast algorithms both for eigenspace approximation and for the determination of the $k$th smallest eigenvalue $\lambda_k$. The latter proves to be extremely efficient under the assumption of locally uniform distribution of the eigenvalue over the spectrum. Finally, we present experiments which show the validity of our method in practice and compare it to state-of-the-art methods for clustering and visualization both on synthetic small-scale datasets and larger real-world problems of millions of nodes. We show that our method allows a better scaling with the number of nodes than all previous methods while achieving an almost perfect reconstruction of the eigenspace formed by the first $k$ eigenvectors. "
794337063223099392,2016-11-04 00:34:48,https://t.co/d6KsptSBMu,High-dimensional regression over disease subgroups. (arXiv:1611.00953v1 [stat.AP]) https://t.co/d6KsptSBMu,1,1," Abstract: We consider high-dimensional regression over subgroups of observations. Our work is motivated by biomedical problems, where disease subtypes, for example, may differ with respect to underlying regression models, but sample sizes at the subgroup-level may be limited. We focus on the case in which subgroup-specific models may be expected to be similar but not necessarily identical. Our approach is to treat subgroups as related problem instances and jointly estimate subgroup-specific regression coefficients. This is done in a penalized framework, combining an $\ell_1$ term with an additional term that penalizes differences between subgroup-specific coefficients. This gives solutions that are globally sparse but that allow information-sharing between the subgroups. We present algorithms for estimation and empirical results on simulated data and using Alzheimer's disease, amyotrophic lateral sclerosis and cancer datasets. These examples demonstrate the gains our approach can offer in terms of prediction and the ability to estimate subgroup-specific sparsity patterns. "
794337059896983552,2016-11-04 00:34:47,https://t.co/HWUFlfaTyw,Multitask Protein Function Prediction Through Task Dissimilarity. (arXiv:1611.00962v1 [stat.ML]) https://t.co/HWUFlfaTyw,0,1," Abstract: Automated protein function prediction is a challenging problem with distinctive features, such as the hierarchical organization of protein functions and the scarcity of annotated proteins for most biological functions. We propose a multitask learning algorithm addressing both issues. Unlike standard multitask algorithms, which use task (protein functions) similarity information as a bias to speed up learning, we show that dissimilarity information enforces separation of rare class labels from frequent class labels, and for this reason is better suited for solving unbalanced protein function prediction problems. We support our claim by showing that a multitask extension of the label propagation algorithm empirically works best when the task relatedness information is represented using a dissimilarity matrix as opposed to a similarity matrix. Moreover, the experimental comparison carried out on three model organism shows that our method has a more stable performance in both ""protein-centric"" and ""function-centric"" evaluation settings. "
794337057804058624,2016-11-04 00:34:47,https://t.co/F2HwkJydce,Learning to Pivot with Adversarial Networks. (arXiv:1611.01046v1 [stat.ML]) https://t.co/F2HwkJydce,1,7," Abstract: Many inference problems involve data generation processes that are not uniquely specified or are uncertain in some way. In a scientific context, the presence of several plausible data generation processes is often associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot -- a quantity whose distribution is invariant to the unknown value of the (categorical or continuous) nuisance parameters that parametrizes this family of generation processes. In this work, we introduce a flexible training procedure based on adversarial networks for enforcing the pivotal property on a predictive model. We derive theoretical results showing that the proposed algorithm tends towards a minimax solution corresponding to a predictive model that is both optimal and independent of the nuisance parameters (if that models exists) or for which one can tune the trade-off between power and robustness. Finally, we demonstrate the effectiveness of this approach with a toy example and an example from particle physics. "
794337055950127104,2016-11-04 00:34:46,https://t.co/xAljhEVOk4,A-Ward_p\b{eta}: Effective hierarchical clustering using the Minkowski metric and a fast k -means initialisation. … https://t.co/xAljhEVOk4,0,2," Abstract: In this paper we make two novel contributions to hierarchical clustering. First, we introduce an anomalous pattern initialisation method for hierarchical clustering algorithms, called A-Ward, capable of substantially reducing the time they take to converge. This method generates an initial partition with a sufficiently large number of clusters. This allows the cluster merging process to start from this partition rather than from a trivial partition composed solely of singletons. Our second contribution is an extension of the Ward and Ward p algorithms to the situation where the feature weight exponent can differ from the exponent of the Minkowski distance. This new method, called A-Ward p\b{eta} , is able to generate a much wider variety of clustering solutions. We also demonstrate that its parameters can be estimated reasonably well by using a cluster validity index. We perform numerous experiments using data sets with two types of noise, insertion of noise features and blurring within-cluster values of some features. These experiments allow us to conclude: (i) our anomalous pattern initialisation method does indeed reduce the time a hierarchical clustering algorithm takes to complete, without negatively impacting its cluster recovery ability; (ii) A-Ward p\b{eta} provides better cluster recovery than both Ward and Ward p. "
794337053563621376,2016-11-04 00:34:46,https://t.co/TPl619hyHK,Spectral community detection in heterogeneous large networks. (arXiv:1611.01096v1 [stat.ML]) https://t.co/TPl619hyHK,0,3," Abstract: In this article, we study spectral methods for community detection based on $ \alpha$-parametrized normalized modularity matrix hereafter called $ {\bf L}_\alpha $ in heterogeneous graph models. We show, in a regime where community detection is not asymptotically trivial, that $ {\bf L}_\alpha $ can be well approximated by a more tractable random matrix which falls in the family of spiked random matrices. The analysis of this equivalent spiked random matrix allows us to improve spectral methods for community detection and assess their performances in the regime under study. In particular, we prove the existence of an optimal value $ \alpha_{\rm opt} $ of the parameter $ \alpha $ for which the detection of communities is best ensured and we provide an on-line estimation of $ \alpha_{\rm opt} $ only based on the knowledge of the graph adjacency matrix. Unlike classical spectral methods for community detection where clustering is performed on the eigenvectors associated with extreme eigenvalues, we show through our theoretical analysis that a regularization should instead be performed on those eigenvectors prior to clustering in heterogeneous graphs. Finally, through a deeper study of the regularized eigenvectors used for clustering, we assess the performances of our new algorithm for community detection. Numerical simulations in the course of the article show that our methods outperform state-of-the-art spectral methods on dense heterogeneous graphs. "
794337051416133633,2016-11-04 00:34:45,https://t.co/QvgqUBuNm5,Cross: Efficient Low-rank Tensor Completion. (arXiv:1611.01129v1 [stat.ME]) https://t.co/QvgqUBuNm5,0,1," Abstract: The completion of tensors, or high-order arrays, attracts significant attention in recent research. Current literature on tensor completion primarily focuses on recovery from a set of uniformly randomly measured entries, and the required number of measurements to achieve recovery is not guaranteed to be optimal. In addition, the implementation of some previous methods are NP-hard. In this article, we propose a framework for low-rank tensor completion via a novel tensor measurement scheme we name Cross. The proposed procedure is efficient and easy to implement. In particular, we show that a third order tensor of Tucker rank-$(r_1, r_2, r_3)$ in $p_1$-by-$p_2$-by-$p_3$ dimensional space can be recovered from as few as $r_1r_2r_3 + r_1(p_1-r_1) + r_2(p_2-r_2) + r_3(p_3-r_3)$ noiseless measurements, which matches the sample complexity lower-bound. In the case of noisy measurements, we also develop a theoretical upper bound and the matching minimax lower bound for recovery error over certain classes of low-rank tensors for the proposed procedure. The results can be further extended to fourth or higher-order tensors. Simulation studies show that the method performs well under a variety of settings. Finally, the procedure is illustrated through a real dataset in neuroimaging. "
794337048027074560,2016-11-04 00:34:44,https://t.co/15JJc4Vnjb,Categorical Reparameterization with Gumbel-Softmax. (arXiv:1611.01144v1 [stat.ML]) https://t.co/15JJc4Vnjb,6,14," Abstract: Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification. "
794337045753827329,2016-11-04 00:34:44,https://t.co/5OzOplUm5h,Finding Local Minima for Nonconvex Optimization in Linear Time. (arXiv:1611.01146v1 [math.OC]) https://t.co/5OzOplUm5h,0,7, Abstract: We design a non-convex second-order optimization algorithm that is guaranteed to return an approximate local minimum in time which is linear in the input representation. The time complexity of our algorithm to find an approximate local minimum is even faster than that of gradient descent to find a critical point. Our algorithm applies to a general class of optimization problems including training a neural network and other non-convex objectives arising in machine learning. 
794337042419302401,2016-11-04 00:34:43,https://t.co/F9mRZgmY3t,Surprising properties of dropout in deep networks. (arXiv:1602.04484v4 [cs.LG] UPDATED) https://t.co/F9mRZgmY3t,2,14," Abstract: We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear, and that dropout is insensitive to various re-scalings of the input features, outputs, and network weights. This last insensitivity implies that there are no isolated local minima of the dropout training criterion. Our work uncovers new properties of dropout, extends our understanding of why dropout succeeds, and lays the foundation for further progress. "
794337039965679617,2016-11-04 00:34:43,https://t.co/FN9QocmAcl,Learning Local Dependence In Ordered Data. (arXiv:1604.07451v2 [math.ST] UPDATED) https://t.co/FN9QocmAcl,0,0," Abstract: In many applications, data come with a natural ordering. This ordering can often induce local dependence among nearby variables. However, in complex data, the width of this dependence may vary, making simple assumptions such as a constant neighborhood size unrealistic. We propose a framework for learning this local dependence based on estimating the inverse of the Cholesky factor of the covariance matrix. Penalized maximum likelihood estimation of this matrix yields a simple regression interpretation for local dependence in which variables are predicted by their neighbors. Our proposed method involves solving a convex, penalized Gaussian likelihood problem with a hierarchical group lasso penalty. The problem decomposes into independent subproblems which can be solved efficiently in parallel using first-order methods. Our method yields a sparse, symmetric, positive definite estimator of the precision matrix, encoding a Gaussian graphical model. We derive theoretical results not found in existing methods attaining this structure. In particular, our conditions for signed support recovery and estimation consistency rates in multiple norms are as mild as those in a regression problem. Empirical results show our method performing favorably compared to existing methods. We apply our method to genomic data to flexibly model linkage disequilibrium. Our method is also applied to improve the performance of discriminant analysis in sound recording classification. "
794337036668928000,2016-11-04 00:34:42,https://t.co/eXdxscPIkP,Generalized Min-Max Kernel and Generalized Consistent Weighted Sampling. (arXiv:1605.05721v3 [cs.LG] UPDATED) https://t.co/eXdxscPIkP,0,2," Abstract: We propose the ""generalized min-max"" (GMM) kernel as a measure of data similarity, where data vectors can have both positive and negative entries. GMM is positive definite as there is an associate hashing method named ""generalized consistent weighted sampling"" (GCWS) which linearizes this (nonlinear) kernel. A natural competitor of GMM is the radial basis function (RBF) kernel, whose corresponding hashing method is known as the ""random Fourier features"" (RFF). An extensive experimental study on classifications of {50} publicly available datasets demonstrates that both the GMM and RBF kernels can often substantially improve over linear classifiers. Furthermore, the GCWS hashing method typically requires substantially fewer samples than (the normalized) RFF in order to achieve similar classification accuracies. To understand the property of random Fourier features (RFF), we derive the theoretical variances of RFF and its normalized version (which we name as NRFF). Overall, the relative (to the expectation) variances of RFF and NRFF are substantially larger than the relative variance of GCWS. This helps explain the superb empirical results of GCWS compared to RFF (NRFF). We expect that GMM and GCWS will be adopted in practice for large-scale statistical machine learning applications and efficient near neighbor search (as GMM generates discrete hash values). "
794337034131410944,2016-11-04 00:34:41,https://t.co/AuFOGRdJvu,Communication-efficient distributed statistical learning. (arXiv:1605.07689v2 [stat.ML] UPDATED) https://t.co/AuFOGRdJvu,0,4," Abstract: We present a Communication-efficient Surrogate Likelihood (CSL) framework for solving distributed statistical inference problems. CSL provides a communication-efficient surrogate to the global likelihood that can be used for low-dimensional estimation, high-dimensional regularized estimation and Bayesian inference. For low-dimensional estimation, CSL provably improves upon naive averaging schemes and facilitates the construction of confidence intervals. For high-dimensional regularized estimation, CSL leads to a minimax-optimal estimator with controlled communication cost. For Bayesian inference, CSL can be used to form a communication-efficient quasi-posterior distribution that converges to the true posterior. This quasi-posterior procedure significantly improves the computational efficiency of MCMC algorithms even in a non-distributed setting. We present both theoretical analysis and experiments to explore the properties of the CSL approximation. "
794337032327884800,2016-11-04 00:34:41,https://t.co/ckCKQztBHE,Homotopy Smoothing for Non-Smooth Problems with Lower Complexity than $O(1/\epsilon)$. (arXiv:1607.03815v2 [math.O… https://t.co/ckCKQztBHE,0,1," Abstract: In this paper, we develop a novel {\bf ho}moto{\bf p}y {\bf s}moothing (HOPS) algorithm for solving a family of non-smooth problems that is composed of a non-smooth term with an explicit max-structure and a smooth term or a simple non-smooth term whose proximal mapping is easy to compute. The best known iteration complexity for solving such non-smooth optimization problems is $O(1/\epsilon)$ without any assumption on the strong convexity. In this work, we will show that the proposed HOPS achieved a lower iteration complexity of $\widetilde O(1/\epsilon^{1-\theta})$\footnote{$\widetilde O()$ suppresses a logarithmic factor.} with $\theta\in(0,1]$ capturing the local sharpness of the objective function around the optimal solutions. To the best of our knowledge, this is the lowest iteration complexity achieved so far for the considered non-smooth optimization problems without strong convexity assumption. The HOPS algorithm employs Nesterov's smoothing technique and Nesterov's accelerated gradient method and runs in stages, which gradually decreases the smoothing parameter in a stage-wise manner until it yields a sufficiently good approximation of the original function. We show that HOPS enjoys a linear convergence for many well-known non-smooth problems (e.g., empirical risk minimization with a piece-wise linear loss function and $\ell_1$ norm regularizer, finding a point in a polyhedron, cone programming, etc). Experimental results verify the effectiveness of HOPS in comparison with Nesterov's smoothing algorithm and the primal-dual style of first-order methods. "
794337029802852352,2016-11-04 00:34:40,https://t.co/2E3D67Itmu,Understanding Neural Sparse Coding with Matrix Factorization. (arXiv:1609.00285v3 [stat.ML] UPDATED) https://t.co/2E3D67Itmu,1,12," Abstract: Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, that are optimal in the class of first-order methods for non-smooth, convex functions, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). However, these methods don't exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks was proposed in \cite{Gregor10}, coined LISTA, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately. In this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails. "
794337026619359237,2016-11-04 00:34:39,https://t.co/YAu3RbNPDK,Semi-Supervised Classification with Graph Convolutional Networks. (arXiv:1609.02907v3 [cs.LG] UPDATED) https://t.co/YAu3RbNPDK,0,9, Abstract: We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin. 
793974299853987841,2016-11-03 00:33:19,https://t.co/8zdSAgc3Y4,Natural-Parameter Networks: A Class of Probabilistic Neural Networks. (arXiv:1611.00448v1 [cs.LG]) https://t.co/8zdSAgc3Y4,1,9," Abstract: Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance. "
793974298054717440,2016-11-03 00:33:18,https://t.co/0OJkyHvWhq,Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks. (arXiv:1611.00454v1 [cs.LG]) https://t.co/0OJkyHvWhq,1,10," Abstract: Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information. "
793974296335020033,2016-11-03 00:33:18,https://t.co/IqDD8AIlov,The Intelligent Voice 2016 Speaker Recognition System. (arXiv:1611.00514v1 [cs.CL]) https://t.co/IqDD8AIlov,0,3," Abstract: This paper presents the Intelligent Voice (IV) system submitted to the NIST 2016 Speaker Recognition Evaluation (SRE). The primary emphasis of SRE this year was on developing speaker recognition technology which is robust for novel languages that are much more heterogeneous than those used in the current state-of-the-art, using significantly less training data, that does not contain meta-data from those languages. The system is based on the state-of-the-art i-vector/PLDA which is developed on the fixed training condition, and the results are reported on the protocol defined on the development set of the challenge. "
793974294422417408,2016-11-03 00:33:17,https://t.co/HU3OtxHq6C,A nonparametric HMM for genetic imputation and coalescent inference. (arXiv:1611.00544v1 [stat.AP]) https://t.co/HU3OtxHq6C,0,1," Abstract: Genetic sequence data are well described by hidden Markov models (HMMs) in which latent states correspond to clusters of similar mutation patterns. Theory from statistical genetics suggests that these HMMs are nonhomogeneous (their transition probabilities vary along the chromosome) and have large support for self transitions. We develop a new nonparametric model of genetic sequence data, based on the hierarchical Dirichlet process, which supports these self transitions and nonhomogeneity. Our model provides a parameterization of the genetic process that is more parsimonious than other more general nonparametric models which have previously been applied to population genetics. We provide truncation-free MCMC inference for our model using a new auxiliary sampling scheme for Bayesian nonparametric HMMs. In a series of experiments on male X chromosome data from the Thousand Genomes Project and also on data simulated from a population bottleneck we show the benefits of our model over the popular finite model fastPHASE, which can itself be seen as a parametric truncation of our model. We find that the number of HMM states found by our model is correlated with the time to the most recent common ancestor in population bottlenecks. This work demonstrates the flexibility of Bayesian nonparametrics applied to large and complex genetic data. "
793974292765622272,2016-11-03 00:33:17,https://t.co/hvF6xw3i6x,Sensitivity Maps of the Hilbert-Schmidt Independence Criterion. (arXiv:1611.00555v1 [stat.ML]) https://t.co/hvF6xw3i6x,0,2," Abstract: Kernel dependence measures yield accurate estimates of nonlinear relations between random variables, and they are also endorsed with solid theoretical properties and convergence rates. Besides, the empirical estimates are easy to compute in closed form just involving linear algebra operations. However, they are hampered by two important problems: the high computational cost involved, as two kernel matrices of the sample size have to be computed and stored, and the interpretability of the measure, which remains hidden behind the implicit feature map. We here address these two issues. We introduce the Sensitivity Maps (SMs) for the Hilbert-Schmidt independence criterion (HSIC). Sensitivity maps allow us to explicitly analyze and visualize the relative relevance of both examples and features on the dependence measure. We also present the randomized HSIC (RHSIC) and its corresponding sensitivity maps to cope with large scale problems. We build upon the framework of random features and the Bochner's theorem to approximate the involved kernels in the canonical HSIC. The power of the RHSIC measure scales favourably with the number of samples, and it approximates HSIC and the sensitivity maps efficiently. Convergence bounds of both the measure and the sensitivity map are also provided. Our proposal is illustrated in synthetic examples, and challenging real problems of dependence estimation, feature selection, and causal inference from empirical data. "
793974290966347777,2016-11-03 00:33:16,https://t.co/HaAJG1olDE,Learning Methods for Dynamic Topic Modeling in Automated Behaviour Analysis. (arXiv:1611.00565v1 [stat.ML]) https://t.co/HaAJG1olDE,1,2," Abstract: Semi-supervised and unsupervised systems provide operators with invaluable support and can tremendously reduce the operators load. In the light of the necessity to process large volumes of video data and provide autonomous decisions, this work proposes new learning algorithms for activity analysis in video. The activities and behaviours are described by a dynamic topic model. Two novel learning algorithms based on the expectation maximisation approach and variational Bayes inference are proposed. Theoretical derivations of the posterior of model parameters are given. The designed learning algorithms are compared with the Gibbs sampling inference scheme introduced earlier in the literature. A detailed comparison of the learning algorithms is presented on real video data. We also propose an anomaly localisation procedure, elegantly embedded in the topic modeling framework. The proposed framework can be applied to a number of areas, including transportation systems, security and surveillance. "
793974289385082881,2016-11-03 00:33:16,https://t.co/6VES0IXQH3,Improving variational methods via pairwise linear response identities. (arXiv:1611.00683v1 [stat.ML]) https://t.co/6VES0IXQH3,0,3," Abstract: Inference methods are often formulated as variational approximations: these approximations allow easy evaluation of statistics by marginalization or linear response, but these estimates can be inconsistent. We show that by introducing constraints on covariance, one can ensure consistency of linear response with the variational parameters, and in so doing inference of marginal probability distributions is improved. For the Bethe approximation and its generalizations, improvements are achieved with simple choices of the constraints. The approximations are presented as variational frameworks; iterative procedures related to message passing are provided for finding the minima. "
793974287602450433,2016-11-03 00:33:16,https://t.co/wtxsfpbWsk,The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables. (arXiv:1611.00712v1 [cs.LG]) https://t.co/wtxsfpbWsk,14,31," Abstract: The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack continuous reparameterizations due to the discontinuous nature of discrete states. In this work we introduce concrete random variables -- continuous relaxations of discrete random variables. The concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate effectiveness of concrete relaxations on density estimation and structured prediction tasks using neural networks. "
793974285522059264,2016-11-03 00:33:15,https://t.co/dnytnrPzR6,When coding meets ranking: A joint framework based on local learning. (arXiv:1409.2232v2 [cs.CV] UPDATED) https://t.co/dnytnrPzR6,0,1," Abstract: Sparse coding, which represents a data point as a sparse reconstruction code with regard to a dictionary, has been a popular data representation method. Meanwhile, in database retrieval problems, learning the ranking scores from data points plays an important role. Up to now, these two problems have always been considered separately, assuming that data coding and ranking are two independent and irrelevant problems. However, is there any internal relationship between sparse coding and ranking score learning? If yes, how to explore and make use of this internal relationship? In this paper, we try to answer these questions by developing the first joint sparse coding and ranking score learning algorithm. To explore the local distribution in the sparse code space, and also to bridge coding and ranking problems, we assume that in the neighborhood of each data point, the ranking scores can be approximated from the corresponding sparse codes by a local linear function. By considering the local approximation error of ranking scores, the reconstruction error and sparsity of sparse coding, and the query information provided by the user, we construct a unified objective function for learning of sparse codes, the dictionary and ranking scores. We further develop an iterative algorithm to solve this optimization problem. "
793974283651416064,2016-11-03 00:33:15,https://t.co/5vUJW3TFO1,Correlated Random Measures. (arXiv:1507.00720v2 [stat.ML] UPDATED) https://t.co/5vUJW3TFO1,0,4," Abstract: We develop correlated random measures, random measures where the atom weights can exhibit a flexible pattern of dependence, and use them to develop powerful hierarchical Bayesian nonparametric models. Hierarchical Bayesian nonparametric models are usually built from completely random measures, a Poisson-process based construction in which the atom weights are independent. Completely random measures imply strong independence assumptions in the corresponding hierarchical model, and these assumptions are often misplaced in real-world settings. Correlated random measures address this limitation. They model correlation within the measure by using a Gaussian process in concert with the Poisson process. With correlated random measures, for example, we can develop a latent feature model for which we can infer both the properties of the latent features and their dependency pattern. We develop several other examples as well. We study a correlated random measure model of pairwise count data. We derive an efficient variational inference algorithm and show improved predictive performance on large data sets of documents, web clicks, and electronic health records. "
793974281780817920,2016-11-03 00:33:14,https://t.co/rB126us7zb,Learning Leading Indicators for Time Series Predictions. (arXiv:1507.01978v3 [cs.LG] UPDATED) https://t.co/rB126us7zb,2,3," Abstract: We consider the problem of learning models for forecasting multiple time-series systems together with discovering the leading indicators that serve as good predictors for the system. We model the systems by linear vector autoregressive models (VAR) and link the discovery of leading indicators to inferring sparse graphs of Granger-causality. We propose new problem formulations and develop two new methods to learn such models, gradually increasing the complexity of assumptions and approaches. While the first method assumes common structures across the whole system, our second method uncovers model clusters based on the Granger-causality and leading indicators together with learning the model parameters. We study the performance of our methods on a comprehensive set of experiments and confirm their efficacy and their advantages over state-of-the-art sparse VAR and graphical Granger learning methods. "
793974279905955840,2016-11-03 00:33:14,https://t.co/JxpZlEuRkW,Operator-valued Kernels for Learning from Functional Response Data. (arXiv:1510.08231v3 [cs.LG] UPDATED) https://t.co/JxpZlEuRkW,5,8," Abstract: In this paper we consider the problems of supervised classification and regression in the case where attributes and labels are functions: a data is represented by a set of functions, and the label is also a function. We focus on the use of reproducing kernel Hilbert space theory to learn from such functional data. Basic concepts and properties of kernel-based learning are extended to include the estimation of function-valued functions. In this setting, the representer theorem is restated, a set of rigorously defined infinite-dimensional operator-valued kernels that can be valuably applied when the data are functions is described, and a learning algorithm for nonlinear functional data analysis is introduced. The methodology is illustrated through speech and audio signal processing experiments. "
793974277976576004,2016-11-03 00:33:13,https://t.co/goDPDqHTiu,Streaming regularization parameter selection via stochastic gradient descent. (arXiv:1511.02187v3 [stat.ML] UPDATE… https://t.co/goDPDqHTiu,0,5," Abstract: We propose a framework to perform streaming covariance selection. Our approach employs regularization constraints where a time-varying sparsity parameter is iteratively estimated via stochastic gradient descent. This allows for the regularization parameter to be efficiently learnt in an online manner. The proposed framework is developed for linear regression models and extended to graphical models via neighbourhood selection. Under mild assumptions, we are able to obtain convergence results in a non-stochastic setting. The capabilities of such an approach are demonstrated using both synthetic data as well as neuroimaging data. "
793974275107528704,2016-11-03 00:33:13,https://t.co/uFsa4f4ahI],Variational Inference: A Review for Statisticians. (arXiv:1601.00670v4 [https://t.co/uFsa4f4ahI] UPDATED) https://t.co/EWMJ69Jw2E,5,21,INDEXERROR
793974272607842304,2016-11-03 00:33:12,https://t.co/ODf3srMEeF,Building Machines That Learn and Think Like People. (arXiv:1604.00289v3 [cs.AI] UPDATED) https://t.co/ODf3srMEeF,3,13," Abstract: Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models. "
793974270280003584,2016-11-03 00:33:12,https://t.co/ZYZcuC2txN,Learning Sensor Multiplexing Design through Back-propagation. (arXiv:1605.07078v2 [cs.LG] UPDATED) https://t.co/ZYZcuC2txN,1,2," Abstract: Recent progress on many imaging and vision tasks has been driven by the use of deep feed-forward neural networks, which are trained by propagating gradients of a loss defined on the final output, back through the network up to the first layer that operates directly on the image. We propose back-propagating one step further---to learn camera sensor designs jointly with networks that carry out inference on the images they capture. In this paper, we specifically consider the design and inference problems in a typical color camera---where the sensor is able to measure only one color channel at each pixel location, and computational inference is required to reconstruct a full color image. We learn the camera sensor's color multiplexing pattern by encoding it as layer whose learnable weights determine which color channel, from among a fixed set, will be measured at each location. These weights are jointly trained with those of a reconstruction network that operates on the corresponding sensor measurements to produce a full color image. Our network achieves significant improvements in accuracy over the traditional Bayer pattern used in most color cameras. It automatically learns to employ a sparse color measurement approach similar to that of a recent design, and moreover, improves upon that design by learning an optimal layout for these measurements. "
793974268627394560,2016-11-03 00:33:11,https://t.co/bI0k6aP5EB,Specialized Support Vector Machines for open-set recognition. (arXiv:1606.03802v2 [cs.LG] UPDATED) https://t.co/bI0k6aP5EB,0,4," Abstract: Often, when dealing with real-world recognition problems, we do not need, and often cannot have, knowledge of the entire set of possible classes that might appear during operational testing. Moreover, sometimes some of these classes may be ill-sampled, not sampled at all or undefined. In such cases, we need to think of robust classification methods able to deal with the ""unknown"" and properly reject samples belonging to classes never seen during training. Notwithstanding, almost all existing classifiers to date were mostly developed for the closed-set scenario, i.e., the classification setup in which it is assumed that all test samples belong to one of the classes with which the classifier was trained. In the open-set scenario, however, a test sample can belong to none of the known classes and the classifier must properly reject it by classifying it as unknown. In this work, we extend upon the well-known Support Vector Machines (SVM) classifier and introduce the Specialized Support Vector Machine (SSVM), which is suitable for recognition in open-set setups. The SSVM balances the empirical risk and the risk of the unknown and ensures that the region of the feature space in which a test sample would be classified as known (one of the known classes) is always bounded, ensuring a finite risk of the unknown. The same cannot be guaranteed by the traditional SVM formulation, even when using the Radial Basis Function (RBF) kernel. In this work, we also highlight the properties of the SVM classifier related to the open-set scenario, and provide necessary and sufficient conditions for an RBF SVM to have bounded open-space risk. An extensive set of experiments compares the proposed method with existing solutions in the literature for open-set recognition and the reported results show its effectiveness. "
793974267100692481,2016-11-03 00:33:11,https://t.co/5GyoQ10SLA,Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data. (arXiv:1610.05755v2 [stat.ML] UPD… https://t.co/5GyoQ10SLA,1,9," Abstract: Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data. The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as ""teachers"" for a ""student"" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning. "
793974264932208641,2016-11-03 00:33:10,https://t.co/nd7mRoBz8k,Distilling Information Reliability and Source Trustworthiness from Digital Traces. (arXiv:1610.07472v2 [cs.SI] UPD… https://t.co/nd7mRoBz8k,0,1," Abstract: Online knowledge repositories typically rely on their users or dedicated editors to evaluate the reliability of their content. These evaluations can be viewed as noisy measurements of both information reliability and information source trustworthiness. Can we leverage these noisy evaluations, often biased, to distill a robust, unbiased and interpretable measure of both notions? In this paper, we argue that the temporal traces left by these noisy evaluations give cues on the reliability of the information and the trustworthiness of the sources. Then, we propose a temporal point process modeling framework that links these temporal traces to robust, unbiased and interpretable notions of information reliability and source trustworthiness. Furthermore, we develop an efficient convex optimization procedure to learn the parameters of the model from historical traces. Experiments on real-world data gathered from Wikipedia and Stack Overflow show that our modeling framework accurately predicts evaluation events, provides an interpretable measure of information reliability and source trustworthiness, and yields interesting insights about real-world events. "
793613439067361280,2016-11-02 00:39:23,https://t.co/NOCfM51JhC,Full-Capacity Unitary Recurrent Neural Networks. (arXiv:1611.00035v1 [stat.ML]) https://t.co/NOCfM51JhC,1,7," Abstract: Recurrent neural networks are powerful models for processing sequential data, but they are generally plagued by vanishing and exploding gradient problems. Unitary recurrent neural networks (uRNNs), which use unitary recurrence matrices, have recently been proposed as a means to avoid these issues. However, in previous experiments, the recurrence matrices were restricted to be a product of parameterized unitary matrices, and an open question remains: when does such a parameterization fail to represent all unitary matrices, and how does this restricted representational capacity limit what can be learned? To address this question, we propose full-capacity uRNNs that optimize their recurrence matrix over all unitary matrices, leading to significantly improved performance over uRNNs that use a restricted-capacity recurrence matrix. Our contribution consists of two main components. First, we provide a theoretical argument to determine if a unitary parameterization has restricted capacity. Using this argument, we show that a recently proposed unitary parameterization has restricted capacity for hidden state dimension greater than 7. Second, we show how a complete, full-capacity unitary recurrence matrix can be optimized over the differentiable manifold of unitary matrices. The resulting multiplicative gradient step is very simple and does not require gradient clipping or learning rate adaptation. We confirm the utility of our claims by empirically evaluating our new full-capacity uRNNs on both synthetic and natural data, achieving superior performance compared to both LSTMs and the original restricted-capacity uRNNs. "
793613437284777984,2016-11-02 00:39:22,https://t.co/K8KbKsyZDA,Kernel Bandwidth Selection for SVDD: Peak Criterion Approach for Large Data. (arXiv:1611.00058v1 [cs.LG]) https://t.co/K8KbKsyZDA,0,2," Abstract: Support Vector Data Description (SVDD) provides a useful approach to construct a description of multivariate data for single-class classification and outlier detection with various practical applications. Gaussian kernel used in SVDD formulation allows flexible data description defined by observations designated as support vectors. The data boundary of such description is non-spherical and conforms to the geometric features of the data. By varying the Gaussian kernel bandwidth parameter, the SVDD-generated boundary can be made either smoother (more spherical) or tighter/jagged. The former case may lead to under-fitting, whereas the latter may result in overfitting. Peak criterion has been proposed to select an optimal value of the kernel bandwidth to strike the balance between the data boundary smoothness and its ability to capture the general geometric shape of the data. Peak criterion involves training SVDD at various values of the kernel bandwidth parameter. When training datasets are large, the time required to obtain the optimal value of the Gaussian kernel bandwidth parameter according to Peak method can become prohibitively large. This paper proposes an extension of Peak method for the case of large data. The proposed method gives good results when applied to several datasets. Two existing alternative methods of computing the Gaussian kernel bandwidth parameter (Coefficient of Variation and Distance to the Farthest Neighbor) were modified to allow comparison with the proposed method on convergence. Empirical comparison demonstrates the advantage of the proposed method. "
793613435250638849,2016-11-02 00:39:22,https://t.co/GeElwlOiit,Bayesian Adaptive Data Analysis Guarantees from Subgaussianity. (arXiv:1611.00065v1 [cs.LG]) https://t.co/GeElwlOiit,1,4," Abstract: The new field of adaptive data analysis seeks to provide algorithms and provable guarantees for models of machine learning that allow researchers to reuse their data, which normally falls outside of the usual statistical paradigm of static data analysis. In 2014, Dwork, Feldman, Hardt, Pitassi, Reingold and Roth introduced one potential model and proposed several solutions based on differential privacy. In previous work in 2016, we described a problem with this model and instead proposed a Bayesian variant, but also found that the analogous Bayesian methods cannot achieve the same statistical guarantees as in the static case. In this paper, we prove the first positive results for the Bayesian model, showing that with a Dirichlet prior, the posterior mean algorithm indeed matches the statistical guarantees of the static case. We conjecture that this is true for any conjugate prior from the exponential family, but can only prove this in special cases. The main ingredient, Theorem 4, shows that the $\text{Beta}(\alpha,\beta)$ distribution is subgaussian with variance proxy $O(1/(\alpha+\beta+1))$, a concentration result also of independent interest. Unlike most moment-based concentration techniques, which bound the centered moments, our proof utilizes a simple condition on the raw moments of a positive random variable. "
793613432834691072,2016-11-02 00:39:21,https://t.co/oozo6P04JA,Online Maximum Likelihood Estimation of the Parameters of Partially Observed Diffusion Processes. (arXiv:1611.0017… https://t.co/oozo6P04JA,0,3," Abstract: We revisit the problem of estimating the parameters of a partially observed stochastic process $(X_t,Y_t)$ with a continuous time parameter, where $X_t$ is the hidden state process and $Y_t$ is the observed process. The estimation is to be done online, i.e. the parameter estimate should be updated recursively based on the observation filtration $\sigma\{Y_s, s\leq t\}$. Online parameter estimation is a challenging problem that needs to be solved for designing adaptive filters and for stochastic control in all cases where the system is unknown or changing over time, with applications in robotics, neuroscience, or finance. Here, we use the representation of the log-likelihood function in terms of the Radon-Nikodym derivative of the probability measure restricted to the observation (the observation likelihood) with respect to a reference measure under which $Y_t$ is a Wiener process. This log-likelihood can be computed by using the stochastic filter. Using stochastic gradient ascent on the likelihood function, we obtain an algorithm for the time evolution of the parameter estimate. Although this approach is based on theoretical results that have been known for several decades, this explicit method of recursive parameter estimation has remained unpublished. "
793613430347431937,2016-11-02 00:39:21,https://t.co/Ba0pWg8uj8,Stationary time-vertex signal processing. (arXiv:1611.00255v1 [cs.LG]) https://t.co/Ba0pWg8uj8,0,1," Abstract: The goal of this paper is to improve learning for multivariate processes whose structure is dependent on some known graph topology. Typically, the graph information is incorporated to the learning process via a smoothness assumption postulating that the values supported on well connected vertices exhibit small variations. We argue that smoothness is not enough. To capture the behavior of complex interconnected systems, such as transportation and biological networks, it is important to train expressive models, being able to reproduce a wide range of graph and temporal behaviors. Motivated by this need, this paper puts forth a novel definition of time-vertex wide-sense stationarity, or joint stationarity for short. We believe that the proposed definition is natural, at it elegantly relates to existing definitions of stationarity in the time and vertex domains. We use joint stationarity to regularize learning and to reduce computational complexity in both estimation and recovery tasks. In particular, we show that for any jointly stationary process: (a) one can learn the covariance structure from O(1) samples, and (b) can solve MMSE recovery problems, such as interpolation, denoising, forecasting, in complexity that is linear to the edges and timesteps. Experiments with three datasets suggest that joint stationarity can yield significant accuracy improvements in the reconstruction effort. "
793613428418109440,2016-11-02 00:39:20,https://t.co/H8g0QleYKI,Causal Compression. (arXiv:1611.00261v1 [stat.ML]) https://t.co/H8g0QleYKI,0,7," Abstract: We propose a new method of discovering causal relationships in temporal data based on the notion of causal compression. To this end, we adopt the Pearlian graph setting and the directed information as an information theoretic tool for quantifying causality. We introduce chain rule for directed information and use it to motivate causal sparsity. We show two applications of the proposed method: causal time series segmentation which selects time points capturing the incoming and outgoing causal flow between time points belonging to different signals, and causal bipartite graph recovery. We prove that modelling of causality in the adopted set-up only requires estimating the copula density of the data distribution and thus does not depend on its marginals. We evaluate the method on time resolved gene expression data. "
793613426144739330,2016-11-02 00:39:20,https://t.co/GjdiCU0Twp,Semi-Supervised Radio Signal Identification. (arXiv:1611.00303v1 [cs.LG]) https://t.co/GjdiCU0Twp,0,1," Abstract: Radio recognition in complex multi-user environments is an important tool for optimizing spectrum utilization, identifying and minimizing interference, and enforcing spectrum policy. Radio data is readily available and easy to obtain, but labeled data is often scarce making supervised learning strategies difficult and time consuming to curate. We demonstrate that semi-supervised learning techniques can be used to scale learning beyond supervised datasets, allowing for both discerning and recalling radio signals of interest by using sparse signal representations based on both unsupervised and supervised methods for nonlinear feature learning. "
793613424299274241,2016-11-02 00:39:19,https://t.co/8B1aVdRtBr,Enhanced Factored Three-Way Restricted Boltzmann Machines for Speech Detection. (arXiv:1611.00326v1 [cs.SD]) https://t.co/8B1aVdRtBr,0,5," Abstract: In this letter, we propose enhanced factored three way restricted Boltzmann machines (EFTW-RBMs) for speech detection. The proposed model incorporates conditional feature learning by multiplying the dynamical state of the third unit, which allows a modulation over the visible-hidden node pairs. Instead of stacking previous frames of speech as the third unit in a recursive manner, the correlation related weighting coefficients are assigned to the contextual neighboring frames. Specifically, a threshold function is designed to capture the long-term features and blend the globally stored speech structure. A factored low rank approximation is introduced to reduce the parameters of the three-dimensional interaction tensor, on which non-negative constraint is imposed to address the sparsity characteristic. The validations through the area-under-ROC-curve (AUC) and signal distortion ratio (SDR) show that our approach outperforms several existing 1D and 2D (i.e., time and time-frequency domain) speech detection algorithms in various noisy environments. "
793613421799477249,2016-11-02 00:39:19,https://t.co/mrgJmo1DrZ,The $\chi$-Divergence for Approximate Inference. (arXiv:1611.00328v1 [stat.ML]) https://t.co/mrgJmo1DrZ,1,2," Abstract: Variational inference enables Bayesian analysis for complex probabilistic models with massive data sets. It works by positing a family of distributions and finding the member in the family that is closest to the posterior. While successful, variational methods can run into pathologies; for example, they typically underestimate posterior uncertainty. We propose CHI-VI, a complementary algorithm to traditional variational inference with KL($q$ || $p$) and an alternative algorithm to EP. CHI-VI is a black box algorithm that minimizes the $\chi$-divergence from the posterior to the family of approximating distributions. In EP, only local minimization of the KL($p$ || $q$) objective is possible. In contrast, CHI-VI optimizes a well-defined global objective. It directly minimizes an upper bound to the model evidence that equivalently minimizes the $\chi$-divergence. In experiments, we illustrate the utility of the upper bound for sandwich estimating the model evidence. We also compare several probabilistic models and a Cox process for basketball data. We find CHI-VI often yields better classification error rates and better posterior uncertainty. "
793613420058804228,2016-11-02 00:39:18,https://t.co/b842m2zJkB,Stochastic Variational Deep Kernel Learning. (arXiv:1611.00336v1 [stat.ML]) https://t.co/b842m2zJkB,3,18," Abstract: Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective. Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra. We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet. "
793613418158780416,2016-11-02 00:39:18,https://t.co/TuRTmaPXwK,Variational Bayes In Private Settings (VIPS). (arXiv:1611.00340v1 [stat.ML]) https://t.co/TuRTmaPXwK,0,3," Abstract: We provide a general framework for privacy-preserving variational Bayes (VB) for a large class of probabilistic models, called the conjugate exponential (CE) family. Our primary observation is that when models are in the CE family, we can privatise the variational posterior distributions simply by perturbing the expected sufficient statistics of the complete-data likelihood. For widely used non-CE models with binomial likelihoods (e.g., logistic regression), we exploit the P{\'o}lya-Gamma data augmentation scheme to bring such models into the CE family, such that inferences in the modified model resemble the original (private) variational Bayes algorithm as closely as possible. The iterative nature of variational Bayes presents a further challenge for privacy preservation, as each iteration increases the amount of noise needed. We overcome this challenge by combining: (1) a relaxed notion of differential privacy, called {\it{concentrated differential privacy}}, which provides a tight bound on the privacy cost of multiple VB iterations and thus significantly decreases the amount of additive noise; and (2) the privacy amplification effect resulting from subsampling mini-batches from large-scale data in stochastic learning. We empirically demonstrate the effectiveness of our method in CE and non-CE models including latent Dirichlet allocation (LDA) and Bayesian logistic regression, evaluated on real-world datasets. "
793613415277268992,2016-11-02 00:39:17,https://t.co/KnLpUeM8hc,Computationally Efficient Influence Maximization in Stochastic and Adversarial Models: Algorithms and Analysis. (a… https://t.co/KnLpUeM8hc,0,5," Abstract: We consider the problem of influence maximization in fixed networks, for both stochastic and adversarial contagion models. The common goal is to select a subset of nodes of a specified size to infect so that the number of infected nodes at the conclusion of the epidemic is as large as possible. In the stochastic setting, the epidemic spreads according to a general triggering model, which includes the popular linear threshold and independent cascade models. We establish upper and lower bounds for the influence of an initial subset of nodes in the network, where the influence is defined as the expected number of infected nodes. Although the problem of exact influence computation is NP-hard in general, our bounds may be evaluated efficiently, leading to scalable algorithms for influence maximization with rigorous theoretical guarantees. In the adversarial spreading setting, an adversary is allowed to specify the edges through which contagion may spread, and the player chooses sets of nodes to infect in successive rounds. Both the adversary and player may behave stochastically, but we limit the adversary to strategies that are oblivious of the player's actions. We establish upper and lower bounds on the minimax pseudo-regret in both undirected and directed networks. "
793613413255639040,2016-11-02 00:39:17,https://t.co/5Co6JYLGyn,Signal Processing on Graphs: Causal Modeling of Unstructured Data. (arXiv:1503.00173v4 [cs.IT] UPDATED) https://t.co/5Co6JYLGyn,1,11," Abstract: Many applications collect a large number of time series, for example, the financial data of companies quoted in a stock exchange, the health care data of all patients that visit the emergency room of a hospital, or the temperature sequences continuously measured by weather stations across the US. These data are often referred to as unstructured. A first task in its analytics is to derive a low dimensional representation, a graph or discrete manifold, that describes well the interrelations among the time series and their intrarelations across time. This paper presents a computationally tractable algorithm for estimating this graph that structures the data. The resulting graph is directed and weighted, possibly capturing causal relations, not just reciprocal correlations as in many existing approaches in the literature. A convergence analysis is carried out. The algorithm is demonstrated on random graph datasets and real network time series datasets, and its performance is compared to that of related methods. The adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested. "
793613411565309952,2016-11-02 00:39:16,https://t.co/hju2oD7BLd,PAC-Bayesian Theory Meets Bayesian Inference. (arXiv:1605.08636v2 [stat.ML] UPDATED) https://t.co/hju2oD7BLd,2,8," Abstract: We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks "
793613409220718592,2016-11-02 00:39:16,https://t.co/u8dVebHvys,Learning Optimized Risk Scores on Large-Scale Datasets. (arXiv:1610.00168v2 [stat.ML] UPDATED) https://t.co/u8dVebHvys,0,3," Abstract: Risk scores are simple classification models that let users quickly assess risk by adding, subtracting and multiplying a few small numbers. These models are used for high-stakes applications in healthcare and criminology, but are difficult to learn from data because they need to be risk-calibrated, use small integer coefficients, and obey operational constraints. In this paper, we present a new approach to learn optimized risk scores from data by solving a discrete optimization problem. We formulate the risk score problem as a mixed integer nonlinear program, and present a new cutting plane algorithm to efficiently recover the optimal solution while avoiding the stalling behavior that occurs when we use existing cutting plane algorithms on non-convex problems. We pair our cutting plane algorithm with specialized procedures to generate feasible solutions, narrow the optimality gap, and reduce data-related computation. The resulting approach can learn optimized risk scores in a way that scales linearly in the number of samples, provides a proof of optimality, and accommodates complex operational constraints. We illustrate the benefits of our approach through extensive numerical experiments. "
793613407387840512,2016-11-02 00:39:15,https://t.co/vnz7Wg4dJ7,Contextual Decision Processes with Low Bellman Rank are PAC-Learnable. (arXiv:1610.09512v1 [cs.LG] CROSS LISTED) https://t.co/vnz7Wg4dJ7,0,2," Abstract: This paper studies systematic exploration for reinforcement learning with rich observations and function approximation. We introduce a new formulation, called contextual decision processes, that unifies and generalizes most prior settings. Our first contribution is a new complexity measure, the Bellman Rank, that we show enables tractable learning of near-optimal behavior in these processes and is naturally small for many well-studied reinforcement learning settings. Our second contribution is a new reinforcement learning algorithm that engages in systematic exploration to learn contextual decision processes with low Bellman Rank. The algorithm provably learns near-optimal behavior with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The algorithm uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for reinforcement learning with function approximation. "
793256042872311808,2016-11-01 00:59:13,https://t.co/Kc24oiyjiv,Dynamic matrix recovery from incomplete observations under an exact low-rank constraint. (arXiv:1610.09420v1 [stat… https://t.co/Kc24oiyjiv,2,6," Abstract: Low-rank matrix factorizations arise in a wide variety of applications -- including recommendation systems, topic models, and source separation, to name just a few. In these and many other applications, it has been widely noted that by incorporating temporal information and allowing for the possibility of time-varying models, significant improvements are possible in practice. However, despite the reported superior empirical performance of these dynamic models over their static counterparts, there is limited theoretical justification for introducing these more complex models. In this paper we aim to address this gap by studying the problem of recovering a dynamically evolving low-rank matrix from incomplete observations. First, we propose the locally weighted matrix smoothing (LOWEMS) framework as one possible approach to dynamic matrix recovery. We then establish error bounds for LOWEMS in both the {\em matrix sensing} and {\em matrix completion} observation models. Our results quantify the potential benefits of exploiting dynamic constraints both in terms of recovery accuracy and sample complexity. To illustrate these benefits we provide both synthetic and real-world experimental results. "
793256040968093697,2016-11-01 00:59:12,https://t.co/e5BTVVufoE,Sparse Signal Recovery for Binary Compressed Sensing by Majority Voting Neural Networks. (arXiv:1610.09463v1 [cs.I… https://t.co/e5BTVVufoE,1,7," Abstract: In this paper, we propose majority voting neural networks for sparse signal recovery in binary compressed sensing. The majority voting neural network is composed of several independently trained feedforward neural networks employing the sigmoid function as an activation function. Our empirical study shows that a choice of a loss function used in training processes for the network is of prime importance. We found a loss function suitable for sparse signal recovery, which includes a cross entropy-like term and an $L_1$ regularized term. From the experimental results, we observed that the majority voting neural network achieves excellent recovery performance, which is approaching the optimal performance as the number of component nets grows. The simple architecture of the majority voting neural networks would be beneficial for both software and hardware implementations. "
793256039177195520,2016-11-01 00:59:12,https://t.co/onk3TM4Ht0,A general multiblock method for structured variable selection. (arXiv:1610.09490v1 [stat.ML]) https://t.co/onk3TM4Ht0,0,1," Abstract: Regularised canonical correlation analysis was recently extended to more than two sets of variables by the multiblock method Regularised generalised canonical correlation analysis (RGCCA). Further, Sparse GCCA (SGCCA) was proposed to address the issue of variable selection. However, for technical reasons, the variable selection offered by SGCCA was restricted to a covariance link between the blocks (i.e., with $\tau=1$). One of the main contributions of this paper is to go beyond the covariance link and to propose an extension of SGCCA for the full RGCCA model (i.e., with $\tau\in[0, 1]$). In addition, we propose an extension of SGCCA that exploits structural relationships between variables within blocks. Specifically, we propose an algorithm that allows structured and sparsity-inducing penalties to be included in the RGCCA optimisation problem. The proposed multiblock method is illustrated on a real three-block high-grade glioma data set, where the aim is to predict the location of the brain tumours, and on a simulated data set, where the aim is to illustrate the method's ability to reconstruct the true underlying weight vectors. "
793256037096775680,2016-11-01 00:59:11,https://t.co/CKXdotXnpw,Solving Large-scale Systems of Random Quadratic Equations via Stochastic Truncated Amplitude Flow. (arXiv:1610.095… https://t.co/CKXdotXnpw,1,5," Abstract: A novel approach termed \emph{stochastic truncated amplitude flow} (STAF) is developed to reconstruct an unknown $n$-dimensional real-/complex-valued signal $\bm{x}$ from $m$ `phaseless' quadratic equations of the form $\psi_i=|\langle\bm{a}_i,\bm{x}\rangle|$. This problem, also known as phase retrieval from magnitude-only information, is \emph{NP-hard} in general. Adopting an amplitude-based nonconvex formulation, STAF leads to an iterative solver comprising two stages: s1) Orthogonality-promoting initialization through a stochastic variance reduced gradient algorithm; and, s2) A series of iterative refinements of the initialization using stochastic truncated gradient iterations. Both stages involve a single equation per iteration, thus rendering STAF a simple, scalable, and fast approach amenable to large-scale implementations that is useful when $n$ is large. When $\{\bm{a}_i\}_{i=1}^m$ are independent Gaussian, STAF provably recovers exactly any $\bm{x}\in\mathbb{R}^n$ exponentially fast based on order of $n$ quadratic equations. STAF is also robust in the presence of additive noise of bounded support. Simulated tests involving real Gaussian $\{\bm{a}_i\}$ vectors demonstrate that STAF empirically reconstructs any $\bm{x}\in\mathbb{R}^n$ exactly from about $2.3n$ magnitude-only measurements, outperforming state-of-the-art approaches and narrowing the gap from the information-theoretic number of equations $m=2n-1$. Extensive experiments using synthetic data and real images corroborate markedly improved performance of STAF over existing alternatives. "
793256034123051008,2016-11-01 00:59:11,https://t.co/hcaIBrb8aR,Conditional Image Synthesis With Auxiliary Classifier GANs. (arXiv:1610.09585v1 [stat.ML]) https://t.co/hcaIBrb8aR,2,9," Abstract: Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data. "
793256031791046656,2016-11-01 00:59:10,https://t.co/D9DpWMMce3,Sparse interpretable estimators for cyclic arrival rates. (arXiv:1610.09600v1 [stat.ML]) https://t.co/D9DpWMMce3,0,2," Abstract: Exploiting the fact that most arrival processes exhibit cyclic (though not necessarily periodic) behaviour, we propose a simple and computationally efficient procedure for estimating the intensity of a nonhomogeneous Poisson process. The estimator is interpretable because it is a simple sum of p sinusoids, although p and the frequency, amplitude, and phase of each wave are not known and need to be estimated. This results in a flexible specification that is suitable for use in modeling as well as in high resolution simulations. Our estimation procedure sits in between classic periodogram methods and atomic/total variation norm thresholding. A novel aspect of our approach is the use of window functions with fast decaying spectral tails, which we show improves frequency resolution as well as prevent strong frequency signals from masking weaker ones nearby. Interestingly the prolate spheriodal window that is usually considered optimal in signal processing is suboptimal for frequency recovery. Under suitable conditions, finite sample performance guarantees for our procedure can be derived that resolve open questions and expand existing results in spectral estimation literature. Preliminary simulations suggest that our procedure can potentially be extended to Cox processes which are used to model overdispersion, although a formal analysis is left for future research. We apply our procedure to arrivals data from an emergency department at an academic hospital in the United States to examine differences in arrival patterns by illness severity. "
793256028909531136,2016-11-01 00:59:09,https://t.co/iLvUUMmW2O,Auxiliary gradient-based sampling algorithms. (arXiv:1610.09641v1 [stat.ML]) https://t.co/iLvUUMmW2O,0,7," Abstract: We introduce a new family of MCMC samplers that combine the strategic use of auxiliary variables and Gibbs sampling with simple Taylor expansions of the target density. Our approach permits the marginalisation over the auxiliary variables yielding marginal samplers, or the augmentation of the auxiliary variables, yielding auxiliary samplers. The well-known Metropolis-adjusted Langevin algorithm (MALA) and preconditioned Crank-Nicolson Langevin (pCNL) algorithm are shown to be special cases of the marginal samplers we propose. We prove that marginal schemes are superior in terms of asymptotic variance, but demonstrate that the acceptance ratios in auxiliary schemes can be computed an order of magnitude faster. In the context of latent Gaussian models we propose new auxiliary and marginal samplers whose implementation requires a single tuning parameter, which can be found automatically during the transient phase. Extensive experimentation shows that the increase in efficiency (measured as effective sample size per unit of computing time) relative to (optimised implementations of) state-of-the-art methods such as pCNL or elliptical slice sampling ranges from 10-fold in binary classification problems to 25-fold in log-Gaussian Cox processes to 100-fold in Gaussian process regression. We provide some insights on this remarkable improvement in terms of the way alternative samplers try to approximate the eigenvalues of the target by shrinking those of the Gaussian prior. Finally, we introduce a novel MCMC sampling scheme for jointly sampling the latent Gaussian and its hyperparameters that builds upon the auxiliary samplers. "
793256026694901762,2016-11-01 00:59:09,https://t.co/f3qlPVuxmd,"Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering. (arXiv:1610.09… https://t.co/f3qlPVuxmd",2,5," Abstract: We propose a methodology to explore and measure the pairwise correlations that exist between variables in a dataset. The methodology leverages copulas for encoding dependence between two variables, state-of-the-art optimal transport for providing a relevant geometry to the copulas, and clustering for summarizing the main dependence patterns found between the variables. Some of the clusters centers can be used to parameterize a novel dependence coefficient which can target or forget specific dependence patterns. Finally, we illustrate and benchmark the methodology on several datasets. Code and numerical experiments are available online for reproducible research. "
793256024069267456,2016-11-01 00:59:08,https://t.co/pq5SlUBeSt,Feature-Augmented Neural Networks for Patient Note De-identification. (arXiv:1610.09704v1 [cs.CL]) https://t.co/pq5SlUBeSt,1,3," Abstract: Patient notes contain a wealth of information of potentially great interest to medical investigators. However, to protect patients' privacy, Protected Health Information (PHI) must be removed from the patient notes before they can be legally released, a process known as patient note de-identification. The main objective for a de-identification system is to have the highest possible recall. Recently, the first neural-network-based de-identification system has been proposed, yielding state-of-the-art results. Unlike other systems, it does not rely on human-engineered features, which allows it to be quickly deployed, but does not leverage knowledge from human experts or from electronic health records (EHRs). In this work, we explore a method to incorporate human-engineered features as well as features derived from EHRs to a neural-network-based de-identification system. Our results show that the addition of features, especially the EHR-derived features, further improves the state-of-the-art in patient note de-identification, including for some of the most sensitive PHI types such as patient names. Since in a real-life setting patient notes typically come with EHRs, we recommend developers of de-identification systems to leverage the information EHRs contain. "
793256021191950336,2016-11-01 00:59:08,https://t.co/C0OUkldaa6,Active Learning from Imperfect Labelers. (arXiv:1610.09730v1 [cs.LG]) https://t.co/C0OUkldaa6,2,7," Abstract: We study active learning where the labeler can not only return incorrect labels but also abstain from labeling. We consider different noise and abstention conditions of the labeler. We propose an algorithm which utilizes abstention responses, and analyze its statistical consistency and query complexity under fairly natural assumptions on the noise and abstention rate of the labeler. This algorithm is adaptive in a sense that it can automatically request less queries with a more informed or less noisy labeler. We couple our algorithm with lower bounds to show that under some technical conditions, it achieves nearly optimal query complexity. "
793256016335044609,2016-11-01 00:59:06,https://t.co/beiZ1sNYjO,Flexible Models for Microclustering with Application to Entity Resolution. (arXiv:1610.09780v1 [stat.ME]) https://t.co/beiZ1sNYjO,0,3," Abstract: Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman--Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some applications, this assumption is inappropriate. For example, when performing entity resolution, the size of each cluster should be unrelated to the size of the data set, and each cluster should contain a negligible fraction of the total number of data points. These applications require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the microclustering property and introducing a new class of models that can exhibit this property. We compare models within this class to two commonly used clustering models using four entity-resolution data sets. "
793251088707252225,2016-11-01 00:39:32,https://t.co/IgGPNIZCqo,Analysis of Nonstationary Time Series Using Locally Coupled Gaussian Processes. (arXiv:1610.09838v1 [stat.ML]) https://t.co/IgGPNIZCqo,0,6," Abstract: The analysis of nonstationary time series is of great importance in many scientific fields such as physics and neuroscience. In recent years, Gaussian process regression has attracted substantial attention as a robust and powerful method for analyzing time series. In this paper, we introduce a new framework for analyzing nonstationary time series using locally stationary Gaussian process analysis with parameters that are coupled through a hidden Markov model. The main advantage of this framework is that arbitrary complex nonstationary covariance functions can be obtained by combining simpler stationary building blocks whose hidden parameters can be estimated in closed-form. We demonstrate the flexibility of the method by analyzing two examples of synthetic nonstationary signals: oscillations with time varying frequency and time series with two dynamical states. Finally, we report an example application on real magnetoencephalographic measurements of brain activity. "
793251087255998464,2016-11-01 00:39:31,https://t.co/geJFi2eaqj,Depth Separation in ReLU Networks for Approximating Smooth Non-Linear Functions. (arXiv:1610.09887v1 [cs.LG]) https://t.co/geJFi2eaqj,1,6," Abstract: We provide a depth-based separation result for feed-forward ReLU neural networks, showing that a wide family of non-linear, twice-differentiable functions on $[0,1]^d$, which can be approximated to accuracy $\epsilon$ by ReLU networks of depth and width $\mathcal{O}(\text{poly}(\log(1/\epsilon)))$, cannot be approximated to similar accuracy by constant-depth ReLU networks, unless their width is at least $\Omega(1/\epsilon)$. "
793251085230174211,2016-11-01 00:39:31,https://t.co/2pOYz3RnQz,Inference Compilation and Universal Probabilistic Programming. (arXiv:1610.09900v1 [cs.AI]) https://t.co/2pOYz3RnQz,7,12," Abstract: We introduce a method for using deep neural networks to amortize the cost of inference in models from the family induced by universal probabilistic programming languages, establishing a framework that combines the strengths of probabilistic programming and deep learning methods. We call what we do ""compilation of inference"" because our method transforms a denotational specification of an inference problem in the form of a probabilistic program written in a universal programming language into a trained neural network denoted in a neural network specification language. When at test time this neural network is fed observational data and executed, it performs approximate inference in the original model specified by the probabilistic program. Our training objective and learning procedure are designed to allow the trained neural network to be used as a proposal distribution in a sequential importance sampling inference engine. We illustrate our method on mixture models and Captcha solving and show significant speedups in the efficiency of inference. "
793251082092904449,2016-11-01 00:39:30,https://t.co/PR6FELpqwM,Complex-Valued Kernel Methods for Regression. (arXiv:1610.09915v1 [stat.ML]) https://t.co/PR6FELpqwM,0,3," Abstract: Usually, complex-valued RKHS are presented as an straightforward application of the real-valued case. In this paper we prove that this procedure yields a limited solution for regression. We show that another kernel, here denoted as pseudo kernel, is needed to learn any function in complex-valued fields. Accordingly, we derive a novel RKHS to include it, the widely RKHS (WRKHS). When the pseudo-kernel cancels, WRKHS reduces to complex-valued RKHS of previous approaches. We address the kernel and pseudo-kernel design, paying attention to the kernel and the pseudo-kernel being complex-valued. In the experiments included we report remarkable improvements in simple scenarios where real a imaginary parts have different similitude relations for given inputs or cases where real and imaginary parts are correlated. In the context of these novel results we revisit the problem of non-linear channel equalization, to show that the WRKHS helps to design more efficient solutions. "
793251080192786432,2016-11-01 00:39:30,https://t.co/W6MLAH85VY,Function Weighted Metric Discovery for Unreliable Functions. (arXiv:1610.10025v1 [stat.ML]) https://t.co/W6MLAH85VY,0,1," Abstract: We consider building a function adapted diffusion operator high dimensional data $X$ when the function $F$ can only be evaluated on large subsets of the data, and possibly only depends on a small subset of the features. Our method breaks $X$ up into hierarchical trees, and determines the importance of each feature in each subset of the population. The resulting metric $\rho_F$ is then used to define a localized filtration of $F$ and estimation values of $F$ at a finer scale than it is reliable naively. We apply this method in several cases in which $F$ is unreliable a priori, and specifically use it to determine personalized risk and treatment effectiveness in drug trials. We validate the model on several synthetic datasets. "
793251077885923328,2016-11-01 00:39:29,https://t.co/xQTJhnmWaw,Optimization for Large-Scale Machine Learning with Distributed Features and Observations. (arXiv:1610.10060v1 [sta… https://t.co/xQTJhnmWaw,1,5," Abstract: As the size of modern data sets exceeds the disk and memory capacities of a single computer, machine learning practitioners have resorted to parallel and distributed computing. Given that optimization is one of the pillars of machine learning and predictive modeling, distributed optimization methods have recently garnered ample attention in the literature. Although previous research has mostly focused on settings where either the observations, or features of the problem at hand are stored in distributed fashion, the situation where both are partitioned across the nodes of a computer cluster (doubly distributed) has barely been studied. In this work we propose two doubly distributed optimization algorithms. The first one falls under the umbrella of distributed dual coordinate ascent methods, while the second one belongs to the class of stochastic gradient/coordinate descent hybrid methods. We conduct numerical experiments in Spark using real-world and simulated data sets and study the scaling properties of our methods. Our empirical evaluation of the proposed algorithms demonstrates the out-performance of a block distributed ADMM method, which, to the best of our knowledge is the only other existing doubly distributed optimization algorithm. "
793251075574919168,2016-11-01 00:39:28,https://t.co/4moCl4MHpN,The Case for Temporal Transparency: Detecting Policy Change Events in Black-Box Decision Making Systems. (arXiv:16… https://t.co/4moCl4MHpN,0,1," Abstract: Bringing transparency to black-box decision making systems (DMS) has been a topic of increasing research interest in recent years. Traditional active and passive approaches to make these systems transparent are often limited by scalability and/or feasibility issues. In this paper, we propose a new notion of black-box DMS transparency, named, temporal transparency, whose goal is to detect if/when the DMS policy changes over time, and is mostly invariant to the drawbacks of traditional approaches. We map our notion of temporal transparency to time series changepoint detection methods, and develop a framework to detect policy changes in real-world DMS's. Experiments on New York Stop-question-and-frisk dataset reveal a number of publicly announced and unannounced policy changes, highlighting the utility of our framework. "
793251073469341696,2016-11-01 00:39:28,https://t.co/u64gt0bO3q,Tensor Switching Networks. (arXiv:1610.10087v1 [cs.NE]) https://t.co/u64gt0bO3q,2,12," Abstract: We present a novel neural network algorithm, the Tensor Switching (TS) network, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to tensor-valued hidden units. The TS network copies its entire input vector to different locations in an expanded representation, with the location determined by its hidden unit activity. In this way, even a simple linear readout from the TS representation can implement a highly expressive deep-network-like function. The TS network hence avoids the vanishing gradient problem by construction, at the cost of larger representation size. We develop several methods to train the TS network, including equivalent kernels for infinitely wide and deep TS networks, a one-pass linear learning algorithm, and two backpropagation-inspired representation learning algorithms. Our experimental results demonstrate that the TS network is indeed more expressive and consistently learns faster than standard ReLU networks. "
793251071758041094,2016-11-01 00:39:28,https://t.co/zSvdMEnmzN,Sequential Convex Programming Methods for A Class of Structured Nonlinear Programming. (arXiv:1210.3039v2 [math.OC… https://t.co/zSvdMEnmzN,1,4," Abstract: In this paper we study a broad class of structured nonlinear programming (SNLP) problems. In particular, we first establish the first-order optimality conditions for them. Then we propose sequential convex programming (SCP) methods for solving them in which each iteration is obtained by solving a convex programming problem exactly or inexactly. Under some suitable assumptions, we establish that any accumulation point of the sequence generated by the methods is a KKT point of the SNLP problems. In addition, we propose a variant of the exact SCP method for SNLP in which nonmonotone scheme and ""local"" Lipschitz constants of the associated functions are used. And a similar convergence result as mentioned above is established. "
793251068960538624,2016-11-01 00:39:27,https://t.co/Kvho8h2LP0,Schatten-$p$ Quasi-Norm Regularized Matrix Optimization via Iterative Reweighted Singular Value Minimization. (arX… https://t.co/Kvho8h2LP0,1,2," Abstract: In this paper we study general Schatten-$p$ quasi-norm (SPQN) regularized matrix minimization problems. In particular, we first introduce a class of first-order stationary points for them, and show that the first-order stationary points introduced in [11] for an SPQN regularized $vector$ minimization problem are equivalent to those of an SPQN regularized $matrix$ minimization reformulation. We also show that any local minimizer of the SPQN regularized matrix minimization problems must be a first-order stationary point. Moreover, we derive lower bounds for nonzero singular values of the first-order stationary points and hence also of the local minimizers of the SPQN regularized matrix minimization problems. The iterative reweighted singular value minimization (IRSVM) methods are then proposed to solve these problems, whose subproblems are shown to have a closed-form solution. In contrast to the analogous methods for the SPQN regularized $vector$ minimization problems, the convergence analysis of these methods is significantly more challenging. We develop a novel approach to establishing the convergence of these methods, which makes use of the expression of a specific solution of their subproblems and avoids the intricate issue of finding the explicit expression for the Clarke subdifferential of the objective of their subproblems. In particular, we show that any accumulation point of the sequence generated by the IRSVM methods is a first-order stationary point of the problems. Our computational results demonstrate that the IRSVM methods generally outperform some recently developed state-of-the-art methods in terms of solution quality and/or speed. "
793251066662051840,2016-11-01 00:39:26,https://t.co/DksRPkS93t,Covariate-assisted spectral clustering. (arXiv:1411.2158v5 [stat.ML] UPDATED) https://t.co/DksRPkS93t,1,3," Abstract: Biological and social systems consist of myriad interacting units. The interactions can be represented in the form of a graph or network. Measurements of these graphs can reveal the underlying structure of these interactions, which provides insight into the systems that generated the graphs. Moreover, in applications such as connectomics, social networks, and genomics, graph data are accompanied by contextualizing measures on each node. We utilize these node covariates to help uncover latent communities in a graph, using a modification of spectral clustering. Statistical guarantees are provided under a joint mixture model that we call the node-contextualized stochastic blockmodel, including a bound on the mis-clustering rate. The bound is used to derive conditions for achieving perfect clustering. For most simulated cases, covariate-assisted spectral clustering yields results superior to regularized spectral clustering without node covariates and to an adaptation of canonical correlation analysis. We apply our clustering method to large brain graphs derived from diffusion MRI data, using the node locations or neurological region membership as covariates. In both cases, covariate-assisted spectral clustering yields clusters that are easier to interpret neurologically. "
793251064384483328,2016-11-01 00:39:26,https://t.co/fZ0eIkjIMF,Compact Compositional Models. (arXiv:1412.3708v4 [cs.CV] UPDATED) https://t.co/fZ0eIkjIMF,0,1," Abstract: Learning compact and interpretable representations is a very natural task, which has not been solved satisfactorily even for simple binary datasets. In this paper, we review various ways of composing experts for binary data and argue that competitive forms of interaction are best suited to learn low-dimensional representations. We propose a new composition rule that discourages experts from focusing on similar structures and that penalizes opposing votes strongly so that abstaining from voting becomes more attractive. We also introduce a novel sequential initialization procedure, which is based on a process of oversimplification and correction. Experiments show that with our approach very intuitive models can be learned. "
793251062455083008,2016-11-01 00:39:25,https://t.co/sMXZHWZNOm,Fast Embedding for JOFC Using the Raw Stress Criterion. (arXiv:1502.03391v3 [stat.ML] UPDATED) https://t.co/sMXZHWZNOm,0,1," Abstract: The Joint Optimization of Fidelity and Commensurability (JOFC) manifold matching methodology embeds an omnibus dissimilarity matrix consisting of multiple dissimilarities on the same set of objects. One approach to this embedding optimizes the preservation of fidelity to each individual dissimilarity matrix together with commensurability of each given observation across modalities via iterative majorization of a raw stress error criterion by successive Guttman transforms. In this paper, we exploit the special structure inherent to JOFC to exactly and efficiently compute the successive Guttman transforms, and as a result we are able to greatly speed up the JOFC procedure for both in-sample and out-of-sample embedding. We demonstrate the scalability of our implementation on both real and simulated data examples. "
793251060513144836,2016-11-01 00:39:25,https://t.co/19CzEF1er1,Iterative Refinement of Approximate Posterior for Training Directed Belief Networks. (arXiv:1511.06382v5 [cs.LG] U… https://t.co/19CzEF1er1,0,4," Abstract: Variational methods that rely on a recognition network to approximate the posterior of directed graphical models offer better inference and learning than previous methods. Recent advances that exploit the capacity and flexibility in this approach have expanded what kinds of models can be trained. However, as a proposal for the posterior, the capacity of the recognition network is limited, which can constrain the representational power of the generative model and increase the variance of Monte Carlo estimates. To address these issues, we introduce an iterative refinement procedure for improving the approximate posterior of the recognition network and show that training with the refined posterior is competitive with state-of-the-art methods. The advantages of refinement are further evident in an increased effective sample size, which implies a lower variance of gradient estimates. "
793251058780897280,2016-11-01 00:39:24,https://t.co/rQndzy9fBM,The p-filter: multi-layer FDR control for grouped hypotheses. (arXiv:1512.03397v3 [stat.ME] UPDATED) https://t.co/rQndzy9fBM,1,2," Abstract: In many practical applications of multiple hypothesis testing using the False Discovery Rate (FDR), the given hypotheses can be naturally partitioned into groups, and one may not only want to control the number of false discoveries (wrongly rejected null hypotheses), but also the number of falsely discovered groups of hypotheses (we say a group is falsely discovered if at least one hypothesis within that group is rejected, when in reality the group contains only nulls). In this paper, we introduce the p-filter, a procedure which unifies and generalizes the standard FDR procedure by Benjamini and Hochberg and global null testing procedure by Simes. We first prove that our proposed method can simultaneously control the overall FDR at the finest level (individual hypotheses treated separately) and the group FDR at coarser levels (when such groups are user-specified). We then generalize the p-filter procedure even further to handle multiple partitions of hypotheses, since that might be natural in many applications. For example, in neuroscience experiments, we may have a hypothesis for every (discretized) location in the brain, and at every (discretized) timepoint: does the stimulus correlate with activity in location x at time t after the stimulus was presented? In this setting, one might want to group hypotheses by location and by time. Importantly, our procedure can handle multiple partitions which are nonhierarchical (i.e. one partition may arrange p-values by voxel, and another partition arranges them by time point; neither one is nested inside the other). We prove that our procedure controls FDR simultaneously across these multiple lay- ers, under assumptions that are standard in the literature: we do not need the hypotheses to be independent, but require a nonnegative dependence condition known as PRDS. "
793251056536973312,2016-11-01 00:39:24,https://t.co/YSr2LJUq0b,Adaptive Ensemble Learning with Confidence Bounds. (arXiv:1512.07446v3 [cs.LG] UPDATED) https://t.co/YSr2LJUq0b,0,4," Abstract: Extracting actionable intelligence from distributed, heterogeneous, correlated and high-dimensional data sources requires run-time processing and learning both locally and globally. In the last decade, a large number of meta-learning techniques have been proposed in which local learners make online predictions based on their locally-collected data instances, and feed these predictions to an ensemble learner, which fuses them and issues a global prediction. However, most of these works do not provide performance guarantees or, when they do, these guarantees are asymptotic. None of these existing works provide confidence estimates about the issued predictions or rate of learning guarantees for the ensemble learner. In this paper, we provide a systematic ensemble learning method called Hedged Bandits, which comes with both long run (asymptotic) and short run (rate of learning) performance guarantees. Moreover, our approach yields performance guarantees with respect to the optimal local prediction strategy, and is also able to adapt its predictions in a data-driven manner. We illustrate the performance of Hedged Bandits in the context of medical informatics and show that it outperforms numerous online and offline ensemble learning methods. "
793251054980829184,2016-11-01 00:39:24,https://t.co/cGhIHwCgqu,Joint Dimensionality Reduction for Two Feature Vectors. (arXiv:1602.04398v3 [stat.ML] UPDATED) https://t.co/cGhIHwCgqu,0,4," Abstract: Many machine learning problems, especially multi-modal learning problems, have two sets of distinct features (e.g., image and text features in news story classification, or neuroimaging data and neurocognitive data in cognitive science research). This paper addresses the joint dimensionality reduction of two feature vectors in supervised learning problems. In particular, we assume a discriminative model where low-dimensional linear embeddings of the two feature vectors are sufficient statistics for predicting a dependent variable. We show that a simple algorithm involving singular value decomposition can accurately estimate the embeddings provided that certain sample complexities are satisfied, without specifying the nonlinear link function (regressor or classifier). The main results establish sample complexities under multiple settings. Sample complexities for different link functions only differ by constant factors. "
793251052007092224,2016-11-01 00:39:23,https://t.co/id6KVR5zCz,Training Input-Output Recurrent Neural Networks through Spectral Methods. (arXiv:1603.00954v5 [cs.LG] UPDATED) https://t.co/id6KVR5zCz,1,11," Abstract: We consider the problem of training input-output recurrent neural networks (RNN) for sequence labeling tasks. We propose a novel spectral approach for learning the network parameters. It is based on decomposition of the cross-moment tensor between the output and a non-linear transformation of the input, based on score functions. We guarantee consistent learning with polynomial sample and computational complexity under transparent conditions such as non-degeneracy of model parameters, polynomial activations for the neurons, and a Markovian evolution of the input sequence. We also extend our results to Bidirectional RNN which uses both previous and future information to output the label at each time point, and is employed in many NLP tasks such as POS tagging. "
792887396606078976,2016-10-31 00:34:21,https://t.co/bgJiF4Dl35,Operator Variational Inference. (arXiv:1610.09033v1 [stat.ML]) https://t.co/bgJiF4Dl35,6,39," Abstract: Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images. "
792887394857091072,2016-10-31 00:34:20,https://t.co/oJwl4R3tgz,Geometric Dirichlet Means algorithm for topic inference. (arXiv:1610.09034v1 [stat.ML]) https://t.co/oJwl4R3tgz,0,4," Abstract: We propose a geometric algorithm for topic learning and inference that is built on the convex geometry of topics arising from the Latent Dirichlet Allocation (LDA) model and its nonparametric extensions. To this end we study the optimization of a geometric loss function, which is a surrogate to the LDA's likelihood. Our method involves a fast optimization based weighted clustering procedure augmented with geometric corrections, which overcomes the computational and statistical inefficiencies encountered by other techniques based on Gibbs sampling and variational inference, while achieving the accuracy comparable to that of a Gibbs sampler. The topic estimates produced by our method are shown to be statistically consistent under some conditions. The algorithm is evaluated with extensive experiments on simulated and real data. "
792887392797650946,2016-10-31 00:34:20,https://t.co/CoM8nuhS4q,Professor Forcing: A New Algorithm for Training Recurrent Networks. (arXiv:1610.09038v1 [stat.ML]) https://t.co/CoM8nuhS4q,2,14," Abstract: The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar. "
792887390771810305,2016-10-31 00:34:19,https://t.co/N6pQkiUsuL,Orthogonal Random Features. (arXiv:1610.09072v1 [cs.LG]) https://t.co/N6pQkiUsuL,0,7," Abstract: We present an intriguing discovery related to Random Fourier Features: in Gaussian kernel approximation, replacing the random Gaussian matrix by a properly scaled random orthogonal matrix significantly decreases kernel approximation error. We call this technique Orthogonal Random Features (ORF), and provide theoretical and empirical justification for this behavior. Motivated by this discovery, we further propose Structured Orthogonal Random Features (SORF), which uses a class of structured discrete orthogonal matrices to speed up the computation. The method reduces the time cost from $\mathcal{O}(d^2)$ to $\mathcal{O}(d \log d)$, where $d$ is the data dimensionality, with almost no compromise in kernel approximation quality compared to ORF. Experiments on several datasets verify the effectiveness of ORF and SORF over the existing methods. We also provide discussions on using the same type of discrete orthogonal structure for a broader range of applications. "
792887388951441409,2016-10-31 00:34:19,https://t.co/0HjYL5YxMF,Missing Data Imputation for Supervised Learning. (arXiv:1610.09075v1 [stat.ML]) https://t.co/0HjYL5YxMF,0,5," Abstract: This paper compares methods for imputing missing categorical data for supervised learning tasks. The ability of researchers to accurately fit a model and yield unbiased estimates may be compromised by missing data, which are prevalent in survey-based social science research. We experiment on two machine learning benchmark datasets with missing categorical data, comparing classifiers trained on non-imputed (i.e., one-hot encoded) or imputed data with different degrees of missing-data perturbation. The results show imputation methods can increase predictive accuracy in the presence of missing-data perturbation. Additionally, we find that for imputed models, missing-data perturbation can improve prediction accuracy by regularizing the classifier. "
792887386816610304,2016-10-31 00:34:18,https://t.co/LhTwV8CIF2,SOL: A Library for Scalable Online Learning Algorithms. (arXiv:1610.09083v1 [cs.LG]) https://t.co/LhTwV8CIF2,1,9," Abstract: SOL is an open-source library for scalable online learning algorithms, and is particularly suitable for learning with high-dimensional data. The library provides a family of regular and sparse online learning algorithms for large-scale binary and multi-class classification tasks with high efficiency, scalability, portability, and extensibility. SOL was implemented in C++, and provided with a collection of easy-to-use command-line tools, python wrappers and library calls for users and developers, as well as comprehensive documents for both beginners and advanced users. SOL is not only a practical machine learning toolbox, but also a comprehensive experimental platform for online learning research. Experiments demonstrate that SOL is highly efficient and scalable for large-scale machine learning with high-dimensional data. "
792887384631283713,2016-10-31 00:34:18,https://t.co/nAY7EgKa5e,Decentralized Clustering and Linking by Networked Agents. (arXiv:1610.09112v1 [math.OC]) https://t.co/nAY7EgKa5e,0,2," Abstract: We consider the problem of decentralized clustering and estimation over multi-task networks, where agents infer and track different models of interest. The agents do not know beforehand which model is generating their own data. They also do not know which agents in their neighborhood belong to the same cluster. We propose a decentralized clustering algorithm aimed at identifying and forming clusters of agents of similar objectives, and at guiding cooperation to enhance the inference performance. One key feature of the proposed technique is the integration of the learning and clustering tasks into a single strategy. We analyze the performance of the procedure and show that the error probabilities of types I and II decay exponentially to zero with the step-size parameter. While links between agents following different objectives are ignored in the clustering process, we nevertheless show how to exploit these links to relay critical information across the network for enhanced performance. Simulation results illustrate the performance of the proposed method in comparison to other useful techniques. "
792887382467051520,2016-10-31 00:34:17,https://t.co/Cg0Ls1L8h2,A framework for adaptive regularization in streaming Lasso models. (arXiv:1610.09127v1 [stat.ML]) https://t.co/Cg0Ls1L8h2,0,6," Abstract: Large scale, streaming datasets are ubiquitous in modern machine learning. Streaming algorithms must be scalable, amenable to incremental training and robust to the presence of non-stationarity. In this work consider the problem of learning $\ell_1$ regularized linear models in the context of streaming data. In particular, the focus of this work revolves around how to select the regularization parameter when data arrives sequentially and the underlying distribution is non-stationary (implying the choice of optimal regularization parameter is itself time-varying). We propose a novel framework through which to infer an adaptive regularization parameter. Our approach employs an $\ell_1$ penalty constraint where the corresponding sparsity parameter is iteratively updated via stochastic gradient descent. This serves to reformulate the choice of regularization parameter in a principled framework for online learning and allows for the derivation of convergence guarantees in a non-stochastic setting. We validate our approach using simulated and real datasets and present an application to a neuroimaging dataset. "
792887380512534528,2016-10-31 00:34:17,https://t.co/X9B0fcExJd,Fuzzy Bayesian Learning. (arXiv:1610.09156v1 [stat.ML]) https://t.co/X9B0fcExJd,0,3, Abstract: In this paper we propose a novel approach for learning from data using rule based fuzzy inference systems where the model parameters are estimated using Bayesian inference and Markov Chain Monte Carlo (MCMC) techniques. We show the applicability of the method for regression and classification tasks using synthetic data-sets and also a real world example in the financial services industry. Then we demonstrate how the method can be extended for knowledge extraction to select the individual rules in a Bayesian way which best explains the given data. Finally we discuss the advantages and pitfalls of using this method over state-of-the-art techniques and highlight the specific class of problems where this would be useful. 
792887378029445121,2016-10-31 00:34:16,https://t.co/rMEMgVB32I,Flexible constrained sampling with guarantees for pattern mining. (arXiv:1610.09263v1 [cs.AI]) https://t.co/rMEMgVB32I,0,2," Abstract: Pattern sampling has been proposed as a potential solution to the infamous pattern explosion. Instead of enumerating all patterns that satisfy the constraints, individual patterns are sampled proportional to a given quality measure. Several sampling algorithms have been proposed, but each of them has its limitations when it comes to 1) flexibility in terms of quality measures and constraints that can be used, and/or 2) guarantees with respect to sampling accuracy. We therefore present Flexics, the first flexible pattern sampler that supports a broad class of quality measures and constraints, while providing strong guarantees regarding sampling accuracy. To achieve this, we leverage the perspective on pattern mining as a constraint satisfaction problem and build upon the latest advances in sampling solutions in SAT as well as existing pattern mining algorithms. Furthermore, the proposed algorithm is applicable to a variety of pattern languages, which allows us to introduce and tackle the novel task of sampling sets of patterns. We introduce and empirically evaluate two variants of Flexics: 1) a generic variant that addresses the well-known itemset sampling task and the novel pattern set sampling task as well as a wide range of expressive constraints within these tasks, and 2) a specialized variant that exploits existing frequent itemset techniques to achieve substantial speed-ups. Experiments show that Flexics is both accurate and efficient, making it a useful tool for pattern-based data exploration. "
792887375781330945,2016-10-31 00:34:16,https://t.co/PJYIQv151A,Toward Implicit Sample Noise Modeling: Deviation-driven Matrix Factorization. (arXiv:1610.09274v1 [cs.LG]) https://t.co/PJYIQv151A,0,1," Abstract: The objective function of a matrix factorization model usually aims to minimize the average of a regression error contributed by each element. However, given the existence of stochastic noises, the implicit deviations of sample data from their true values are almost surely diverse, which makes each data point not equally suitable for fitting a model. In this case, simply averaging the cost among data in the objective function is not ideal. Intuitively we would like to emphasize more on the reliable instances (i.e., those contain smaller noise) while training a model. Motivated by such observation, we derive our formula from a theoretical framework for optimal weighting under heteroscedastic noise distribution. Specifically, by modeling and learning the deviation of data, we design a novel matrix factorization model. Our model has two advantages. First, it jointly learns the deviation and conducts dynamic reweighting of instances, allowing the model to converge to a better solution. Second, during learning the deviated instances are assigned lower weights, which leads to faster convergence since the model does not need to overfit the noise. The experiments are conducted in clean recommendation and noisy sensor datasets to test the effectiveness of the model in various scenarios. The results show that our model outperforms the state-of-the-art factorization and deep learning models in both accuracy and efficiency. "
792887373558345729,2016-10-31 00:34:15,https://t.co/Vh7wBaicIL,Improving Sampling from Generative Autoencoders with Markov Chains. (arXiv:1610.09296v1 [cs.LG]) https://t.co/Vh7wBaicIL,0,9," Abstract: We focus on generative autoencoders, such as variational or adversarial autoencoders, which jointly learn a generative model alongside an inference model. Generative autoencoders are those which are trained to softly enforce a prior on the latent distribution learned by the inference model. However, the inference model may not always map inputs to latent samples that are consistent with the prior. We formulate a Markov chain Monte Carlo (MCMC) sampling process, equivalent to iteratively encoding and decoding, which allows us to sample from the learned latent distribution. Using this, we can improve the quality of samples drawn from the model, especially when the learned distribution is far from the prior. Using MCMC sampling, we are able to reveal previously unseen differences between generative autoencoders trained either with or without a denoising criterion. "
792887370819444736,2016-10-31 00:34:14,https://t.co/ZooxYh7qsF,Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods. (arXiv:1610.0… https://t.co/ZooxYh7qsF,0,7," Abstract: The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward neural networks can be trained globally optimal with a linear convergence rate with our nonlinear spectral method. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one and two hidden layer networks. Our experiments confirm that these models are rich enough to achieve good performance on a series of real-world datasets. "
792887368978235392,2016-10-31 00:34:14,https://t.co/AFyPHUGIuL,Correlated-PCA: Principal Components' Analysis when Data and Noise are Correlated. (arXiv:1610.09307v1 [cs.LG]) https://t.co/AFyPHUGIuL,1,9," Abstract: Given a matrix of observed data, Principal Components Analysis (PCA) computes a small number of orthogonal directions that contain most of its variability. Provably accurate solutions for PCA have been in use for a long time. However, to the best of our knowledge, all existing theoretical guarantees for it assume that the data and the corrupting noise are mutually independent, or at least uncorrelated. This is valid in practice often, but not always. In this paper, we study the PCA problem in the setting where the data and noise can be correlated. Such noise is often also referred to as ""data-dependent noise"". We obtain a correctness result for the standard eigenvalue decomposition (EVD) based solution to PCA under simple assumptions on the data-noise correlation. We also develop and analyze a generalization of EVD, cluster-EVD, that improves upon EVD in certain regimes. "
792887367073992705,2016-10-31 00:34:14,https://t.co/nbfxXn0WhE,Homotopy Method for Tensor Principal Component Analysis. (arXiv:1610.09322v1 [stat.ML]) https://t.co/nbfxXn0WhE,0,4," Abstract: Developing efficient and guaranteed nonconvex algorithms has been an important challenge in modern machine learning. Algorithms with good empirical performance such as stochastic gradient descent often lack theoretical guarantees. In this paper, we analyze the class of homotopy or continuation methods for global optimization of nonconvex functions. These methods start from an objective function that is efficient to optimize (e.g. convex), and progressively modify it to obtain the required objective, and the solutions are passed along the homotopy path. For the challenging problem of tensor PCA, we prove global convergence of the homotopy method in the ""high noise"" regime. The signal-to-noise requirement for our algorithm is tight in the sense that it matches the recovery guarantee for the best degree-4 sum-of-squares algorithm. In addition, we prove a phase transition along the homotopy path for tensor PCA. This allows to simplify the homotopy method to a local search algorithm, viz., tensor power iterations, with a specific initialization and a noise injection procedure, while retaining the theoretical guarantees. "
792887365090078720,2016-10-31 00:34:13,https://t.co/CpPtkLbXen,Improved Gibbs Sampling Parameter Estimators for Latent Dirichlet Allocation. (arXiv:1505.02065v5 [stat.ML] UPDATE… https://t.co/CpPtkLbXen,0,8," Abstract: Latent Dirichlet Allocation (LDA) is a generative probabilistic model for discovering the underlying structure of discrete data. LDA and its extensions have been successfully used for both unsupervised and supervised learning tasks across a variety of data types including textual, image, and biological data. After more than a decade of intensive research on training algorithms for LDA, the Collapsed Gibbs Sampler (CGS), in which the parameters are marginalized out, remains one of the most popular LDA inference algorithms. We introduce a novel approach for estimating LDA parameters from collapsed Gibbs samples, by leveraging the full conditional distributions over the latent variable assignments to efficiently average over multiple samples, for little more computational cost than drawing a single sample. Our approach can be understood as adapting the soft clustering methodology of Collapsed Variational Bayes (CVB0) to CGS parameter estimation, in order to get the best of both techniques. We perform extensive empirical comparisons of our estimators with those of standard collapsed inference algorithms on real-world data for both unsupervised LDA and Prior-LDA, a supervised variant of LDA for multi-label classification. Our results show a consistent advantage of our approach over traditional CGS under all experimental conditions, and over CVB0 inference in the majority of conditions. More broadly, our results highlight the importance of averaging over multiple samples in LDA parameter estimation, and the use of efficient computational techniques to do so. "
792887363382943744,2016-10-31 00:34:13,https://t.co/g8I02ouGhn,R\'enyi Divergence Variational Inference. (arXiv:1602.02311v3 [stat.ML] UPDATED) https://t.co/g8I02ouGhn,1,6," Abstract: This paper introduces the variational R\'enyi bound (VR) that extends traditional variational inference to R\'enyi's alpha-divergences. This new family of variational methods unifies a number of existing approaches, and enables a smooth interpolation from the evidence lower-bound to the log (marginal) likelihood that is controlled by the value of alpha that parametrises the divergence. The reparameterization trick, Monte Carlo approximation and stochastic optimisation methods are deployed to obtain a tractable and unified framework for optimisation. We further consider negative alpha values and propose a novel variational inference method as a new special case in the proposed framework. Experiments on Bayesian neural networks and variational auto-encoders demonstrate the wide applicability of the VR bound. "
792887361520689152,2016-10-31 00:34:12,https://t.co/3Rob0T8OvH,A Simple Practical Accelerated Method for Finite Sums. (arXiv:1602.02442v2 [stat.ML] UPDATED) https://t.co/3Rob0T8OvH,0,1," Abstract: We describe a novel optimization method for finite sums (such as empirical risk minimization problems) building on the recently introduced SAGA method. Our method achieves an accelerated convergence rate on strongly convex smooth problems. Our method has only one parameter (a step size), and is radically simpler than other accelerated methods for finite sums. Additionally it can be applied when the terms are non-smooth, yielding a method applicable in many areas where operator splitting methods would traditionally be applied. "
792887359746478080,2016-10-31 00:34:12,https://t.co/Q0EuiRgLzu,PAC Reinforcement Learning with Rich Observations. (arXiv:1602.02722v4 [cs.LG] UPDATED) https://t.co/Q0EuiRgLzu,0,6," Abstract: We propose and study a new model for reinforcement learning with rich observations, generalizing contextual bandits to sequential decision making. These models require an agent to take actions based on observations (features) with the goal of achieving long-term performance competitive with a large set of policies. To avoid barriers to sample-efficient learning associated with large observation spaces and general POMDPs, we focus on problems that can be summarized by a small number of hidden states and have long-term rewards that are predictable by a reactive function class. In this setting, we design and analyze a new reinforcement learning algorithm, Least Squares Value Elimination by Exploration. We prove that the algorithm learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. Our result provides theoretical justification for reinforcement learning with function approximation. "
792887357687078912,2016-10-31 00:34:11,https://t.co/iYImAYz4JD,Theoretical Comparisons of Positive-Unlabeled Learning against Positive-Negative Learning. (arXiv:1603.03130v3 [cs… https://t.co/iYImAYz4JD,0,3," Abstract: In PU learning, a binary classifier is trained from positive (P) and unlabeled (U) data without negative (N) data. Although N data is missing, it sometimes outperforms PN learning (i.e., ordinary supervised learning). Hitherto, neither theoretical nor experimental analysis has been given to explain this phenomenon. In this paper, we theoretically compare PU (and NU) learning against PN learning based on the upper bounds on estimation errors. We find simple conditions when PU and NU learning are likely to outperform PN learning, and we prove that, in terms of the upper bounds, either PU or NU learning (depending on the class-prior probability and the sizes of P and N data) given infinite U data will improve on PN learning. Our theoretical findings well agree with the experimental results on artificial and benchmark data even when the experimental setup does not match the theoretical assumptions exactly. "
792887355740946438,2016-10-31 00:34:11,https://t.co/wNKhoL7gEu,Statistical Inference for Cluster Trees. (arXiv:1605.06416v2 [math.ST] UPDATED) https://t.co/wNKhoL7gEu,0,2," Abstract: A cluster tree provides a highly-interpretable summary of a density function by representing the hierarchy of its high-density clusters. It is estimated using the empirical tree, which is the cluster tree constructed from a density estimator. This paper addresses the basic question of quantifying our uncertainty by assessing the statistical significance of features of an empirical cluster tree. We first study a variety of metrics that can be used to compare different trees, analyze their properties and assess their suitability for inference. We then propose methods to construct and summarize confidence sets for the unknown true cluster tree. We introduce a partial ordering on cluster trees which we use to prune some of the statistically insignificant features of the empirical tree, yielding interpretable and parsimonious cluster trees. Finally, we illustrate the proposed methods on a variety of synthetic examples and furthermore demonstrate their utility in the analysis of a Graft-versus-Host Disease (GvHD) data set. "
792887353819926528,2016-10-31 00:34:10,https://t.co/FdYWU4mAj7,Interpretable Distribution Features with Maximum Testing Power. (arXiv:1605.06796v2 [stat.ML] UPDATED) https://t.co/FdYWU4mAj7,0,2," Abstract: Two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features). The features are chosen so as to maximize the distinguishability of the distributions, by optimizing a lower bound on test power for a statistical test using these features. The result is a parsimonious and interpretable indication of how and where two distributions differ locally. An empirical estimate of the test power criterion converges with increasing sample size, ensuring the quality of the returned features. In real-world benchmarks on high-dimensional text and image data, linear-time tests using the proposed semimetrics achieve comparable performance to the state-of-the-art quadratic-time maximum mean discrepancy test, while returning human-interpretable features that explain the test results. "
792887352066772992,2016-10-31 00:34:10,https://t.co/jeeQJ6PIXO,Convergence guarantees for kernel-based quadrature rules in misspecified settings. (arXiv:1605.07254v2 [stat.ML] U… https://t.co/jeeQJ6PIXO,2,8," Abstract: Kernel-based quadrature rules are becoming important in machine learning and statistics, as they achieve super-$\sqrt{n}$ convergence rates in numerical integration, and thus provide alternatives to Monte Carlo integration in challenging settings where integrands are expensive to evaluate or where integrands are high dimensional. These rules are based on the assumption that the integrand has a certain degree of smoothness, which is expressed as that the integrand belongs to a certain reproducing kernel Hilbert space (RKHS). However, this assumption can be violated in practice (e.g., when the integrand is a black box function), and no general theory has been established for the convergence of kernel quadratures in such misspecified settings. Our contribution is in proving that kernel quadratures can be consistent even when the integrand does not belong to the assumed RKHS, i.e., when the integrand is less smooth than assumed. Specifically, we derive convergence rates that depend on the (unknown) lesser smoothness of the integrand, where the degree of smoothness is expressed via powers of RKHSs or via Sobolev spaces. "
792887350347128832,2016-10-31 00:34:10,https://t.co/0mZoao7KHO,On Valid Optimal Assignment Kernels and Applications to Graph Classification. (arXiv:1606.01141v2 [cs.LG] UPDATED) https://t.co/0mZoao7KHO,0,2," Abstract: The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel. "
792887348300316672,2016-10-31 00:34:09,https://t.co/8VGSCutDo3,Estimating individual treatment effect: generalization bounds and algorithms. (arXiv:1606.03976v3 [stat.ML] UPDATE… https://t.co/8VGSCutDo3,1,2," Abstract: There is intense interest in applying machine learning to problems of causal inference in healthcare, economics, education, and other fields. In particular, individual-level causal inference has applications such as precision medicine and personalized advertising. We give a new theoretical analysis and family of algorithms for estimating individual treatment effect (ITE) from observational data. The algorithm itself learns a ""balanced"" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distance. Experiments on real and simulated data show the new algorithms match or outperform state-of-the-art methods. "
792887345985032192,2016-10-31 00:34:09,https://t.co/PGbzXCS8sU,High-dimensional regression adjustments in randomized experiments. (arXiv:1607.06801v3 [stat.ME] UPDATED) https://t.co/PGbzXCS8sU,0,2," Abstract: We study the problem of treatment effect estimation in randomized experiments with high-dimensional covariate information, and show that essentially any risk-consistent regression adjustment can be used to obtain efficient estimates of the average treatment effect. Our results considerably extend the range of settings where high-dimensional regression adjustments are guaranteed to provide valid inference about the population average treatment effect. We then propose cross-estimation, a simple method for obtaining finite-sample-unbiased treatment effect estimates that leverages high-dimensional regression adjustments. Our method can be used when the regression model is estimated using the lasso, the elastic net, subset selection, etc. Finally, we extend our analysis to allow for adaptive specification search via cross-validation, and flexible non-parametric regression adjustments with machine learning methods such as random forests or neural networks. "
792887342323343364,2016-10-31 00:34:08,https://t.co/QduPOYnMw5,Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back. (arXiv:1608.04414v2 [cs.LG] U… https://t.co/QduPOYnMw5,0,7," Abstract: In stochastic convex optimization the goal is to minimize a convex function $F(x) \doteq {\mathbf E}_{{\mathbf f}\sim D}[{\mathbf f}(x)]$ over a convex set $\cal K \subset {\mathbb R}^d$ where $D$ is some unknown distribution and each $f(\cdot)$ in the support of $D$ is convex over $\cal K$. The optimization is commonly based on i.i.d.~samples $f^1,f^2,\ldots,f^n$ from $D$. A standard approach to such problems is empirical risk minimization (ERM) that optimizes $F_S(x) \doteq \frac{1}{n}\sum_{i\leq n} f^i(x)$. Here we consider the question of how many samples are necessary for ERM to succeed and the closely related question of uniform convergence of $F_S$ to $F$ over $\cal K$. We demonstrate that in the standard $\ell_p/\ell_q$ setting of Lipschitz-bounded functions over a $\cal K$ of bounded radius, ERM requires sample size that scales linearly with the dimension $d$. This nearly matches standard upper bounds and improves on $\Omega(\log d)$ dependence proved for $\ell_2/\ell_2$ setting by Shalev-Shwartz et al. (2009). In stark contrast, these problems can be solved using dimension-independent number of samples for $\ell_2/\ell_2$ setting and $\log d$ dependence for $\ell_1/\ell_\infty$ setting using other approaches. We further show that our lower bound applies even if the functions in the support of $D$ are smooth and efficiently computable and even if an $\ell_1$ regularization term is added. Finally, we demonstrate that for a more general class of bounded-range (but not Lipschitz-bounded) stochastic convex programs an infinite gap appears already in dimension 2. "
792887339697799169,2016-10-31 00:34:07,https://t.co/KQYhgxyuwp,Hybrid CPU-GPU Framework for Network Motifs. (arXiv:1608.05138v2 [cs.DC] UPDATED) https://t.co/KQYhgxyuwp,1,2," Abstract: Massively parallel architectures such as the GPU are becoming increasingly important due to the recent proliferation of data. In this paper, we propose a key class of hybrid parallel graphlet algorithms that leverages multiple CPUs and GPUs simultaneously for computing k-vertex induced subgraph statistics (called graphlets). In addition to the hybrid multi-core CPU-GPU framework, we also investigate single GPU methods (using multiple cores) and multi-GPU methods that leverage all available GPUs simultaneously for computing induced subgraph statistics. Both methods leverage GPU devices only, whereas the hybrid multi-core CPU-GPU framework leverages all available multi-core CPUs and multiple GPUs for computing graphlets in large networks. Compared to recent approaches, our methods are orders of magnitude faster, while also more cost effective enjoying superior performance per capita and per watt. In particular, the methods are up to 300 times faster than the recent state-of-the-art method. To the best of our knowledge, this is the first work to leverage multiple CPUs and GPUs simultaneously for computing induced subgraph statistics. "
792887335784476672,2016-10-31 00:34:06,https://t.co/AWW5isLHyb,A penalized likelihood method for classification with matrix-valued predictors. (arXiv:1609.07386v2 [stat.ML] UPDA… https://t.co/AWW5isLHyb,0,3," Abstract: We propose a penalized likelihood method to fit the linear discriminant analysis model when the predictor is matrix valued. We simultaneously estimate the means and the precision matrix, which we assume has a Kronecker product decomposition. Our penalties encourage pairs of response category mean matrices to have equal entries and also encourage zeros in the precision matrix. To compute our estimators, we use a blockwise coordinate descent algorithm. To update the optimization variables corresponding to response category mean matrices, we use an alternating minimization algorithm that takes advantage of the Kronecker structure of the precision matrix. We show that our method can outperform relevant competitors in classification, even when our modeling assumptions are violated. We analyze an EEG dataset to demonstrate our method's interpretability and classification accuracy. "
791800284993445888,2016-10-28 00:34:33,https://t.co/QswYvyjM2x,Causal Network Learning from Multiple Interventions of Unknown Manipulated Targets. (arXiv:1610.08611v1 [stat.ML]) https://t.co/QswYvyjM2x,0,3," Abstract: In this paper, we discuss structure learning of causal networks from multiple data sets obtained by external intervention experiments where we do not know what variables are manipulated. For example, the conditions in these experiments are changed by changing temperature or using drugs, but we do not know what target variables are manipulated by the external interventions. From such data sets, the structure learning becomes more difficult. For this case, we first discuss the identifiability of causal structures. Next we present a graph-merging method for learning causal networks for the case that the sample sizes are large for these interventions. Then for the case that the sample sizes of these interventions are relatively small, we propose a data-pooling method for learning causal networks in which we pool all data sets of these interventions together for the learning. Further we propose a re-sampling approach to evaluate the edges of the causal network learned by the data-pooling method. Finally we illustrate the proposed learning methods by simulations. "
791800283470893056,2016-10-28 00:34:33,https://t.co/zge6ucWtMv,Poisson intensity estimation with reproducing kernels. (arXiv:1610.08623v1 [stat.ML]) https://t.co/zge6ucWtMv,1,4," Abstract: Despite the fundamental nature of the inhomogeneous Poisson process in the theory and application of stochastic processes, and its attractive generalizations (e.g.~Cox process), few tractable nonparametric modeling approaches of intensity functions exist, especially in high dimensional settings. In this paper we develop a new, computationally tractable Reproducing Kernel Hilbert Space (RKHS) formulation for the inhomogeneous Poisson process. We model the square root of the intensity as an RKHS function. The modeling challenge is that the usual representer theorem arguments no longer apply due to the form of the inhomogeneous Poisson process likelihood. However, we prove that the representer theorem does hold in an appropriately transformed RKHS, guaranteeing that the optimization of the penalized likelihood can be cast as a tractable finite-dimensional problem. The resulting approach is simple to implement, and readily scales to high dimensions and large-scale datasets. "
791800281520570368,2016-10-28 00:34:32,https://t.co/uBxfKGejpd,PCM and APCM Revisited: An Uncertainty Perspective. (arXiv:1610.08624v1 [cs.CV]) https://t.co/uBxfKGejpd,0,1," Abstract: In this paper, we take a new look at the possibilistic c-means (PCM) and adaptive PCM (APCM) clustering algorithms from the perspective of uncertainty. This new perspective offers us insights into the clustering process, and also provides us greater degree of flexibility. We analyze the clustering behavior of PCM-based algorithms and introduce parameters $\sigma_v$ and $\alpha$ to characterize uncertainty of estimated bandwidth and noise level of the dataset respectively. Then uncertainty (fuzziness) of membership values caused by uncertainty of the estimated bandwidth parameter is modeled by a conditional fuzzy set, which is a new formulation of the type-2 fuzzy set. Experiments show that parameters $\sigma_v$ and $\alpha$ make the clustering process more easy to control, and main features of PCM and APCM are unified in this new clustering framework (UPCM). More specifically, UPCM reduces to PCM when we set a small $\alpha$ or a large $\sigma_v$, and UPCM reduces to APCM when clusters are confined in their physical clusters and possible cluster elimination are ensured. Finally we present further researches of this paper. "
791800279951876097,2016-10-28 00:34:32,https://t.co/KbdOHstbUA,Regret Bounds for Lifelong Learning. (arXiv:1610.08628v1 [stat.ML]) https://t.co/KbdOHstbUA,1,3," Abstract: We consider the problem of transfer learning in an online setting. Different tasks are presented sequentially and processed by a within-task algorithm. We propose a lifelong learning strategy which refines the underlying data representation used by the within-task algorithm, thereby transferring information from one task to the next. We show that when the within-task algorithm comes with some regret bound, our strategy inherits this good property. Our bounds are in expectation for a general loss function, and uniform for a convex loss. We discuss applications to dictionary learning and finite set of predictors. In the latter case, we improve previous $O(1/\sqrt{m})$ bounds to $O(1/m)$ where $m$ is the per task sample size. "
791800278379094016,2016-10-28 00:34:31,https://t.co/gntvdZb9zN,Statistical Inference for Model Parameters in Stochastic Gradient Descent. (arXiv:1610.08637v1 [stat.ML]) https://t.co/gntvdZb9zN,0,3," Abstract: The stochastic gradient descent (SGD) algorithm has been widely used in statistical estimation for large-scale data due to its computational and memory efficiency. While most existing work focuses on the convergence of the objective function or the error of the obtained solution, we investigate the problem of statistical inference of the true model parameters based on SGD. To this end, we propose two consistent estimators of the asymptotic covariance of the average iterate from SGD: (1) an intuitive plug-in estimator and (2) a computationally more efficient batch-means estimator, which only uses the iterates from SGD. As the SGD process forms a time-inhomogeneous Markov chain, our batch-means estimator with carefully chosen increasing batch sizes generalizes the classical batch-means estimator designed for time-homogenous Markov chains. The proposed batch-means estimator is of independent interest, which can be potentially used for estimating the covariance of other time-inhomogeneous Markov chains. Both proposed estimators allow us to construct asymptotically exact confidence intervals and hypothesis tests. We further discuss an extension to conducting inference based on SGD for high-dimensional linear regression. Using a variant of the SGD algorithm, we construct a debiased estimator of each regression coefficient that is asymptotically normal. This gives a one-pass algorithm for computing both the sparse regression coefficient estimator and confidence intervals, which is computationally attractive and applicable to online data. "
791800276890030080,2016-10-28 00:34:31,https://t.co/9wfm4GMes9,Learning Bound for Parameter Transfer Learning. (arXiv:1610.08696v1 [stat.ML]) https://t.co/9wfm4GMes9,0,4," Abstract: We consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then, we introduce the notion of the local stability and parameter transfer learnability of parametric feature mapping,and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in self-taught learning. Although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance, their theoretical analysis has not been studied. In this paper, we also provide the first theoretical learning bound for self-taught learning. "
791800275162001408,2016-10-28 00:34:31,https://t.co/en2R9iMplQ,GPflow: A Gaussian process library using TensorFlow. (arXiv:1610.08733v1 [stat.ML]) https://t.co/en2R9iMplQ,52,70," Abstract: GPflow is a Gaussian process library that uses TensorFlow for its core computations and Python for its front end. The distinguishing features of GPflow are that it uses variational inference as the primary approximation method, provides concise code through the use of automatic differentiation, has been engineered with a particular emphasis on software testing and is able to exploit GPU hardware. "
791800273522008064,2016-10-28 00:34:30,https://t.co/0Zfe3Pw6Mz,Stratification of patient trajectories using covariate latent variable models. (arXiv:1610.08735v1 [stat.ML]) https://t.co/0Zfe3Pw6Mz,0,4," Abstract: Standard models assign disease progression to discrete categories or stages based on well-characterized clinical markers. However, such a system is potentially at odds with our understanding of the underlying biology, which in highly complex systems may support a (near-)continuous evolution of disease from inception to terminal state. To learn such a continuous disease score one could infer a latent variable from dynamic omics data such as RNA-seq that correlates with an outcome of interest such as survival time. However, such analyses may be confounded by additional data such as clinical covariates measured in electronic health records (EHRs). As a solution to this we introduce covariate latent variable models, a novel type of latent variable model that learns a low-dimensional data representation in the presence of two (asymmetric) views of the same data source. We apply our model to TCGA colorectal cancer RNA-seq data and demonstrate how incorporating microsatellite-instability (MSI) and metastatic status as external covariates allows us to identify genes that stratify patients on an immune-response trajectory. Finally, we propose an extension termed Covariate Gaussian Process Latent Variable Models for learning nonparametric, nonlinear representations. "
791800271915585536,2016-10-28 00:34:30,https://t.co/B9FdihzAoM,Compressive K-means. (arXiv:1610.08738v1 [cs.LG]) https://t.co/B9FdihzAoM,1,5," Abstract: The Lloyd-Max algorithm is a classical approach to perform K-means clustering. Unfortunately, its cost becomes prohibitive as the training dataset grows large. We propose a compressive version of K-means (CKM), that estimates cluster centers from a sketch, i.e. from a drastically compressed representation of the training dataset. We demonstrate empirically that CKM performs similarly to Lloyd-Max, for a sketch size proportional to the number of cen-troids times the ambient dimension, and independent of the size of the original dataset. Given the sketch, the computational complexity of CKM is also independent of the size of the dataset. Unlike Lloyd-Max which requires several replicates, we further demonstrate that CKM is almost insensitive to initialization. For a large dataset of 10^7 data points, we show that CKM can run two orders of magnitude faster than five replicates of Lloyd-Max, with similar clustering performance on artificial data. Finally, CKM achieves lower classification errors on handwritten digits classification. "
791800270384660480,2016-10-28 00:34:30,https://t.co/vTDdRjQt9G,Differentially Private Variational Inference for Non-conjugate Models. (arXiv:1610.08749v1 [stat.ML]) https://t.co/vTDdRjQt9G,1,6," Abstract: As collecting huge amounts of personal data from individuals has been established as a standard nowadays, it is really important to use these data in a conscientious way. For example, when performing inference using these data, one has to make sure individuals' identities or the privacy of the data are not compromised. Differential privacy is a powerful framework that introduces stochasticity into the computation to guarantee that it is difficult to breach the privacy using the output of the computation. Differentially private versions of many important machine learning methods have been proposed, but still there is a long way to pave towards an efficient unified approach applicable to handle many models. In this paper, we propose a differentially private variational inference method with a very wide applicability. The variational inference is based on stochastic gradient ascent and can handle non-conjugate models as well as conjugate ones. Differential privacy is achieved by perturbing the gradients. We explore ways to make the algorithm more efficient through privacy amplification from subsampling and through clipping the gradients to limit the amount of information they leak. We explore the effect of different parameter combinations in logistic regression problems where the method can reach an accuracy close to non-private level under reasonably strong privacy guarantees. "
791800267880685568,2016-10-28 00:34:29,https://t.co/XkUTX08zRl,Sparse Signal Subspace Decomposition Based on Adaptive Over-complete Dictionary. (arXiv:1610.08813v1 [stat.ML]) https://t.co/XkUTX08zRl,0,7," Abstract: This paper proposes a subspace decomposition method based on an over-complete dictionary in sparse representation, called ""Sparse Signal Subspace Decomposition"" (or 3SD) method. This method makes use of a novel criterion based on the occurrence frequency of atoms of the dictionary over the data set. This criterion, well adapted to subspace-decomposition over a dependent basis set, adequately re ects the intrinsic characteristic of regularity of the signal. The 3SD method combines variance, sparsity and component frequency criteria into an unified framework. It takes benefits from using an over-complete dictionary which preserves details and from subspace decomposition which rejects strong noise. The 3SD method is very simple with a linear retrieval operation. It does not require any prior knowledge on distributions or parameters. When applied to image denoising, it demonstrates high performances both at preserving fine details and suppressing strong noise. "
791800265657778176,2016-10-28 00:34:28,https://t.co/m65rn9CKeu,A Category Space Approach to Supervised Dimensionality Reduction. (arXiv:1610.08838v1 [stat.ML]) https://t.co/m65rn9CKeu,0,3," Abstract: Supervised dimensionality reduction has emerged as an important theme in the last decade. Despite the plethora of models and formulations, there is a lack of a simple model which aims to project the set of patterns into a space defined by the classes (or categories). To this end, we set up a model in which each class is represented as a 1D subspace of the vector space formed by the features. Assuming the set of classes does not exceed the cardinality of the features, the model results in multi-class supervised learning in which the features of each class are projected into the class subspace. Class discrimination is automatically guaranteed via the imposition of orthogonality of the 1D class sub-spaces. The resulting optimization problem - formulated as the minimization of a sum of quadratic functions on a Stiefel manifold - while being non-convex (due to the constraints), nevertheless has a structure for which we can identify when we have reached a global minimum. After formulating a version with standard inner products, we extend the formulation to reproducing kernel Hilbert spaces in a straightforward manner. The optimization approach also extends in a similar fashion to the kernel version. Results and comparisons with the multi-class Fisher linear (and kernel) discriminants and principal component analysis (linear and kernel) showcase the relative merits of this approach to dimensionality reduction. "
791800264181293060,2016-10-28 00:34:28,https://t.co/9ftm4Zeoln,On Bochner's and Polya's Characterizations of Positive-Definite Kernels and the Respective Random Feature Maps. (a… https://t.co/9ftm4Zeoln,0,3," Abstract: Positive-definite kernel functions are fundamental elements of kernel methods and Gaussian processes. A well-known construction of such functions comes from Bochner's characterization, which connects a positive-definite function with a probability distribution. Another construction, which appears to have attracted less attention, is Polya's criterion that characterizes a subset of these functions. In this paper, we study the latter characterization and derive a number of novel kernels little known previously. In the context of large-scale kernel machines, Rahimi and Recht (2007) proposed a random feature map (random Fourier) that approximates a kernel function, through independent sampling of the probability distribution in Bochner's characterization. The authors also suggested another feature map (random binning), which, although not explicitly stated, comes from Polya's characterization. We show that with the same number of random samples, the random binning map results in an Euclidean inner product closer to the kernel than does the random Fourier map. The superiority of the random binning map is confirmed empirically through regressions and classifications in the reproducing kernel Hilbert space. "
791800261413105664,2016-10-28 00:34:27,https://t.co/L31wr9R1JU,Voice Conversion using Convolutional Neural Networks. (arXiv:1610.08927v1 [stat.ML]) https://t.co/L31wr9R1JU,0,4," Abstract: The human auditory system is able to distinguish the vocal source of thousands of speakers, yet not much is known about what features the auditory system uses to do this. Fourier Transforms are capable of capturing the pitch and harmonic structure of the speaker but this alone proves insufficient at identifying speakers uniquely. The remaining structure, often referred to as timbre, is critical to identifying speakers but we understood little about it. In this paper we use recent advances in neural networks in order to manipulate the voice of one speaker into another by transforming not only the pitch of the speaker, but the timbre. We review generative models built with neural networks as well as architectures for creating neural networks that learn analogies. Our preliminary results converting voices from one speaker to another are encouraging. "
791800258527424513,2016-10-28 00:34:27,https://t.co/1QAwEjOSfN,Rapid Posterior Exploration in Bayesian Non-negative Matrix Factorization. (arXiv:1610.08928v1 [stat.ML]) https://t.co/1QAwEjOSfN,0,3," Abstract: Non-negative Matrix Factorization (NMF) is a popular tool for data exploration. Bayesian NMF promises to also characterize uncertainty in the factorization. Unfortunately, current inference approaches such as MCMC mix slowly and tend to get stuck on single modes. We introduce a novel approach using rapidly-exploring random trees (RRTs) to asymptotically cover regions of high posterior density. These are placed in a principled Bayesian framework via an online extension to nonparametric variational inference. On experiments on real and synthetic data, we obtain greater coverage of the posterior and higher ELBO values than standard NMF inference approaches. "
791800256908390400,2016-10-28 00:34:26,https://t.co/bjV4sqJ4Wy,Learning Scalable Deep Kernels with Recurrent Structure. (arXiv:1610.08936v1 [cs.LG]) https://t.co/bjV4sqJ4Wy,1,9," Abstract: Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic procedure and exploit the structure of these kernels for fast and scalable training and prediction. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable. "
791800255075479552,2016-10-28 00:34:26,https://t.co/q1b8QBXn0h,Concentration for matrix martingales in continuous time and microscopic activity of social networks. (arXiv:1412.7… https://t.co/q1b8QBXn0h,1,6, Abstract: This paper gives new concentration inequalities for the spectral norm of a wide class of matrix martingales in continuous time. These results extend previously established Freedman and Bernstein inequalities for series of random matrices to the class of continuous time processes. Our analysis relies on a new supermartingale property of the trace exponential proved within the framework of stochastic calculus. We provide also several examples that illustrate the fact that our results allow us to recover easily several formerly obtained sharp bounds for discrete time matrix martingales. 
791800253569699840,2016-10-28 00:34:26,https://t.co/4KOjpsQNt8,Optimal model-assisted design of experiments for network correlated outcomes suggests new notions of network balan… https://t.co/4KOjpsQNt8,0,2," Abstract: We consider the problem of how to assign treatment in a randomized experiment, in which the correlation among the outcomes is informed by a network available pre-intervention. Working within the potential outcome causal framework, we develop a class of models that posit such a correlation structure among the outcomes. Then we leverage these models to develop rerandomization strategies for allocating treatment optimally, by minimizing the mean squared error of the estimated average treatment effect. Analytical decompositions of the mean squared error, due both to the model and to the randomization distribution, provide insights into aspects of the optimal designs. In particular, the analysis suggests new notions of balance based on specific network quantities, in addition to classical covariate balance. The resulting balanced, optimal rerandomization strategies are still design unbiased, in situations where the model used to derive them does not hold. We illustrate how the proposed treatment allocation strategies improve on allocations that ignore the network structure, with extensive simulations. "
791800252068233216,2016-10-28 00:34:25,https://t.co/9ZANzjkWrH,Distributed Bayesian Learning with Stochastic Natural-gradient Expectation Propagation and the Posterior Server. (… https://t.co/9ZANzjkWrH,2,8," Abstract: This paper makes two contributions to Bayesian machine learning algorithms. Firstly, we propose stochastic natural gradient expectation propagation (SNEP), a novel alternative to expectation propagation (EP), a popular variational inference algorithm. SNEP is a black box variational algorithm, in that it does not require any simplifying assumptions on the distribution of interest, beyond the existence of some Monte Carlo sampler for estimating the moments of the EP tilted distributions. Further, as opposed to EP which has no guarantee of convergence, SNEP can be shown to be convergent, even when using Monte Carlo moment estimates. Secondly, we propose a novel architecture for distributed Bayesian learning which we call the posterior server. The posterior server allows scalable and robust Bayesian learning in cases where a dataset is stored in a distributed manner across a cluster, with each compute node containing a disjoint subset of data. An independent Monte Carlo sampler is run on each compute node, with direct access only to the local data subset, but which targets an approximation to the global posterior distribution given all data across the whole cluster. This is achieved by using a distributed asynchronous implementation of SNEP to pass messages across the cluster. We demonstrate SNEP and the posterior server on distributed Bayesian learning of logistic regression and neural networks. Keywords: Distributed Learning, Large Scale Learning, Deep Learning, Bayesian Learning, Variational Inference, Expectation Propagation, Stochastic Approximation, Natural Gradient, Markov chain Monte Carlo, Parameter Server, Posterior Server. "
791800250553991168,2016-10-28 00:34:25,https://t.co/9oyPo85yBR,Double Thompson Sampling for Dueling Bandits. (arXiv:1604.07101v2 [cs.LG] UPDATED) https://t.co/9oyPo85yBR,0,4," Abstract: In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for dueling bandit problems. As indicated by its name, D-TS selects both the first and the second candidates according to Thompson Sampling. Specifically, D-TS maintains a posterior distribution for the preference matrix, and chooses the pair of arms for comparison by sampling twice from the posterior distribution. This simple algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as its special case. For general Copeland dueling bandits, we show that D-TS achieves $O(K^2 \log T)$ regret. For Condorcet dueling bandits, we further simplify the D-TS algorithm and show that the simplified D-TS algorithm achieves $O(K \log T + K^2 \log \log T)$ regret. Simulation results based on both synthetic and real-world data demonstrate the efficiency of the proposed D-TS algorithm. "
791800249169879040,2016-10-28 00:34:24,https://t.co/uFsa4flLGi],Coresets for Scalable Bayesian Logistic Regression. (arXiv:1605.06423v2 [https://t.co/uFsa4flLGi] UPDATED) https://t.co/eCgLurJ77V,0,2,INDEXERROR
791800247446011904,2016-10-28 00:34:24,https://t.co/DdytqXbHbW,Reshaped Wirtinger Flow and Incremental Algorithm for Solving Quadratic System of Equations. (arXiv:1605.07719v2 [… https://t.co/DdytqXbHbW,0,2," Abstract: We study the phase retrieval problem, which solves quadratic system of equations, i.e., recovers a vector $\boldsymbol{x}\in \mathbb{R}^n$ from its magnitude measurements $y_i=|\langle \boldsymbol{a}_i, \boldsymbol{x}\rangle|, i=1,..., m$. We develop a gradient-like algorithm (referred to as RWF representing reshaped Wirtinger flow) by minimizing a nonconvex nonsmooth loss function. In comparison with existing nonconvex Wirtinger flow (WF) algorithm \cite{candes2015phase}, although the loss function becomes nonsmooth, it involves only the second power of variable and hence reduces the complexity. We show that for random Gaussian measurements, RWF enjoys geometric convergence to a global optimal point as long as the number $m$ of measurements is on the order of $n$, the dimension of the unknown $\boldsymbol{x}$. This improves the sample complexity of WF, and achieves the same sample complexity as truncated Wirtinger flow (TWF) \cite{chen2015solving}, but without truncation in gradient loop. Furthermore, RWF costs less computationally than WF, and runs faster numerically than both WF and TWF. We further develop the incremental (stochastic) reshaped Wirtinger flow (IRWF) and show that IRWF converges linearly to the true signal. We further establish performance guarantee of an existing Kaczmarz method for the phase retrieval problem based on its connection to IRWF. We also empirically demonstrate that IRWF outperforms existing ITWF algorithm (stochastic version of TWF) as well as other batch algorithms. "
791800245961256960,2016-10-28 00:34:24,https://t.co/7pKsagGWzG,Tight Complexity Bounds for Optimizing Composite Objectives. (arXiv:1605.08003v2 [math.OC] UPDATED) https://t.co/7pKsagGWzG,0,2," Abstract: We provide tight upper and lower bounds on the complexity of minimizing the average of $m$ convex functions using gradient and prox oracles of the component functions. We show a significant gap between the complexity of deterministic vs randomized optimization. For smooth functions, we show that accelerated gradient descent (AGD) and an accelerated variant of SVRG are optimal in the deterministic and randomized settings respectively, and that a gradient oracle is sufficient for the optimal rate. For non-smooth functions, having access to prox oracles reduces the complexity and we present optimal methods based on smoothing that improve over methods using just gradient accesses. "
791800244510060544,2016-10-28 00:34:23,https://t.co/3x5Z4TKKv4,Contextual Bandits with Latent Confounders: An NMF Approach. (arXiv:1606.00119v3 [cs.LG] UPDATED) https://t.co/3x5Z4TKKv4,1,4," Abstract: Motivated by online recommendation and advertising systems, we consider a causal model for stochastic contextual bandits with a latent low-dimensional confounder. In our model, there are $L$ observed contexts and $K$ arms of the bandit. The observed context influences the reward obtained through a latent confounder variable with cardinality $m$ ($m \ll L,K$). The arm choice and the latent confounder causally determines the reward while the observed context is correlated with the confounder. Under this model, the $L \times K$ mean reward matrix $\mathbf{U}$ (for each context in $[L]$ and each arm in $[K]$) factorizes into non-negative factors $\mathbf{A}$ ($L \times m$) and $\mathbf{W}$ ($m \times K$). This insight enables us to propose an $\epsilon$-greedy NMF-Bandit algorithm that designs a sequence of interventions (selecting specific arms), that achieves a balance between learning this low-dimensional structure and selecting the best arm to minimize regret. Our algorithm achieves a regret of $\mathcal{O}\left(L\mathrm{poly}(m, \log K) \log T \right)$ at time $T$, as compared to $\mathcal{O}(LK\log T)$ for conventional contextual bandits, assuming a constant gap between the best arm and the rest for each context. These guarantees are obtained under mild sufficiency conditions on the factors that are weaker versions of the well-known Statistical RIP condition. We further propose a class of generative models that satisfy our sufficient conditions, and derive a lower bound of $\mathcal{O}\left(Km\log T\right)$. These are the first regret guarantees for online matrix completion with bandit feedback, when the rank is greater than one. We further compare the performance of our algorithm with the state of the art, on synthetic and real world data-sets. "
791800242941353984,2016-10-28 00:34:23,https://t.co/172DkPmEg4,Scaling Factorial Hidden Markov Models: Stochastic Variational Inference without Messages. (arXiv:1608.03817v2 [st… https://t.co/172DkPmEg4,0,6," Abstract: Factorial Hidden Markov Models (FHMMs) are powerful models for sequential data but they do not scale well with long sequences. We propose a scalable inference and learning algorithm for FHMMs that draws on ideas from the stochastic variational inference, neural network and copula literatures. Unlike existing approaches, the proposed algorithm requires no message passing procedure among latent variables and can be distributed to a network of computers to speed up learning. Our experiments corroborate that the proposed algorithm does not introduce further approximation bias compared to the proven structured mean-field algorithm, and achieves better performance with long sequences and large FHMMs. "
791800241448181760,2016-10-28 00:34:23,https://t.co/t5NtGx2Ar1,Parallelizing Stochastic Approximation Through Mini-Batching and Tail-Averaging. (arXiv:1610.03774v2 [stat.ML] UPD… https://t.co/t5NtGx2Ar1,0,3," Abstract: This work characterizes the benefits of averaging techniques widely used in conjunction with stochastic gradient descent (SGD). In particular, this work sharply analyzes: (1) mini-batching, a method of averaging many samples of the gradient to both reduce the variance of a stochastic gradient estimate and for parallelizing SGD and (2) tail-averaging, a method involving averaging the final few iterates of SGD in order to decrease the variance in SGD's final iterate. This work presents the first tight non-asymptotic generalization error bounds for these schemes for the stochastic approximation problem of least squares regression. Furthermore, this work establishes a precise problem-dependent extent to which mini-batching can be used to yield provable near-linear parallelization speedups over SGD with batch size one. These results are utilized in providing a highly parallelizable SGD algorithm that obtains the optimal statistical error rate with nearly the same number of serial updates as batch gradient descent, which improves significantly over existing SGD-style methods. Finally, this work sheds light on some fundamental differences in SGD's behavior when dealing with agnostic noise in the (non-realizable) least squares regression problem. In particular, the work shows that the stepsizes that ensure optimal statistical error rates for the agnostic case must be a function of the noise properties. The central analysis tools used by this paper are obtained through generalizing the operator view of averaged SGD, introduced by Defossez and Bach (2015) followed by developing a novel analysis in bounding these operators to characterize the generalization error. These techniques may be of broader interest in analyzing various computational aspects of stochastic approximation. "
791438914485649408,2016-10-27 00:38:36,https://t.co/RInkXBafLh,Gaussian Process Kernels for Popular State-Space Time Series Models. (arXiv:1610.08074v1 [stat.ML]) https://t.co/RInkXBafLh,0,12," Abstract: In this paper we investigate a link between state- space models and Gaussian Processes (GP) for time series modeling and forecasting. In particular, several widely used state- space models are transformed into continuous time form and corresponding Gaussian Process kernels are derived. Experimen- tal results demonstrate that the derived GP kernels are correct and appropriate for Gaussian Process Regression. An experiment with a real world dataset shows that the modeling is identical with state-space models and with the proposed GP kernels. The considered connection allows the researchers to look at their models from a different angle and facilitate sharing ideas between these two different modeling approaches. "
791438912631742464,2016-10-27 00:38:35,https://t.co/07Tm1PfMZK,A statistical framework for fair predictive algorithms. (arXiv:1610.08077v1 [stat.ML]) https://t.co/07Tm1PfMZK,1,8," Abstract: Predictive modeling is increasingly being employed to assist human decision-makers. One purported advantage of replacing human judgment with computer models in high stakes settings-- such as sentencing, hiring, policing, college admissions, and parole decisions-- is the perceived ""neutrality"" of computers. It is argued that because computer models do not hold personal prejudice, the predictions they produce will be equally free from prejudice. There is growing recognition that employing algorithms does not remove the potential for bias, and can even amplify it, since training data were inevitably generated by a process that is itself biased. In this paper, we provide a probabilistic definition of algorithmic bias. We propose a method to remove bias from predictive models by removing all information regarding protected variables from the permitted training data. Unlike previous work in this area, our framework is general enough to accommodate arbitrary data types, e.g. binary, continuous, etc. Motivated by models currently in use in the criminal justice system that inform decisions on pre-trial release and paroling, we apply our proposed method to a dataset on the criminal histories of individuals at the time of sentencing to produce ""race-neutral"" predictions of re-arrest. In the process, we demonstrate that the most common approach to creating ""race-neutral"" models-- omitting race as a covariate-- still results in racially disparate predictions. We then demonstrate that the application of our proposed method to these data removes racial disparities from predictions with minimal impact on predictive accuracy. "
791438910958276608,2016-10-27 00:38:35,https://t.co/Gxk1j5xs8m,Infinite-dimensional Log-Determinant divergences II: Alpha-Beta divergences. (arXiv:1610.08087v1 [math.FA]) https://t.co/Gxk1j5xs8m,0,4," Abstract: This work presents a parametrized family of divergences, namely Alpha-Beta Log- Determinant (Log-Det) divergences, between positive definite unitized trace class operators on a Hilbert space. This is a generalization of the Alpha-Beta Log-Determinant divergences between symmetric, positive definite matrices to the infinite-dimensional setting. The family of Alpha-Beta Log-Det divergences is highly general and contains many divergences as special cases, including the recently formulated infinite dimensional affine-invariant Riemannian distance and the infinite-dimensional Alpha Log-Det divergences between positive definite unitized trace class operators. In particular, it includes a parametrized family of metrics between positive definite trace class operators, with the affine-invariant Riemannian distance and the square root of the symmetric Stein divergence being special cases. For the Alpha-Beta Log-Det divergences between covariance operators on a Reproducing Kernel Hilbert Space (RKHS), we obtain closed form formulas via the corresponding Gram matrices. "
791438909343404032,2016-10-27 00:38:34,https://t.co/xHPm1WOkZK,Fast Bayesian Non-Negative Matrix Factorisation and Tri-Factorisation. (arXiv:1610.08127v1 [cs.LG]) https://t.co/xHPm1WOkZK,0,4," Abstract: We present a fast variational Bayesian algorithm for performing non-negative matrix factorisation and tri-factorisation. We show that our approach achieves faster convergence per iteration and timestep (wall-clock) than Gibbs sampling and non-probabilistic approaches, and do not require additional samples to estimate the posterior. We show that in particular for matrix tri-factorisation convergence is difficult, but our variational Bayesian approach offers a fast solution, allowing the tri-factorisation approach to be used more effectively. "
791438907674066944,2016-10-27 00:38:34,https://t.co/oBzQkfk0gn,Automatic measurement of vowel duration via structured prediction. (arXiv:1610.08166v1 [stat.ML]) https://t.co/oBzQkfk0gn,0,2," Abstract: A key barrier to making phonetic studies scalable and replicable is the need to rely on subjective, manual annotation. To help meet this challenge, a machine learning algorithm was developed for automatic measurement of a widely used phonetic measure: vowel duration. Manually-annotated data were used to train a model that takes as input an arbitrary length segment of the acoustic signal containing a single vowel that is preceded and followed by consonants and outputs the duration of the vowel. The model is based on the structured prediction framework. The input signal and a hypothesized set of a vowel's onset and offset are mapped to an abstract vector space by a set of acoustic feature functions. The learning algorithm is trained in this space to minimize the difference in expectations between predicted and manually-measured vowel durations. The trained model can then automatically estimate vowel durations without phonetic or orthographic transcription. Results comparing the model to three sets of manually annotated data suggest it out-performed the current gold standard for duration measurement, an HMM-based forced aligner (which requires orthographic or phonetic transcription as an input). "
791438905967017984,2016-10-27 00:38:34,https://t.co/PpMZ5TMUXe,Tensor Decompositions for Identifying Directed Graph Topologies and Tracking Dynamic Networks. (arXiv:1610.08189v1… https://t.co/PpMZ5TMUXe,3,14," Abstract: Directed networks are pervasive both in nature and engineered systems, often underlying the complex behavior observed in biological systems, microblogs and social interactions over the web, as well as global financial markets. Since their structures are often unobservable, in order to facilitate network analytics, one generally resorts to approaches capitalizing on measurable nodal processes to infer the unknown topology. Structural equation models (SEMs) are capable of incorporating exogenous inputs to resolve inherent directional ambiguities. However, conventional SEMs assume full knowledge of exogenous inputs, which may not be readily available in some practical settings. The present paper advocates a novel SEM-based topology inference approach that entails factorization of a three-way tensor, constructed from the observed nodal data, using the well-known parallel factor (PARAFAC) decomposition. It turns out that second-order piecewise stationary statistics of exogenous variables suffice to identify the hidden topology. Capitalizing on the uniqueness properties inherent to high-order tensor factorizations, it is shown that topology identification is possible under reasonably mild conditions. In addition, to facilitate real-time operation and inference of time-varying networks, an adaptive (PARAFAC) tensor decomposition scheme which tracks the topology-revealing tensor factors is developed. Extensive tests on simulated and real stock quote data demonstrate the merits of the novel tensor-based approach. "
791438904431894528,2016-10-27 00:38:33,https://t.co/X21ePTnRkH,Things Bayes can't do. (arXiv:1610.08239v1 [cs.LG]) https://t.co/X21ePTnRkH,3,16," Abstract: The problem of forecasting conditional probabilities of the next event given the past is considered in a general probabilistic setting. Given an arbitrary (large, uncountable) set C of predictors, we would like to construct a single predictor that performs asymptotically as well as the best predictor in C, on any data. Here we show that there are sets C for which such predictors exist, but none of them is a Bayesian predictor with a prior concentrated on C. In other words, there is a predictor with sublinear regret, but every Bayesian predictor must have a linear regret. This negative finding is in sharp contrast with previous results that establish the opposite for the case when one of the predictors in $C$ achieves asymptotically vanishing error. In such a case, if there is a predictor that achieves asymptotically vanishing error for any measure in C, then there is a Bayesian predictor that also has this property, and whose prior is concentrated on (a countable subset of) C. "
791438902812958726,2016-10-27 00:38:33,https://t.co/gL2ONvIkht,Universal adversarial perturbations. (arXiv:1610.08401v1 [cs.CV]) https://t.co/gL2ONvIkht,1,11," Abstract: Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images. "
791438901151924224,2016-10-27 00:38:32,https://t.co/GlwfIIEwJO,Probabilistic Linear Multistep Methods. (arXiv:1610.08417v1 [math.NA]) https://t.co/GlwfIIEwJO,0,5," Abstract: We present a derivation and theoretical investigation of the Adams-Bashforth and Adams-Moulton family of linear multistep methods for solving ordinary differential equations, starting from a Gaussian process (GP) framework. In the limit, this formulation coincides with the classical deterministic methods, which have been used as higher-order initial value problem solvers for over a century. Furthermore, the natural probabilistic framework provided by the GP formulation allows us to derive probabilistic versions of these methods, in the spirit of a number of other probabilistic ODE solvers presented in the recent literature. In contrast to higher-order Runge-Kutta methods, which require multiple intermediate function evaluations per step, Adams family methods make use of previous function evaluations, so that increased accuracy arising from a higher-order multistep approach comes at very little additional computational cost. We show that through a careful choice of covariance function for the GP, the posterior mean and standard deviation over the numerical solution can be made to exactly coincide with the value given by the deterministic method and its local truncation error respectively. We provide a rigorous proof of the convergence of these new methods, as well as an empirical investigation (up to fifth order) demonstrating their convergence rates in practice. "
791438899444850696,2016-10-27 00:38:32,https://t.co/7JptZ8RHp5,New Liftable Classes for First-Order Probabilistic Inference. (arXiv:1610.08445v1 [cs.AI]) https://t.co/7JptZ8RHp5,0,3," Abstract: Statistical relational models provide compact encodings of probabilistic dependencies in relational domains, but result in highly intractable graphical models. The goal of lifted inference is to carry out probabilistic inference without needing to reason about each individual separately, by instead treating exchangeable, undistinguished objects as a whole. In this paper, we study the domain recursion inference rule, which, despite its central role in early theoretical results on domain-lifted inference, has later been believed redundant. We show that this rule is more powerful than expected, and in fact significantly extends the range of models for which lifted inference runs in time polynomial in the number of individuals in the domain. This includes an open problem called S4, the symmetric transitivity model, and a first-order logic encoding of the birthday paradox. We further identify new classes S2FO2 and S2RU of domain-liftable theories, which respectively subsume FO2 and recursively unary theories, the largest classes of domain-liftable theories known so far, and show that using domain recursion can achieve exponential speedup even in theories that cannot fully be lifted with the existing set of inference rules. "
791438897133813760,2016-10-27 00:38:31,https://t.co/KdkxS9bgMB,Body movement to sound interface with vector autoregressive hierarchical hidden Markov models. (arXiv:1610.08450v1… https://t.co/KdkxS9bgMB,0,2," Abstract: Interfacing a kinetic action of a person to an action of a machine system is an important research topic in many application areas. One of the key factors for intimate human-machine interaction is the ability of the control algorithm to detect and classify different user commands with shortest possible latency, thus making a highly correlated link between cause and effect. In our research, we focused on the task of mapping user kinematic actions into sound samples. The presented methodology relies on the wireless sensor nodes equipped with inertial measurement units and the real-time algorithm dedicated for early detection and classification of a variety of movements/gestures performed by a user. The core algorithm is based on the approximate Bayesian inference of Vector Autoregressive Hierarchical Hidden Markov Models (VAR-HHMM), where models database is derived from the set of motion gestures. The performance of the algorithm was compared with an online version of the K-nearest neighbours (KNN) algorithm, where we used offline expert based classification as the benchmark. In almost all of the evaluation metrics (e.g. confusion matrix, recall and precision scores) the VAR-HHMM algorithm outperformed KNN. Furthermore, the VAR-HHMM algorithm, in some cases, achieved faster movement onset detection compared with the offline standard. The proposed concept, although envisioned for movement-to-sound application, could be implemented in other human-machine interfaces. "
791438895586152448,2016-10-27 00:38:31,https://t.co/91WTmQcy7Z,Fairness Beyond Disparate Treatment &amp; Disparate Impact: Learning Classification without Disparate Mistreatment. (a… https://t.co/91WTmQcy7Z,0,4," Abstract: As the use of automated decision making systems becomes wide-spread, there is a growing concern about their potential unfairness towards people with certain traits. Anti-discrimination laws in various countries prohibit unfair treatment of individuals based on specific traits, also called sensitive attributes (e.g., gender, race). In many learning scenarios, the trained algorithms (classifiers) make decisions with certain inaccuracy (misclassification rate). As learning mechanisms target minimizing the error rate for all decisions, it is quite possible that the optimally trained algorithm makes decisions for users belonging to different sensitive attribute groups with different error rates (e.g., decision errors for females are higher than for males). To account for and avoid such unfairness when learning, in this paper, we introduce a new notion of unfairness, disparate mistreatment, which is defined in terms of misclassification rates. We then propose an intuitive measure of disparate mistreatment for decision boundary-based classifiers, which can be easily incorporated into their formulation as a convex-concave constraint. Experiments on synthetic as well as real world datasets show that our methodology is effective at avoiding disparate mistreatment, often at a small cost in terms of accuracy. "
791438893786759168,2016-10-27 00:38:31,https://t.co/gb0urjn9kg,Bayesian latent structure discovery from multi-neuron recordings. (arXiv:1610.08465v1 [stat.ML]) https://t.co/gb0urjn9kg,1,5," Abstract: Neural circuits contain heterogeneous groups of neurons that differ in type, location, connectivity, and basic response properties. However, traditional methods for dimensionality reduction and clustering are ill-suited to recovering the structure underlying the organization of neural circuits. In particular, they do not take advantage of the rich temporal dependencies in multi-neuron recordings and fail to account for the noise in neural spike trains. Here we describe new tools for inferring latent structure from simultaneously recorded spike train data using a hierarchical extension of a multi-neuron point process model commonly known as the generalized linear model (GLM). Our approach combines the GLM with flexible graph-theoretic priors governing the relationship between latent features and neural connectivity patterns. Fully Bayesian inference via P\'olya-gamma augmentation of the resulting model allows us to classify neurons and infer latent dimensions of circuit organization from correlated spike trains. We demonstrate the effectiveness of our method with applications to synthetic data and multi-neuron recordings in primate retina, revealing latent patterns of neural types and locations from spike trains alone. "
791438891702226944,2016-10-27 00:38:30,https://t.co/xZTEnjxHFb,Recurrent switching linear dynamical systems. (arXiv:1610.08466v1 [stat.ML]) https://t.co/xZTEnjxHFb,0,11," Abstract: Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics. We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. Building on switching linear dynamical systems (SLDS), we present a new model class that not only discovers these dynamical units, but also explains how their switching behavior depends on observations or continuous latent states. These ""recurrent"" switching linear dynamical systems provide further insight by discovering the conditions under which each unit is deployed, something that traditional SLDS models fail to do. We leverage recent algorithmic advances in approximate inference to make Bayesian inference in these models easy, fast, and scalable. "
791438889701474304,2016-10-27 00:38:30,https://t.co/Y9MOGRhcL2,Estimating the Size of a Large Network and its Communities from a Random Sample. (arXiv:1610.08473v1 [stat.ML]) https://t.co/Y9MOGRhcL2,0,3," Abstract: Most real-world networks are too large to be measured or studied directly and there is substantial interest in estimating global network properties from smaller sub-samples. One of the most important global properties is the number of vertices/nodes in the network. Estimating the number of vertices in a large network is a major challenge in computer science, epidemiology, demography, and intelligence analysis. In this paper we consider a population random graph G = (V;E) from the stochastic block model (SBM) with K communities/blocks. A sample is obtained by randomly choosing a subset W and letting G(W) be the induced subgraph in G of the vertices in W. In addition to G(W), we observe the total degree of each sampled vertex and its block membership. Given this partial information, we propose an efficient PopULation Size Estimation algorithm, called PULSE, that correctly estimates the size of the whole population as well as the size of each community. To support our theoretical analysis, we perform an exhaustive set of experiments to study the effects of sample size, K, and SBM model parameters on the accuracy of the estimates. The experimental results also demonstrate that PULSE significantly outperforms a widely-used method called the network scale-up estimator in a wide variety of scenarios. We conclude with extensions and directions for future work. "
791438887629512705,2016-10-27 00:38:29,https://t.co/82QERrHEJn,Adaptive matching pursuit for sparse signal recovery. (arXiv:1610.08495v1 [cs.LG]) https://t.co/82QERrHEJn,1,4," Abstract: Spike and Slab priors have been of much recent interest in signal processing as a means of inducing sparsity in Bayesian inference. Applications domains that benefit from the use of these priors include sparse recovery, regression and classification. It is well-known that solving for the sparse coefficient vector to maximize these priors results in a hard non-convex and mixed integer programming problem. Most existing solutions to this optimization problem either involve simplifying assumptions/relaxations or are computationally expensive. We propose a new greedy and adaptive matching pursuit (AMP) algorithm to directly solve this hard problem. Essentially, in each step of the algorithm, the set of active elements would be updated by either adding or removing one index, whichever results in better improvement. In addition, the intermediate steps of the algorithm are calculated via an inexpensive Cholesky decomposition which makes the algorithm much faster. Results on simulated data sets as well as real-world image recovery challenges confirm the benefits of the proposed AMP, particularly in providing a superior cost-quality trade-off over existing alternatives. "
791438885364568064,2016-10-27 00:38:29,https://t.co/UcfHe9k1pe,Relevant sparse codes with variational information bottleneck. (arXiv:1605.07332v2 [stat.ML] UPDATED) https://t.co/UcfHe9k1pe,0,5," Abstract: In many applications, it is desirable to extract only the relevant aspects of data. A principled way to do this is the information bottleneck (IB) method, where one seeks a code that maximizes information about a 'relevance' variable, Y, while constraining the information encoded about the original data, X. Unfortunately however, the IB method is computationally demanding when data are high-dimensional and/or non-gaussian. Here we propose an approximate variational scheme for maximizing a lower bound on the IB objective, analogous to variational EM. Using this method, we derive an IB algorithm to recover features that are both relevant and sparse. Finally, we demonstrate how kernelized versions of the algorithm can be used to address a broad range of problems with non-linear relation between X and Y. "
791438883435184128,2016-10-27 00:38:28,https://t.co/HZEUKKLwLV,Kernel Bayesian Inference with Posterior Regularization. (arXiv:1607.02011v2 [stat.ML] UPDATED) https://t.co/HZEUKKLwLV,0,4," Abstract: We propose a vector-valued regression problem whose solution is equivalent to the reproducing kernel Hilbert space (RKHS) embedding of the Bayesian posterior distribution. This equivalence provides a new understanding of kernel Bayesian inference. Moreover, the optimization problem induces a new regularization for the posterior embedding estimator, which is faster and has comparable performance to the squared regularization in kernel Bayes' rule. This regularization coincides with a former thresholding approach used in kernel POMDPs whose consistency remains to be established. Our theoretical work solves this open problem and provides consistency analysis in regression settings. Based on our optimizational formulation, we propose a flexible Bayesian posterior regularization framework which for the first time enables us to put regularization at the distribution level. We apply this method to nonparametric state-space filtering tasks with extremely nonlinear dynamics and show performance gains over all other baselines. "
791438880209833985,2016-10-27 00:38:27,https://t.co/FKxj1LetAs,Comparing the Performance of Graphical Structure Learning Algorithms with TETRAD. (arXiv:1607.08110v3 [stat.ML] UP… https://t.co/FKxj1LetAs,2,4," Abstract: In this report we describe a tool for comparing the performance of causal structure learning algorithms implemented in the TETRAD freeware suite of causal analysis methods. Currently the tool is available as a package in the TETRAD source code (written in Java), which can be loaded up in an Integrated Development Environment (IDE) such as IntelliJ IDEA. Simulations can be done varying the number of runs, sample sizes, and data modalities. Performance on this simulated data can then be compared for a number of algorithms, with parameters varied and with performance statistics as selected, producing a publishable report. The order of the algorithms in the output can be adjusted to the user's preference using a utility function over the statistics. Data sets from simulation can be saved along with their graphs to a file and loaded back in for further analysis, or used for analysis by other tools. "
791438878196502528,2016-10-27 00:38:27,https://t.co/GqMGMGpj2c,A Non-convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing. (arXiv:1608.059… https://t.co/GqMGMGpj2c,0,4," Abstract: We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from $d$ dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank $k$, our algorithm converges linearly, achieves $O(\epsilon)$ recovery error after retrieving $O(k^{3}d\log(1/\epsilon))$ training instances, consumes $O(kd)$ memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval. "
791438875692523520,2016-10-27 00:38:26,https://t.co/d1mDA8Gybn,Indirect Gaussian Graph Learning beyond Gaussianity. (arXiv:1610.02590v2 [stat.ML] UPDATED) https://t.co/d1mDA8Gybn,0,3," Abstract: This paper studies how to capture dependency graph structures from real data which may not be multivariate Gaussian. Starting from marginal loss functions not necessarily derived from probability distributions, we use an additive over-parametrization with shrinkage to incorporate variable dependencies into the criterion. An iterative Gaussian graph learning algorithm is proposed with ease in implementation. Statistical analysis shows that with the error measured in terms of a proper Bregman divergence, the estimators have fast rate of convergence. Real-life examples in different settings are given to demonstrate the efficacy of the proposed methodology. "
791075283470254080,2016-10-26 00:33:39,https://t.co/QNJ3t65C44,A Theoretical Analysis of Noisy Sparse Subspace Clustering on Dimensionality-Reduced Data. (arXiv:1610.07650v1 [st… https://t.co/QNJ3t65C44,2,11," Abstract: Subspace clustering is the problem of partitioning unlabeled data points into a number of clusters so that data points within one cluster lie approximately on a low-dimensional linear subspace. In many practical scenarios, the dimensionality of data points to be clustered are compressed due to constraints of measurement, computation or privacy. In this paper, we study the theoretical properties of a popular subspace clustering algorithm named sparse subspace clustering (SSC) and establish formal success conditions of SSC on dimensionality-reduced data. Our analysis applies to the most general fully deterministic model where both underlying subspaces and data points within each subspace are deterministically positioned, and also a wide range of dimensionality reduction techniques (e.g., Gaussian random projection, uniform subsampling, sketching) that fall into a subspace embedding framework (Meng & Mahoney, 2013; Avron et al., 2014). Finally, we apply our analysis to a differentially private SSC algorithm and established both privacy and utility guarantees of the proposed method. "
791075281859649536,2016-10-26 00:33:39,https://t.co/UOW98vRUCc,A Bayesian Ensemble for Unsupervised Anomaly Detection. (arXiv:1610.07677v1 [stat.ML]) https://t.co/UOW98vRUCc,3,14," Abstract: Methods for unsupervised anomaly detection suffer from the fact that the data is unlabeled, making it difficult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classification and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classifier combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data. "
791075280383279104,2016-10-26 00:33:38,https://t.co/Db8WrvuU77,Scalable Dynamic Topic Modeling with Clustered Latent Dirichlet Allocation (CLDA). (arXiv:1610.07703v1 [cs.IR]) https://t.co/Db8WrvuU77,3,5," Abstract: Topic modeling is an increasingly important component of Big Data analytics, enabling the sense-making of highly dynamic and diverse streams of text data. Traditional methods such as Dynamic Topic Modeling (DTM), while mathematically elegant, do not lend themselves well to direct parallelization because of dependencies from one time step to another. Data decomposition approaches that partition data across time segments and then combine results in a global view of the dynamic change of topics enable execution of topic models on much larger datasets than is possibly without data decomposition. However, these methods are difficult to analyze mathematically and are relatively untested for quality of topics and performance on parallel systems. In this paper, we introduce and empirically analyze Clustered Latent Dirichlet Allocation (CLDA), a method for extracting dynamic latent topics from a collection of documents. CLDA uses a data decomposition strategy to partition data. CLDA takes advantage of parallelism, enabling fast execution for even very large datasets and a large number of topics. A large corpus is split into local segments to extract textual information from different time steps. Latent Dirichlet Allocation (LDA) is applied to infer topics at local segments. The results are merged, and clustering is used to combine topics from different segments into global topics. Results show that the perplexity is comparable and that topics generated by this algorithm are similar to those generated by DTM. In addition, CLDA is two orders of magnitude faster than existing approaches and allows for more freedom of experiment design. In this paper CLDA is applied successfully to seventeen years of NIPS conference papers, seventeen years of computer science journal abstracts, and to forty years of the PubMed corpus. "
791075278663585792,2016-10-26 00:33:38,https://t.co/DunU7vhaPr,Approximate cross-validation formula for Bayesian linear regression. (arXiv:1610.07733v1 [stat.ML]) https://t.co/DunU7vhaPr,0,5," Abstract: Cross-validation (CV) is a technique for evaluating the ability of statistical models/learning systems based on a given data set. Despite its wide applicability, the rather heavy computational cost can prevent its use as the system size grows. To resolve this difficulty in the case of Bayesian linear regression, we develop a formula for evaluating the leave-one-out CV error approximately without actually performing CV. The usefulness of the developed formula is tested by statistical mechanical analysis for a synthetic model. This is confirmed by application to a real-world supernova data set as well. "
791075277103362049,2016-10-26 00:33:38,https://t.co/1nLwJYFgHb,"Balancing, Regression, Difference-In-Differences and Synthetic Control Methods: A Synthesis. (arXiv:1610.07748v1 [… https://t.co/1nLwJYFgHb",1,5," Abstract: In a seminal paper Abadie, Diamond, and Hainmueller [2010] (ADH) develop the synthetic control procedure for estimating the effect of a treatment, in the presence of a single treated unit and a number of control units, with pre-treatment outcomes observed for all units. The method constructs a set of weights such that covariates and pre-treatment outcomes of the treated unit are approximately matched by a weighted average of control units. The weights are restricted to be nonnegative and sum to one, which allows the procedure to obtain the weights even when the number of lagged outcomes is modest relative to the number of control units, a setting that is not uncommon in applications. In the current paper we propose a more general class of synthetic control estimators that allows researchers to relax some of the restrictions in the ADH method. We allow the weights to be negative, do not necessarily restrict the sum of the weights, and allow for a permanent additive difference between the treated unit and the controls, similar to difference-in-difference procedures. The weights directly minimize the distance between the lagged outcomes for the treated and the control units, using regularization methods to deal with a potentially large number of possible control units. "
791075275408834560,2016-10-26 00:33:37,https://t.co/2iRYHZ3N6q,Frank-Wolfe Algorithms for Saddle Point Problems. (arXiv:1610.07797v1 [math.OC]) https://t.co/2iRYHZ3N6q,3,10," Abstract: We extend the Frank-Wolfe (FW) optimization algorithm to solve constrained smooth convex-concave saddle point (SP) problems. Remarkably, the method only requires access to linear minimization oracles. Leveraging recent advances in FW optimization, we provide the first proof of convergence of a FW-type saddle point solver over polytopes, thereby partially answering a 30 year-old conjecture. We also survey other convergence results and highlight gaps in the theoretical underpinnings of FW-style algorithms. Motivating applications without known efficient alternatives are explored through structured prediction with combinatorial penalties as well as games over matching polytopes involving an exponential number of constraints. "
791075273659846656,2016-10-26 00:33:37,https://t.co/lafJrgRkw1,On the convergence rate of the three operator splitting scheme. (arXiv:1610.07830v1 [stat.ML]) https://t.co/lafJrgRkw1,0,2," Abstract: The three operator splitting scheme was recently proposed by [Davis and Yin, 2015] as a method to optimize composite objective functions with one convex smooth term and two convex (possibly non-smooth) terms for which we have access to their proximity operator. In this short note we provide an alternative proof for the sublinear and linear rate of convergence of this method. "
791075271671738372,2016-10-26 00:33:36,https://t.co/3Bg3n6jNmf,Hybrid clustering-classification neural network in the medical diagnostics of reactive arthritis. (arXiv:1610.0785… https://t.co/3Bg3n6jNmf,1,5," Abstract: The hybrid clustering-classification neural network is proposed. This network allows increasing a quality of information processing under the condition of overlapping classes due to the rational choice of a learning rate parameter and introducing a special procedure of fuzzy reasoning in the clustering process, which occurs both with an external learning signal (supervised) and without the one (unsupervised). As similarity measure neighborhood function or membership one, cosine structures are used, which allow to provide a high flexibility due to self-learning-learning process and to provide some new useful properties. Many realized experiments have confirmed the efficiency of proposed hybrid clustering-classification neural network; also, this network was used for solving diagnostics task of reactive arthritis. "
791075269964664832,2016-10-26 00:33:36,https://t.co/7qgdRQd8rC,Formulas for Counting the Sizes of Markov Equivalence Classes of Directed Acyclic Graphs. (arXiv:1610.07921v1 [sta… https://t.co/7qgdRQd8rC,0,2," Abstract: The sizes of Markov equivalence classes of directed acyclic graphs play important roles in measuring the uncertainty and complexity in causal learning. A Markov equivalence class can be represented by an essential graph and its undirected subgraphs determine the size of the class. In this paper, we develop a method to derive the formulas for counting the sizes of Markov equivalence classes. We first introduce a new concept of core graph. The size of a Markov equivalence class of interest is a polynomial of the number of vertices given its core graph. Then, we discuss the recursive and explicit formula of the polynomial, and provide an algorithm to derive the size formula via symbolic computation for any given core graph. The proposed size formula derivation sheds light on the relationships between the size of a Markov equivalence class and its representation graph, and makes size counting efficient, even when the essential graphs contain non-sparse undirected subgraphs. "
791075268479815681,2016-10-26 00:33:36,https://t.co/WaCsAgM07y,Parallelizable sparse inverse formulation Gaussian processes (SpInGP). (arXiv:1610.08035v1 [stat.ML]) https://t.co/WaCsAgM07y,1,5," Abstract: We propose a parallelizable sparse inverse formulation Gaussian process (SpInGP) al- gorithm for temporal Gaussian process mod- els. It uses a sparse precision GP formulation and sparse matrix routines to speed up the computations. Due to the state-space formu- lation used in the algorithm, the time com- plexity of the basic SpInGP is linear, and be- cause all the computations are parallelizable, the parallel form of the algorithm is sublin- ear in the number of data points. We provide example algorithms to implement the sparse matrix routines and experimentally test the method using both simulated and real data. "
791075265262809088,2016-10-26 00:33:35,https://t.co/LhXgHA0wDe,Characteristic Kernels and Infinitely Divisible Distributions. (arXiv:1403.7304v3 [stat.ML] UPDATED) https://t.co/LhXgHA0wDe,0,2," Abstract: We connect shift-invariant characteristic kernels to infinitely divisible distributions on $\mathbb{R}^{d}$. Characteristic kernels play an important role in machine learning applications with their kernel means to distinguish any two probability measures. The contribution of this paper is two-fold. First, we show, using the L\'evy-Khintchine formula, that any shift-invariant kernel given by a bounded, continuous and symmetric probability density function (pdf) of an infinitely divisible distribution on $\mathbb{R}^d$ is characteristic. We also present some closure property of such characteristic kernels under addition, pointwise product, and convolution. Second, in developing various kernel mean algorithms, it is fundamental to compute the following values: (i) kernel mean values $m_P(x)$, $x \in \mathcal{X}$, and (ii) kernel mean RKHS inner products ${\left\langle m_P, m_Q \right\rangle_{\mathcal{H}}}$, for probability measures $P, Q$. If $P, Q$, and kernel $k$ are Gaussians, then computation (i) and (ii) results in Gaussian pdfs that is tractable. We generalize this Gaussian combination to more general cases in the class of infinitely divisible distributions. We then introduce a {\it conjugate} kernel and {\it convolution trick}, so that the above (i) and (ii) have the same pdf form, expecting tractable computation at least in some cases. As specific instances, we explore $\alpha$-stable distributions and a rich class of generalized hyperbolic distributions, where the Laplace, Cauchy and Student-t distributions are included. "
791075263597670400,2016-10-26 00:33:34,https://t.co/H94NdKoLdo,End-to-End Kernel Learning with Supervised Convolutional Kernel Networks. (arXiv:1605.06265v2 [stat.ML] UPDATED) https://t.co/H94NdKoLdo,8,16," Abstract: In this paper, we introduce a new image representation based on a multilayer kernel machine. Unlike traditional kernel methods where data representation is decoupled from the prediction task, we learn how to shape the kernel with supervision. We proceed by first proposing improvements of the recently-introduced convolutional kernel networks (CKNs) in the context of unsupervised learning; then, we derive backpropagation rules to take advantage of labeled training data. The resulting model is a new type of convolutional neural network, where optimizing the filters at each layer is equivalent to learning a linear subspace in a reproducing kernel Hilbert space (RKHS). We show that our method achieves reasonably competitive performance for image classification on some standard ""deep learning"" datasets such as CIFAR-10 and SVHN, and also for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks. "
791075262003867648,2016-10-26 00:33:34,https://t.co/Cvc0G0QC0d,Linear dynamical neural population models through nonlinear embeddings. (arXiv:1605.08454v2 [q-bio.NC] UPDATED) https://t.co/Cvc0G0QC0d,0,3," Abstract: A body of recent work in modeling neural activity focuses on recovering low-dimensional latent features that capture the statistical structure of large-scale neural populations. Most such approaches have focused on linear generative models, where inference is computationally tractable. Here, we propose fLDS, a general class of nonlinear generative models that permits the firing rate of each neuron to vary as an arbitrary smooth function of a latent, linear dynamical state. This extra flexibility allows the model to capture a richer set of neural variability than a purely linear model, but retains an easily visualizable low-dimensional latent space. To fit this class of non-conjugate models we propose a variational inference scheme, along with a novel approximate posterior capable of capturing rich temporal correlations across time. We show that our techniques permit inference in a wide class of generative models.We also show in application to two neural datasets that, compared to state-of-the-art neural population models, fLDS captures a much larger proportion of neural variability with a small number of latent dimensions, providing superior predictive performance and interpretability. "
791075260032581632,2016-10-26 00:33:34,https://t.co/QR2EX4DEkD,Deep Survival: A Deep Cox Proportional Hazards Network. (arXiv:1606.00931v2 [stat.ML] UPDATED) https://t.co/QR2EX4DEkD,1,9," Abstract: Previous research has shown that neural networks can model survival data in situations in which some patients' death times are unknown, e.g. right-censored. However, neural networks have rarely been shown to outperform their linear counterparts such as the Cox proportional hazards model. In this paper, we run simulated experiments and use real survival data to build upon the risk-regression architecture proposed by Faraggi and Simon. We demonstrate that our model, DeepSurv, not only works as well as other survival models but actually outperforms in predictive ability on survival data with linear and nonlinear risk functions. We then show that the neural network can also serve as a recommender system by including a categorical variable representing a treatment group. This can be used to provide personalized treatment recommendations based on an individual's calculated risk. We provide an open source Python module that implements these methods in order to advance research on deep learning and survival analysis. "
791075258572898304,2016-10-26 00:33:33,https://t.co/YAu3RbNPDK,Semi-Supervised Classification with Graph Convolutional Networks. (arXiv:1609.02907v2 [cs.LG] UPDATED) https://t.co/YAu3RbNPDK,5,18, Abstract: We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin. 
791075256458960901,2016-10-26 00:33:33,https://t.co/M4nE6QXHgg,Multiplicative LSTM for sequence modelling. (arXiv:1609.07959v2 [cs.NE] UPDATED) https://t.co/M4nE6QXHgg,1,11," Abstract: This paper introduces multiplicative LSTM, a novel hybrid recurrent neural network architecture for sequence modelling that combines the long short-term memory (LSTM) and multiplicative recurrent neural network architectures. Multiplicative LSTM is motivated by its flexibility to have very different recurrent transition functions for each possible input, which we argue helps make it more expressive in autoregressive density estimation. We show empirically that multiplicative LSTM outperforms standard LSTM and its deep variants for a range of character level modelling tasks. We also found that this improvement increases as the complexity of the task scales up. This model achieves a test error of 1.19 bits/character on the last 4 million characters of the Hutter prize dataset when combined with dynamic evaluation. "
790712948645912576,2016-10-25 00:33:52,https://t.co/s2pkLRKeTy,Mean-Field Variational Inference for Gradient Matching with Gaussian Processes. (arXiv:1610.06949v1 [stat.ML]) https://t.co/s2pkLRKeTy,1,20," Abstract: Gradient matching with Gaussian processes is a promising tool for learning parameters of ordinary differential equations (ODE's). The essence of gradient matching is to model the prior over state variables as a Gaussian process which implies that the joint distribution given the ODE's and GP kernels is also Gaussian distributed. The state-derivatives are integrated out analytically since they are modelled as latent variables. However, the state variables themselves are also latent variables because they are contaminated by noise. Previous work sampled the state variables since integrating them out is \textit{not} analytically tractable. In this paper we use mean-field approximation to establish tight variational lower bounds that decouple state variables and are therefore, in contrast to the integral over state variables, analytically tractable and even concave for a restricted family of ODE's, including nonlinear and periodic ODE's. Such variational lower bounds facilitate ""hill climbing"" to determine the maximum a posteriori estimate of ODE parameters. An additional advantage of our approach over sampling methods is the determination of a proxy to the intractable posterior distribution over state variables given observations and the ODE's. "
790712947198857217,2016-10-25 00:33:52,https://t.co/O4o4MKtq2J,Independent Component Analysis by Entropy Maximization with Kernels. (arXiv:1610.07104v1 [stat.ML]) https://t.co/O4o4MKtq2J,3,8," Abstract: Independent component analysis (ICA) is the most popular method for blind source separation (BSS) with a diverse set of applications, such as biomedical signal processing, video and image analysis, and communications. Maximum likelihood (ML), an optimal theoretical framework for ICA, requires knowledge of the true underlying probability density function (PDF) of the latent sources, which, in many applications, is unknown. ICA algorithms cast in the ML framework often deviate from its theoretical optimality properties due to poor estimation of the source PDF. Therefore, accurate estimation of source PDFs is critical in order to avoid model mismatch and poor ICA performance. In this paper, we propose a new and efficient ICA algorithm based on entropy maximization with kernels, (ICA-EMK), which uses both global and local measuring functions as constraints to dynamically estimate the PDF of the sources with reasonable complexity. In addition, the new algorithm performs optimization with respect to each of the cost function gradient directions separately, enabling parallel implementations on multi-core computers. We demonstrate the superior performance of ICA-EMK over competing ICA algorithms using simulated as well as real-world data. "
790712945726615552,2016-10-25 00:33:51,https://t.co/vilNWMbOq4,Fast and Reliable Parameter Estimation from Nonlinear Observations. (arXiv:1610.07108v1 [stat.ML]) https://t.co/vilNWMbOq4,0,11," Abstract: In this paper we study the problem of recovering a structured but unknown parameter ${\bf{\theta}}^*$ from $n$ nonlinear observations of the form $y_i=f(\langle {\bf{x}}_i,{\bf{\theta}}^*\rangle)$ for $i=1,2,\ldots,n$. We develop a framework for characterizing time-data tradeoffs for a variety of parameter estimation algorithms when the nonlinear function $f$ is unknown. This framework includes many popular heuristics such as projected/proximal gradient descent and stochastic schemes. For example, we show that a projected gradient descent scheme converges at a linear rate to a reliable solution with a near minimal number of samples. We provide a sharp characterization of the convergence rate of such algorithms as a function of sample size, amount of a-prior knowledge available about the parameter and a measure of the nonlinearity of the function $f$. These results provide a precise understanding of the various tradeoffs involved between statistical and computational resources as well as a-prior side information available for such nonlinear parameter estimation problems. "
790712944241827840,2016-10-25 00:33:51,https://t.co/vIM6EyV7SV,Online Classification with Complex Metrics. (arXiv:1610.07116v1 [stat.ML]) https://t.co/vIM6EyV7SV,0,2," Abstract: We present a framework and analysis of consistent binary classification for complex and non-decomposable performance metrics such as the F-measure and the Jaccard measure. The proposed framework is general, as it applies to both batch and online learning, and to both linear and non-linear models. Our work follows recent results showing that the Bayes optimal classifier for many complex metrics is given by a thresholding of the conditional probability of the positive class. This manuscript extends this thresholding characterization -- showing that the utility is strictly locally quasi-concave with respect to the threshold for a wide range of models and performance metrics. This, in turn, motivates simple normalized gradient ascent updates for threshold estimation. We present a finite-sample regret analysis for the resulting procedure. In particular, the risk for the batch case converges to the Bayes risk at the same rate as that of the underlying conditional probability estimation, and the risk of proposed online algorithm converges at a rate that depends on the conditional probability estimation risk. For instance, in the special case where the conditional probability model is logistic regression, our procedure achieves $O(\frac{1}{\sqrt{n}})$ sample complexity, both for batch and online training. Empirical evaluation shows that the proposed algorithms out-perform alternatives in practice, with comparable or better prediction performance and reduced run time for various metrics and datasets. "
790712942715146240,2016-10-25 00:33:50,https://t.co/flIOvBJNDi,Stochastic inference with spiking neurons in the high-conductance state. (arXiv:1610.07161v1 [q-bio.NC]) https://t.co/flIOvBJNDi,3,3," Abstract: The highly variable dynamics of neocortical circuits observed in vivo have been hypothesized to represent a signature of ongoing stochastic inference but stand in apparent contrast to the deterministic response of neurons measured in vitro. Based on a propagation of the membrane autocorrelation across spike bursts, we provide an analytical derivation of the neural activation function that holds for a large parameter space, including the high-conductance state. On this basis, we show how an ensemble of leaky integrate-and-fire neurons with conductance-based synapses embedded in a spiking environment can attain the correct firing statistics for sampling from a well-defined target distribution. For recurrent networks, we examine convergence toward stationarity in computer simulations and demonstrate sample-based Bayesian inference in a mixed graphical model. This points to a new computational role of high-conductance states and establishes a rigorous link between deterministic neuron models and functional stochastic dynamics on the network level. "
790712941318402048,2016-10-25 00:33:50,https://t.co/FL6npNbZN4,Learning Deep Architectures for Interaction Prediction in Structure-based Virtual Screening. (arXiv:1610.07187v1 [… https://t.co/FL6npNbZN4,3,7," Abstract: We introduce a deep learning architecture for structure-based virtual screening that generates fixed-sized fingerprints of proteins and small molecules by applying learnable atom convolution and softmax operations to each compound separately. These fingerprints are further transformed non-linearly, their inner-product is calculated and used to predict the binding potential. Moreover, we show that widely used benchmark datasets may be insufficient for testing structure-based virtual screening methods that utilize machine learning. Therefore, we introduce a new benchmark dataset, which we constructed based on DUD-E and PDBBind databases. "
790712939766579201,2016-10-25 00:33:50,https://t.co/OosCLJ1LE6,Simpler PAC-Bayesian Bounds for Hostile Data. (arXiv:1610.07193v1 [stat.ML]) https://t.co/OosCLJ1LE6,0,3," Abstract: PAC-Bayesian learning bounds are of the utmost interest to the learning community. Their role is to connect the generalization ability of an aggregation distribution $\rho$ to its empirical risk and to its Kullback-Leibler divergence with respect to some prior distribution $\pi$. Unfortunately, most of the available bounds typically rely on heavy assumptions such as boundedness and independence of the observations. This paper aims at relaxing these constraints and provides PAC-Bayesian learning bounds that hold for dependent, heavy-tailed observations (hereafter referred to as \emph{hostile data}). In these bounds the Kullack-Leibler divergence is replaced with a general version of Csisz\'ar's $f$-divergence. We prove a general PAC-Bayesian bound, and show how to use it in various hostile settings. "
790712938256592900,2016-10-25 00:33:49,https://t.co/phW2YSQs2x,Inertial Regularization and Selection (IRS): Sequential Regression in High-Dimension and Sparsity. (arXiv:1610.072… https://t.co/phW2YSQs2x,0,1," Abstract: In this paper, we develop a new sequential regression modeling approach for data streams. Data streams are commonly found around us, e.g in a retail enterprise sales data is continuously collected every day. A demand forecasting model is an important outcome from the data that needs to be continuously updated with the new incoming data. The main challenge in such modeling arises when there is a) high dimensional and sparsity, b) need for an adaptive use of prior knowledge, and/or c) structural changes in the system. The proposed approach addresses these challenges by incorporating an adaptive L1-penalty and inertia terms in the loss function, and thus called Inertial Regularization and Selection (IRS). The former term performs model selection to handle the first challenge while the latter is shown to address the last two challenges. A recursive estimation algorithm is developed, and shown to outperform the commonly used state-space models, such as Kalman Filters, in experimental studies and real data. "
790712936830472192,2016-10-25 00:33:49,https://t.co/IH5jsidUTL,Bayesian Nonparametric Modeling of Heterogeneous Groups of Censored Data. (arXiv:1610.07262v1 [stat.ML]) https://t.co/IH5jsidUTL,0,5," Abstract: Applied statisticians often encounter large samples of time-to-event data arising from a number of different groups with only a small number of observations per group. Bayesian nonparametric modelling approaches can be used to model such datasets given their ability to flexibly share information across groups. In this paper, we will compare three popular Bayesian nonparametric methods for modelling the survival functions of heterogeneous groups. Specifically, we will first compare the modelilng accuracy of the Dirichlet process, the hierarchical Dirichlet process, and the nested Dirichlet process on simulated datasets of different sizes, where group survival curves differ in shape or in expectation. We then will compare the models on two real world injury datasets. "
790712935433789441,2016-10-25 00:33:49,https://t.co/F8Ih3N7SjC,Truncated Variance Reduction: A Unified Approach to Bayesian Optimization and Level-Set Estimation. (arXiv:1610.07… https://t.co/F8Ih3N7SjC,1,7," Abstract: We present a new algorithm, truncated variance reduction (TruVaR), that treats Bayesian optimization (BO) and level-set estimation (LSE) with Gaussian processes in a unified fashion. The algorithm greedily shrinks a sum of truncated variances within a set of potential maximizers (BO) or unclassified points (LSE), which is updated based on confidence bounds. TruVaR is effective in several important settings that are typically non-trivial to incorporate into myopic algorithms, including pointwise costs and heteroscedastic noise. We provide a general theoretical guarantee for TruVaR covering these aspects, and use it to recover and strengthen existing results on BO and LSE. Moreover, we provide a new result for a setting where one can select from a number of noise levels having associated costs. We demonstrate the effectiveness of the algorithm on both synthetic and real-world data sets. "
790712933911257089,2016-10-25 00:33:48,https://t.co/FdQOGALfmq,"C-mix: a high dimensional mixture model for censored durations, with applications to genetic data. (arXiv:1610.074… https://t.co/FdQOGALfmq",0,2," Abstract: We introduce a mixture model for censored durations (C-mix), and develop maximum likelihood inference for the joint estimation of the time distributions and latent regression parameters of the model. We consider a high-dimensional setting, with datasets containing a large number of biomedical covariates. We therefore penalize the negative log-likelihood by the Elastic-Net, which leads to a sparse parameterization of the model. Inference is achieved using an efficient Quasi-Newton Expectation Maximization (QNEM) algorithm, for which we provide convergence properties. We then propose a score by assessing the patients risk of early adverse event. The statistical performance of the method is examined on an extensive Monte Carlo simulation study, and finally illustrated on three genetic datasets with high-dimensional covariates. We show that our approach outperforms the state-of-the-art, namely both the CURE and Cox proportional hazards models for this task, both in terms of C-index and AUC(t). "
790712932330078208,2016-10-25 00:33:48,https://t.co/ApIVyHk4os,A Framework for Parallel and Distributed Training of Neural Networks. (arXiv:1610.07448v1 [stat.ML]) https://t.co/ApIVyHk4os,0,8," Abstract: The aim of this paper is to develop a general framework for training neural networks (NNs) in a distributed environment, where training data is partitioned over a set of agents that communicate with each other through a sparse, possibly time-varying, connectivity pattern. In such distributed scenario, the training problem can be formulated as the (regularized) optimization of a non-convex social cost function, given by the sum of local (non-convex) costs, where each agent contributes with a single error term defined with respect to its local dataset. To devise a flexible and efficient solution, we customize a recently proposed framework for non-convex optimization over networks, which hinges on a (primal) convexification-decomposition technique to handle non-convexity, and a dynamic consensus procedure to diffuse information among the agents. Several typical choices for the training criterion (e.g., squared loss, cross entropy, etc.) and regularization (e.g., $\ell_2$ norm, sparsity inducing penalties, etc.) are included in the framework and explored along the paper. Convergence to a stationary solution of the social non-convex problem is guaranteed under mild assumptions. Additionally, we show a principled way allowing each agent to exploit a multi-core architecture (e.g., a local cloud) in order to parallelize its local optimization step, resulting in strategies that are both distributed (across the agents) and parallel (inside each agent) in nature. A comprehensive set of experimental results validate the proposed approach. "
790712930761408512,2016-10-25 00:33:48,https://t.co/nd7mRoBz8k,Distilling Information Reliability and Source Trustworthiness from Digital Traces. (arXiv:1610.07472v1 [cs.SI]) https://t.co/nd7mRoBz8k,0,4," Abstract: Online knowledge repositories typically rely on their users or dedicated editors to evaluate the reliability of their content. These evaluations can be viewed as noisy measurements of both information reliability and information source trustworthiness. Can we leverage these noisy evaluations, often biased, to distill a robust, unbiased and interpretable measure of both notions? In this paper, we argue that the temporal traces left by these noisy evaluations give cues on the reliability of the information and the trustworthiness of the sources. Then, we propose a temporal point process modeling framework that links these temporal traces to robust, unbiased and interpretable notions of information reliability and source trustworthiness. Furthermore, we develop an efficient convex optimization procedure to learn the parameters of the model from historical traces. Experiments on real-world data gathered from Wikipedia and Stack Overflow show that our modeling framework accurately predicts evaluation events, provides an interpretable measure of information reliability and source trustworthiness, and yields interesting insights about real-world events. "
790712929322762241,2016-10-25 00:33:47,https://t.co/UdyBE67V04,Parallelizing Spectral Algorithms for Kernel Learning. (arXiv:1610.07487v1 [math.ST]) https://t.co/UdyBE67V04,0,7," Abstract: We consider a distributed learning approach in supervised learning for a large class of spectral regularization methods in an RKHS framework. The data set of size n is partitioned into $m=O(n^\alpha)$, $\alpha \leq \frac{1}{2}$, disjoint subsets. On each subset, some spectral regularization method (belonging to a large class, including in particular Kernel Ridge Regression, $L^2$-boosting and spectral cut-off) is applied. The regression function $f$ is then estimated via simple averaging, leading to a substantial reduction in computation time. We show that minimax optimal rates of convergence are preserved if m grows sufficiently slowly (corresponding to an upper bound for $\alpha$) as $n \to \infty$, depending on the smoothness assumptions on $f$ and the intrinsic dimensionality. In spirit, our approach is classical. "
790712927825358852,2016-10-25 00:33:47,https://t.co/cDbzpC1J03,Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. (arXiv:1610.07524v1 [… https://t.co/cDbzpC1J03,0,2," Abstract: Recidivism prediction instruments provide decision makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. While such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This paper discusses a fairness criterion originating in the field of educational and psychological testing that has recently been applied to assess the fairness of recidivism prediction instruments. We demonstrate how adherence to the criterion may lead to considerable disparate impact when recidivism prevalence differs across groups. "
790712926046982144,2016-10-25 00:33:46,https://t.co/psYlAGmNeX,Geometry of Polysemy. (arXiv:1610.07569v1 [cs.CL]) https://t.co/psYlAGmNeX,0,2," Abstract: Vector representations of words have heralded a transformational approach to classical problems in NLP; the most popular example is word2vec. However, a single vector does not suffice to model the polysemous nature of many (frequent) words, i.e., words with multiple meanings. In this paper, we propose a three-fold approach for unsupervised polysemy modeling: (a) context representations, (b) sense induction and disambiguation and (c) lexeme (as a word and sense pair) representations. A key feature of our work is the finding that a sentence containing a target word is well represented by a low rank subspace, instead of a point in a vector space. We then show that the subspaces associated with a particular sense of the target word tend to intersect over a line (one-dimensional subspace), which we use to disambiguate senses using a clustering algorithm that harnesses the Grassmannian geometry of the representations. The disambiguation algorithm, which we call $K$-Grassmeans, leads to a procedure to label the different senses of the target word in the corpus -- yielding lexeme vector representations, all in an unsupervised manner starting from a large (Wikipedia) corpus in English. Apart from several prototypical target (word,sense) examples and a host of empirical studies to intuit and justify the various geometric representations, we validate our algorithms on standard sense induction and disambiguation datasets and present new state-of-the-art results. "
790712924620881920,2016-10-25 00:33:46,https://t.co/2bbKAshoGg,Boltzmann-Machine Learning of Prior Distributions of Binarized Natural Images. (arXiv:1412.7012v4 [stat.ML] UPDATE… https://t.co/2bbKAshoGg,1,7," Abstract: Prior distributions of binarized natural images are learned by using a Boltzmann machine. According the results of this study, there emerges a structure with two sublattices in the interactions, and the nearest-neighbor and next-nearest-neighbor interactions correspondingly take two discriminative values, which reflects the individual characteristics of the three sets of pictures that we process. Meanwhile, in a longer spatial scale, a longer-range, although still rapidly decaying, ferromagnetic interaction commonly appears in all cases. The characteristic length scale of the interactions is universally up to approximately four lattice spacings $\xi \approx 4$. These results are derived by using the mean-field method, which effectively reduces the computational time required in a Boltzmann machine. An improved mean-field method called the Bethe approximation also gives the same results, as well as the Monte Carlo method does for small size images. These reinforce the validity of our analysis and findings. Relations to criticality, frustration, and simple-cell receptive fields are also discussed. "
790712923027042304,2016-10-25 00:33:46,https://t.co/R2d3lvjdfQ,Switching nonparametric regression models for multi-curve data. (arXiv:1504.02813v2 [stat.ME] UPDATED) https://t.co/R2d3lvjdfQ,0,2," Abstract: We develop and apply an approach for analyzing multi-curve data where each curve is driven by a latent state process. The state at any particular point determines a smooth function, forcing the individual curve to switch from one function to another. Thus each curve follows what we call a switching nonparametric regression model. We develop an EM algorithm to estimate the model parameters. We also obtain standard errors for the parameter estimates of the state process. We consider several types of state processes: independent and identically distributed, independent but depending on a covariate and Markov. Simulation studies show the frequentist properties of our estimates. We apply our methods to a data set of a building's power usage. "
790712921420619776,2016-10-25 00:33:45,https://t.co/Mj5DDn2eNy,"A Theory of Local Learning, the Learning Channel, and the Optimality of Backpropagation. (arXiv:1506.06472v2 [cs.L… https://t.co/Mj5DDn2eNy",3,7," Abstract: In a physical neural system, where storage and processing are intimately intertwined, the rules for adjusting the synaptic weights can only depend on variables that are available locally, such as the activity of the pre- and post-synaptic neurons, resulting in local learning rules. A systematic framework for studying the space of local learning rules is obtained by first specifying the nature of the local variables, and then the functional form that ties them together into each learning rule. Such a framework enables also the systematic discovery of new learning rules and exploration of relationships between learning rules and group symmetries. We study polynomial local learning rules stratified by their degree and analyze their behavior and capabilities in both linear and non-linear units and networks. Stacking local learning rules in deep feedforward networks leads to deep local learning. While deep local learning can learn interesting representations, it cannot learn complex input-output functions, even when targets are available for the top layer. Learning complex input-output functions requires local deep learning where target information is communicated to the deep layers through a backward learning channel. The nature of the communicated information about the targets and the structure of the learning channel partition the space of learning algorithms. We estimate the learning channel capacity associated with several algorithms and show that backpropagation outperforms them by simultaneously maximizing the information rate and minimizing the computational cost, even in recurrent networks. The theory clarifies the concept of Hebbian learning, establishes the power and limitations of local learning rules, introduces the learning channel which enables a formal analysis of the optimality of backpropagation, and explains the sparsity of the space of learning rules discovered so far. "
790712919965196288,2016-10-25 00:33:45,https://t.co/q61pRPcsuw,Search Improves Label for Active Learning. (arXiv:1602.07265v2 [cs.LG] UPDATED) https://t.co/q61pRPcsuw,1,3, Abstract: We investigate active learning with access to two distinct oracles: Label (which is standard) and Search (which is not). The Search oracle models the situation where a human searches a database to seed or counterexample an existing solution. Search is stronger than Label while being natural to implement in many situations. We show that an algorithm using both oracles can provide exponentially large problem-dependent improvements over Label alone. 
790712918581059584,2016-10-25 00:33:45,https://t.co/py6ArWh08H,A Multi-Batch L-BFGS Method for Machine Learning. (arXiv:1605.06049v2 [math.OC] UPDATED) https://t.co/py6ArWh08H,1,4," Abstract: The question of how to parallelize the stochastic gradient descent (SGD) method has received much attention in the literature. In this paper, we focus instead on batch methods that use a sizeable fraction of the training set at each iteration to facilitate parallelism, and that employ second-order information. In order to improve the learning process, we follow a multi-batch approach in which the batch changes at each iteration. This can cause difficulties because L-BFGS employs gradient differences to update the Hessian approximations, and when these gradients are computed using different data points the process can be unstable. This paper shows how to perform stable quasi-Newton updating in the multi-batch setting, illustrates the behavior of the algorithm in a distributed computing platform, and studies its convergence properties for both the convex and nonconvex cases. "
790712916748148740,2016-10-25 00:33:44,https://t.co/6jxar8oNMJ,Fast $\epsilon$-free Inference of Simulation Models with Bayesian Conditional Density Estimation. (arXiv:1605.0637… https://t.co/6jxar8oNMJ,2,5," Abstract: Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an $\epsilon$-ball around the observed data, which is only correct in the limit $\epsilon\!\rightarrow\!0$. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as $\epsilon\!\rightarrow\!0$, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior. "
790712915154333696,2016-10-25 00:33:44,https://t.co/gBBKrymm6c,MCMC assisted by Belief Propagaion. (arXiv:1605.09042v3 [stat.ML] UPDATED) https://t.co/gBBKrymm6c,1,7," Abstract: Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most popular algorithms for computational inference in Graphical Models (GM). In principle, MCMC is an exact probabilistic method which, however, often suffers from exponentially slow mixing. In contrast, BP is a deterministic method, which is typically fast, empirically very successful, however in general lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC algorithms correcting the approximation error of BP, i.e., we provide a way to compensate for BP errors via a consecutive BP-aware MCMC. Our framework is based on the Loop Calculus (LC) approach which allows to express the BP error as a sum of weighted generalized loops. Although the full series is computationally intractable, it is known that a truncated series, summing up all 2-regular loops, is computable in polynomial-time for planar pair-wise binary GMs and it also provides a highly accurate approximation empirically. Motivated by this, we first propose a polynomial-time approximation MCMC scheme for the truncated series of general (non-planar) pair-wise binary models. Our main idea here is to use the Worm algorithm, known to provide fast mixing in other (related) problems, and then design an appropriate rejection scheme to sample 2-regular loops. Furthermore, we also design an efficient rejection-free MCMC scheme for approximating the full series. The main novelty underlying our design is in utilizing the concept of cycle basis, which provides an efficient decomposition of the generalized loops. In essence, the proposed MCMC schemes run on transformed GM built upon the non-trivial BP solution, and our experiments show that this synthesis of BP and MCMC outperforms both direct MCMC and bare BP schemes. "
790712913594028032,2016-10-25 00:33:43,https://t.co/4IBAjMXbH2,Semi-Supervised Learning with Generative Adversarial Networks. (arXiv:1606.01583v2 [stat.ML] UPDATED) https://t.co/4IBAjMXbH2,3,24," Abstract: We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels. We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes. At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G. We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN. "
790712912163774464,2016-10-25 00:33:43,https://t.co/aTOy3RCxPl,Deep Learning with Differential Privacy. (arXiv:1607.00133v2 [stat.ML] UPDATED) https://t.co/aTOy3RCxPl,0,9," Abstract: Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality. "
790712910058233856,2016-10-25 00:33:43,https://t.co/z0UPvlZzm7,A Greedy Approach to Adapting the Trace Parameter for Temporal Difference Learning. (arXiv:1607.00446v2 [cs.AI] UP… https://t.co/z0UPvlZzm7,0,3," Abstract: One of the main obstacles to broad application of reinforcement learning methods is the parameter sensitivity of our core learning algorithms. In many large-scale applications, online computation and function approximation represent key strategies in scaling up reinforcement learning algorithms. In this setting, we have effective and reasonably well understood algorithms for adapting the learning-rate parameter, online during learning. Such meta-learning approaches can improve robustness of learning and enable specialization to current task, improving learning speed. For temporal-difference learning algorithms which we study here, there is yet another parameter, $\lambda$, that similarly impacts learning speed and stability in practice. Unfortunately, unlike the learning-rate parameter, $\lambda$ parametrizes the objective function that temporal-difference methods optimize. Different choices of $\lambda$ produce different fixed-point solutions, and thus adapting $\lambda$ online and characterizing the optimization is substantially more complex than adapting the learning-rate parameter. There are no meta-learning method for $\lambda$ that can achieve (1) incremental updating, (2) compatibility with function approximation, and (3) maintain stability of learning under both on and off-policy sampling. In this paper we contribute a novel objective function for optimizing $\lambda$ as a function of state rather than time. We derive a new incremental, linear complexity $\lambda$-adaption algorithm that does not require offline batch updating or access to a model of the world, and present a suite of experiments illustrating the practicality of our new algorithm in three different settings. Taken together, our contributions represent a concrete step towards black-box application of temporal-difference learning methods in real world problems. "
790712908430843905,2016-10-25 00:33:42,https://t.co/itdhCEBMOQ,Convex Relaxation for Community Detection with Covariates. (arXiv:1607.02675v2 [stat.ME] UPDATED) https://t.co/itdhCEBMOQ,0,5," Abstract: Community detection in networks is an important problem in many applied areas. In this paper, we investigate this in the presence of node covariates. Recently, an emerging body of theoretical work has been focused on leveraging information from both the edges in the network and the node covariates to infer community memberships. However, so far the role of the network and that of the covariates have not been examined closely. In essence, in most parameter regimes, one of the sources of information provides enough information to infer the hidden cluster labels, thereby making the other source redundant. To our knowledge, this is the first work which shows that when the network and the covariates carry ""orthogonal"" pieces of information about the cluster memberships, one can get asymptotically consistent clustering by using them both, while each of them fails individually. "
790712907008991232,2016-10-25 00:33:42,https://t.co/zsM0HO5ehK,Direct Feedback Alignment Provides Learning in Deep Neural Networks. (arXiv:1609.01596v4 [stat.ML] UPDATED) https://t.co/zsM0HO5ehK,0,6," Abstract: Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task. "
790712905280917509,2016-10-25 00:33:42,https://t.co/UAeCyEY0WC,Variance-based regularization with convex objectives. (arXiv:1610.02581v2 [stat.ML] UPDATED) https://t.co/UAeCyEY0WC,1,7," Abstract: We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error. Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certificates of optimality and achieves faster rates of convergence in more general settings than empirical risk minimization by virtue of trading off approximation and estimation error optimally. We give corroborating empirical evidence that suggests that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems. "
790350222597718017,2016-10-24 00:32:31,https://t.co/DsqIYqwIMu,Combinatorial Multi-Armed Bandit with General Reward Functions. (arXiv:1610.06603v1 [cs.LG]) https://t.co/DsqIYqwIMu,1,4," Abstract: In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the $\max()$ function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve $O(\log T)$ distribution-dependent regret and $\tilde{O}(\sqrt{T})$ distribution-independent regret, where $T$ is the time horizon. We apply our results to the $K$-MAX problem and expected utility maximization problems. In particular, for $K$-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first $\tilde{O}(\sqrt T)$ bound on the $(1-\epsilon)$-approximation regret of its online problem, for any $\epsilon>0$. "
790350220127264768,2016-10-24 00:32:31,https://t.co/Q7U3gZnRE1,Single Pass PCA of Matrix Products. (arXiv:1610.06656v1 [stat.ML]) https://t.co/Q7U3gZnRE1,3,5," Abstract: In this paper we present a new algorithm for computing a low rank approximation of the product $A^TB$ by taking only a single pass of the two matrices $A$ and $B$. The straightforward way to do this is to (a) first sketch $A$ and $B$ individually, and then (b) find the top components using PCA on the sketch. Our algorithm in contrast retains additional summary information about $A,B$ (e.g. row and column norms etc.) and uses this additional information to obtain an improved approximation from the sketches. Our main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in addition we also provide results from an Apache Spark implementation that shows better computational and statistical performance on real-world and synthetic evaluation datasets. "
790350218386608128,2016-10-24 00:32:30,https://t.co/pQuBQ6lE8u,Stochastic Gradient MCMC with Stale Gradients. (arXiv:1610.06664v1 [stat.ML]) https://t.co/pQuBQ6lE8u,1,12," Abstract: Stochastic gradient MCMC (SG-MCMC) has played an important role in large-scale Bayesian learning, with well-developed theoretical convergence properties. In such applications of SG-MCMC, it is becoming increasingly popular to employ distributed systems, where stochastic gradients are computed based on some outdated parameters, yielding what are termed stale gradients. While stale gradients could be directly used in SG-MCMC, their impact on convergence properties has not been well studied. In this paper we develop theory to show that while the bias and MSE of an SG-MCMC algorithm depend on the staleness of stochastic gradients, its estimation variance (relative to the expected estimate, based on a prescribed number of samples) is independent of it. In a simple Bayesian distributed system with SG-MCMC, where stale gradients are computed asynchronously by a set of workers, our theory indicates a linear speedup on the decrease of estimation variance w.r.t. the number of workers. Experiments on synthetic data and deep neural networks validate our theory, demonstrating the effectiveness and scalability of SG-MCMC with stale gradients. "
790350216570503168,2016-10-24 00:32:30,https://t.co/8i5zZlGK4K,On the Convergence of Stochastic Gradient MCMC Algorithms with High-Order Integrators. (arXiv:1610.06665v1 [stat.M… https://t.co/8i5zZlGK4K,0,4," Abstract: Recent advances in Bayesian learning with large-scale data have witnessed emergence of stochastic gradient MCMC algorithms (SG-MCMC), such as stochastic gradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian MCMC (SGHMC), and the stochastic gradient thermostat. While finite-time convergence properties of the SGLD with a 1st-order Euler integrator have recently been studied, corresponding theory for general SG-MCMCs has not been explored. In this paper we consider general SG-MCMCs with high-order integrators, and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures. Our theoretical results show faster convergence rates and more accurate invariant measures for SG-MCMCs with higher-order integrators. For example, with the proposed efficient 2nd-order symmetric splitting integrator, the {\em mean square error} (MSE) of the posterior average for the SGHMC achieves an optimal convergence rate of $L^{-4/5}$ at $L$ iterations, compared to $L^{-2/3}$ for the SGHMC and SGLD with 1st-order Euler integrators. Furthermore, convergence results of decreasing-step-size SG-MCMCs are also developed, with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence. Experiments on both synthetic and real datasets verify our theory, and show advantages of the proposed method in two large-scale real applications. "
790350213391249408,2016-10-24 00:32:29,https://t.co/JqhYl7GW1d,End-to-End Training Approaches for Discriminative Segmental Models. (arXiv:1610.06700v1 [cs.CL]) https://t.co/JqhYl7GW1d,0,1," Abstract: Recent work on discriminative segmental models has shown that they can achieve competitive speech recognition performance, using features based on deep neural frame classifiers. However, segmental models can be more challenging to train than standard frame-based approaches. While some segmental models have been successfully trained end to end, there is a lack of understanding of their training under different settings and with different losses. We investigate a model class based on recent successful approaches, consisting of a linear model that combines segmental features based on an LSTM frame classifier. Similarly to hybrid HMM-neural network models, segmental models of this class can be trained in two stages (frame classifier training followed by linear segmental model weight training), end to end (joint training of both frame classifier and linear weights), or with end-to-end fine-tuning after two-stage training. We study segmental models trained end to end with hinge loss, log loss, latent hinge loss, and marginal log loss. We consider several losses for the case where training alignments are available as well as where they are not. We find that in general, marginal log loss provides the most consistent strong performance without requiring ground-truth alignments. We also find that training with dropout is very important in obtaining good performance with end-to-end training. Finally, the best results are typically obtained by a combination of two-stage training and fine-tuning. "
790350211763830784,2016-10-24 00:32:29,https://t.co/YIVkhiaXgs,Minimax Error of Interpolation and Optimal Design of Experiments for Variable Fidelity Data. (arXiv:1610.06731v1 [… https://t.co/YIVkhiaXgs,0,1," Abstract: Engineering problems often involve data sources of variable fidelity with different costs of obtaining an observation. In particular, one can use both a cheap low fidelity function (e.g. a computational experiment with a CFD code) and an expensive high fidelity function (e.g. a wind tunnel experiment) to generate a data sample in order to construct a regression model of a high fidelity function. The key question in this setting is how the sizes of the high and low fidelity data samples should be selected in order to stay within a given computational budget and maximize accuracy of the regression model prior to committing resources on data acquisition. In this paper we obtain minimax interpolation errors for single and variable fidelity scenarios for a multivariate Gaussian process regression. Evaluation of the minimax errors allows us to identify cases when the variable fidelity data provides better interpolation accuracy than the exclusively high fidelity data for the same computational budget. These results allow us to calculate the optimal shares of variable fidelity data samples under the given computational budget constraint. Real and synthetic data experiments suggest that using the obtained optimal shares often outperforms natural heuristics in terms of the regression accuracy. "
790350209922568192,2016-10-24 00:32:28,https://t.co/c65BeY5vwE,Maximally Divergent Intervals for Anomaly Detection. (arXiv:1610.06761v1 [stat.ML]) https://t.co/c65BeY5vwE,1,3, Abstract: We present new methods for batch anomaly detection in multivariate time series. Our methods are based on maximizing the Kullback-Leibler divergence between the data distribution within and outside an interval of the time series. An empirical analysis shows the benefits of our algorithms compared to methods that treat each time step independently from each other without optimizing with respect to all possible intervals. 
790350208416776192,2016-10-24 00:32:28,https://t.co/GdqWmElBHi,Variational approximation of molecular kinetics from short off-equilibrium simulations. (arXiv:1610.06773v1 [stat.… https://t.co/GdqWmElBHi,1,2," Abstract: Markov state models (MSMs) and Master equation models are popular approaches to approximate molecular kinetics, equilibria, metastable states, and reaction coordinates in terms of a state space discretization usually obtained by clustering. Recently, a powerful generalization of MSMs has been introduced, the variational approach of conformation dynamics (VAC) and its special case the time-lagged independent component analysis (TICA), which allow us to approximate molecular kinetics and reaction coordinates by linear combinations of smooth basis functions or order parameters. While MSMs can be learned from trajectories whose starting points are not sampled from an equilibrium ensemble, TICA and VAC have as yet not enjoyed this property, and thus previous TICA/VAC estimates have been strongly biased when used with ensembles of short trajectories. Here, we employ Koopman operator theory and ideas from dynamic mode decomposition (DMD) to show how TICA/VAC can be used to estimate the unbiased equilibrium distribution from short-trajectory data and further this result in order to construct unbiased estimators for expectations, covariance matrices, TICA/VAC eigenvectors, relaxation timescales, and reaction coordinates. "
790350206781034500,2016-10-24 00:32:27,https://t.co/YYIfd71KmL,Robust training on approximated minimal-entropy set. (arXiv:1610.06806v1 [cs.LG]) https://t.co/YYIfd71KmL,0,2," Abstract: In this paper, we propose a general framework to learn a robust large-margin binary classifier when corrupt measurements, called anomalies, caused by sensor failure might be present in the training set. The goal is to minimize the generalization error of the classifier on non-corrupted measurements while controlling the false alarm rate associated with anomalous samples. By incorporating a non-parametric regularizer based on an empirical entropy estimator, we propose a Geometric-Entropy-Minimization regularized Maximum Entropy Discrimination (GEM-MED) method to learn to classify and detect anomalies in a joint manner. We demonstrate using simulated data and a real multimodal data set. Our GEM-MED method can yield improved performance over previous robust classification methods in terms of both classification accuracy and anomaly detection rate. "
790350205078102016,2016-10-24 00:32:27,https://t.co/VKsvTBgkQg,Convex Formulation for Kernel PCA and its Use in Semi-Supervised Learning. (arXiv:1610.06811v1 [cs.LG]) https://t.co/VKsvTBgkQg,4,8," Abstract: In this paper, Kernel PCA is reinterpreted as the solution to a convex optimization problem. Actually, there is a constrained convex problem for each principal component, so that the constraints guarantee that the principal component is indeed a solution, and not a mere saddle point. Although these insights do not imply any algorithmic improvement, they can be used to further understand the method, formulate possible extensions and properly address them. As an example, a new convex optimization problem for semi-supervised classification is proposed, which seems particularly well-suited whenever the number of known labels is small. Our formulation resembles a Least Squares SVM problem with a regularization parameter multiplied by a negative sign, combined with a variational principle for Kernel PCA. Our primal optimization principle for semi-supervised learning is solved in terms of the Lagrange multipliers. Numerical experiments in several classification tasks illustrate the performance of the proposed model in problems with only a few labeled data. "
790350203090006016,2016-10-24 00:32:27,https://t.co/GWQyPQFYee,An Efficient Minibatch Acceptance Test for Metropolis-Hastings. (arXiv:1610.06848v1 [cs.LG]) https://t.co/GWQyPQFYee,1,2," Abstract: We present a novel Metropolis-Hastings method for large datasets that uses small expected-size minibatches of data. Previous work on reducing the cost of Metropolis-Hastings tests yield variable data consumed per sample, with only constant factor reductions versus using the full dataset for each sample. Here we present a method that can be tuned to provide arbitrarily small batch sizes, by adjusting either proposal step size or temperature. Our test uses the noise-tolerant Barker acceptance test with a novel additive correction variable. The resulting test has similar cost to a normal SGD update. Our experiments demonstrate several order-of-magnitude speedups over previous work. "
790350201508753412,2016-10-24 00:32:26,https://t.co/esExh5ZtuB,Dictionary Learning Strategies for Compressed Fiber Sensing Using a Probabilistic Sparse Model. (arXiv:1610.06902v… https://t.co/esExh5ZtuB,1,3," Abstract: We present a sparse estimation and dictionary learning framework for compressed fiber sensing based on a probabilistic hierarchical sparse model. To handle severe dictionary coherence, selective shrinkage is achieved using a Weibull prior, which can be related to non-convex optimization with $p$-norm constraints for $0 < p < 1$. In addition, we leverage the specific dictionary structure to promote collective shrinkage based on a local similarity model. This is incorporated in form of a kernel function in the joint prior density of the sparse coefficients, thereby establishing a Markov random field-relation. Approximate inference is accomplished using a hybrid technique that combines Hamilton Monte Carlo and Gibbs sampling. To estimate the dictionary parameter, we pursue two strategies, relying on either a deterministic or a probabilistic model for the dictionary parameter. In the first strategy, the parameter is estimated based on alternating estimation. In the second strategy, it is jointly estimated along with the sparse coefficients. The performance is evaluated in comparison to an existing method in various scenarios using simulations and experimental data. "
790350199872950272,2016-10-24 00:32:26,https://t.co/rt79EYuzhF,Learning Theory for Distribution Regression. (arXiv:1411.2066v4 [math.ST] UPDATED) https://t.co/rt79EYuzhF,1,3," Abstract: We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the one-stage sampled minimax optimal rate [Caponnetto and De Vito, 2007; Steinwart et al., 2009]. This result answers a 17-year-old open question, establishing the consistency of the classical set kernel [Haussler, 1999; Gaertner et. al, 2002] in regression. We also cover consistency for more recent kernels on distributions, including those due to [Christmann and Steinwart, 2010]. "
790350198165962752,2016-10-24 00:32:25,https://t.co/AUMOO4cHsX,Modeling sequences and temporal networks with dynamic community structures. (arXiv:1509.04740v2 [cs.SI] UPDATED) https://t.co/AUMOO4cHsX,1,4," Abstract: Methods for identification of dynamical patterns in networks suffer from effects of arbitrary time scales that need to be imposed a priori. Here we develop a principled method to identify patterns on dynamics that take place on network systems, as well as on the dynamics that shape the network themselves, without requiring the stipulation of relevant time scales, which instead are determined solely from data. Our approach is based on a variable-order hidden Markov chain model that generalizes the stochastic block model for discrete time-series as well as temporal networks, without requiring the aggregation of events into discrete intervals. We formulate an efficient nonparametric Bayesian framework that can infer the most appropriate Markov order and number of communities, based solely on statistical evidence and without overfitting. "
790350196081299456,2016-10-24 00:32:25,https://t.co/FC9pLr0BQI,Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-\L{}ojasiewicz Condition. (arXiv:160… https://t.co/FC9pLr0BQI,0,2," Abstract: In 1963, Polyak proposed a simple condition that is sufficient to show a global linear convergence rate for gradient descent. This condition is a special case of the \L{}ojasiewicz inequality proposed in the same year, and it does not require strong convexity (or even convexity). In this work, we show that this much-older Polyak-\L{}ojasiewicz (PL) inequality is actually weaker than the main conditions that have been explored to show linear convergence rates without strong convexity over the last 25 years. We also use the PL inequality to give new analyses of randomized and greedy coordinate descent methods, sign-based gradient descent methods, and stochastic gradient methods in the classic setting (with decreasing or constant step-sizes) as well as the variance-reduced setting. We further propose a generalization that applies to proximal-gradient methods for non-smooth optimization, leading to simple proofs of linear convergence of these methods. Along the way, we give simple convergence results for a wide variety of problems in machine learning: least squares, logistic regression, boosting, resilient backpropagation, L1-regularization, support vector machines, stochastic dual coordinate ascent, and stochastic variance-reduced gradient methods. "
790350193967366144,2016-10-24 00:32:24,https://t.co/vqMOC78Pl6,Stochastic Heavy Ball. (arXiv:1609.04228v2 [math.ST] UPDATED) https://t.co/vqMOC78Pl6,0,1," Abstract: This paper deals with a natural stochastic optimization procedure derived from the so-called Heavy-ball method differential equation, which was introduced by Polyak in the 1960s with his seminal contribution [Pol64]. The Heavy-ball method is a second-order dynamics that was investigated to minimize convex functions f . The family of second-order methods recently received a large amount of attention, until the famous contribution of Nesterov [Nes83], leading to the explosion of large-scale optimization problems. This work provides an in-depth description of the stochastic heavy-ball method, which is an adaptation of the deterministic one when only unbiased evalutions of the gradient are available and used throughout the iterations of the algorithm. We first describe some almost sure convergence results in the case of general non-convex coercive functions f . We then examine the situation of convex and strongly convex potentials and derive some non-asymptotic results about the stochastic heavy-ball method. We end our study with limit theorems on several rescaled algorithms. "
790350192474292224,2016-10-24 00:32:24,https://t.co/MVV80rEKVc,Robust Bayesian Compressed sensing. (arXiv:1610.02807v2 [stat.ML] UPDATED) https://t.co/MVV80rEKVc,1,9," Abstract: We consider the problem of robust compressed sensing whose objective is to recover a high-dimensional sparse signal from compressed measurements corrupted by outliers. A new sparse Bayesian learning method is developed for robust compressed sensing. The basic idea of the proposed method is to identify and remove the outliers from sparse signal recovery. To automatically identify the outliers, we employ a set of binary indicator hyperparameters to indicate which observations are outliers. These indicator hyperparameters are treated as random variables and assigned a beta process prior such that their values are confined to be binary. In addition, a Gaussian-inverse Gamma prior is imposed on the sparse signal to promote sparsity. Based on this hierarchical prior model, we develop a variational Bayesian method to estimate the indicator hyperparameters as well as the sparse signal. Simulation results show that the proposed method achieves a substantial performance improvement over existing robust compressed sensing techniques. "
790350189835980801,2016-10-24 00:32:23,https://t.co/rdAZSfrLOX,Generalization bound for kernel similarity learning. (arXiv:1610.03899v2 [stat.ML] UPDATED) https://t.co/rdAZSfrLOX,1,2," Abstract: Similarity learning has received a large amount of interest and is an important tool for many scientific and industrial applications. In this framework, we wish to infer the distance (similarity) between points with respect to an arbitrary distance function $d$. Here, we formulate the problem as a regression from a feature space $\mathcal{X}$ to an arbitrary vector space $\mathcal{Y}$, where the Euclidean distance is proportional to $d$. We then give Rademacher complexity bounds on the generalization error. We find that with high probability, the complexity is bounded by the maximum of the radius of $\mathcal{X}$ and the radius of $\mathcal{Y}$. "
789264522125148160,2016-10-21 00:38:20,https://t.co/1IkEqnghkT,Robust and Parallel Bayesian Model Selection. (arXiv:1610.06194v1 [stat.ML]) https://t.co/1IkEqnghkT,1,7," Abstract: Effective and accurate model selection that takes into account model uncertainty is an important but challenging problem in modern data analysis. One of the major challenges is the computational burden required to infer huge data sets which, in general, cannot be stored or processed on one machine. Moreover, in many real data modeling scenarios we may encounter the presence of outliers and contaminations that will damage the quality of our model and variable selection. We can overcome both of these problems through a simple ""divide and conquer"" strategy in which we divide the observations of the full data set equally into subsets and perform inference and model selections independently on each subset. After local subset inference, we can aggregate the optimal subset model or aggregate the local model/variable selection criteria to obtain a final model. We show that by aggregating with the geometric median, we obtain results that are robust to outliers and contamination of an unknown nature. "
789264520522989568,2016-10-21 00:38:20,https://t.co/tHRLGW1SAA,Enhancing ICA Performance by Exploiting Sparsity: Application to FMRI Analysis. (arXiv:1610.06235v1 [stat.ML]) https://t.co/tHRLGW1SAA,0,3," Abstract: Independent component analysis (ICA) is a powerful method for blind source separation based on the assumption that sources are statistically independent. Though ICA has proven useful and has been employed in many applications, complete statistical independence can be too restrictive an assumption in practice. Additionally, important prior information about the data, such as sparsity, is usually available. Sparsity is a natural property of the data, a form of diversity, which, if incorporated into the ICA model, can relax the independence assumption, resulting in an improvement in the overall separation performance. In this work, we propose a new variant of ICA by entropy bound minimization (ICA-EBM)-a flexible, yet parameter-free algorithm-through the direct exploitation of sparsity. Using this new SparseICA-EBM algorithm, we study the synergy of independence and sparsity through simulations on synthetic as well as functional magnetic resonance imaging (fMRI)-like data. "
789264518954221568,2016-10-21 00:38:19,https://t.co/IvrvLUbcVw,Using Fast Weights to Attend to the Recent Past. (arXiv:1610.06258v1 [stat.ML]) https://t.co/IvrvLUbcVw,0,7," Abstract: Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These ""fast weights"" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns. "
789264517372993536,2016-10-21 00:38:19,https://t.co/Omqjy5XPMN,Kernel Alignment for Unsupervised Transfer Learning. (arXiv:1610.06434v1 [stat.ML]) https://t.co/Omqjy5XPMN,0,7," Abstract: The ability of a human being to extrapolate previously gained knowledge to other domains inspired a new family of methods in machine learning called transfer learning. Transfer learning is often based on the assumption that objects in both target and source domains share some common feature and/or data space. In this paper, we propose a simple and intuitive approach that minimizes iteratively the distance between source and target task distributions by optimizing the kernel target alignment (KTA). We show that this procedure is suitable for transfer learning by relating it to Hilbert-Schmidt Independence Criterion (HSIC) and Quadratic Mutual Information (QMI) maximization. We run our method on benchmark computer vision data sets and show that it can outperform some state-of-art methods. "
789264515380678656,2016-10-21 00:38:18,https://t.co/MDj9WNwhn6,Regularized Optimal Transport and the Rot Mover's Distance. (arXiv:1610.06447v1 [stat.ML]) https://t.co/MDj9WNwhn6,0,3," Abstract: This paper presents a unified framework for smooth convex regularization of discrete optimal transport problems. In this context, the regularized optimal transport turns out to be equivalent to a matrix nearness problem with respect to Bregman divergences. Our framework thus naturally generalizes a previously proposed regularization based on the Boltzmann-Shannon entropy related to the Kullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We call the regularized optimal transport distance the rot mover's distance in reference to the classical earth mover's distance. We develop two generic schemes that we respectively call the alternate scaling algorithm and the non-negative alternate scaling algorithm, to compute efficiently the regularized optimal plans depending on whether the domain of the regularizer lies within the non-negative orthant or not. These schemes are based on Dykstra's algorithm with alternate Bregman projections, and further exploit the Newton-Raphson method for separable divergences. We enhance the separable case with a sparse extension to deal with high data dimensions. We also instantiate our proposed framework and discuss the inherent specificities for well-known regularizers and statistical divergences in the machine learning and information geometry communities. Finally, we demonstrate our methods with an experiment on synthetic input data that illustrates the effect of different regularizers and penalties on the output solutions. "
789264513656848385,2016-10-21 00:38:18,https://t.co/8p9lCzmB99,Change-point Detection Methods for Body-Worn Video. (arXiv:1610.06453v1 [cs.CV]) https://t.co/8p9lCzmB99,0,3," Abstract: Body-worn video (BWV) cameras are increasingly utilized by police departments to provide a record of police-public interactions. However, large-scale BWV deployment produces terabytes of data per week, necessitating the development of effective computational methods to identify salient changes in video. In work carried out at the 2016 RIPS program at IPAM, UCLA, we present a novel two-stage framework for video change-point detection. First, we employ state-of-the-art machine learning methods including convolutional neural networks and support vector machines for scene classification. We then develop and compare change-point detection algorithms utilizing mean squared-error minimization, forecasting methods, hidden Markov models, and maximum likelihood estimation to identify noteworthy changes. We test our framework on detection of vehicle exits and entrances in a BWV data set provided by the Los Angeles Police Department and achieve over 90% recall and nearly 70% precision -- demonstrating robustness to rapid scene changes, extreme luminance differences, and frequent camera occlusions. "
789264509315710980,2016-10-21 00:38:17,https://t.co/0Kn7mdhrkE,Reasoning with Memory Augmented Neural Networks for Language Comprehension. (arXiv:1610.06454v1 [cs.CL]) https://t.co/0Kn7mdhrkE,0,8," Abstract: Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets. "
789264507650633729,2016-10-21 00:38:17,https://t.co/4fI2QrNXcE,Efficient Estimation of Compressible State-Space Models with Application to Calcium Signal Deconvolution. (arXiv:1… https://t.co/4fI2QrNXcE,0,3," Abstract: In this paper, we consider linear state-space models with compressible innovations and convergent transition matrices in order to model spatiotemporally sparse transient events. We perform parameter and state estimation using a dynamic compressed sensing framework and develop an efficient solution consisting of two nested Expectation-Maximization (EM) algorithms. Under suitable sparsity assumptions on the innovations, we prove recovery guarantees and derive confidence bounds for the state estimates. We provide simulation studies as well as application to spike deconvolution from calcium imaging data which verify our theoretical results and show significant improvement over existing algorithms. "
789264505582780417,2016-10-21 00:38:16,https://t.co/BW4cwlhefW,Gaussian process modeling in approximate Bayesian computation to estimate horizontal gene transfer in bacteria. (a… https://t.co/BW4cwlhefW,3,4," Abstract: Approximate Bayesian computation (ABC) can be used for model fitting when the likelihood function is intractable but simulating from the model is feasible. However, even a single evaluation of a complex model may take several hours, limiting the number of model evaluations available. Modeling the discrepancy between the simulated and observed data using a Gaussian process (GP) can be used to reduce the number of model evaluations required by ABC, but the sensitivity of this approach to a specific GP formulation has not been thoroughly investigated. We begin with a comprehensive empirical evaluation of using GPs in ABC, including various transformations of the discrepancies and two novel GP formulations. Our results indicate the choice of GP may significantly affect the accuracy of the estimated posterior distribution. Selection of an appropriate GP model is thus important. We define expected utility to measure the accuracy of classifying discrepancies below or above the ABC threshold, and show that by using this utility, the GP model selection step can be made automatic. Finally, based on the understanding gained with toy examples, we fit a population genetic model for bacteria, providing insight into horizontal gene transfer events within the population and from external origins. "
789264503976460288,2016-10-21 00:38:16,https://t.co/s2EDxWmxBA,ChoiceRank: Identifying Preferences from Node Traffic in Networks. (arXiv:1610.06525v1 [stat.ML]) https://t.co/s2EDxWmxBA,0,2," Abstract: Understanding how users navigate in a network is of high interest in many applications. We consider a setting where only aggregate node-level traffic is observed and tackle the task of learning edge transition probabilities. We cast it as a preference learning problem, and we study a model where choices follow Luce's axiom. In this case, the $O(n)$ marginal counts of node visits are a sufficient statistic for the $O(n^2)$ transition probabilities. We show how to make the inference problem well-posed regardless of the network's structure, and we present ChoiceRank, an iterative algorithm that scales to networks that contains billions of nodes and edges. We apply the model to a month-long clickstream of the English Wikipedia and one year of rides on New York City's bicycle-sharing system. In both cases, we successfully recover the transition probabilities using only the network structure and marginal (node-level) traffic data. "
789264502256721920,2016-10-21 00:38:15,https://t.co/dd1VmefDGW,Revisiting Classifier Two-Sample Tests for GAN Evaluation and Causal Discovery. (arXiv:1610.06545v1 [stat.ML]) https://t.co/dd1VmefDGW,4,15," Abstract: The goal of two-sample tests is to assess whether two samples, $S_P \sim P^n$ and $S_Q \sim Q^m$, are drawn from the same distribution. Perhaps intriguingly, one relatively unexplored method to build two-sample tests is the use of binary classifiers. In particular, construct a dataset by pairing the $n$ examples in $S_P$ with a positive label, and by pairing the $m$ examples in $S_Q$ with a negative label. If the null hypothesis ""$P = Q$"" is true, then the classification accuracy of a binary classifier on a held-out subset of this dataset should remain near chance-level. As we will show, such Classifier Two-Sample Tests (C2ST) learn a suitable representation of the data on the fly, return test statistics in interpretable units, have a simple null distribution, and their predictive uncertainty allow to interpret where $P$ and $Q$ differ. The goal of this paper is to establish the properties, performance, and uses of C2ST. First, we analyze their main theoretical properties. Second, we compare their performance against a variety of state-of-the-art alternatives. Third, we propose their use to evaluate the sample quality of generative models with intractable likelihoods, such as Generative Adversarial Networks (GANs). Fourth, we showcase the novel application of GANs together with C2ST for causal discovery. "
789264500474146816,2016-10-21 00:38:15,https://t.co/LnzRlifss3,Nonlinear Structural Vector Autoregressive Models for Inferring Effective Brain Network Connectivity. (arXiv:1610.… https://t.co/LnzRlifss3,0,4," Abstract: Structural equation models (SEMs) and vector autoregressive models (VARMs) are two broad families of approaches that have been shown useful in effective brain connectivity studies. While VARMs postulate that a given region of interest in the brain is directionally connected to another one by virtue of time-lagged influences, SEMs assert that causal dependencies arise due to contemporaneous effects, and may even be adopted when nodal measurements are not necessarily multivariate time series. To unify these complementary perspectives, linear structural vector autoregressive models (SVARMs) that leverage both contemporaneous and time-lagged nodal data have recently been put forth. Albeit simple and tractable, linear SVARMs are quite limited since they are incapable of modeling nonlinear dependencies between neuronal time series. To this end, the overarching goal of the present paper is to considerably broaden the span of linear SVARMs by capturing nonlinearities through kernels, which have recently emerged as a powerful nonlinear modeling framework in canonical machine learning tasks, e.g., regression, classification, and dimensionality reduction. The merits of kernel-based methods are extended here to the task of learning the effective brain connectivity, and an efficient regularized estimator is put forth to leverage the edge sparsity inherent to real-world complex networks. Judicious kernel choice from a preselected dictionary of kernels is also addressed using a data-driven approach. Extensive numerical tests on ECoG data captured through a study on epileptic seizures demonstrate that it is possible to unveil previously unknown causal links between brain regions of interest. "
789264498565742592,2016-10-21 00:38:14,https://t.co/Y2L2juRhFF,Selective Factor Extraction in High Dimensions. (arXiv:1403.6212v3 [stat.ME] UPDATED) https://t.co/Y2L2juRhFF,0,4," Abstract: This paper studies simultaneous feature selection and extraction in supervised and unsupervised learning. We propose and investigate selective reduced rank regression for constructing optimal explanatory factors from a parsimonious subset of input features. The proposed estimators enjoy sharp oracle inequalities, and with a predictive information criterion for model selection, they adapt to unknown sparsity by controlling both rank and row support of the coefficient matrix. A class of algorithms is developed that can accommodate various convex and nonconvex sparsity-inducing penalties, and can be used for rank-constrained variable screening in high-dimensional multivariate data. The paper also showcases applications in macroeconomics and computer vision to demonstrate how low-dimensional data structures can be effectively captured by joint variable selection and projection. "
789264496577679361,2016-10-21 00:38:14,https://t.co/zZbXeHb30Y,Group Regularized Estimation under Structural Hierarchy. (arXiv:1411.4691v2 [math.ST] UPDATED) https://t.co/zZbXeHb30Y,0,3," Abstract: Variable selection for models including interactions between explanatory variables often needs to obey certain hierarchical constraints. The weak or strong structural hierarchy requires that the existence of an interaction term implies at least one or both associated main effects to be present in the model. Lately, this problem has attracted a lot of attention, but existing computational algorithms converge slow even with a moderate number of predictors. Moreover, in contrast to the rich literature on ordinary variable selection, there is a lack of statistical theory to show reasonably low error rates of hierarchical variable selection. This work investigates a new class of estimators that make use of multiple group penalties to capture structural parsimony. We give the minimax lower bounds for strong and weak hierarchical variable selection and show that the proposed estimators enjoy sharp rate oracle inequalities. A general-purpose algorithm is developed with guaranteed convergence and global optimality. Simulations and real data experiments demonstrate the efficiency and efficacy of the proposed approach. "
789264494178496512,2016-10-21 00:38:13,https://t.co/sL4widIDjg,Sparse Partially Collapsed MCMC for Parallel Inference in Topic Models. (arXiv:1506.03784v2 [stat.ML] UPDATED) https://t.co/sL4widIDjg,0,3," Abstract: Topic models are widely used for probabilistic modeling of text and images. MCMC sampling from the posterior distribution is typically performed using a collapsed Gibbs sampler. We propose a parallel sparse partially collapsed Gibbs sampler and compare its speed and efficiency to state-of-the-art samplers for topic models on five text corpora of differing sizes and properties. In particular, we propose and compare two different strategies for sampling the parameter block with latent topic indicators. The experiments, which are performed on well-known corpora, show that the expected increase in statistical inefficiency from only partial collapsing is smaller than commonly assumed. This minor inefficiency can be more than compensated by the speed-up from parallelization of larger corpora. We also prove that the partially collapsed samplers scale well with the size of the corpus. The proposed algorithm is fast, efficient, exact, and can be used in more modeling situations than the ordinary collapsed sampler. "
789264492563668992,2016-10-21 00:38:13,https://t.co/5hTJzeE5L2,Probabilistic Integration: A Role for Statisticians in Numerical Analysis?. (arXiv:1512.00933v5 [stat.ML] UPDATED) https://t.co/5hTJzeE5L2,0,8," Abstract: A research frontier has emerged in scientific computation, founded on the principle that numerical error entails epistemic uncertainty that ought to be subjected to statistical analysis. This viewpoint raises several interesting challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational pipeline. This paper examines thoroughly the case for probabilistic numerical methods in statistical computation and a specific case study is presented for Markov chain and Quasi Monte Carlo methods. A probabilistic integrator is equipped with a full distribution over its output, providing a measure of epistemic uncertainty that is shown to be statistically valid at finite computational levels, as well as in asymptotic regimes. The approach is motivated by expensive integration problems, where, as in krigging, one is willing to expend, at worst, cubic computational effort in order to gain uncertainty quantification. There, probabilistic integrators enjoy the ""best of both worlds"", leveraging the sampling efficiency of Monte Carlo methods whilst providing a principled route to assessment of the impact of numerical error on scientific conclusions. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and uncertainty quantification in oil reservoir modelling. "
789264490932137984,2016-10-21 00:38:13,https://t.co/iAzfN2xFnk,DOLDA - a regularized supervised topic model for high-dimensional multi-class regression. (arXiv:1602.00260v2 [sta… https://t.co/iAzfN2xFnk,0,2," Abstract: Generating user interpretable multi-class predictions in data rich environments with many classes and explanatory covariates is a daunting task. We introduce Diagonal Orthant Latent Dirichlet Allocation (DOLDA), a supervised topic model for multi-class classification that can handle both many classes as well as many covariates. To handle many classes we use the recently proposed Diagonal Orthant (DO) probit model (Johndrow et al., 2013) together with an efficient Horseshoe prior for variable selection/shrinkage (Carvalho et al., 2010). We propose a computationally efficient parallel Gibbs sampler for the new model. An important advantage of DOLDA is that learned topics are directly connected to individual classes without the need for a reference class. We evaluate the model's predictive accuracy on two datasets and demonstrate DOLDA's advantage in interpreting the generated predictions. "
789264489459965954,2016-10-21 00:38:12,https://t.co/LDhuktkZoI,Efficient Learning with a Family of Nonconvex Regularizers by Redistributing Nonconvexity. (arXiv:1606.03841v2 [ma… https://t.co/LDhuktkZoI,1,6," Abstract: The use of convex regularizers allow for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, for a large class of nonconvex regularizers, we propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex regularizer, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the standard proximal algorithm and Frank-Wolfe algorithm). Moreover, it can be shown that critical points of the transformed problem are also critical points of the original problem. Extensive experiments on a number of nonconvex regularization problems show that the proposed procedure is much faster than the state-of-the-art nonconvex solvers. "
789264487664803840,2016-10-21 00:38:12,https://t.co/mkllQ3R31Y,Membership Inference Attacks against Machine Learning Models. (arXiv:1610.05820v1 [cs.CR] CROSS LISTED) https://t.co/mkllQ3R31Y,0,5," Abstract: We investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine whether the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference attack model to recognize differences in the target model's predictions on inputs that it trained on versus inputs that it did not use during training. We empirically evaluate our inference techniques on classification models trained by commercial ""machine learning as a service"" providers such as Google and Amazon. Using realistic datasets and classification tasks, we show that these models can be significantly vulnerable to membership inference attacks. "
789264485219532800,2016-10-21 00:38:11,https://t.co/PWpjPzfHvq,Making brain-machine interfaces robust to future neural variability. (arXiv:1610.05872v1 [q-bio.NC] CROSS LISTED) https://t.co/PWpjPzfHvq,0,2," Abstract: A major hurdle to clinical translation of brain-machine interfaces (BMIs) is that current decoders, which are trained from a small quantity of recent data, become ineffective when neural recording conditions subsequently change. We tested whether a decoder could be made more robust to future neural variability by training it to handle a variety of recording conditions sampled from months of previously collected data as well as synthetic training data perturbations. We developed a new multiplicative recurrent neural network BMI decoder that successfully learned a large variety of neural-to- kinematic mappings and became more robust with larger training datasets. When tested with a non-human primate preclinical BMI model, this decoder was robust under conditions that disabled a state-of-the-art Kalman filter based decoder. These results validate a new BMI strategy in which accumulated data history is effectively harnessed, and may facilitate reliable daily BMI use by reducing decoder retraining downtime. "
788900912135806976,2016-10-20 00:33:29,https://t.co/CCYg9T3A38,RedQueen: An Online Algorithm for Smart Broadcasting in Social Networks. (arXiv:1610.05773v1 [stat.ML]) https://t.co/CCYg9T3A38,1,5," Abstract: Users in social networks whose posts stay at the top of their followers'{} feeds the longest time are more likely to be noticed. Can we design an online algorithm to help them decide when to post to stay at the top? In this paper, we address this question as a novel optimal control problem for jump stochastic differential equations. For a wide variety of feed dynamics, we show that the optimal broadcasting intensity for any user is surprisingly simple -- it is given by the position of her most recent post on each of her follower's feeds. As a consequence, we are able to develop a simple and highly efficient online algorithm, RedQueen, to sample the optimal times for the user to post. Experiments on both synthetic and real data gathered from Twitter show that our algorithm is able to consistently make a user's posts more visible over time, is robust to volume changes on her followers' feeds, and significantly outperforms the state of the art. "
788900909875130368,2016-10-20 00:33:28,https://t.co/YVMcMaUwZB,Modeling the Dynamics of Online Learning Activity. (arXiv:1610.05775v1 [stat.ML]) https://t.co/YVMcMaUwZB,0,3," Abstract: People are increasingly relying on the Web and social media to find solutions to their problems in a wide range of domains. In this online setting, closely related problems often lead to the same characteristic learning pattern, in which people sharing these problems visit related pieces of information, perform almost identical queries or, more generally, take a series of similar actions. In this paper, we introduce a novel modeling framework for clustering continuous-time grouped streaming data, the hierarchical Dirichlet Hawkes process (HDHP), which allows us to automatically uncover a wide variety of learning patterns from detailed traces of learning activity. Our model allows for efficient inference, scaling to millions of actions taken by thousands of users. Experiments on real data gathered from Stack Overflow reveal that our framework can recover meaningful learning patterns in terms of both content and temporal dynamics, as well as accurately track users' interests and goals over time. "
788900907224293376,2016-10-20 00:33:28,https://t.co/6TlO8Y0gqu,Big Batch SGD: Automated Inference using Adaptive Batch Sizes. (arXiv:1610.05792v1 [cs.LG]) https://t.co/6TlO8Y0gqu,0,15," Abstract: Classical stochastic gradient methods for optimization rely on noisy gradient approximations that become progressively less accurate as iterates approach a solution. The large noise and small signal in the resulting gradients makes it difficult to use them for adaptive stepsize selection and automatic stopping. We propose alternative ""big batch"" SGD schemes that adaptively grow the batch size over time to maintain a nearly constant signal-to-noise ratio in the gradient approximation. The resulting methods have similar convergence rates to classical SGD methods without requiring convexity of the objective function. The high fidelity gradients enable automated learning rate selection and do not require stepsize decay. For this reason, big batch methods are easily automated and can run with little or no user oversight. "
788900904992931841,2016-10-20 00:33:27,https://t.co/IqTjpzaXS0,Learning Determinantal Point Processes in Sublinear Time. (arXiv:1610.05925v1 [stat.ML]) https://t.co/IqTjpzaXS0,0,2," Abstract: We propose a new class of determinantal point processes (DPPs) which can be manipulated for inference and parameter learning in potentially sublinear time in the number of items. This class, based on a specific low-rank factorization of the marginal kernel, is particularly suited to a subclass of continuous DPPs and DPPs defined on exponentially many items. We apply this new class to modelling text documents as sampling a DPP of sentences, and propose a conditional maximum likelihood formulation to model topic proportions, which is made possible with no approximation for our class of DPPs. We present an application to document summarization with a DPP on $2^{500}$ items. "
788900902484738048,2016-10-20 00:33:26,https://t.co/DtU6XJsMVp,Consistent Kernel Mean Estimation for Functions of Random Variables. (arXiv:1610.05950v1 [stat.ML]) https://t.co/DtU6XJsMVp,1,5," Abstract: We provide a theoretical foundation for non-parametric estimation of functions of random variables using kernel mean embeddings. We show that for any continuous function $f$, consistent estimators of the mean embedding of a random variable $X$ lead to consistent estimators of the mean embedding of $f(X)$. For Mat\'ern kernels and sufficiently smooth functions we also provide rates of convergence. Our results extend to functions of multiple random variables. If the variables are dependent, we require an estimator of the mean embedding of their joint distribution as a starting point; if they are independent, it is sufficient to have separate estimators of the mean embeddings of their marginal distributions. In either case, our results cover both mean embeddings based on i.i.d. samples as well as ""reduced set"" expansions in terms of dependent expansion points. The latter serves as a justification for using such expansions to limit memory resources when applying the approach as a basis for probabilistic programming. "
788900899980705795,2016-10-20 00:33:26,https://t.co/YoTr2UKcBt,Clustering by connection center evolution. (arXiv:1610.05956v1 [stat.ML]) https://t.co/YoTr2UKcBt,0,2," Abstract: The determination of cluster centers generally depends on the scale that we use to analyze the data to be clustered. Inappropriate scale usually leads to unreasonable cluster centers and thus unreasonable results. In this study, we first consider the similarity of elements in the data as the connectivity of nodes in an undirected graph, then present the concept of a connection center and regard it as the cluster center of the data. Based on this definition, the determination of cluster centers and the assignment of class are very simple, natural and effective. One more crucial finding is that the cluster centers of different scales can be obtained easily by the different powers of a similarity matrix and the change of power from small to large leads to the dynamic evolution of cluster centers from local (microscopic) to global (microscopic). Further, in this process of evolution, the number of categories changes discontinuously, which means that the presented method can automatically skip the unreasonable number of clusters, suggest appropriate observation scales and provide corresponding cluster results. "
788900897250217984,2016-10-20 00:33:25,https://t.co/7Td1xTsn7G,Learning to Learn Neural Networks. (arXiv:1610.06072v1 [cs.LG]) https://t.co/7Td1xTsn7G,0,14," Abstract: Meta-learning consists in learning learning algorithms. We use a Long Short Term Memory (LSTM) based network to learn to compute on-line updates of the parameters of another neural network. These parameters are stored in the cell state of the LSTM. Our framework allows to compare learned algorithms to hand-made algorithms within the traditional train and test methodology. In an experiment, we learn a learning algorithm for a one-hidden layer Multi-Layer Perceptron (MLP) on non-linearly separable datasets. The learned algorithm is able to update parameters of both layers and generalise well on similar datasets. "
788900894070939648,2016-10-20 00:33:24,https://t.co/FA6iJzWJch,A global optimization algorithm for sparse mixed membership matrix factorization. (arXiv:1610.06145v1 [stat.ME]) https://t.co/FA6iJzWJch,0,7," Abstract: Mixed membership factorization is a popular approach for analyzing data sets that have within-sample heterogeneity. In recent years, several algorithms have been developed for mixed membership matrix factorization, but they only guarantee estimates from a local optimum. Here, we derive a global optimization (GOP) algorithm that provides a guaranteed $\epsilon$-global optimum for a sparse mixed membership matrix factorization problem. We test the algorithm on simulated data and find the algorithm always bounds the global optimum across random initializations and explores multiple modes efficiently. "
788900891839569920,2016-10-20 00:33:24,https://t.co/TSaE5BuPcw,Sparse Quadratic Discriminant Analysis and Community Bayes. (arXiv:1407.4543v2 [stat.ML] UPDATED) https://t.co/TSaE5BuPcw,0,2," Abstract: We develop a class of rules spanning the range between quadratic discriminant analysis and naive Bayes, through a path of sparse graphical models. A group lasso penalty is used to introduce shrinkage and encourage a similar pattern of sparsity across precision matrices. It gives sparse estimates of interactions and produces interpretable models. Inspired by the connected-components structure of the estimated precision matrices, we propose the community Bayes model, which partitions features into several conditional independent communities and splits the classification problem into separate smaller ones. The community Bayes idea is quite general and can be applied to non-Gaussian data and likelihood-based classifiers. "
788900887733436416,2016-10-20 00:33:23,https://t.co/bs9G2tQuEa,The Impact of Estimation: A New Method for Clustering and Trajectory Estimation in Patient Flow Modeling. (arXiv:1… https://t.co/bs9G2tQuEa,0,4," Abstract: The ability to accurately forecast and control inpatient census, and thereby workloads, is a critical and longstanding problem in hospital management. Majority of current literature focuses on optimal scheduling of inpatients, but largely ignores the process of accurate estimation of the trajectory of patients throughout the treatment and recovery process. The result is that current scheduling models are optimizing based on inaccurate input data. We developed a Clustering and Scheduling Integrated (CSI) approach to capture patient flows through a network of hospital services. CSI functions by clustering patients into groups based on similarity of trajectory using a novel Semi-Markov model (SMM)-based clustering scheme proposed in this paper, as opposed to clustering by admit type or condition as in previous literature. The methodology is validated by simulation and then applied to real patient data from a partner hospital where we see it outperforms current methods. Further, we demonstrate that extant optimization methods achieve significantly better results on key hospital performance measures under CSI, compared with traditional estimation approaches, increasing elective admissions by 97% and utilization by 22% compared to 30% and 8% using traditional estimation techniques. From a theoretical standpoint, the SMM-clustering is a novel approach applicable to any temporal-spatial stochastic data that is prevalent in many industries and application areas. "
788900885699133440,2016-10-20 00:33:22,https://t.co/Kxy58BeZGy,Learning Summary Statistic for Approximate Bayesian Computation via Deep Neural Network. (arXiv:1510.02175v2 [stat… https://t.co/Kxy58BeZGy,2,9," Abstract: Approximate Bayesian Computation (ABC) methods are used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Both the accuracy and computational efficiency of ABC depend on the choice of summary statistic, but outside of special cases where the optimal summary statistics are known, it is unclear which guiding principles can be used to construct effective summary statistics. In this paper we explore the possibility of automating the process of constructing summary statistics by training deep neural networks to predict the parameters from artificially generated data: the resulting summary statistics are approximately posterior means of the parameters. With minimal model-specific tuning, our method constructs summary statistics for the Ising model and the moving-average model, which match or exceed theoretically-motivated summary statistics in terms of the accuracies of the resulting posteriors. "
788900882805059584,2016-10-20 00:33:22,https://t.co/v46gG8pTv5,Recognizing Semantic Features in Faces using Deep Learning. (arXiv:1512.00743v2 [cs.LG] UPDATED) https://t.co/v46gG8pTv5,0,3," Abstract: The human face constantly conveys information, both consciously and subconsciously. However, as basic as it is for humans to visually interpret this information, it is quite a big challenge for machines. Conventional semantic facial feature recognition and analysis techniques are already in use and are based on physiological heuristics, but they suffer from lack of robustness and high computation time. This thesis aims to explore ways for machines to learn to interpret semantic information available in faces in an automated manner without requiring manual design of feature detectors, using the approach of Deep Learning. This thesis provides a study of the effects of various factors and hyper-parameters of deep neural networks in the process of determining an optimal network configuration for the task of semantic facial feature recognition. This thesis explores the effectiveness of the system to recognize the various semantic features (like emotions, age, gender, ethnicity etc.) present in faces. Furthermore, the relation between the effect of high-level concepts on low level features is explored through an analysis of the similarities in low-level descriptors of different semantic features. This thesis also demonstrates a novel idea of using a deep network to generate 3-D Active Appearance Models of faces from real-world 2-D images. For a more detailed report on this work, please see [arXiv:1512.00743v1]. "
788900880376619008,2016-10-20 00:33:21,https://t.co/AEPXm3ln8O,Optimality of Belief Propagation for Crowdsourced Classification: Proof for Arbitrary Number of Per-worker Assignm… https://t.co/AEPXm3ln8O,0,2," Abstract: Crowdsourcing systems are popular for solving large-scale labelling tasks with low-paid (or even non-paid) workers. We study the problem of recovering the true labels from the possibly erroneous crowdsourced labels under the popular Dawid-Skene model. To address this inference problem, several algorithms have recently been proposed, but the best known guarantee is still significantly larger than the fundamental limit. In our previous work, we closed this gap under a canonical assumption where each worker is assigned only two tasks, i.e., $r=2$, and each task is assigned to sufficiently but constantly many workers, $\ell \ge C_r$. In this work, we further remove the condition on r and show that for all $r \ge 1$, Belief Propagation exactly matches a lower bound on the fundamental limit if $\ell \ge C_r$. The guaranteed optimality of BP is the strongest in the sense that it is information-theoretically impossible for any other algorithm to correctly label a larger fraction of the tasks. In the general setting, regardless of the number of workers assigned to a task, we establish the dominance result on BP that it outperforms all existing algorithms with provable guarantees. Experimental results suggest that BP is close to optimal for all regimes considered, while all other algorithms show suboptimal performances in certain regimes. "
788900877784473604,2016-10-20 00:33:20,https://t.co/i00RnBdKt5,Optimal dictionary for least squares representation. (arXiv:1603.02074v2 [cs.LG] UPDATED) https://t.co/i00RnBdKt5,0,3," Abstract: Dictionaries are collections of vectors used for representations of random vectors in Euclidean spaces. Recent research on optimal dictionaries is focused on constructing dictionaries that offer sparse representations, i.e., $\ell_0$-optimal representations. Here we consider the problem of finding optimal dictionaries with which representations of samples of a random vector are optimal in an $\ell_2$-sense: optimality of representation is defined as attaining the minimal average $\ell_2$-norm of the coefficients used to represent the random vector. With the help of recent results on rank-$1$ decompositions of symmetric positive semidefinite matrices, we provide an explicit description of $\ell_2$-optimal dictionaries as well as their algorithmic constructions in polynomial time. "
788900875313876992,2016-10-20 00:33:20,https://t.co/xfXKPbMbda,From Behavior to Sparse Graphical Games: Efficient Recovery of Equilibria. (arXiv:1607.02959v2 [cs.GT] UPDATED) https://t.co/xfXKPbMbda,0,2," Abstract: In this paper we study the problem of exact recovery of the pure-strategy Nash equilibria (PSNE) set of a graphical game from noisy observations of joint actions of the players alone. We consider sparse linear influence games --- a parametric class of graphical games with linear payoffs, and represented by directed graphs of n nodes (players) and in-degree of at most k. We present an $\ell_1$-regularized logistic regression based algorithm for recovering the PSNE set exactly, that is both computationally efficient --- i.e. runs in polynomial time --- and statistically efficient --- i.e. has logarithmic sample complexity. Specifically, we show that the sufficient number of samples required for exact PSNE recovery scales as $\mathcal{O}(\mathrm{poly}(k) \log n)$. We also validate our theoretical results using synthetic experiments. "
788900872591933440,2016-10-20 00:33:19,https://t.co/AOxUnkK4Zm,Sequence Graph Transform (SGT): A Feature Extraction Function for Sequence Data Mining. (arXiv:1608.03533v5 [stat.… https://t.co/AOxUnkK4Zm,0,4," Abstract: The ubiquitous presence of sequence data across fields such as the web, healthcare, bioinformatics, and text mining has made sequence mining a vital research area. However, sequence mining is particularly challenging because of difficulty in finding (dis)similarity/distance between sequences. This is because a distance measure between sequences is not obvious due to their unstructuredness---arbitrary strings of arbitrary length. Feature representations, such as n-grams, are often used but they either compromise on extracting both short- and long-term sequence patterns or have a high computation. We propose a new function, Sequence Graph Transform (SGT), that extracts the short- and long-term sequence features and embeds them in a finite-dimensional feature space. Importantly, SGT has low computation and can extract any amount of short- to long-term patterns without any increase in the computation, also proved theoretically in this paper. Due to this, SGT yields superior result with significantly higher accuracy and lower computation compared to the existing methods. We show it via several experimentation and SGT's real world application for clustering, classification, search and visualization as examples. "
788900870398283776,2016-10-20 00:33:19,https://t.co/XpT8weP5KD,Human Pose Estimation in Space and Time using 3D CNN. (arXiv:1609.00036v3 [cs.CV] UPDATED) https://t.co/XpT8weP5KD,0,9," Abstract: This paper explores the capabilities of convolutional neural networks to deal with a task that is easily manageable for humans: perceiving 3D pose of a human body from varying angles. However, in our approach, we are restricted to using a monocular vision system. For this purpose, we apply a convolutional neural network approach on RGB videos and extend it to three dimensional convolutions. This is done via encoding the time dimension in videos as the 3\ts{rd} dimension in convolutional space, and directly regressing to human body joint positions in 3D coordinate space. This research shows the ability of such a network to achieve state-of-the-art performance on the selected Human3.6M dataset, thus demonstrating the possibility of successfully representing temporal data with an additional dimension in the convolutional operation. "
788539802740228096,2016-10-19 00:38:33,https://t.co/95FsU7o8sL,How Well Do Local Algorithms Solve Semidefinite Programs?. (arXiv:1610.05350v1 [cs.DM]) https://t.co/95FsU7o8sL,1,4," Abstract: Several probabilistic models from high-dimensional statistics and machine learning reveal an intriguing --and yet poorly understood-- dichotomy. Either simple local algorithms succeed in estimating the object of interest, or even sophisticated semi-definite programming (SDP) relaxations fail. In order to explore this phenomenon, we study a classical SDP relaxation of the minimum graph bisection problem, when applied to Erd\H{o}s-Renyi random graphs with bounded average degree $d>1$, and obtain several types of results. First, we use a dual witness construction (using the so-called non-backtracking matrix of the graph) to upper bound the SDP value. Second, we prove that a simple local algorithm approximately solves the SDP to within a factor $2d^2/(2d^2+d-1)$ of the upper bound. In particular, the local algorithm is at most $8/9$ suboptimal, and $1+O(1/d)$ suboptimal for large degree. We then analyze a more sophisticated local algorithm, which aggregates information according to the harmonic measure on the limiting Galton-Watson (GW) tree. The resulting lower bound is expressed in terms of the conductance of the GW tree and matches surprisingly well the empirically determined SDP values on large-scale Erd\H{o}s-Renyi graphs. We finally consider the planted partition model. In this case, purely local algorithms are known to fail, but they do succeed if a small amount of side information is available. Our results imply quantitative bounds on the threshold for partial recovery using SDP in this model. "
788539800139825152,2016-10-19 00:38:33,https://t.co/5nycoI1rp3,AutoGP: Exploring the Capabilities and Limitations of Gaussian Process Models. (arXiv:1610.05392v1 [stat.ML]) https://t.co/5nycoI1rp3,1,10," Abstract: We investigate the capabilities and limitations of Gaussian process (GP) models by jointly exploring three complementary directions: (i) scalable and statistically efficient inference; (ii) flexible kernels; and (iii) objective functions for hyperparameter learning alternative to the marginal likelihood. Our approach outperforms all previously reported GP methods on the standard MNIST dataset; achieves state-of-the-art performance in a task particularly hard for kernel-based methods using the RECTANGLES-IMAGE dataset; and breaks the 1% error-rate barrier in GP models using the MNIST8M dataset, showing along the way the scalability of our method at unprecedented scale for GP models (8 million observations) in classification problems. Overall, our approach represents a significant breakthrough in kernel methods and GP models, bridging the gap between deep learning approaches and kernel machines. "
788539797321216001,2016-10-19 00:38:32,https://t.co/NJsB1nq1QR,Going off the Grid: Iterative Model Selection for Biclustered Matrix Completion. (arXiv:1610.05400v1 [stat… https://t.co/NJsB1nq1QR,0,4," Abstract: We consider the problem of performing matrix completion with side information on row-by-row and column-by-column similarities. We build upon recent proposals for matrix estimation with smoothness constraints with respect to row and column graphs. We present a novel iterative procedure for directly minimizing an information criterion in order to select an appropriate amount row and column smoothing, namely perform model selection. We also discuss how to exploit the special structure of the problem to scale up the estimation and model selection procedure via the Hutchinson estimator. We present simulation results and an application to predicting associations in imaging-genomics studies. "
788539793072386052,2016-10-19 00:38:31,https://t.co/fjuyE4x9Ix,Generalization error minimization: a new approach to model evaluation and selection with an application to penaliz… https://t.co/fjuyE4x9Ix,0,8," Abstract: We study model evaluation and model selection from the perspective of generalization ability (GA): the ability of a model to predict outcomes in new samples from the same population. We believe that GA is one way formally to address concerns about the external validity of a model. The GA of a model estimated on a sample can be measured by its empirical out-of-sample errors, called the generalization errors (GE). We derive upper bounds for the GE, which depend on sample sizes, model complexity and the distribution of the loss function. The upper bounds can be used to evaluate the GA of a model, ex ante. We propose using generalization error minimization (GEM) as a framework for model selection. Using GEM, we are able to unify a big class of penalized regression estimators, including lasso, ridge and bridge, under the same set of assumptions. We establish finite-sample and asymptotic properties (including $\mathcal{L}_2$-consistency) of the GEM estimator for both the $n \geqslant p$ and the $n < p$ cases. We also derive the $\mathcal{L}_2$-distance between the penalized and corresponding unpenalized regression estimates. In practice, GEM can be implemented by validation or cross-validation. We show that the GE bounds can be used for selecting the optimal number of folds in $K$-fold cross-validation. We propose a variant of $R^2$, the $GR^2$, as a measure of GA, which considers both both in-sample and out-of-sample goodness of fit. Simulations are used to demonstrate our key results. "
788539790123761665,2016-10-19 00:38:30,https://t.co/0OTbfBj19m,Analysis and Implementation of an Asynchronous Optimization Algorithm for the Parameter Server. (arXiv:1610.05507v… https://t.co/0OTbfBj19m,0,7," Abstract: This paper presents an asynchronous incremental aggregated gradient algorithm and its implementation in a parameter server framework for solving regularized optimization problems. The algorithm can handle both general convex (possibly non-smooth) regularizers and general convex constraints. When the empirical data loss is strongly convex, we establish linear convergence rate, give explicit expressions for step-size choices that guarantee convergence to the optimum, and bound the associated convergence factors. The expressions have an explicit dependence on the degree of asynchrony and recover classical results under synchronous operation. Simulations and implementations on commercial compute clouds validate our findings. "
788539786902593536,2016-10-19 00:38:30,https://t.co/iHrhXOwsfS,Dynamic Assortment Personalization in High Dimensions. (arXiv:1610.05604v1 [stat.ML]) https://t.co/iHrhXOwsfS,0,2," Abstract: We demonstrate the importance of structural priors for effective, efficient large-scale dynamic assortment personalization. Assortment personalization is the problem of choosing, for each individual or consumer segment (type), a best assortment of products, ads, or other offerings (items) so as to maximize revenue. This problem is central to revenue management in e-commerce, online advertising, and multi-location brick-and-mortar retail, where both items and types can number in the thousands-to-millions. Data efficiency is paramount in this large-scale setting. A good personalization strategy must dynamically balance the need to learn consumer preferences and to maximize revenue. We formulate the dynamic assortment personalization problem as a discrete-contextual bandit with $m$ contexts (customer types) and many arms (assortments of the $n$ items). We assume that each type's preferences follow a simple parametric model with $n$ parameters. In all, there are $mn$ parameters, and existing literature suggests that order optimal regret scales as $mn$. However, this figure is orders of magnitude larger than the data available in large-scale applications, and imposes unacceptably high regret. In this paper, we impose natural structure on the problem -- a small latent dimension, or low rank. In the static setting, we show that this model can be efficiently learned from surprisingly few interactions, using a time- and memory-efficient optimization algorithm that converges globally whenever the model is learnable. In the dynamic setting, we show that structure-aware dynamic assortment personalization can have regret that is an order of magnitude smaller than structure-ignorant approaches. We validate our theoretical results empirically. "
788539784717303808,2016-10-19 00:38:29,https://t.co/do80zdWAOB,Markov Chain Truncation for Doubly-Intractable Inference. (arXiv:1610.05672v1 [stat.ML]) https://t.co/do80zdWAOB,0,3," Abstract: Computing partition functions, the normalizing constants of probability distributions, is often hard. Variants of importance sampling give unbiased estimates of a normalizer Z, however, unbiased estimates of the reciprocal 1/Z are harder to obtain. Unbiased estimates of 1/Z allow Markov chain Monte Carlo sampling of ""doubly-intractable"" distributions, such as the parameter posterior for Markov Random Fields or Exponential Random Graphs. We demonstrate how to construct unbiased estimates for 1/Z given access to black-box importance sampling estimators for Z. We adapt recent work on random series truncation and Markov chain coupling, producing estimators with lower variance and a higher percentage of positive estimates than before. Our debiasing algorithms are simple to implement, and have some theoretical and empirical advantages over existing methods. "
788539782590857216,2016-10-19 00:38:29,https://t.co/XQE4q3uvgi,Rejection Sampling Variational Inference. (arXiv:1610.05683v1 [stat.ML]) https://t.co/XQE4q3uvgi,1,14," Abstract: Variational inference using the reparameterization trick has enabled large-scale approximate Bayesian inference in complex probabilistic models, leveraging stochastic optimization to sidestep intractable expectations. The reparameterization trick is applicable when we can simulate a random variable by applying a (differentiable) deterministic function on an auxiliary random variable whose distribution is fixed. For many distributions of interest (such as the gamma or Dirichlet), simulation of random variables relies on rejection sampling. The discontinuity introduced by the accept--reject step means that standard reparameterization tricks are not applicable. We propose a new method that lets us leverage reparameterization gradients even when variables are outputs of a rejection sampling algorithm. Our approach enables reparameterization on a larger class of variational distributions. In several studies of real and synthetic data, we show that the variance of the estimator of the gradient is significantly lower than other state-of-the-art methods. This leads to faster convergence of stochastic optimization variational inference. "
788539780351004674,2016-10-19 00:38:28,https://t.co/DQFWXAxUVu,Deep Amortized Inference for Probabilistic Programs. (arXiv:1610.05735v1 [cs.AI]) https://t.co/DQFWXAxUVu,0,10," Abstract: Probabilistic programming languages (PPLs) are a powerful modeling tool, able to represent any computable probability distribution. Unfortunately, probabilistic program inference is often intractable, and existing PPLs mostly rely on expensive, approximate sampling-based methods. To alleviate this problem, one could try to learn from past inferences, so that future inferences run faster. This strategy is known as amortized inference; it has recently been applied to Bayesian networks and deep generative models. This paper proposes a system for amortized inference in PPLs. In our system, amortization comes in the form of a parameterized guide program. Guide programs have similar structure to the original program, but can have richer data flow, including neural network components. These networks can be optimized so that the guide approximately samples from the posterior distribution defined by the original program. We present a flexible interface for defining guide programs and a stochastic gradient-based scheme for optimizing guide parameters, as well as some preliminary results on automatically deriving guide programs. We explore in detail the common machine learning pattern in which a 'local' model is specified by 'global' random values and used to generate independent observed data points; this gives rise to amortized local inference supporting global model learning. "
788539776941035520,2016-10-19 00:38:27,https://t.co/5GyoQ10SLA,Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data. (arXiv:1610.05755v1 [stat.ML]) https://t.co/5GyoQ10SLA,4,15," Abstract: Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data. The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as ""teachers"" for a ""student"" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning. "
788539774944550912,2016-10-19 00:38:27,https://t.co/oj5ClRb7st,Modeling community structure and topics in dynamic text networks. (arXiv:1610.05756v1 [cs.SI]) https://t.co/oj5ClRb7st,0,3," Abstract: The last decade has seen great progress in both dynamic network modeling and topic modeling. This paper draws upon both areas to create a Bayesian method that allows topic discovery to inform the latent network model and the network structure to facilitate topic identification. We apply this method to the 467 top political blogs of 2012. Our results find complex community structure within this set of blogs, where community membership depends strongly upon the set of topics in which the blogger is interested. "
788539772117590016,2016-10-19 00:38:26,https://t.co/Kd4lz4gycw,Fast Sampling for Bayesian Max-Margin Models. (arXiv:1504.07107v5 [stat.ML] UPDATED) https://t.co/Kd4lz4gycw,0,4," Abstract: Bayesian max-margin models have shown superiority in various practical applications, such as text categorization, collaborative prediction, social network link prediction and crowdsourcing, and they conjoin the flexibility of Bayesian modeling and predictive strengths of max-margin learning. However, Monte Carlo sampling for these models still remains challenging, especially for applications that involve large-scale datasets. In this paper, we present the stochastic subgradient Hamiltonian Monte Carlo (HMC) methods, which are easy to implement and computationally efficient. We show the approximate detailed balance property of subgradient HMC which reveals a natural and validated generalization of the ordinary HMC. Furthermore, we investigate the variants that use stochastic subsampling and thermostats for better scalability and mixing. Using stochastic subgradient Markov Chain Monte Carlo (MCMC), we efficiently solve the posterior inference task of various Bayesian max-margin models and extensive experimental results demonstrate the effectiveness of our approach. "
788539768883806209,2016-10-19 00:38:25,https://t.co/OPKg8KRcuS,Sparse Convex Clustering. (arXiv:1601.04586v3 [stat.ME] UPDATED) https://t.co/OPKg8KRcuS,2,9," Abstract: Convex clustering, a convex relaxation of k-means clustering and hierarchical clustering, has drawn recent attentions since it nicely addresses the instability issue of traditional nonconvex clustering methods. Although its computational and statistical properties have been recently studied, the performance of convex clustering has not yet been investigated in the high-dimensional clustering scenario, where the data contains a large number of features and many of them carry no information about the clustering structure. In this paper, we demonstrate that the performance of convex clustering could be distorted when the uninformative features are included in the clustering. To overcome it, we introduce a new clustering method, referred to as Sparse Convex Clustering, to simultaneously cluster observations and conduct feature selection. The key idea is to formulate convex clustering in a form of regularization, with an adaptive group-lasso penalty term on cluster centers. In order to optimally balance the tradeoff between the cluster fitting and sparsity, a tuning criterion based on clustering stability is developed. In theory, we provide an unbiased estimator for the degrees of freedom of the proposed sparse convex clustering method. Finally, the effectiveness of the sparse convex clustering is examined through a variety of numerical experiments and a real data application. "
788539765670961152,2016-10-19 00:38:25,https://t.co/8LviylK8kH,Further properties of the forward-backward envelope with applications to difference-of-convex programming. (arXiv:… https://t.co/8LviylK8kH,0,3," Abstract: In this paper, we further study the forward-backward envelope first introduced in [28] and [30] for problems whose objective is the sum of a proper closed convex function and a twice continuously differentiable possibly nonconvex function with Lipschitz continuous gradient. We derive sufficient conditions on the original problem for the corresponding forward-backward envelope to be a level-bounded and Kurdyka-{\L}ojasiewicz function with an exponent of $\frac12$; these results are important for the efficient minimization of the forward-backward envelope by classical optimization algorithms. In addition, we demonstrate how to minimize some difference-of-convex regularized least squares problems by minimizing a suitably constructed forward-backward envelope. Our preliminary numerical results on randomly generated instances of large-scale $\ell_{1-2}$ regularized least squares problems [37] illustrate that an implementation of this approach with a limited-memory BFGS scheme usually outperforms standard first-order methods such as the nonmonotone proximal gradient method in [35]. "
788539763712225282,2016-10-19 00:38:24,https://t.co/NnnVP5l8Ig,A constrained L1 minimization approach for estimating multiple Sparse Gaussian or Nonparanormal Graphical Models. … https://t.co/NnnVP5l8Ig,1,3," Abstract: Identifying context-specific entity networks from aggregated data is an important task, arising often in bioinformatics and neuroimaging. Computationally, this task can be formulated as jointly estimating multiple different, but related, sparse Undirected Graphical Models (UGM) from aggregated samples across several contexts. Previous joint-UGM studies have mostly focused on sparse Gaussian Graphical Models (sGGMs) and can't identify context-specific edge patterns directly. We, therefore, propose a novel approach, SIMULE (detecting Shared and Individual parts of MULtiple graphs Explicitly) to learn multi-UGM via a constrained L1 minimization. SIMULE automatically infers both specific edge patterns that are unique to each context and shared interactions preserved among all the contexts. Through the L1 constrained formulation, this problem is cast as multiple independent subtasks of linear programming that can be solved efficiently in parallel. In addition to Gaussian data, SIMULE can also handle multivariate Nonparanormal data that greatly relaxes the normality assumption that many real-world applications do not follow. We provide a novel theoretical proof showing that SIMULE achieves a consistent result at the rate O(log(Kp)/n_{tot}). On multiple synthetic datasets and two biomedical datasets, SIMULE shows significant improvement over state-of-the-art multi-sGGM and single-UGM baselines. "
788539761355096065,2016-10-19 00:38:24,https://t.co/ox1nyRXTY0,On Robustness of Kernel Clustering. (arXiv:1606.01869v2 [stat.ML] UPDATED) https://t.co/ox1nyRXTY0,0,5," Abstract: Clustering is one of the most important unsupervised problems in machine learning and statistics. Among many existing algorithms, kernel \km has drawn much research attention due to its ability to find non-linear cluster boundaries and its inherent simplicity. There are two main approaches for kernel k-means: SVD of the kernel matrix and convex relaxations. Despite the attention kernel clustering has received both from theoretical and applied quarters, not much is known about robustness of the methods. In this paper we first introduce a semidefinite programming relaxation for the kernel clustering problem, then prove that under a suitable model specification, both the K-SVD and SDP approaches are consistent in the limit, albeit SDP is strongly consistent, i.e. achieves exact recovery, whereas K-SVD is weakly consistent, i.e. the fraction of misclassified nodes vanish. "
788539758871994368,2016-10-19 00:38:23,https://t.co/ae6HEebjhv,Learning Structured Sparsity in Deep Neural Networks. (arXiv:1608.03665v4 [cs.NE] UPDATED) https://t.co/ae6HEebjhv,3,11," Abstract: High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation. Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by around ~1%. Open source code is in this https URL "
788539756741292033,2016-10-19 00:38:23,https://t.co/PoOalbxTLB,Neural networks for the prediction organic chemistry reactions. (arXiv:1608.06296v2 [physics.chem-ph] UPDATED) https://t.co/PoOalbxTLB,2,8," Abstract: Reaction prediction remains one of the major challenges for organic chemistry, and is a pre-requisite for efficient synthetic planning. It is desirable to develop algorithms that, like humans, ""learn"" from being exposed to examples of the application of the rules of organic chemistry. We explore the use of neural networks for predicting reaction types, using a new reaction fingerprinting method. We combine this predictor with SMARTS transformations to build a system which, given a set of reagents and re- actants, predicts the likely products. We test this method on problems from a popular organic chemistry textbook. "
788539752966414337,2016-10-19 00:38:22,https://t.co/0flQG10Rvc,Latent Sequence Decompositions. (arXiv:1610.03035v2 [stat.ML] UPDATED) https://t.co/0flQG10Rvc,1,9," Abstract: We present the Latent Sequence Decompositions (LSD) framework. LSD decomposes sequences with variable lengthed output units as a function of both the input sequence and the output sequence. We present a training algorithm which samples valid extensions and an approximate decoding algorithm. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9% WER compared to a character baseline of 14.8% WER. When combined with a convolutional network on the encoder, we achieve 9.2% WER. "
788177446994644992,2016-10-18 00:38:41,https://t.co/yixofDr5Hg,"Simultaneous Learning of Trees and Representations for Extreme Classification, with Application to Language Modeli… https://t.co/yixofDr5Hg",1,5," Abstract: This paper addresses the problem of multi-class classification with an extremely large number of classes, where the class predictor is learned jointly with the data representation, as is the case in language modeling problems. The predictor admits a hierarchical structure, which allows for efficient handling of settings that deal with a very large number of labels. The predictive power of the model however can heavily depend on the structure of the tree. We address this problem with an algorithm for tree construction and training that is based on a new objective function which favors balanced and easily-separable node partitions. We describe theoretical properties of this objective function and show that it gives rise to a boosting algorithm for which we provide a bound on classification error, i.e. we show that if the objective is weakly optimized in the internal nodes of the tree, then our algorithm will amplify this weak advantage to build a tree achieving any desired level of accuracy. We apply the algorithm to the task of language modeling by re-framing conditional density estimation as a variant of the hierarchical classification problem. We empirically demonstrate on text data that the proposed approach leads to high-quality trees in terms of perplexity and computational running time compared to its non-hierarchical counterpart. "
788177444654157824,2016-10-18 00:38:41,https://t.co/V1Ad9c15fY,Unsupervised clustering under the Union of Polyhedral Cones (UOPC) model. (arXiv:1610.04751v1 [stat.ML]) https://t.co/V1Ad9c15fY,0,2," Abstract: In this paper, we consider clustering data that is assumed to come from one of finitely many pointed convex polyhedral cones. This model is referred to as the Union of Polyhedral Cones (UOPC) model. Similar to the Union of Subspaces (UOS) model where each data from each subspace is generated from a (unknown) basis, in the UOPC model each data from each cone is assumed to be generated from a finite number of (unknown) \emph{extreme rays}.To cluster data under this model, we consider several algorithms - (a) Sparse Subspace Clustering by Non-negative constraints Lasso (NCL), (b) Least squares approximation (LSA), and (c) K-nearest neighbor (KNN) algorithm to arrive at affinity between data points. Spectral Clustering (SC) is then applied on the resulting affinity matrix to cluster data into different polyhedral cones. We show that on an average KNN outperforms both NCL and LSA and for this algorithm we provide the deterministic conditions for correct clustering. For an affinity measure between the cones it is shown that as long as the cones are not very coherent and as long as the density of data within each cone exceeds a threshold, KNN leads to accurate clustering. Finally, simulation results on real datasets (MNIST and YaleFace datasets) depict that the proposed algorithm works well on real data indicating the utility of the UOPC model and the proposed algorithm. "
788177442166960129,2016-10-18 00:38:40,https://t.co/vVWgdMbNuc,An Adaptive Test of Independence with Analytic Kernel Embeddings. (arXiv:1610.04782v1 [stat.ML]) https://t.co/vVWgdMbNuc,2,7," Abstract: A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratic-time HSIC test, and outperform competing O(n) and O(n log n) tests. "
788177440204021761,2016-10-18 00:38:40,https://t.co/PalgX7uWgD,Communication-efficient Distributed Sparse Linear Discriminant Analysis. (arXiv:1610.04798v1 [stat.ML]) https://t.co/PalgX7uWgD,0,2," Abstract: We propose a communication-efficient distributed estimation method for sparse linear discriminant analysis (LDA) in the high dimensional regime. Our method distributes the data of size $N$ into $m$ machines, and estimates a local sparse LDA estimator on each machine using the data subset of size $N/m$. After the distributed estimation, our method aggregates the debiased local estimators from $m$ machines, and sparsifies the aggregated estimator. We show that the aggregated estimator attains the same statistical rate as the centralized estimation method, as long as the number of machines $m$ is chosen appropriately. Moreover, we prove that our method can attain the model selection consistency under a milder condition than the centralized method. Experiments on both synthetic and real datasets corroborate our theory. "
788177437637144576,2016-10-18 00:38:39,https://t.co/BvajV0cQPC,Dynamic Stacked Generalization for Node Classification on Networks. (arXiv:1610.04804v1 [stat.ML]) https://t.co/BvajV0cQPC,0,1," Abstract: We propose a novel stacked generalization (stacking) method as a dynamic ensemble technique using a pool of heterogeneous classifiers for node label classification on networks. The proposed method assigns component models a set of functional coefficients, which can vary smoothly with certain topological features of a node. Compared to the traditional stacking model, the proposed method can dynamically adjust the weights of individual models as we move across the graph and provide a more versatile and significantly more accurate stacking model for label prediction on a network. We demonstrate the benefits of the proposed model using both a simulation study and real data analysis. "
788177435305078785,2016-10-18 00:38:38,https://t.co/gfZooZ7zKR,Estimation of low rank density matrices by Pauli measurements. (arXiv:1610.04811v1 [stat.ML]) https://t.co/gfZooZ7zKR,0,2," Abstract: Density matrices are positively semi-definite Hermitian matrices with unit trace that describe the states of quantum systems. Many quantum systems of physical interest can be represented as high-dimensional low rank density matrices. A popular problem in {\it quantum state tomography} (QST) is to estimate the unknown low rank density matrix of a quantum system by conducting Pauli measurements. Our main contribution is twofold. First, we establish the minimax lower bounds in Schatten $p$-norms with $1\leq p\leq +\infty$ for low rank density matrices estimation by Pauli measurements. In our previous paper, these minimax lower bounds are proved under the trace regression model with Gaussian noise and the noise is assumed to have common variance. In this paper, we prove these bounds under the Binomial observation model which meets the actual model in QST. Second, we study the Dantzig estimator (DE) for estimating the unknown low rank density matrix under the Binomial observation model by using Pauli measurements. In our previous papers, we studied the least squares estimator and the projection estimator, where we proved the optimal convergence rates for the least squares estimator in Schatten $p$-norms with $1\leq p\leq 2$ and, under a stronger condition, the optimal convergence rates for the projection estimator in Schatten $p$-norms with $1\leq p\leq +\infty$. In this paper, we show that the results of these two distinct estimators can be simultaneously obtained by the Dantzig estimator. Moreover, better convergence rates in Schatten norm distances can be proved for Dantzig estimator under conditions weaker than those needed in previous papers. When the objective function of DE is replaced by the negative von Neumann entropy, we obtain sharp convergence rate in Kullback-Leibler divergence. "
788177433606356992,2016-10-18 00:38:38,https://t.co/RFvwryn8WG,Probabilistic Dimensionality Reduction via Structure Learning. (arXiv:1610.04929v1 [stat.ML]) https://t.co/RFvwryn8WG,1,9," Abstract: We propose a novel probabilistic dimensionality reduction framework that can naturally integrate the generative model and the locality information of data. Based on this framework, we present a new model, which is able to learn a smooth skeleton of embedding points in a low-dimensional space from high-dimensional noisy data. The formulation of the new model can be equivalently interpreted as two coupled learning problem, i.e., structure learning and the learning of projection matrix. This interpretation motivates the learning of the embedding points that can directly form an explicit graph structure. We develop a new method to learn the embedding points that form a spanning tree, which is further extended to obtain a discriminative and compact feature representation for clustering problems. Unlike traditional clustering methods, we assume that centers of clusters should be close to each other if they are connected in a learned graph, and other cluster centers should be distant. This can greatly facilitate data visualization and scientific discovery in downstream analysis. Extensive experiments are performed that demonstrate that the proposed framework is able to obtain discriminative feature representations, and correctly recover the intrinsic structures of various real-world datasets. "
788177431639228416,2016-10-18 00:38:37,https://t.co/hAVOMJjX0w,Efficient Metric Learning for the Analysis of Motion Data. (arXiv:1610.05083v1 [cs.LG]) https://t.co/hAVOMJjX0w,0,3," Abstract: We investigate metric learning in the context of dynamic time warping (DTW), the by far most popular dissimilarity measure used for the comparison and analysis of motion capture data. While metric learning enables a problem-adapted representation of data, the majority of meth- ods has been proposed for vectorial data only. In this contribution, we extend the popular principle offered by the large margin nearest neighbours learner (LMNN) to DTW by treating the resulting component-wise dissimilarity values as features. We demonstrate, that this principle greatly enhances the classification accuracy in several benchmarks. Further, we show that recent auxiliary concepts such as metric regularisation can be transferred from the vectorial case to component-wise DTW in a similar way. We illustrate, that metric regularisation constitutes a crucial prerequisite for the interpretation of the resulting relevance profiles. "
788177429571465216,2016-10-18 00:38:37,https://t.co/Qbn2DDLbFz,The xyz algorithm for fast interaction search in high-dimensional data. (arXiv:1610.05108v1 [stat.ML]) https://t.co/Qbn2DDLbFz,0,3," Abstract: When performing regression on a dataset with $p$ variables, it is often of interest to go beyond using main linear effects and include interactions as products between individual variables. For small-scale problems, these interactions can be computed explicitly but this leads to a computational complexity of at least $\mathcal{O}(p^2)$ if done naively. This cost can be prohibitive if $p$ is very large. We introduce a new randomised algorithm that is able to discover interactions with high probability and under mild conditions has a runtime that is subquadratic in $p$. We show that strong interactions can be discovered in almost linear time, whilst finding weaker interactions requires $\mathcal{O}(p^\alpha)$ operations for $1 < \alpha < 2$ depending on their strength. The underlying idea is to transform interaction search into a closestpair problem which can be solved efficiently in subquadratic time. The algorithm is called $\mathit{xyz}$ and is implemented in the language R. We demonstrate its efficiency for application to genome-wide association studies, where more than $10^{11}$ interactions can be screened in under $280$ seconds with a single-core $1.2$ GHz CPU. "
788177426195046400,2016-10-18 00:38:36,https://t.co/nM680dQU8E,Risk-Aware Algorithms for Adversarial Contextual Bandits. (arXiv:1610.05129v1 [cs.LG]) https://t.co/nM680dQU8E,0,3," Abstract: In this work we consider adversarial contextual bandits with risk constraints. At each round, nature prepares a context, a cost for each arm, and additionally a risk for each arm. The learner leverages the context to pull an arm and then receives the corresponding cost and risk associated with the pulled arm. In addition to minimizing the cumulative cost, the learner also needs to satisfy long-term risk constraints -- the average of the cumulative risk from all pulled arms should not be larger than a pre-defined threshold. To address this problem, we first study the full information setting where in each round the learner receives an adversarial convex loss and a convex constraint. We develop a meta algorithm leveraging online mirror descent for the full information setting and extend it to contextual bandit with risk constraints setting using expert advice. Our algorithms can achieve near-optimal regret in terms of minimizing the total cost, while successfully maintaining a sublinear growth of cumulative risk constraint violation. "
788177422772539393,2016-10-18 00:38:35,https://t.co/GYkfbpw6xi,The Peaking Phenomenon in Semi-supervised Learning. (arXiv:1610.05160v1 [stat.ML]) https://t.co/GYkfbpw6xi,0,6," Abstract: For the supervised least squares classifier, when the number of training objects is smaller than the dimensionality of the data, adding more data to the training set may first increase the error rate before decreasing it. This, possibly counterintuitive, phenomenon is known as peaking. In this work, we observe that a similar but more pronounced version of this phenomenon also occurs in the semi-supervised setting, where instead of labeled objects, unlabeled objects are added to the training set. We explain why the learning curve has a more steep incline and a more gradual decline in this setting through simulation studies and by applying an approximation of the learning curve based on the work by Raudys & Duin. "
788177420201365504,2016-10-18 00:38:35,https://t.co/ZgwI0iecTA,Spatio-temporal Gaussian processes modeling of dynamical systems in systems biology. (arXiv:1610.05163v1 [stat.ML]) https://t.co/ZgwI0iecTA,1,9," Abstract: Quantitative modeling of post-transcriptional regulation process is a challenging problem in systems biology. A mechanical model of the regulatory process needs to be able to describe the available spatio-temporal protein concentration and mRNA expression data and recover the continuous spatio-temporal fields. Rigorous methods are required to identify model parameters. A promising approach to deal with these difficulties is proposed using Gaussian process as a prior distribution over the latent function of protein concentration and mRNA expression. In this study, we consider a partial differential equation mechanical model with differential operators and latent function. Since the operators at stake are linear, the information from the physical model can be encoded into the kernel function. Hybrid Monte Carlo methods are employed to carry out Bayesian inference of the partial differential equation parameters and Gaussian process kernel parameters. The spatio-temporal field of protein concentration and mRNA expression are reconstructed without explicitly solving the partial differential equation. "
788177418007773184,2016-10-18 00:38:34,https://t.co/gq6U6tXKR4,Decentralized Collaborative Learning of Personalized Models over Networks. (arXiv:1610.05202v1 [cs.LG]) https://t.co/gq6U6tXKR4,0,3," Abstract: We consider a set of learning agents in a collaborative peer-to-peer network, where each agent learns a personalized model according to its own learning objective. The question addressed in this paper is: how can agents improve upon their locally trained model by communicating with other agents that have similar objectives? We introduce and analyze two asynchronous gossip algorithms running in a fully decentralized manner. Our first approach, inspired from label propagation, aims to smooth pre-trained local models over the network while accounting for the confidence that each agent has in its initial model. In our second approach, agents jointly learn and propagate their model by making iterative updates based on both their local dataset and the behavior of their neighbors. Our algorithm to optimize this challenging objective in a decentralized way is based on ADMM. "
788177415545782273,2016-10-18 00:38:34,https://t.co/9j7fzv9E2y,A polynomial-time relaxation of the Gromov-Hausdorff distance. (arXiv:1610.05214v1 [math.GT]) https://t.co/9j7fzv9E2y,0,2," Abstract: The Gromov-Hausdorff distance provides a metric on the set of isometry classes of compact metric spaces. Unfortunately, computing this metric directly is believed to be computationally intractable. Motivated by applications in shape matching and point-cloud comparison, we study a semidefinite programming relaxation of the Gromov-Hausdorff metric. This relaxation can be computed in polynomial time, and somewhat surprisingly is itself a pseudometric. We describe the induced topology on the set of compact metric spaces. Finally, we demonstrate the numerical performance of various algorithms for computing the relaxed distance and apply these algorithms to several relevant data sets. In particular we propose a greedy algorithm for finding the best correspondence between finite metric spaces that can handle hundreds of points. "
788177412307558400,2016-10-18 00:38:33,https://t.co/DZ8hterNzT,BET on Independence. (arXiv:1610.05246v1 [math.ST]) https://t.co/DZ8hterNzT,0,2," Abstract: We study the problem of model-free dependence detection. This problem can be difficult even when the marginal distributions are known. We explain this difficulty by showing the impossibility to uniformly consistently distinguish degeneracy from independence with any single test. To make model-free dependence detection a tractable problem, we introduce the concept of binary expansion statistics (BEStat) and propose the binary expansion testing (BET) framework. Through simple mathematics, we convert the dependence detection problem to a multiple testing problem. Besides being model-free, the BET also enjoys many other advantages which include (1) invariance to monotone marginal transformations, (2) clear interpretability of local relationships upon rejection, and (3) close connections to computing for efficient algorithms. We illustrate the BET by studying the distribution of the brightest stars in the night sky. "
788177409770127360,2016-10-18 00:38:32,https://t.co/rEA8nYGsPv,Black-box Importance Sampling. (arXiv:1610.05247v1 [stat.ML]) https://t.co/rEA8nYGsPv,1,4," Abstract: Importance sampling is widely used in machine learning and statistics, but its power is limited by the restriction of using simple proposals for which the importance weights can be tractably calculated. We address this problem by studying black-box importance sampling methods that calculate importance weights for samples generated from any unknown proposal or black-box mechanism. Our method allows us to use better and richer proposals to solve difficult problems, and (somewhat counter-intuitively) also has the additional benefit of improving the estimation accuracy beyond typical importance sampling. Both theoretical and empirical analyses are provided. "
788177407345848321,2016-10-18 00:38:32,https://t.co/YlbPELKsnF,A probabilistic model for the numerical solution of initial value problems. (arXiv:1610.05261v1 [math.NA]) https://t.co/YlbPELKsnF,0,2," Abstract: Like many numerical methods, solvers for initial value problems (IVPs) on ordinary differential equations estimate an analytically intractable quantity, using the results of tractable computations as inputs. This structure is closely connected to the notion of inference on latent variables in statistics. We describe a class of algorithms that formulate the solution to an IVP as inference on a latent path that is a draw from a Gaussian process probability measure (or equivalently, the solution of a linear stochastic differential equation). We then show that certain members of this class are identified exactly with existing generalized linear methods for ODEs, in particular a number of Runge--Kutta methods and Nordsieck methods. This probabilistic formulation of classic methods is valuable in two ways: analytically, it highlights implicit prior assumptions favoring certain approximate solutions to the IVP over others, and gives a precise meaning to the old observation that these methods act like filters. Practically, it endows the classic solvers with `docking points' for notions of uncertainty and prior information about the initial value, the value of the ODE itself, and the solution of the problem. "
788177405114458113,2016-10-18 00:38:31,https://t.co/eYxKeGjlsO,A Unified Computational and Statistical Framework for Nonconvex Low-Rank Matrix Estimation. (arXiv:1610.05275v1 [s… https://t.co/eYxKeGjlsO,0,7," Abstract: We propose a unified framework for estimating low-rank matrices through nonconvex optimization based on gradient descent algorithm. Our framework is quite general and can be applied to both noisy and noiseless observations. In the general case with noisy observations, we show that our algorithm is guaranteed to linearly converge to the unknown low-rank matrix up to minimax optimal statistical error, provided an appropriate initial estimator. While in the generic noiseless setting, our algorithm converges to the unknown low-rank matrix at a linear rate and enables exact recovery with optimal sample complexity. In addition, we develop a new initialization algorithm to provide a desired initial estimator, which outperforms existing initialization algorithms for nonconvex low-rank matrix estimation. We illustrate the superiority of our framework through three examples: matrix regression, matrix completion, and one-bit matrix completion. We also corroborate our theory through extensive experiments on synthetic data. "
788177402904141828,2016-10-18 00:38:31,https://t.co/CpPtkLbXen,Improved Gibbs Sampling Parameter Estimators for Latent Dirichlet Allocation. (arXiv:1505.02065v3 [stat.ML] UPDATE… https://t.co/CpPtkLbXen,0,3," Abstract: Latent Dirichlet Allocation (LDA) is a generative probabilistic model for discovering the underlying structure of discrete data. LDA and its extensions have been successfully used for both unsupervised and supervised learning tasks across a variety of data types including textual, image, and biological data. After more than a decade of intensive research on training algorithms for LDA, the Collapsed Gibbs Sampler (CGS), in which the parameters are marginalized out, remains one of the most popular LDA inference algorithms. We introduce a novel approach for estimating LDA parameters from collapsed Gibbs samples, by leveraging the full conditional distributions over the latent variable assignments to efficiently average over multiple samples, for little more computational cost than drawing a single sample. Our approach can be understood as adapting the soft clustering methodology of Collapsed Variational Bayes (CVB0) to CGS parameter estimation, in order to get the best of both techniques. We perform extensive empirical comparisons of our estimators with those of standard collapsed inference algorithms on real-world data for both unsupervised LDA and Prior-LDA, a supervised variant of LDA for multi-label classification. Our results show a consistent advantage of our approach over traditional CGS under all experimental conditions, and over CVB0 inference in the majority of conditions. More broadly, our results highlight the importance of averaging over multiple samples in LDA parameter estimation, and the use of efficient computational techniques to do so. "
788177399380934657,2016-10-18 00:38:30,https://t.co/hF1j6Uih92,A Deep Bag-of-Features Model for Music Auto-Tagging. (arXiv:1508.04999v3 [cs.LG] UPDATED) https://t.co/hF1j6Uih92,0,2," Abstract: Feature learning and deep learning have drawn great attention in recent years as a way of transforming input data into more effective representations using learning algorithms. Such interest has grown in the area of music information retrieval (MIR) as well, particularly in music audio classification tasks such as auto-tagging. In this paper, we present a two-stage learning model to effectively predict multiple labels from music audio. The first stage learns to project local spectral patterns of an audio track onto a high-dimensional sparse space in an unsupervised manner and summarizes the audio track as a bag-of-features. The second stage successively performs the unsupervised learning on the bag-of-features in a layer-by-layer manner to initialize a deep neural network and finally fine-tunes it with the tag labels. Through the experiment, we rigorously examine training choices and tuning parameters, and show that the model achieves high performance on Magnatagatune, a popularly used dataset in music auto-tagging. "
788177396293890048,2016-10-18 00:38:29,https://t.co/HyBvRw0K98,Identifying collusion groups using spectral clustering. (arXiv:1509.06457v2 [q-fin.TR] UPDATED) https://t.co/HyBvRw0K98,0,2," Abstract: In an illiquid stock, traders can collude and place orders on a predetermined price and quantity at a fixed schedule. This is usually done to manipulate the price of the stock or to create artificial liquidity in the stock, which may mislead genuine investors. Here, the problem is to identify such group of colluding traders. We modeled the problem instance as a graph, where each trader corresponds to a vertex of the graph and trade corresponds to edges of the graph. Further, we assign weights on edges depending on total volume, total number of trades, maximum change in the price and commonality between two vertices. Spectral clustering algorithms are used on the constructed graph to identify colluding group(s). We have compared our results with simulated data to show the effectiveness of spectral clustering to detecting colluding groups. Moreover, we also have used parameters of real data to test the effectiveness of our algorithm. "
788177393357844480,2016-10-18 00:38:28,https://t.co/MaXgVn7Yo4,Towards Unifying Hamiltonian Monte Carlo and Slice Sampling. (arXiv:1602.07800v4 [stat.ML] UPDATED) https://t.co/MaXgVn7Yo4,0,8," Abstract: We unify slice sampling and Hamiltonian Monte Carlo (HMC) sampling, demonstrating their connection via the Hamiltonian-Jacobi equation from Hamiltonian mechanics. This insight enables extension of HMC and slice sampling to a broader family of samplers, called Monomial Gamma Samplers (MGS). We provide a theoretical analysis of the mixing performance of such samplers, proving that in the limit of a single parameter, the MGS draws decorrelated samples from the desired target distribution. We further show that as this parameter tends toward this limit, performance gains are achieved at a cost of increasing numerical difficulty and some practical convergence issues. Our theoretical results are validated with synthetic data and real-world applications. "
788177390165979136,2016-10-18 00:38:28,https://t.co/OLDjGNDU2V,Without-Replacement Sampling for Stochastic Gradient Methods: Convergence Results and Application to Distributed O… https://t.co/OLDjGNDU2V,0,6," Abstract: Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled \emph{with} replacement. In practice, however, sampling \emph{without} replacement is very common, easier to implement in many cases, and often performs better. In this paper, we provide competitive convergence guarantees for without-replacement sampling, under various scenarios, for three types of algorithms: Any algorithm with online regret guarantees, stochastic gradient descent, and SVRG. A useful application of our SVRG analysis is a nearly-optimal algorithm for regularized least squares in a distributed setting, in terms of both communication complexity and runtime complexity, when the data is randomly partitioned and the condition number can be as large as the data size per machine (up to logarithmic factors). Our proof techniques combine ideas from stochastic optimization, adversarial online learning, and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems. "
788177386907004929,2016-10-18 00:38:27,https://t.co/CsPBnybCxm,Online Nonnegative Matrix Factorization with Outliers. (arXiv:1604.02634v2 [stat.ML] UPDATED) https://t.co/CsPBnybCxm,0,4," Abstract: We propose a unified and systematic framework for performing online nonnegative matrix factorization in the presence of outliers. Our framework is particularly suited to large-scale data. We propose two solvers based on projected gradient descent and the alternating direction method of multipliers. We prove that the sequence of objective values converges almost surely by appealing to the quasi-martingale convergence theorem. We also show the sequence of learned dictionaries converges to the set of stationary points of the expected loss function almost surely. In addition, we extend our basic problem formulation to various settings with different constraints and regularizers. We also adapt the solvers and analyses to each setting. We perform extensive experiments on both synthetic and real datasets. These experiments demonstrate the computational efficiency and efficacy of our algorithms on tasks such as (parts-based) basis learning, image denoising, shadow removal and foreground-background separation. "
788177383828447232,2016-10-18 00:38:26,https://t.co/oHFoUMYapr,A Bayesian Group Sparse Multi-Task Regression Model for Imaging Genetics. (arXiv:1605.02234v2 [stat.ME] UPDATED) https://t.co/oHFoUMYapr,0,3," Abstract: Motivation: Recent advances in technology for brain imaging and high-throughput genotyping have motivated studies examining the influence of genetic variation on brain structure. Wang et al. (Bioinformatics, 2012) have developed an approach for the analysis of imaging genomic studies using penalized multi-task regression with regularization based on a novel group $l_{2,1}$-norm penalty which encourages structured sparsity at both the gene level and SNP level. While incorporating a number of useful features, the proposed method only furnishes a point estimate of the regression coefficients; techniques for conducting statistical inference are not provided. A new Bayesian method is proposed here to overcome this limitation. Results: We develop a Bayesian hierarchical modeling formulation where the posterior mode corresponds to the estimator proposed by Wang et al. (Bioinformatics, 2012), and an approach that allows for full posterior inference including the construction of interval estimates for the regression parameters. We show that the proposed hierarchical model can be expressed as a three-level Gaussian scale mixture and this representation facilitates the use of a Gibbs sampling algorithm for posterior simulation. Simulation studies demonstrate that the interval estimates obtained using our approach achieve adequate coverage probabilities that outperform those obtained from the nonparametric bootstrap. Our proposed methodology is applied to the analysis of neuroimaging and genetic data collected as part of the Alzheimer's Disease Neuroimaging Initiative (ADNI), and this analysis of the ADNI cohort demonstrates clearly the value added of incorporating interval estimation beyond only point estimation when relating SNPs to brain imaging endophenotypes. "
788177380988813312,2016-10-18 00:38:25,https://t.co/zHBDOJqNyt,Solving Systems of Random Quadratic Equations via Truncated Amplitude Flow. (arXiv:1605.08285v4 [stat.ML] UPDATED) https://t.co/zHBDOJqNyt,0,3," Abstract: This paper puts forth a new algorithm, termed \emph{truncated amplitude flow} (TAF), to recover an unknown $n$-dimensional real-/complex-valued vector $\bm{x}$ from $m$ quadratic equations of the form $y_i=|\langle\bm{a}_i,\bm{x}\rangle|^2$. This problem is known to be \emph{NP-hard} in general. We prove that as soon as the number of equations $m$ is on the order of the number of unknowns $n$, TAF recovers the solution exactly (up to a global unimodular constant) with high probability and complexity growing linearly with the time required to read the data. Our method adopts the \emph{amplitude-based} cost function and proceeds in two stages: In stage one, we introduce an \emph{orthogonality-promoting} initialization that is obtained with a few simple power iterations. Stage two refines the initial estimate by successive updates of scalable \emph{truncated generalized gradient iterations}. The former is in sharp contrast to existing spectral initializations, while the latter handles the rather challenging nonconvex and nonsmooth amplitude-based cost function. In particular for real-valued vectors, our gradient truncation rule provably eliminates the erroneously estimated signs with high probability to markedly improve upon its untruncated version. Numerical tests demonstrate that our initialization method returns more accurate and robust estimates relative to its spectral counterparts. Furthermore, even under the same initialization, our amplitude-based refinement outperforms Wirtinger-based alternatives, corroborating the superior performance of TAF over state-of-the-art algorithms. "
788177378262548480,2016-10-18 00:38:25,https://t.co/Eupogf2BmW,Asymptotic Analysis of Objectives based on Fisher Information in Active Learning. (arXiv:1605.08798v2 [stat.ML] UP… https://t.co/Eupogf2BmW,0,3," Abstract: Obtaining labels can be costly and time-consuming. Active learning allows a learning algorithm to intelligently query samples to be labeled for efficient learning. Fisher information ratio (FIR) has been used as an objective for selecting queries in active learning. However, little is known about the theory behind the use of FIR for active learning. There is a gap between the underlying theory and the motivation of its usage in practice. In this paper, we attempt to fill this gap and provide a rigorous framework for analyzing existing FIR-based active learning methods. In particular, we show that FIR can be asymptotically viewed as an upper bound of the expected variance of the log-likelihood ratio. Additionally, our analysis suggests a unifying framework that not only enables us to make theoretical comparisons among the existing querying methods based on FIR, but also allows us to give insight into the development of new active learning approaches based on this objective. "
788177374462480384,2016-10-18 00:38:24,https://t.co/Evajas2fn6,On the Identification and Mitigation of Weaknesses in the Knowledge Gradient Policy for Multi-Armed Bandits. (arXi… https://t.co/Evajas2fn6,0,4," Abstract: The Knowledge Gradient (KG) policy was originally proposed for online ranking and selection problems but has recently been adapted for use in online decision making in general and multi-armed bandit problems (MABs) in particular. We study its use in a class of exponential family MABs and identify weaknesses, including a propensity to take actions which are dominated with respect to both exploitation and exploration. We propose variants of KG which avoid such errors. These new policies include an index heuristic which deploys a KG approach to develop an approximation to the Gittins index. A numerical study shows this policy to perform well over a range of MABs including those for which index policies are not optimal. While KG does not make dominated actions when bandits are Gaussian, it fails to be index consistent and appears not to enjoy a performance advantage over competitor policies when arms are correlated to compensate for its greater computational demands. "
788177371648196608,2016-10-18 00:38:23,https://t.co/XCY9Y6Gbwe,Using Neural Network Formalism to Solve Multiple-Instance Problems. (arXiv:1609.07257v2 [cs.LG] UPDATED) https://t.co/XCY9Y6Gbwe,0,3," Abstract: Many objects in the real world are difficult to describe by a single numerical vector of a fixed length, whereas describing them by a set of vectors is more natural. Therefore, Multiple instance learning (MIL) techniques have been constantly gaining on importance throughout last years. MIL formalism represents each object (sample) by a set (bag) of feature vectors (instances) of fixed length where knowledge about objects (e.g., class label) is available on bag level but not necessarily on instance level. Many standard tools including supervised classifiers have been already adapted to MIL setting since the problem got formalized in late nineties. In this work we propose a neural network (NN) based formalism that intuitively bridges the gap between MIL problem definition and the vast existing knowledge-base of standard models and classifiers. We show that the proposed NN formalism is effectively optimizable by a modified back-propagation algorithm and can reveal unknown patterns inside bags. Comparison to eight types of classifiers from the prior art on a set of 14 publicly available benchmark datasets confirms the advantages and accuracy of the proposed solution. "
787813751454138369,2016-10-17 00:33:29,https://t.co/ZdogGYjPvu,MML is not consistent for Neyman-Scott. (arXiv:1610.04336v1 [stat.ML]) https://t.co/ZdogGYjPvu,0,1," Abstract: Minimum Message Length (MML) is a popular method for statistical inference, belonging to the Minimum Description Length (MDL) family. It is a general name for any of several computationally-feasible approximations to the generally NP-Hard Strict Minimum Message Length (SMML) estimator. One often-cited showcase for the power of MML is the Neyman-Scott estimation problem, where most popular estimation algorithms fail to produce a consistent result. MML's performance on Neyman-Scott was analysed by Dowe and Wallace (1997) and by Wallace (2005) and MML was shown to be consistent for the problem. However, this analysis was not performed on SMML, but rather on two SMML approximations: Wallace-Freeman and Ideal Group. As for most estimation problems, the exact SMML solution is not known for Neyman-Scott. We analyse the Dowe-Wallace solution, and show that it hinges critically on the use of an unnatural prior for the problem. We argue that the Jeffreys prior is a more natural prior to assume in this case. Re-analysing the problem over its Jeffreys prior, we show that both the Ideal Group and the Wallace-Freeman approximations converge to the (inconsistent) Maximum Likelihood (ML) solution. We develop novel techniques that enable determining properties of the SMML estimator for some general families of estimation problems without requiring a full construction of the estimator, and use these to show that for many problems, including Neyman-Scott, the SMML estimator is not a point-estimator at all. Rather, it maps each observation to an entire continuum of estimates. Furthermore, using the tools developed we show that for Neyman-Scott the SMML estimate is inconsistent for all parameter sets as well as asymptotically. We discuss methodological problems in the arguments put forward by previous authors, who argued that MML is consistent for Neyman-Scott and in general. "
787813746857172992,2016-10-17 00:33:28,https://t.co/dR8W5ZXzbd,A Language-independent and Compositional Model for Personality Trait Recognition from Short Texts. (arXiv:1610.043… https://t.co/dR8W5ZXzbd,0,3," Abstract: Many methods have been used to recognize author personality traits from text, typically combining linguistic feature engineering with shallow learning models, e.g. linear regression or Support Vector Machines. This work uses deep-learning-based models and atomic features of text, the characters, to build hierarchical, vectorial word and sentence representations for trait inference. This method, applied to a corpus of tweets, shows state-of-the-art performance across five traits and three languages (English, Spanish and Italian) compared with prior work in author profiling. The results, supported by preliminary visualisation work, are encouraging for the ability to detect complex human traits. "
787813744546021377,2016-10-17 00:33:28,https://t.co/6L3o4XgoN6,Semi-supervised Graph Embedding Approach to Dynamic Link Prediction. (arXiv:1610.04351v1 [stat.ML]) https://t.co/6L3o4XgoN6,0,5," Abstract: We propose a simple discrete time semi-supervised graph embedding approach to link prediction in dynamic networks. The learned embedding reflects information from both the temporal and cross-sectional network structures, which is performed by defining the loss function as a weighted sum of the supervised loss from past dynamics and the unsupervised loss of predicting the neighborhood context in the current network. Our model is also capable of learning different embeddings for both formation and dissolution dynamics. These key aspects contributes to the predictive performance of our model and we provide experiments with three real--world dynamic networks showing that our method is comparable to state of the art methods in link formation prediction and outperforms state of the art baseline methods in link dissolution prediction. "
787813741979131904,2016-10-17 00:33:27,https://t.co/JKxEY6mROK,"Aboveground biomass mapping in French Guiana by combining remote sensing, forest inventories and environmental dat… https://t.co/JKxEY6mROK",0,0," Abstract: Mapping forest aboveground biomass (AGB) has become an important task, particularly for the reporting of carbon stocks and changes. AGB can be mapped using synthetic aperture radar data (SAR) or passive optical data. However, these data are insensitive to high AGB levels (\textgreater{}150 Mg/ha, and \textgreater{}300 Mg/ha for P-band), which are commonly found in tropical forests. Studies have mapped the rough variations in AGB by combining optical and environmental data at regional and global scales. Nevertheless, these maps cannot represent local variations in AGB in tropical forests. In this paper, we hypothesize that the problem of misrepresenting local variations in AGB and AGB estimation with good precision occurs because of both methodological limits (signal saturation or dilution bias) and a lack of adequate calibration data in this range of AGB values. We test this hypothesis by developing a calibrated regression model to predict variations in high AGB values (mean \textgreater{}300 Mg/ha) in French Guiana by a methodological approach for spatial extrapolation with data from the optical geoscience laser altimeter system (GLAS), forest inventories, radar, optics, and environmental variables for spatial inter-and extrapolation. Given their higher point count, GLAS data allow a wider coverage of AGB values. We find that the metrics from GLAS footprints are correlated with field AGB estimations (R 2 =0.54, RMSE=48.3 Mg/ha) with no bias for high values. First, predictive models, including remote-sensing, environmental variables and spatial correlation functions, allow us to obtain ""wall-to-wall"" AGB maps over French Guiana with an RMSE for the in situ AGB estimates of ~51 Mg/ha and R${}^2$=0.48 at a 1-km grid size. We conclude that a calibrated regression model based on GLAS with dependent environmental data can produce good AGB predictions even for high AGB values if the calibration data fit the AGB range. We also demonstrate that small temporal and spatial mismatches between field data and GLAS footprints are not a problem for regional and global calibrated regression models because field data aim to predict large and deep tendencies in AGB variations from environmental gradients and do not aim to represent high but stochastic and temporally limited variations from forest dynamics. Thus, we advocate including a greater variety of data, even if less precise and shifted, to better represent high AGB values in global models and to improve the fitting of these models for high values. "
787813739802271744,2016-10-17 00:33:27,https://t.co/f2g8vJP6DU,Practical Learning of Deep Gaussian Processes via Random Fourier Features. (arXiv:1610.04386v1 [stat.ML]) https://t.co/f2g8vJP6DU,1,11," Abstract: The composition of multiple Gaussian Processes as a Deep Gaussian Process (DGP) enables a deep probabilistic approach to flexibly quantify uncertainty and carry out model selection in various learning scenarios. In this work, we introduce a novel formulation of DGPs based on random Fourier features that we train using stochastic variational inference. Our proposal yields an efficient way of training DGP architectures without compromising on predictive performance. Through a series of experiments, we illustrate how our model compares favorably to other state-of-the-art inference methods for DGPs for both regression and classification tasks. We also demonstrate how an asynchronous implementation of stochastic gradient optimization can exploit the computational power of distributed systems for large-scale DGP learning. "
787813737562509316,2016-10-17 00:33:26,https://t.co/TAGckYIOsl,Theoretical Analysis of Domain Adaptation with Optimal Transport. (arXiv:1610.04420v1 [stat.ML]) https://t.co/TAGckYIOsl,0,5," Abstract: Domain adaptation (DA) is an important and emerging field of machine learning that tackles the problem occurring when the distributions of training (source domain) and test (target domain) data are similar but different. Current theoretical results show that the efficiency of DA algorithms depends on their capacity of minimizing the divergence between source and target probability distributions. In this paper, we provide a theoretical study on the advantages that concepts borrowed from optimal transportation theory can bring to DA. In particular, we show that the Wasserstein metric can be used as a divergence measure between distributions to obtain generalization guarantees for three different learning settings: (i) classic DA with unsupervised target data (ii) DA combining source and target labeled data, (iii) multiple source DA. Based on the obtained results, we provide some insights showing when this analysis can be tighter than other existing frameworks. We also show that in the context of multiple source DA, the problem of estimating of the best joint hypothesis between source and target labeling functions can be reformulated using a Wasserstein distance-based loss function. We think that these results open the door to novel ideas and directions for DA. "
787813735519887360,2016-10-17 00:33:26,https://t.co/OyNPv5XCLv,A Reduction Theorem for the Sample Mean in Dynamic Time Warping Spaces. (arXiv:1610.04460v1 [cs.CV]) https://t.co/OyNPv5XCLv,0,1," Abstract: Though the concept of sample mean in dynamic time warping (DTW) spaces is used in pattern recognition applications, its existence has neither been proved nor called into question. This article shows that a sample mean exists under general conditions that cover common variations of different DTW-spaces mentioned in the literature. The existence proofs are based on a Reduction Theorem that bounds the length of the candidate solutions we need to consider. The proposed results place the concept of sample mean in DTW-spaces on a sound mathematical foundation and serves as a first step towards a statistical theory of DTW-spaces. "
787813733196263424,2016-10-17 00:33:25,https://t.co/jkYDES3bBr,Amortised MAP Inference for Image Super-resolution. (arXiv:1610.04490v1 [cs.CV]) https://t.co/jkYDES3bBr,1,9," Abstract: Image Super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data, achieving particularly good results in photo-realistic texture SR. "
787813730654490624,2016-10-17 00:33:24,https://t.co/86sPrL8YFp,The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear Bandits. (arXiv:1610.04491v1 [stat.ML]) https://t.co/86sPrL8YFp,0,2," Abstract: Stochastic linear bandits are a natural and simple generalisation of finite-armed bandits with numerous practical applications. Current approaches focus on generalising existing techniques for finite-armed bandits, notably the optimism principle and Thompson sampling. While prior work has mostly been in the worst-case setting, we analyse the asymptotic instance-dependent regret and show matching upper and lower bounds on what is achievable. Surprisingly, our results show that no algorithm based on optimism or Thompson sampling will ever achieve the optimal rate, and indeed, can be arbitrarily far from optimal, even in very simple cases. This is a disturbing result because these techniques are standard tools that are widely used for sequential optimisation. For example, for generalised linear bandits and reinforcement learning. "
787813728305737728,2016-10-17 00:33:24,https://t.co/iMd1wgL836,Generalization Error of Invariant Classifiers. (arXiv:1610.04574v1 [stat.ML]) https://t.co/iMd1wgL836,0,2," Abstract: This paper studies the generalization error of invariant classifiers. In particular, we consider the common scenario where the classification task is invariant to certain transformations of the input, and that the classifier is constructed (or learned) to be invariant to these transformations. Our approach relies on factoring the input space into a product of a base space and a set of transformations. We show that whereas the generalization error of a non-invariant classifier is proportional to the complexity of the input space, the generalization error of an invariant classifier is proportional to the complexity of the base space. We also derive a set of sufficient conditions on the geometry of the base space and the set of transformations that ensure that the complexity of the base space is much smaller than the complexity of the input space. Our analysis applies to general classifiers such as convolutional neural networks. We demonstrate the implications of the developed theory for such classifiers with experiments on the MNIST and CIFAR-10 datasets. "
787813726309187584,2016-10-17 00:33:23,https://t.co/dwtIYNVmKk,Improved Strongly Adaptive Online Learning using Coin Betting. (arXiv:1610.04578v1 [stat.ML]) https://t.co/dwtIYNVmKk,0,1," Abstract: This paper describes a new parameter-free online learning algorithm for changing environments. In comparing against algorithms with the same time complexity as ours, we obtain a strongly adaptive regret bound that is a factor of at least $\sqrt{\log(T)}$ better, where $T$ is the time horizon. Empirical results show that our algorithm outperforms state-of-the-art methods in learning with expert advice and metric learning scenarios. "
787813724308529152,2016-10-17 00:33:23,https://t.co/A09C1bPKEg,Message-passing algorithms for synchronization problems over compact groups. (arXiv:1610.04583v1 [cs.IT]) https://t.co/A09C1bPKEg,0,2," Abstract: Various alignment problems arising in cryo-electron microscopy, community detection, time synchronization, computer vision, and other fields fall into a common framework of synchronization problems over compact groups such as Z/L, U(1), or SO(3). The goal of such problems is to estimate an unknown vector of group elements given noisy relative observations. We present an efficient iterative algorithm to solve a large class of these problems, allowing for any compact group, with measurements on multiple 'frequency channels' (Fourier modes, or more generally, irreducible representations of the group). Our algorithm is a highly efficient iterative method following the blueprint of approximate message passing (AMP), which has recently arisen as a central technique for inference problems such as structured low-rank estimation and compressed sensing. We augment the standard ideas of AMP with ideas from representation theory so that the algorithm can work with distributions over compact groups. Using standard but non-rigorous methods from statistical physics we analyze the behavior of our algorithm on a Gaussian noise model, identifying phases where the problem is easy, (computationally) hard, and (statistically) impossible. In particular, such evidence predicts that our algorithm is information-theoretically optimal in many cases, and that the remaining cases show evidence of statistical-to-computational gaps. "
787813721515122688,2016-10-17 00:33:22,https://t.co/EUWpKvtD99,"Data-Driven Threshold Machine: Scan Statistics, Change-Point Detection, and Extreme Bandits. (arXiv:1610.04599v1 [… https://t.co/EUWpKvtD99",1,3," Abstract: We present a novel distribution-free approach, the data-driven threshold machine (DTM), for a fundamental problem at the core of many learning tasks: choose a threshold for a given pre-specified level that bounds the tail probability of the maximum of a (possibly dependent but stationary) random sequence. We do not assume data distribution, but rather relying on the asymptotic distribution of extremal values, and reduce the problem to estimate three parameters of the extreme value distributions and the extremal index. We specially take care of data dependence via estimating extremal index since in many settings, such as scan statistics, change-point detection, and extreme bandits, where dependence in the sequence of statistics can be significant. Key features of our DTM also include robustness and the computational efficiency, and it only requires one sample path to form a reliable estimate of the threshold, in contrast to the Monte Carlo sampling approach which requires drawing a large number of sample paths. We demonstrate the good performance of DTM via numerical examples in various dependent settings. "
787813719409586177,2016-10-17 00:33:22,https://t.co/sdKZVyeIRg,Second Order Stochastic Optimization in Linear Time. (arXiv:1602.03943v4 [stat.ML] UPDATED) https://t.co/sdKZVyeIRg,2,12," Abstract: First-order stochastic methods are the state-of-the-art in large-scale machine learning optimization owing to efficient per-iteration complexity. Second-order methods, while able to provide faster convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop second-order stochastic methods for optimization problems in machine learning that match the per-iteration cost of gradient based methods, and in certain settings improves upon the overall running time upon the state-of-the-art. Furthermore, our algorithm has the desirable property of being implementable in time linear in the sparsity of the input data. "
787813717090197504,2016-10-17 00:33:21,https://t.co/mjXzEoR6p3,Localized Lasso for High-Dimensional Regression. (arXiv:1603.06743v3 [stat.ML] UPDATED) https://t.co/mjXzEoR6p3,3,12," Abstract: We introduce the localized Lasso, which is suited for learning models that are both interpretable and have a high predictive power in problems with high dimensionality $d$ and small sample size $n$. More specifically, we consider a function defined by local sparse models, one at each data point. We introduce sample-wise network regularization to borrow strength across the models, and sample-wise exclusive group sparsity (a.k.a., $\ell_{1,2}$ norm) to introduce diversity into the choice of feature sets in the local models. The local models are interpretable in terms of similarity of their sparsity patterns. The cost function is convex, and thus has a globally optimal solution. Moreover, we propose a simple yet efficient iterative least-squares based optimization procedure for the localized Lasso, which does not need a tuning parameter, and is guaranteed to converge to a globally optimal solution. The solution is empirically shown to outperform alternatives for both simulated and genomic personalized medicine data. "
787813714498052096,2016-10-17 00:33:21,https://t.co/TxOmJL6gkI,Uncovering Causality from Multivariate Hawkes Integrated Cumulants. (arXiv:1607.06333v2 [stat.ML] UPDATED) https://t.co/TxOmJL6gkI,0,3," Abstract: We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each nodes of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. A consequence is that it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the third-order integrated cumulants of the process. We show on numerical experiments that our approach is indeed very robust to the shape of the kernels, and gives appealing results on the MemeTracker database. "
787813711880851456,2016-10-17 00:33:20,https://t.co/QTDmqYGjhD,Higher-Order Factorization Machines. (arXiv:1607.07195v2 [stat.ML] UPDATED) https://t.co/QTDmqYGjhD,2,5," Abstract: Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy. We demonstrate the proposed approaches on four different link prediction tasks. "
787813709800407040,2016-10-17 00:33:19,https://t.co/3mb7faDLHG,Learning Tree-Structured Detection Cascades for Heterogeneous Networks of Embedded Devices. (arXiv:1608.00159v2 [s… https://t.co/3mb7faDLHG,0,2," Abstract: In this paper, we present a new approach to learning cascaded classifiers for use in computing environments that involve networks of heterogeneous and resource-constrained, low-power embedded compute and sensing nodes. We present a generalization of the classical linear detection cascade to the case of tree-structured cascades where different branches of the tree execute on different physical compute nodes in the network. Different nodes have access to different features, as well as access to potentially different computation and energy resources. We concentrate on the problem of jointly learning the parameters for all of the classifiers in the cascade given a fixed cascade architecture and a known set of costs required to carry out the computation at each node.To accomplish the objective of joint learning of all detectors, we propose a novel approach to combining classifier outputs during training that better matches the hard cascade setting in which the learned system will be deployed. This work is motivated by research in the area of mobile health where energy efficient real time detectors integrating information from multiple wireless on-body sensors and a smart phone are needed for real-time monitoring and delivering just-in-time adaptive interventions. We apply our framework to the problem of cigarette smoking detection from a combination of wrist-worn actigraphy data and respiration chest band data. "
787813707652923392,2016-10-17 00:33:19,https://t.co/fsQ9SXGqW0,Stochastic Rank-1 Bandits. (arXiv:1608.03023v2 [cs.LG] UPDATED) https://t.co/fsQ9SXGqW0,0,2," Abstract: We propose stochastic rank-$1$ bandits, a class of online learning problems where at each step a learning agent chooses a pair of row and column arms, and receives the product of their values as a reward. The challenge is that the values of the row and column are unobserved. These values are stochastic and drawn independently of each other. We propose an efficient algorithm for solving our problem, Rank1Elim, and derive a $O((K + L) (1 / \Delta) \log n)$ upper bound on its $n$-step regret, where $K$ is the number of rows, $L$ is the number of columns, and $\Delta$ is the minimum of the row and column gaps. This is the first bandit algorithm for finding the maximum entry of a rank-$1$ matrix whose regret is linear in $K + L$, $1 / \Delta$, and $\log n$. We evaluate our proposed algorithm on both synthetic and real-world problems, and observe that it leverages the structure of our problems and can learn near-optimal solutions even when our modeling assumptions are mildly violated. "
787813704578588672,2016-10-17 00:33:18,https://t.co/CdiJMof8iJ,Gated End-to-End Memory Networks. (arXiv:1610.04211v1 [cs.CL] CROSS LISTED) https://t.co/CdiJMof8iJ,0,5," Abstract: Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks, MemN2N, have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architecture, GMemN2N. From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art. "
786727672089042944,2016-10-14 00:37:48,https://t.co/rdAZSfrLOX,Generalization bound for kernel similarity learning. (arXiv:1610.03899v1 [stat.ML]) https://t.co/rdAZSfrLOX,0,4," Abstract: Similarity learning has received a large amount of interest and is an important tool for many scientific and industrial applications. In this framework, we wish to infer the distance (similarity) between points with respect to an arbitrary distance function $d$. Here, we formulate the problem as a regression from a feature space $\mathcal{X}$ to an arbitrary vector space $\mathcal{Y}$, where the Euclidean distance is proportional to $d$. We then give Rademacher complexity bounds on the generalization error. We find that with high probability, the complexity is bounded by the maximum of the radius of $\mathcal{X}$ and the radius of $\mathcal{Y}$. "
786727670432268288,2016-10-14 00:37:47,https://t.co/bHA7kBd2O8,Statistical Inference Using Mean Shift Denoising. (arXiv:1610.03927v1 [stat.ME]) https://t.co/bHA7kBd2O8,0,6," Abstract: In this paper, we study how the mean shift algorithm can be used to denoise a dataset. We introduce a new framework to analyze the mean shift algorithm as a denoising approach by viewing the algorithm as an operator on a distribution function. We investigate how the mean shift algorithm changes the distribution and show that data points shifted by the mean shift concentrate around high density regions of the underlying density function. By using the mean shift as a denoising method, we enhance the performance of several clustering techniques, improve the power of two-sample tests, and obtain a new method for anomaly detection. "
786727668616134656,2016-10-14 00:37:47,https://t.co/lNk3gKS0ZG,A Survey of Voice Translation Methodologies - Acoustic Dialect Decoder. (arXiv:1610.03934v1 [cs.CL]) https://t.co/lNk3gKS0ZG,0,4," Abstract: Speech Translation has always been about giving source text or audio input and waiting for system to give translated output in desired form. In this paper, we present the Acoustic Dialect Decoder (ADD) - a voice to voice ear-piece translation device. We introduce and survey the recent advances made in the field of Speech Engineering, to employ in the ADD, particularly focusing on the three major processing steps of Recognition, Translation and Synthesis. We tackle the problem of machine understanding of natural language by designing a recognition unit for source audio to text, a translation unit for source language text to target language text, and a synthesis unit for target language text to target language speech. Speech from the surroundings will be recorded by the recognition unit present on the ear-piece and translation will start as soon as one sentence is successfully read. This way, we hope to give translated output as and when input is being read. The recognition unit will use Hidden Markov Models (HMMs) Based Tool-Kit (HTK), hybrid RNN systems with gated memory cells, and the synthesis unit, HMM based speech synthesis system HTS. This system will initially be built as an English to Tamil translation device. "
786727667055923201,2016-10-14 00:37:47,https://t.co/zyfGN7gw9i,Dictionary Update for NMF-based Voice Conversion Using an Encoder-Decoder Network. (arXiv:1610.03988v1 [stat.ML]) https://t.co/zyfGN7gw9i,0,5," Abstract: In this paper, we propose a dictionary update method for Nonnegative Matrix Factorization (NMF) with high dimensional data in a spectral conversion (SC) task. Voice conversion has been widely studied due to its potential applications such as personalized speech synthesis and speech enhancement. Exemplar-based NMF (ENMF) emerges as an effective and probably the simplest choice among all techniques for SC, as long as a source-target parallel speech corpus is given. ENMF-based SC systems usually need a large amount of bases (exemplars) to ensure the quality of the converted speech. However, a small and effective dictionary is desirable but hard to obtain via dictionary update, in particular when high-dimensional features such as STRAIGHT spectra are used. Therefore, we propose a dictionary update framework for NMF by means of an encoder-decoder reformulation. Regarding NMF as an encoder-decoder network makes it possible to exploit the whole parallel corpus more effectively and efficiently when applied to SC. Our experiments demonstrate significant gains of the proposed system with small dictionaries over conventional ENMF-based systems with dictionaries of same or much larger size. "
786727665206165504,2016-10-14 00:37:46,https://t.co/1WZe5okbDG,Semi-Supervised Active Learning for Support Vector Machines: A Novel Approach that Exploits Structure Information … https://t.co/1WZe5okbDG,3,6," Abstract: In our today's information society more and more data emerges, e.g.~in social networks, technical applications, or business applications. Companies try to commercialize these data using data mining or machine learning methods. For this purpose, the data are categorized or classified, but often at high (monetary or temporal) costs. An effective approach to reduce these costs is to apply any kind of active learning (AL) methods, as AL controls the training process of a classifier by specific querying individual data points (samples), which are then labeled (e.g., provided with class memberships) by a domain expert. However, an analysis of current AL research shows that AL still has some shortcomings. In particular, the structure information given by the spatial pattern of the (un)labeled data in the input space of a classification model (e.g.,~cluster information), is used in an insufficient way. In addition, many existing AL techniques pay too little attention to their practical applicability. To meet these challenges, this article presents several techniques that together build a new approach for combining AL and semi-supervised learning (SSL) for support vector machines (SVM) in classification tasks. Structure information is captured by means of probabilistic models that are iteratively improved at runtime when label information becomes available. The probabilistic models are considered in a selection strategy based on distance, density, diversity, and distribution (4DS strategy) information for AL and in a kernel function (Responsibility Weighted Mahalanobis kernel) for SVM. The approach fuses generative and discriminative modeling techniques. With 20 benchmark data sets and with the MNIST data set it is shown that our new solution yields significantly better results than state-of-the-art methods. "
786727663591428096,2016-10-14 00:37:46,https://t.co/HN3HuLDwiU,Voice Conversion from Non-parallel Corpora Using Variational Auto-encoder. (arXiv:1610.04019v1 [stat.ML]) https://t.co/HN3HuLDwiU,0,4," Abstract: We propose a flexible framework for spectral conversion (SC) that facilitates training with unaligned corpora. Many SC frameworks require parallel corpora, phonetic alignments, or explicit frame-wise correspondence for learning conversion functions or for synthesizing a target spectrum with the aid of alignments. However, these requirements gravely limit the scope of practical applications of SC due to scarcity or even unavailability of parallel corpora. We propose an SC framework based on variational auto-encoder which enables us to exploit non-parallel corpora. The framework comprises an encoder that learns speaker-independent phonetic representations and a decoder that learns to reconstruct the designated speaker. It removes the requirement of parallel corpora or phonetic alignments to train a spectral conversion system. We report objective and subjective evaluations to validate our proposed method and compare it to SC methods that have access to aligned corpora. "
786727661955579905,2016-10-14 00:37:45,https://t.co/eTfxTpjVd3,Towards end-to-end optimisation of functional image analysis pipelines. (arXiv:1610.04079v1 [cs.CV]) https://t.co/eTfxTpjVd3,0,2," Abstract: The study of neurocognitive tasks requiring accurate localisation of activity often rely on functional Magnetic Resonance Imaging, a widely adopted technique that makes use of a pipeline of data processing modules, each involving a variety of parameters. These parameters are frequently set according to the local goal of each specific module, not accounting for the rest of the pipeline. Given recent success of neural network research in many different domains, we propose to convert the whole data pipeline into a deep neural network, where the parameters involved are jointly optimised by the network to best serve a common global goal. As a proof of concept, we develop a module able to adaptively apply the most suitable spatial smoothing to every brain volume for each specific neuroimaging task, and we validate its results in a standard brain decoding experiment. "
786727660512772097,2016-10-14 00:37:45,https://t.co/4SwyaN9538,Tensorial Mixture Models. (arXiv:1610.04167v1 [cs.LG]) https://t.co/4SwyaN9538,1,8," Abstract: We introduce a generative model, we call Tensorial Mixture Models (TMMs) based on mixtures of basic component distributions over local structures (e.g. patches in an image) where the dependencies between the local-structures are represented by a ""priors tensor"" holding the prior probabilities of assigning a component distribution to each local-structure. In their general form, TMMs are intractable as the prior tensor is typically of exponential size. However, when the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the TMM into a Convolutional Arithmetic Circuit (ConvAC). A ConvAC corresponds to a shallow (single hidden layer) network when the priors tensor is decomposed by a CP (sum of rank-1) approach and corresponds to a deep network when the decomposition follows the Hierarchical Tucker (HT) model. The ConvAC representation of a TMM possesses several attractive properties. First, the inference is tractable and is implemented by a forward pass through a deep network. Second, the architectural design of the model follows the deep networks community design, i.e., the structure of TMMs is determined by just two easily understood factors: size of pooling windows and number of channels. Finally, we demonstrate the effectiveness of our model when tackling the problem of classification with missing data, leveraging TMMs unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution. "
786727657845161984,2016-10-14 00:37:44,https://t.co/wqJLy3U0Kb,Removal of Batch Effects using Distribution-Matching Residual Networks. (arXiv:1610.04181v1 [stat.ML]) https://t.co/wqJLy3U0Kb,0,4," Abstract: Sources of variability in experimentally derived data include measurement error in addition to the physical phenomena of interest. This measurement error is a combination of systematic components, originating from the measuring instrument, and random measurement errors. Several novel biological technologies, such as mass cytometry and single-cell RNA-seq, are plagued with systematic errors that may severely affect statistical analysis if the data is not properly calibrated. Here, we propose a novel deep learning approach for removing systematic batch effects. Our method is based on a residual network, trained to minimize the Maximum Mean Discrepancy (MMD) between the multivariate distributions of two replicates, measured in different batches. We apply our method to mass cytometry and single-cell RNA-seq datasets, and demonstrate that it effectively attenuates batch effects, and outperforms several popular methods. "
786727656293367808,2016-10-14 00:37:44,https://t.co/5NIc9QG8AU,Phase Retrieval Meets Statistical Learning Theory: A Flexible Convex Relaxation. (arXiv:1610.04210v1 [cs.IT]) https://t.co/5NIc9QG8AU,0,5," Abstract: We propose a flexible convex relaxation for the phase retrieval problem that operates in the natural domain of the signal. Therefore, we avoid the prohibitive computational cost associated with ""lifting"" and semidefinite programming (SDP) in methods such as PhaseLift and compete with recently developed non-convex techniques for phase retrieval. We relax the quadratic equations for phaseless measurements to inequality constraints each of which representing a symmetric ""slab"". Through a simple convex program, our proposed estimator finds an extreme point of the intersection of these slabs that is best aligned with a given anchor vector. We characterize geometric conditions that certify success of the proposed estimator. Furthermore, using classic results in statistical learning theory, we show that for random measurements the geometric certificates hold with high probability at an optimal sample complexity. Phase transition of our estimator is evaluated through simulations. Our numerical experiments also suggest that the proposed method can solve phase retrieval problems with coded diffraction measurements as well. "
786727654644940801,2016-10-14 00:37:44,https://t.co/joCDpvntEh,Tucker Gaussian Process for Regression and Collaborative Filtering. (arXiv:1605.07025v2 [stat.ML] UPDATED) https://t.co/joCDpvntEh,0,6," Abstract: We introduce the Tucker Gaussian Process (TGP), a model for regression that regularises a Gaussian Process (GP) towards simpler regression functions for enhanced generalisation performance. We derive it using a novel approach to scalable GP learning, and show that our model is particularly well-suited to grid-structured data and problems where the dependence on covariates is close to being separable. A prime example is collaborative filtering, for which our model provides an effective GP based method that has a low-rank matrix factorisation at its core. We show that TGP generalises classical Bayesian matrix factorisation models, and goes beyond them to give a natural and elegant method for incorporating side information. "
786727652661026816,2016-10-14 00:37:43,https://t.co/uFsa4flLGi],Asymptotically exact inference in likelihood-free models. (arXiv:1605.07826v3 [https://t.co/uFsa4flLGi] UPDATED) https://t.co/JnvUtHqD9h,3,7,INDEXERROR
786727650719109121,2016-10-14 00:37:43,https://t.co/zbno8xGRh2,"Dialog state tracking, a machine reading approach using Memory Network. (arXiv:1606.04052v4 [cs.CL] UPDATED) https://t.co/zbno8xGRh2",0,2," Abstract: In an end-to-end dialog system, the aim of dialog state tracking is to accurately estimate a compact representation of the current dialog status from a sequence of noisy observations produced by the speech recognition and the natural language understanding modules. This paper introduces a novel method of dialog state tracking based on the general paradigm of machine reading and proposes to solve it using an End-to-End Memory Network, MemN2N, a memory-enhanced neural network architecture. We evaluate the proposed approach on the second Dialog State Tracking Challenge (DSTC-2) dataset. The corpus has been converted for the occasion in order to frame the hidden state variable inference as a question-answering task based on a sequence of utterances extracted from a dialog. We show that the proposed tracker gives encouraging results. Then, we propose to extend the DSTC-2 dataset with specific reasoning capabilities requirement like counting, list maintenance, yes-no question answering and indefinite knowledge management. Finally, we present encouraging results using our proposed MemN2N based tracking model. "
786727648575848449,2016-10-14 00:37:42,https://t.co/Xuk9rj0mGR,An Operator Theoretic Approach to Nonparametric Mixture Models. (arXiv:1607.00071v2 [stat.ML] UPDATED) https://t.co/Xuk9rj0mGR,1,10," Abstract: When estimating finite mixture models, it is common to make assumptions on the mixture components, such as parametric assumptions. In this work, we make no distributional assumptions on the mixture components and instead assume that observations from the mixture model are grouped, such that observations in the same group are known to be drawn from the same mixture component. We precisely characterize the number of observations $n$ per group needed for the mixture model to be identifiable, as a function of the number $m$ of mixture components. In addition to our assumption-free analysis, we also study the settings where the mixture components are either linearly independent or jointly irreducible. Furthermore, our analysis considers two kinds of identifiability -- where the mixture model is the simplest one explaining the data, and where it is the only one. As an application of these results, we precisely characterize identifiability of multinomial mixture models. Our analysis relies on an operator-theoretic framework that associates mixture models in the grouped-sample setting with certain infinite-dimensional tensors. Based on this framework, we introduce general spectral algorithms for recovering the mixture components and illustrate their use on a synthetic data set. "
786727645979537408,2016-10-14 00:37:42,https://t.co/yGRd2T0hgA,Saddle-free Hessian-free Optimization. (arXiv:1506.00059v2 [cs.NA] CROSS LISTED) https://t.co/yGRd2T0hgA,2,15," Abstract: Nonconvex optimization problems such as the ones in training deep neural networks suffer from a phenomenon called saddle point proliferation. This means that there are a vast number of high error saddle points present in the loss function. Second order methods have been tremendously successful and widely adopted in the convex optimization community, while their usefulness in deep learning remains limited. This is due to two problems: computational complexity and the methods being driven towards the high error saddle points. We introduce a novel algorithm specially designed to solve these two issues, providing a crucial first step to take the widely known advantages of Newton's method to the nonconvex optimization community, especially in high dimensional settings. "
786727644079489024,2016-10-14 00:37:41,https://t.co/R9riqWWEd1,Fast Nonsmooth Regularized Risk Minimization with Continuation. (arXiv:1602.07844v1 [cs.LG] CROSS LISTED) https://t.co/R9riqWWEd1,0,3," Abstract: In regularized risk minimization, the associated optimization problem becomes particularly difficult when both the loss and regularizer are nonsmooth. Existing approaches either have slow or unclear convergence properties, are restricted to limited problem subclasses, or require careful setting of a smoothing parameter. In this paper, we propose a continuation algorithm that is applicable to a large class of nonsmooth regularized risk minimization problems, can be flexibly used with a number of existing solvers for the underlying smoothed subproblem, and with convergence results on the whole algorithm rather than just one of its subproblems. In particular, when accelerated solvers are used, the proposed algorithm achieves the fastest known rates of $O(1/T^2)$ on strongly convex problems, and $O(1/T)$ on general convex problems. Experiments on nonsmooth classification and regression tasks demonstrate that the proposed algorithm outperforms the state-of-the-art. "
786727642200440832,2016-10-14 00:37:41,https://t.co/7iAG2hLIzk,Machine learning applied to single-shot x-ray diagnostics in an XFEL. (arXiv:1610.03378v1 [physics.data-an] CROSS … https://t.co/7iAG2hLIzk,3,9," Abstract: X-ray free-electron lasers (XFELs) are the only sources currently able to produce bright few-fs pulses with tunable photon energies from 100 eV to more than 10 keV. Due to the stochastic SASE operating principles and other technical issues the output pulses are subject to large fluctuations, making it necessary to characterize the x-ray pulses on every shot for data sorting purposes. We present a technique that applies machine learning tools to predict x-ray pulse properties using simple electron beam and x-ray parameters as input. Using this technique at the Linac Coherent Light Source (LCLS), we report mean errors below 0.3 eV for the prediction of the photon energy at 530 eV and below 1.6 fs for the prediction of the delay between two x-ray pulses. We also demonstrate spectral shape prediction with a mean agreement of 97%. This approach could potentially be used at the next generation of high-repetition-rate XFELs to provide accurate knowledge of complex x-ray pulses at the full repetition rate. "
786364796530810880,2016-10-13 00:35:52,https://t.co/DmcfDf4S6N,Towards a Theoretical Analysis of PCA for Heteroscedastic Data. (arXiv:1610.03595v1 [math.ST]) https://t.co/DmcfDf4S6N,0,5," Abstract: Principal Component Analysis (PCA) is a method for estimating a subspace given noisy samples. It is useful in a variety of problems ranging from dimensionality reduction to anomaly detection and the visualization of high dimensional data. PCA performs well in the presence of moderate noise and even with missing data, but is also sensitive to outliers. PCA is also known to have a phase transition when noise is independent and identically distributed; recovery of the subspace sharply declines at a threshold noise variance. Effective use of PCA requires a rigorous understanding of these behaviors. This paper provides a step towards an analysis of PCA for samples with heteroscedastic noise, that is, samples that have non-uniform noise variances and so are no longer identically distributed. In particular, we provide a simple asymptotic prediction of the recovery of a one-dimensional subspace from noisy heteroscedastic samples. The prediction enables: a) easy and efficient calculation of the asymptotic performance, and b) qualitative reasoning to understand how PCA is impacted by heteroscedasticity (such as outliers). "
786364793527599105,2016-10-13 00:35:51,https://t.co/WeIADgvSzy,Optimistic Semi-supervised Least Squares Classification. (arXiv:1610.03713v1 [stat.ML]) https://t.co/WeIADgvSzy,0,2, Abstract: The goal of semi-supervised learning is to improve supervised classifiers by using additional unlabeled training examples. In this work we study a simple self-learning approach to semi-supervised learning applied to the least squares classifier. We show that a soft-label and a hard-label variant of self-learning can be derived by applying block coordinate descent to two related but slightly different objective functions. The resulting soft-label approach is related to an idea about dealing with missing data that dates back to the 1930s. We show that the soft-label variant typically outperforms the hard-label variant on benchmark datasets and partially explain this behaviour by studying the relative difficulty of finding good local minima for the corresponding objective functions. 
786364791770247168,2016-10-13 00:35:50,https://t.co/NGtTfOWe4L,The Weak Efficient Market Hypothesis in Light of Statistical Learning. (arXiv:1610.03724v1 [stat.AP]) https://t.co/NGtTfOWe4L,0,3," Abstract: We make an unprecedented evaluation of statistical learning methods to forecast daily returns. Using a randomization test to adjust for data snooping, several models are found statistically significant on the tested equity indices: CSI 300, FTSE, and S&P 500. A best Sharpe ratio portfolio has abnormal returns on the S&P 500, breaking even with the market at 10 bps in round trip costs. The returns produce statistically significant intercept for factor regression models, qualifying as a new anomalous 3-day crisis persistency factor. These results open the path towards a standardized usage of statistical learning methods in finance. "
786364790126116864,2016-10-13 00:35:50,https://t.co/c2OYIrvcJV,Post Selection Inference with Kernels. (arXiv:1610.03725v1 [stat.ML]) https://t.co/c2OYIrvcJV,1,12," Abstract: We propose a novel kernel based post selection inference (PSI) algorithm, which can not only handle non-linearity in data but also structured output such as multi-dimensional and multi-label outputs. Specifically, we develop a PSI algorithm for independence measures, and propose the Hilbert-Schmidt Independence Criterion (HSIC) based PSI algorithm (hsicInf). The novelty of the proposed algorithm is that it can handle non-linearity and/or structured data through kernels. Namely, the proposed algorithm can be used for wider range of applications including nonlinear multi-class classification and multi-variate regressions, while existing PSI algorithms cannot handle them. Through synthetic experiments, we show that the proposed approach can find a set of statistically significant features for both regression and classification problems. Moreover, we apply the hsicInf algorithm to a real-world data, and show that hsicInf can successfully identify important features. "
786364788477681664,2016-10-13 00:35:50,https://t.co/VgYO4uahdR,Exploring the Entire Regularization Path for the Asymmetric Cost Linear Support Vector Machine. (arXiv:1610.03738v… https://t.co/VgYO4uahdR,0,2, Abstract: We propose an algorithm for exploring the entire regularization path of asymmetric-cost linear support vector machines. Empirical evidence suggests the predictive power of support vector machines depends on the regularization parameters of the training algorithms. The algorithms exploring the entire regularization paths have been proposed for single-cost support vector machines thereby providing the complete knowledge on the behavior of the trained model over the hyperparameter space. Considering the problem in two-dimensional hyperparameter space though enables our algorithm to maintain greater flexibility in dealing with special cases and sheds light on problems encountered by algorithms building the paths in one-dimensional spaces. We demonstrate two-dimensional regularization paths for linear support vector machines that we train on synthetic and real data. 
786364786586025984,2016-10-13 00:35:49,https://t.co/JaxYl3YngW,Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble of Autoencoders. (arXiv:1610.03761v1 [cs.… https://t.co/JaxYl3YngW,1,3," Abstract: A fall is an abnormal activity that occurs rarely, so it is hard to collect real data for falls. It is, therefore, difficult to use supervised learning methods to automatically detect falls. Another challenge in using machine learning methods to automatically detect falls is the choice of features. In this paper, we propose to use an ensemble of autoencoders to extract features from different channels of wearable sensor data trained only on normal activities. We show that choosing a threshold as maximum of the reconstruction error on the training normal data is not the right way to identify unseen falls. We propose two methods for automatic tightening of reconstruction error from only the normal activities for better identification of unseen falls. We present our results on two activity recognition datasets and show the efficacy of our proposed method against traditional autoencoder models and two standard one-class classification methods. "
786364784992251904,2016-10-13 00:35:49,https://t.co/t5NtGx2Ar1,Parallelizing Stochastic Approximation Through Mini-Batching and Tail-Averaging. (arXiv:1610.03774v1 [stat.ML]) https://t.co/t5NtGx2Ar1,0,3," Abstract: This work characterizes the benefits of averaging techniques widely used in conjunction with stochastic gradient descent (SGD). In particular, this work sharply analyzes: (1) mini-batching, a method of averaging many samples of the gradient to both reduce the variance of a stochastic gradient estimate and for parallelizing SGD and (2) tail-averaging, a method involving averaging the final few iterates of SGD in order to decrease the variance in SGD's final iterate. This work presents the first tight non-asymptotic generalization error bounds for these schemes for the stochastic approximation problem of least squares regression. Furthermore, this work establishes a precise problem-dependent extent to which mini-batching can be used to yield provable near-linear parallelization speedups over SGD with batch size one. These results are utilized in providing a highly parallelizable SGD algorithm that obtains the optimal statistical error rate with nearly the same number of serial updates as batch gradient descent, which improves significantly over existing SGD-style methods. Finally, this work sheds light on some fundamental differences in SGD's behavior when dealing with agnostic noise in the (non-realizable) least squares regression problem. In particular, the work shows that the stepsizes that ensure optimal statistical error rates for the agnostic case must be a function of the noise properties. The central analysis tools used by this paper are obtained through generalizing the operator view of averaged SGD, introduced by Defossez and Bach (2015) followed by developing a novel analysis in bounding these operators to characterize the generalization error. These techniques may be of broader interest in analyzing various computational aspects of stochastic approximation. "
786364783155093504,2016-10-13 00:35:48,https://t.co/SAnQ6ECloN,The Power of Localization for Efficiently Learning Linear Separators with Noise. (arXiv:1307.8371v8 [cs.LG] UPDATE… https://t.co/SAnQ6ECloN,0,3," Abstract: We introduce a new approach for designing computationally efficient learning algorithms that are tolerant to noise, and demonstrate its effectiveness by designing algorithms with improved noise tolerance guarantees for learning linear separators. We consider both the malicious noise model and the adversarial label noise model. For malicious noise, where the adversary can corrupt both the label and the features, we provide a polynomial-time algorithm for learning linear separators in $\Re^d$ under isotropic log-concave distributions that can tolerate a nearly information-theoretically optimal noise rate of $\eta = \Omega(\epsilon)$. For the adversarial label noise model, where the distribution over the feature vectors is unchanged, and the overall probability of a noisy label is constrained to be at most $\eta$, we also give a polynomial-time algorithm for learning linear separators in $\Re^d$ under isotropic log-concave distributions that can handle a noise rate of $\eta = \Omega\left(\epsilon\right)$. We show that, in the active learning model, our algorithms achieve a label complexity whose dependence on the error parameter $\epsilon$ is polylogarithmic. This provides the first polynomial-time active learning algorithm for learning linear separators in the presence of malicious noise or adversarial label noise. "
786364781267677184,2016-10-13 00:35:48,https://t.co/grUlZHbgAu,Bayesian multi-tensor factorization. (arXiv:1412.4679v5 [stat.ML] UPDATED) https://t.co/grUlZHbgAu,2,7," Abstract: We introduce Bayesian multi-tensor factorization, a model that is the first Bayesian formulation for joint factorization of multiple matrices and tensors. The research problem generalizes the joint matrix-tensor factorization problem to arbitrary sets of tensors of any depth, including matrices, can be interpreted as unsupervised multi-view learning from multiple data tensors, and can be generalized to relax the usual trilinear tensor factorization assumptions. The result is a factorization of the set of tensors into factors shared by any subsets of the tensors, and factors private to individual tensors. We demonstrate the performance against existing baselines in multiple tensor factorization tasks in structural toxicogenomics and functional neuroimaging. "
786364779308978176,2016-10-13 00:35:47,https://t.co/XVO7U0ioDR,Learning and Free Energies for Vector Approximate Message Passing. (arXiv:1602.08207v2 [cs.IT] UPDATED) https://t.co/XVO7U0ioDR,0,5," Abstract: Vector approximate message passing (VAMP) is a computationally simple approach to the recovery of a signal $\mathbf{x}$ from noisy linear measurements $\mathbf{y}=\mathbf{Ax}+\mathbf{w}$. Like the AMP proposed by Donoho, Maleki, and Montanari in 2009, VAMP is characterized by a rigorous state evolution (SE) that holds under certain large random matrices and that matches the replica prediction of optimality. But while AMP's SE holds only for large i.i.d. sub-Gaussian $\mathbf{A}$, VAMP's SE holds under the much larger class: right-rotationally invariant $\mathbf{A}$. To run VAMP, however, one must specify the statistical parameters of the signal and noise. This work combines VAMP with Expectation-Maximization to yield an algorithm, EM-VAMP, that can jointly recover $\mathbf{x}$ while learning those statistical parameters. The fixed points of the proposed EM-VAMP algorithm are shown to be stationary points of a certain constrained free-energy, providing a variational interpretation of the algorithm. Numerical simulations show that EM-VAMP is robust to highly ill-conditioned $\mathbf{A}$ with performance nearly matching oracle-parameter VAMP. "
786364777396367360,2016-10-13 00:35:47,https://t.co/DxGWT3teHR,On the Influence of Momentum Acceleration on Online Learning. (arXiv:1603.04136v4 [math.OC] UPDATED) https://t.co/DxGWT3teHR,0,6," Abstract: The article examines in some detail the convergence rate and mean-square-error performance of momentum stochastic gradient methods in the constant step-size and slow adaptation regime. The results establish that momentum methods are equivalent to the standard stochastic gradient method with a re-scaled (larger) step-size value. The size of the re-scaling is determined by the value of the momentum parameter. The equivalence result is established for all time instants and not only in steady-state. The analysis is carried out for general strongly convex and smooth risk functions, and is not limited to quadratic risks. One notable conclusion is that the well-known bene ts of momentum constructions for deterministic optimization problems do not necessarily carry over to the adaptive online setting when small constant step-sizes are used to enable continuous adaptation and learn- ing in the presence of persistent gradient noise. From simulations, the equivalence between momentum and standard stochastic gradient methods is also observed for non-differentiable and non-convex problems. "
786364775483707392,2016-10-13 00:35:47,https://t.co/OVOL0dgpl4,Stochastic Variance-Reduced ADMM. (arXiv:1604.07070v2 [cs.LG] UPDATED) https://t.co/OVOL0dgpl4,0,7," Abstract: The alternating direction method of multipliers (ADMM) is a powerful optimization solver in machine learning. Recently, stochastic ADMM has been integrated with variance reduction methods for stochastic gradient, leading to SAG-ADMM and SDCA-ADMM that have fast convergence rates and low iteration complexities. However, their space requirements can still be high. In this paper, we propose an integration of ADMM with the method of stochastic variance reduced gradient (SVRG). Unlike another recent integration attempt called SCAS-ADMM, the proposed algorithm retains the fast convergence benefits of SAG-ADMM and SDCA-ADMM, but is more advantageous in that its storage requirement is very low, even independent of the sample size $n$. We also extend the proposed method for nonconvex problems, and obtain a convergence rate of $O(1/T)$. Experimental results demonstrate that it is as fast as SAG-ADMM and SDCA-ADMM, much faster than SCAS-ADMM, and can be used on much bigger data sets. "
786364773889929216,2016-10-13 00:35:46,https://t.co/MIPMEB4sb0,Quantifying the accuracy of approximate diffusions and Markov chains. (arXiv:1605.06420v2 [math.ST] UPDATED) https://t.co/MIPMEB4sb0,0,2," Abstract: Markov chains and diffusion processes are indispensable tools in machine learning and statistics that are used for inference, sampling, and modeling. With the growth of large-scale datasets, the computational cost associated with simulating these stochastic processes can be considerable, and many algorithms have been proposed to approximate the underlying Markov chain or diffusion. A fundamental question is how the computational savings trade off against the statistical error incurred due to approximations. This paper develops general results that address this question. We bound the Wasserstein distance between the equilibrium distributions of two diffusions as a function of their mixing rates and the deviation in their drifts. We show that this error bound is tight in simple Gaussian settings. Our general result on continuous diffusions can be discretized to provide insights into the computational--statistical trade-off of Markov chains. As an illustration, we apply our framework to derive finite-sample error bounds of approximate unadjusted Langevin dynamics. We characterize computation-constrained settings where, by using fast-to-compute approximate gradients in the Langevin dynamics, we obtain more accurate samples compared to using the exact gradients. Finally, as an additional application of our approach, we quantify the accuracy of approximate Zig-Zag sampling. Our theoretical analyses are supported by simulation experiments. "
786364772145127428,2016-10-13 00:35:46,https://t.co/AegaSr2kx4,Sparse principal component regression for generalized linear models. (arXiv:1609.08886v2 [stat.ML] UPDATED) https://t.co/AegaSr2kx4,0,4," Abstract: Principal component regression (PCR) is a widely used two-stage procedure: principal component analysis (PCA), followed by regression in which the selected principal components are regarded as new explanatory variables in the model. Note that PCA is based only on the explanatory variables, so the principal components are not selected using the information on the response variable. In this paper, we propose a one-stage procedure for PCR in the framework of generalized linear models. The basic loss function is based on a combination of the regression loss and PCA loss. An estimate of the regression parameter is obtained as the minimizer of the basic loss function with a sparse penalty. We call the proposed method sparse principal component regression for generalized linear models (SPCR-glm). Taking the two loss function into consideration simultaneously, SPCR-glm enables us to obtain sparse principal component loadings that are related to a response variable. However, a combination of loss functions may cause a parameter identification problem, but this potential problem is avoided by virtue of the sparse penalty. Thus, the sparse penalty plays two roles in this method. The parameter estimation procedure is proposed using various update algorithms with the coordinate descent algorithm. We apply SPCR-glm to two real datasets, doctor visits data and mouse consomic strain data. SPCR-glm provides more easily interpretable principal component (PC) scores and clearer classification on PC plots than the usual PCA. "
786364767741116416,2016-10-13 00:35:45,https://t.co/NbtSZ4ys0f,Unitary Evolution Recurrent Neural Networks. (arXiv:1511.06464v4 [cs.LG] CROSS LISTED) https://t.co/NbtSZ4ys0f,0,6," Abstract: Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies. "
786001541320802304,2016-10-12 00:32:25,https://t.co/VhjwlnrkJo,Truncated Variational Expectation Maximization. (arXiv:1610.03113v1 [stat.ML]) https://t.co/VhjwlnrkJo,1,8," Abstract: We derive a novel variational expectation maximization approach based on truncated variational distributions. The truncated distributions are proportional to exact posteriors in a subset of a discrete state space and equal zero otherwise. In contrast to factored variational approximations or Gaussian approximations, truncated approximations neither assume posterior independence nor mono-modal posteriors. The novel variational approach is closely related to Expectation Truncation (L\""ucke and Eggert, 2010) - a preselection based EM approximation. It shares with Expectation Truncation the central idea of truncated distributions and the application domain of discrete hidden variables. In contrast to Expectation Truncation we here show how truncated distributions can be included into the theoretical framework of variational EM approximations. A fully variational treatment of truncated distributions then allows for derivations of novel general and mathematically grounded results, which in turn can be used to formulate novel efficient algorithms for parameter optimization of probabilistic data models. Apart from showing that truncated distributions are fully consistent with the variational free-energy framework, we find the free-energy that corresponds to truncated distributions to be given by compact and efficiently computable expressions, while update equations for model parameters (M-steps) remain in their standard form. Furthermore, expectation values w.r.t. truncated distributions are given in a generic form. Based on these observations, we show how an efficient and easily applicable meta-algorithm can be formulated that guarantees a monotonic increase of the free-energy. More generally, the obtained variational framework developed here links variational E-steps to discrete optimization, and it provides a theoretical basis to tightly couple sampling and variational approaches. "
786001538741403648,2016-10-12 00:32:24,https://t.co/pVg01yyiUO,Error Asymmetry in Causal and Anticausal Regression. (arXiv:1610.03263v1 [cs.AI]) https://t.co/pVg01yyiUO,1,6," Abstract: It is generally difficult to make any statements about the expected prediction error in an univariate setting without further knowledge about how the data were generated. Recent work showed that knowledge about the real underlying causal structure of a data generation process has implications for various machine learning settings. Assuming an additive noise and an independence between data generating mechanism and its input, we draw a novel connection between the intrinsic causal relationship of two variables and the expected prediction error. We formulate the theorem that the expected error of the true data generating function as prediction model is generally smaller when the effect is predicted from its cause and, on the contrary, greater when the cause is predicted from its effect. The theorem implies an asymmetry in the error depending on the prediction direction. This is further corroborated with empirical evaluations in artificial and real-world data sets. "
786001536325386240,2016-10-12 00:32:24,https://t.co/ARBHOcLORx,Assisted Dictionary Learning for fMRI Data Analysis. (arXiv:1610.03276v1 [stat.ML]) https://t.co/ARBHOcLORx,0,3," Abstract: Extracting information from functional magnetic resonance (fMRI) images has been a major area of research for more than two decades. The goal of this work is to present a new method for the analysis of fMRI data sets, that is capable to incorporate a priori available information, via an efficient optimization framework. Tests on synthetic data sets demonstrate significant performance gains over existing methods of this kind. "
786001533028761600,2016-10-12 00:32:23,https://t.co/miUHXiFCzM,"Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving. (arXiv:1610.03295v1 [cs.AI]) https://t.co/miUHXiFCzM",3,8," Abstract: Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained. In this paper we apply deep reinforcement learning to the problem of forming long term driving strategies. We note that there are two major challenges that make autonomous driving different from other robotic tasks. First, is the necessity for ensuring functional safety - something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances. Second, the Markov Decision Process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario. We make three contributions in our work. First, we show how policy gradient iterations can be used without Markovian assumptions. Second, we decompose the problem into a composition of a Policy for Desires (which is to be learned) and trajectory planning with hard constraints (which is not learned). The goal of Desires is to enable comfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce a hierarchical temporal abstraction we call an ""Option Graph"" with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further. "
786001530507984897,2016-10-12 00:32:22,https://t.co/QWxIS1MUFs,Maximum entropy models capture melodic styles. (arXiv:1610.03414v1 [stat.ML]) https://t.co/QWxIS1MUFs,1,3," Abstract: We introduce a Maximum Entropy model able to capture the statistics of melodies in music. The model can be used to generate new melodies that emulate the style of the musical corpus which was used to train it. Instead of using the $n-$body interactions of $(n-1)-$order Markov models, traditionally used in automatic music generation, we use a $k-$nearest neighbour model with pairwise interactions only. In that way, we keep the number of parameters low and avoid over-fitting problems typical of Markov models. We show that long-range musical phrases don't need to be explicitly enforced using high-order Markov interactions, but can instead emerge from multiple, competing, pairwise interactions. We validate our Maximum Entropy model by contrasting how much the generated sequences capture the style of the original corpus without plagiarizing it. To this end we use a data-compression approach to discriminate the levels of borrowing and innovation featured by the artificial sequences. The results show that our modelling scheme outperforms both fixed-order and variable-order Markov models. This shows that, despite being based only on pairwise interactions, this Maximum Entropy scheme opens the possibility to generate musically sensible alterations of the original phrases, providing a way to generate innovation. "
786001527626498048,2016-10-12 00:32:22,https://t.co/6P79LkqSaL,Statistics of Robust Optimization: A Generalized Empirical Likelihood Approach. (arXiv:1610.03425v1 [stat.ML]) https://t.co/6P79LkqSaL,1,5," Abstract: We study statistical inference and robust solution methods for stochastic optimization problems, focusing on giving calibrated and adaptive confidence intervals for optimal values and solutions for a range of stochastic problems. As part of this, we develop a generalized empirical likelihood framework---based on distributional uncertainty sets constructed from nonparametric $f$-divergence balls---for Hadamard differentiable functionals, and in particular, stochastic optimization problems. As consequences of this theory, we provide principled methods of choosing distributional uncertainty regions so as to provide calibrated one- and two-sided confidence intervals. We also give an asymptotic expansion for our distributionally robust formulation, showing how robustification regularizes problems by their variance. Finally, we show that optimizers of the distributionally robust formulations we study enjoy (essentially) the same consistency properties as those in classical sample average approximations. "
786001525336371200,2016-10-12 00:32:21,https://t.co/4vFlLq6DFq,Learning in Implicit Generative Models. (arXiv:1610.03483v1 [stat.ML]) https://t.co/4vFlLq6DFq,11,40," Abstract: Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination. "
786001523071447041,2016-10-12 00:32:20,https://t.co/fjmeJMpl6R,A Characterization of Deterministic Sampling Patterns for Low-Rank Matrix Completion. (arXiv:1503.02596v3 [stat.ML… https://t.co/fjmeJMpl6R,0,6," Abstract: Low-rank matrix completion (LRMC) problems arise in a wide variety of applications. Previous theory mainly provides conditions for completion under missing-at-random samplings. This paper studies deterministic conditions for completion. An incomplete $d \times N$ matrix is finitely rank-$r$ completable if there are at most finitely many rank-$r$ matrices that agree with all its observed entries. Finite completability is the tipping point in LRMC, as a few additional samples of a finitely completable matrix guarantee its unique completability. The main contribution of this paper is a deterministic sampling condition for finite completability. We use this to also derive deterministic sampling conditions for unique completability that can be efficiently verified. We also show that under uniform random sampling schemes, these conditions are satisfied with high probability if $O(\max\{r,\log d\})$ entries per column are observed. These findings have several implications on LRMC regarding lower bounds, sample and computational complexity, the role of coherence, adaptive settings and the validation of any completion algorithm. We complement our theoretical results with experiments that support our findings and motivate future analysis of uncharted sampling regimes. "
786001520374456320,2016-10-12 00:32:20,https://t.co/r8r61SM08H,Recursion-Free Online Multiple Incremental/Decremental Analysis Based on Ridge Support Vector Learning. (arXiv:160… https://t.co/r8r61SM08H,0,4," Abstract: This study presents a rapid multiple incremental and decremental mechanism based on Weight-Error Curves (WECs) for support-vector analysis. Recursion-free computation is proposed for predicting the Lagrangian multipliers of new samples. This study examines Ridge Support Vector Models, subsequently devising a recursion-free function derived from WECs. With the proposed function, all the new Lagrangian multipliers can be computed at once without using any gradual step sizes. Moreover, such a function relaxes a constraint, where the increment of new multiple Lagrangian multipliers should be the same in the previous work, thereby easily satisfying the requirement of KKT conditions. The proposed mechanism no longer requires typical bookkeeping strategies, which compute the step size by checking all the training samples in each incremental round. "
785640289365008384,2016-10-11 00:36:56,https://t.co/CJZ7wo55ON,A nonparametric sequential test for online randomized experiments. (arXiv:1610.02490v1 [stat.ML]) https://t.co/CJZ7wo55ON,0,1," Abstract: We propose a nonparametric sequential test that aims to address two practical problems pertinent to online randomized experiments: (i) how to do a hypothesis test for complex metrics; (ii) how to prevent type $1$ error inflation under continuous monitoring. The proposed test does not require knowledge of the underlying probability distribution generating the data. We use the bootstrap to estimate the likelihood for blocks of data followed by mixture sequential probability ratio test. We validate this procedure on data from a major online e-commerce website. We show that the proposed test controls type $1$ error at any time, has good power, is robust to misspecification in the distribution generating the data, and allows quick inference in online randomized experiments. "
785640287641141248,2016-10-11 00:36:55,https://t.co/vK56z9yQUN,SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs. (arXiv:1610.02496v1 [cs.DC]) https://t.co/vK56z9yQUN,0,2," Abstract: Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete count data such as text and images. Applications require LDA to handle both large datasets and a large number of topics. Though distributed CPU systems have been used, GPU-based systems have emerged as a promising alternative because of the high computational power and memory bandwidth of GPUs. However, existing GPU-based LDA systems cannot support a large number of topics because they use algorithms on dense data structures whose time and space complexity is linear to the number of topics. In this paper, we propose SaberLDA, a GPU-based LDA system that implements a sparsity-aware algorithm to achieve sublinear time complexity and scales well to learn a large number of topics. To address the challenges introduced by sparsity, we propose a novel data layout, a new warp-based sampling kernel, and an efficient sparse count matrix updating algorithm that improves locality, makes efficient utilization of GPU warps, and reduces memory consumption. Experiments show that SaberLDA can learn from billions-token-scale data with up to 10,000 topics, which is almost two orders of magnitude larger than that of the previous GPU-based systems. With a single GPU card, SaberLDA is able to learn 10,000 topics from a dataset of billions of tokens in a few hours, which is only achievable with clusters with tens of machines before. "
785640285787320320,2016-10-11 00:36:55,https://t.co/U0fHnoXcoA,Revisiting Multiple Instance Neural Networks. (arXiv:1610.02501v1 [stat.ML]) https://t.co/U0fHnoXcoA,1,4," Abstract: Recently neural networks and multiple instance learning are both attractive topics in Artificial Intelligence related research fields. Deep neural networks have achieved great success in supervised learning problems, and multiple instance learning as a typical weakly-supervised learning method is effective for many applications in computer vision, biometrics, nature language processing, etc. In this paper, we revisit the problem of solving multiple instance learning problems using neural networks. Neural networks are appealing for solving multiple instance learning problem. The multiple instance neural networks perform multiple instance learning in an end-to-end way, which take a bag with various number of instances as input and directly output bag label. All of the parameters in a multiple instance network are able to be optimized via back-propagation. We propose a new multiple instance neural network to learn bag representations, which is different from the existing multiple instance neural networks that focus on estimating instance label. In addition, recent tricks developed in deep learning have been studied in multiple instance networks, we find deep supervision is effective for boosting bag classification accuracy. In the experiments, the proposed multiple instance networks achieve state-of-the-art or competitive performance on several MIL benchmarks. Moreover, it is extremely fast for both testing and training, e.g., it takes only 0.0003 second to predict a bag and a few seconds to train on a MIL datasets on a moderate CPU. "
785640284038303744,2016-10-11 00:36:54,https://t.co/UAeCyEY0WC,Variance-based regularization with convex objectives. (arXiv:1610.02581v1 [stat.ML]) https://t.co/UAeCyEY0WC,0,4," Abstract: We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error. Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certificates of optimality and achieves faster rates of convergence in more general settings than empirical risk minimization by virtue of trading off approximation and estimation error optimally. We give corroborating empirical evidence that suggests that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems. "
785640282306084864,2016-10-11 00:36:54,https://t.co/79TU3l76MG,Iterative proportional scaling revisited: a modern optimization perspective on big count data. (arXiv:1610.02588v1… https://t.co/79TU3l76MG,0,3," Abstract: We revisit the classic iterative proportional scaling (IPS) for contingency table analysis, from a modern optimization perspective. In contrast to the criticisms made in the literature, we show that based on a coordinate descent characterization, IPS can be slightly modified to deliver coefficient estimates, and from a majorization-minimization standpoint, IPS can be extended to handle log-affine models with general designs. The optimization techniques help accelerate IPS to provide highly salable algorithms for big count data applications, and can adapt IPS to shrinkage estimation to deal with a large number of variables. "
785640280439582720,2016-10-11 00:36:54,https://t.co/d1mDA8Gybn,Indirect Gaussian Graph Learning beyond Gaussianity. (arXiv:1610.02590v1 [stat.ML]) https://t.co/d1mDA8Gybn,0,1," Abstract: This paper studies how to capture dependency graph structures from real data which may not be multivariate Gaussian. Starting from marginal loss functions not necessarily derived from probability distributions, we use an additive over-parametrization with shrinkage to incorporate variable dependencies into the criterion. An iterative Gaussian graph learning algorithm is proposed with ease in implementation. Statistical analysis shows that with the error measured in terms of a proper Bregman divergence, the estimators have fast rate of convergence. Real-life examples in different settings are given to demonstrate the efficacy of the proposed methodology. "
785640278552092672,2016-10-11 00:36:53,https://t.co/t6yVEAujII,A new selection strategy for selective cluster ensemble based on Diversity and Independency. (arXiv:1610.02649v1 [… https://t.co/t6yVEAujII,0,2," Abstract: This research introduces a new strategy in cluster ensemble selection by using Independency and Diversity metrics. In recent years, Diversity and Quality, which are two metrics in evaluation procedure, have been used for selecting basic clustering results in the cluster ensemble selection. Although quality can improve the final results in cluster ensemble, it cannot control the procedures of generating basic results, which causes a gap in prediction of the generated basic results' accuracy. Instead of quality, this paper introduces Independency as a supplementary method to be used in conjunction with Diversity. Therefore, this paper uses a heuristic metric, which is based on the procedure of converting code to graph in Software Testing, in order to calculate the Independency of two basic clustering algorithms. Moreover, a new modeling language, which we called as ""Clustering Algorithms Independency Language"" (CAIL), is introduced in order to generate graphs which depict Independency of algorithms. Also, Uniformity, which is a new similarity metric, has been introduced for evaluating the diversity of basic results. As a credential, our experimental results on varied different standard data sets show that the proposed framework improves the accuracy of final results dramatically in comparison with other cluster ensemble methods. "
785640277033771008,2016-10-11 00:36:53,https://t.co/IJPLU9MAAb,Nonparametric Bayesian inference of the microcanonical stochastic block model. (arXiv:1610.02703v1 [physics.data-a… https://t.co/IJPLU9MAAb,0,5," Abstract: A principled approach to characterize the hidden modular structure of networks is to formulate generative models, and then infer their parameters from data. When the desired structure is composed of modules or ""communities"", a suitable choice for this task is the stochastic block model (SBM), where nodes are divided into groups, and the placement of edges is conditioned on the group memberships. Here, we present a nonparametric Bayesian method to infer the modular structure of empirical networks, including the number of modules and their hierarchical organization. We focus on a microcanonical variant of the SBM, where the structure is imposed via hard constraints. We show how this simple model variation allows simultaneously for two important improvements over more traditional inference approaches: 1. Deeper Bayesian hierarchies, with noninformative priors replaced by sequences of priors and hyperpriors, that not only remove limitations that seriously degrade the inference on large networks, but also reveal structures at multiple scales; 2. A very efficient inference algorithm that scales well not only for networks with a large number of nodes and edges, but also with an unlimited number of modules. We show also how this approach can be used to sample modular hierarchies from the posterior distribution, as well as to perform model selection. Furthermore, we expose a direct equivalence between our microcanonical approach and alternative derivations based on the canonical SBM. "
785640275096039424,2016-10-11 00:36:52,https://t.co/0T8xbTnfoE,Accelerate Monte Carlo Simulations with Restricted Boltzmann Machines. (arXiv:1610.02746v1 [physics.comp-ph]) https://t.co/0T8xbTnfoE,1,5," Abstract: Despite their exceptional flexibility and popularity, the Monte Carlo methods often suffer from slow mixing times for challenging statistical physics problems. We present a general strategy to overcome this difficulty by adopting ideas and techniques from the machine learning community. We fit the unnormalized probability of the physical model to a feedforward neural network and reinterpret the architecture as a restricted Boltzmann machine. Then, exploiting its feature detection ability, we utilize the restricted Boltzmann machine for efficient Monte Carlo updates and to speed up the simulation of the original physical system. We implement these ideas for the Falicov-Kimball model and demonstrate improved acceptance ratio and autocorrelation time near the phase transition point. "
785640273472790528,2016-10-11 00:36:52,https://t.co/kBfuxuJBU9,Dataiku's Solution to SPHERE's Activity Recognition Challenge. (arXiv:1610.02757v1 [stat.ML]) https://t.co/kBfuxuJBU9,0,2," Abstract: Our team won the second prize of the Safe Aging with SPHERE Challenge organized by SPHERE, in conjunction with ECML-PKDD and Driven Data. The goal of the competition was to recognize activities performed by humans, using sensor data. This paper presents our solution. It is based on a rich pre-processing and state of the art machine learning methods. From the raw train data, we generate a synthetic train set with the same statistical characteristics as the test set. We then perform feature engineering. The machine learning modeling part is based on stacking weak learners through a grid searched XGBoost algorithm. Finally, we use post-processing to smooth our predictions over time. "
785640271761596416,2016-10-11 00:36:51,https://t.co/GWuPufJjM5,Stochastic Alternating Direction Method of Multipliers with Variance Reduction for Nonconvex Optimization. (arXiv:… https://t.co/GWuPufJjM5,0,7," Abstract: In this work, we study the stochastic alternating direction method of multipliers (ADMM) method for optimizing nonconvex problems, and propose two classes of nonconvex stochastic ADMM with variance reduction. The first class is the nonconvex stochastic variance reduced gradient ADMM (SVRG-ADMM), which uses a multi-stage strategy to progressively reduce the variance of stochastic gradients. The second is the nonconvex stochastic average gradient ADMM (SAGA-ADMM), which additionally uses the old gradients estimated in the previous iteration. Theoretically, we analyze convergence of the SVRG-ADMM and SAGA-ADMM, and prove that they enjoy the iteration complexity bound of $O(1/\epsilon)$ to reach an $\epsilon$-stationary solution. In particular, we provide a general framework to analyze convergence and iteration complexity of the nonconvex stochastic ADMM with variance reduction. In addition, we prove that the simple stochastic ADMM (S-ADMM), in which the variance of the stochastic gradients is free, is divergent under some conditions. Finally, the experimental results on some real datasets back up our theoretical results. To the best of our knowledge, this is the first study of iteration complexity of the stochastic ADMM for the noncovex problems. "
785640270092181505,2016-10-11 00:36:51,https://t.co/MVV80rEKVc,Robust Bayesian Compressed sensing. (arXiv:1610.02807v1 [stat.ML]) https://t.co/MVV80rEKVc,0,7," Abstract: We consider the problem of robust compressed sensing whose objective is to recover a high-dimensional sparse signal from compressed measurements corrupted by outliers. A new sparse Bayesian learning method is developed for robust compressed sensing. The basic idea of the proposed method is to identify and remove the outliers from sparse signal recovery. To automatically identify the outliers, we employ a set of binary indicator hyperparameters to indicate which observations are outliers. These indicator hyperparameters are treated as random variables and assigned a beta process prior such that their values are confined to be binary. In addition, a Gaussian-inverse Gamma prior is imposed on the sparse signal to promote sparsity. Based on this hierarchical prior model, we develop a variational Bayesian method to estimate the indicator hyperparameters as well as the sparse signal. Simulation results show that the proposed method achieves a substantial performance improvement over existing robust compressed sensing techniques. "
785640268422844417,2016-10-11 00:36:51,https://t.co/BCLT5KlFug,Phase transitions and optimal algorithms in high-dimensional Gaussian mixture clustering. (arXiv:1610.02918v1 [sta… https://t.co/BCLT5KlFug,0,4," Abstract: We consider the problem of Gaussian mixture clustering in the high-dimensional limit where the data consists of $m$ points in $n$ dimensions, $n,m \rightarrow \infty$ and $\alpha = m/n$ stays finite. Using exact but non-rigorous methods from statistical physics, we determine the critical value of $\alpha$ and the distance between the clusters at which it becomes information-theoretically possible to reconstruct the membership into clusters better than chance. We also determine the accuracy achievable by the Bayes-optimal estimation algorithm. In particular, we find that when the number of clusters is sufficiently large, $r > 4 + 2 \sqrt{\alpha}$, there is a gap between the threshold for information-theoretically optimal performance and the threshold at which known algorithms succeed. "
785640266627739649,2016-10-11 00:36:50,https://t.co/feMKIK6mxA,Generative Adversarial Nets from a Density Ratio Estimation Perspective. (arXiv:1610.02920v1 [stat.ML]) https://t.co/feMKIK6mxA,8,31," Abstract: Generative adversarial networks (GANs) are successful deep generative models. GANs are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful. "
785640265025515520,2016-10-11 00:36:50,https://t.co/Jo6qnRq6wT,Low-rank Approximation and Dynamic Mode Decomposition. (arXiv:1610.02962v1 [stat.ML]) https://t.co/Jo6qnRq6wT,0,4," Abstract: Dynamic Mode Decomposition (DMD) has emerged as a powerful tool for analyzing the dynamics of non-linear systems from experimental datasets. Recently, several attempts have extended DMD to the context of low-rank approximations. This extension is of particular interest for reduced-order modeling in various applicative domains, e.g. for climate prediction, to study molecular dynamics or micro-electromechanical devices. This low-rank extension takes the form of a nonconvex optimization problem. To the best of our knowledge, only sub-optimal algorithms have been proposed in the literature to compute the solution of this problem. In this paper, we prove that there exists a closed-form optimal solution to this problem and design an effective algorithm to compute it based on Singular Value Decomposition (SVD). Based on this solution, we then propose efficient procedures for reduced-order modeling and for the identification of the the low-rank DMD modes and amplitudes. Experiments illustrates the gain in performance of the proposed algorithm compared to state-of-the-art techniques. "
785640263431680000,2016-10-11 00:36:49,https://t.co/uQqljZ64bF,Distributed Convex Optimization with Many Convex Constraints. (arXiv:1610.02967v1 [math.OC]) https://t.co/uQqljZ64bF,0,4," Abstract: We address the problem of solving convex optimization problems with many convex constraints in a distributed setting. Our approach is based on an extension of the alternating direction method of multipliers (ADMM) that recently gained a lot of attention in the Big Data context. Although it has been invented decades ago, ADMM so far can be applied only to unconstrained problems and problems with linear equality or inequality constraints. Our extension can handle arbitrary inequality constraints directly. It combines the ability of ADMM to solve convex optimization problems in a distributed setting with the ability of the Augmented Lagrangian method to solve constrained optimization problems, and as we show, it inherits the convergence guarantees of ADMM and the Augmented Lagrangian method. "
785640261124780036,2016-10-11 00:36:49,https://t.co/uGDUcrvmZi,Linear Hypothesis Testing in Dense High-Dimensional Linear Models. (arXiv:1610.02987v1 [stat.ME]) https://t.co/uGDUcrvmZi,0,3," Abstract: Providing asymptotically valid methods for testing general linear functions of the regression parameters in high-dimensional models is extremely challenging -- especially without making restrictive or unverifiable assumptions on the number of non-zero elements, i.e., the model sparsity. In this article, we propose a new methodology that transforms the original hypothesis into a moment condition and demonstrate that valid tests can be created without making any assumptions on the model sparsity. We formulate a restructured regression problem with the new features synthesized according to the null hypothesis directly; further, with the help of such new features, we have designed a valid test for the transformed moment condition. This construction enables us to test the null hypothesis, even if the original model cannot be estimated well. Although the linear tests in high dimensions are by nature very difficult to analyze we establish theoretical guarantees for Type I error control, allowing both the model and the vector representing the hypothesis to be non-sparse. The assumptions that are necessary to establish Type I error guarantees are shown to be weaker than the weakest known assumptions that are necessary to construct confidence intervals in the high-dimensional regression. Our methods are also shown to achieve certain optimality in detecting deviations from the null hypothesis. We demonstrate favorable finite-sample performance of the proposed methods, via both a numerical and a real data example. "
785640259342204932,2016-10-11 00:36:48,https://t.co/0flQG10Rvc,Latent Sequence Decompositions. (arXiv:1610.03035v1 [stat.ML]) https://t.co/0flQG10Rvc,0,2," Abstract: We present the Latent Sequence Decompositions (LSD) framework. LSD decomposes sequences with variable lengthed output units as a function of both the input sequence and the output sequence. We present a training algorithm which samples valid extensions and an approximate decoding algorithm. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9% WER compared to a character baseline of 14.8% WER. When combined with a convolutional network on the encoder, we achieve 9.2% WER. "
785640257731649536,2016-10-11 00:36:48,https://t.co/UkcHMiZjzW,Sketching Meets Random Projection in the Dual: A Provable Recovery Algorithm for Big and High-dimensional Data. (a… https://t.co/UkcHMiZjzW,1,5," Abstract: Sketching techniques have become popular for scaling up machine learning algorithms by reducing the sample size or dimensionality of massive data sets, while still maintaining the statistical power of big data. In this paper, we study sketching from an optimization point of view: we first show that the iterative Hessian sketch is an optimization process with preconditioning, and develop accelerated iterative Hessian sketch via the searching the conjugate direction; we then establish primal-dual connections between the Hessian sketch and dual random projection, and apply the preconditioned conjugate gradient approach on the dual problem, which leads to the accelerated iterative dual random projection methods. Finally to tackle the challenges from both large sample size and high-dimensionality, we propose the primal-dual sketch, which iteratively sketches the primal and dual formulations. We show that using a logarithmic number of calls to solvers of small scale problem, primal-dual sketch is able to recover the optimum of the original problem up to arbitrary precision. The proposed algorithms are validated via extensive experiments on synthetic and real data sets which complements our theoretical results. "
785640256053841920,2016-10-11 00:36:48,https://t.co/C8IX4hbkgW,Online and stochastic Douglas-Rachford splitting method for large scale machine learning. (arXiv:1308.4757v7 [cs.N… https://t.co/C8IX4hbkgW,0,2," Abstract: Online and stochastic learning has emerged as powerful tool in large scale optimization. In this work, we generalize the Douglas-Rachford splitting (DRs) method for minimizing composite functions to online and stochastic settings (to our best knowledge this is the first time DRs been generalized to sequential version). We first establish an $O(1/\sqrt{T})$ regret bound for batch DRs method. Then we proved that the online DRs splitting method enjoy an $O(1)$ regret bound and stochastic DRs splitting has a convergence rate of $O(1/\sqrt{T})$. The proof is simple and intuitive, and the results and technique can be served as a initiate for the research on the large scale machine learning employ the DRs method. Numerical experiments of the proposed method demonstrate the effectiveness of the online and stochastic update rule, and further confirm our regret and convergence analysis. "
785640254321590272,2016-10-11 00:36:47,https://t.co/Vq4lwSU54H,Online semi-parametric learning for inverse dynamics modeling. (arXiv:1603.05412v2 [math.OC] UPDATED) https://t.co/Vq4lwSU54H,0,5," Abstract: This paper presents a semi-parametric algorithm for online learning of a robot inverse dynamics model. It combines the strength of the parametric and non-parametric modeling. The former exploits the rigid body dynamics equa- tion, while the latter exploits a suitable kernel function. We provide an extensive comparison with other methods from the literature using real data from the iCub humanoid robot. In doing so we also compare two different techniques, namely cross validation and marginal likelihood optimization, for estimating the hyperparameters of the kernel function. "
785640252392214528,2016-10-11 00:36:47,https://t.co/zTAhx5w4Ag,Condorcet's Jury Theorem for Consensus Clustering and its Implications for Diversity. (arXiv:1604.07711v2 [stat.ML… https://t.co/zTAhx5w4Ag,0,2," Abstract: Condorcet's Jury Theorem has been invoked for ensemble classifiers to indicate that the combination of many classifiers can have better predictive performance than a single classifier. Such a theoretical underpinning is unknown for consensus clustering. This article extends Condorcet's Jury Theorem to the mean partition approach under the additional assumptions that a unique ground-truth partition exists and sample partitions are drawn from a sufficiently small ball containing the ground-truth. As an implication of practical relevance, we question the claim that the quality of consensus clustering depends on the diversity of the sample partitions. Instead, we conjecture that limiting the diversity of the mean partitions is necessary for controlling the quality. "
785640250815156224,2016-10-11 00:36:46,https://t.co/SgiZ2vfxkH,Fast Randomized Semi-Supervised Clustering. (arXiv:1605.06422v3 [cs.LG] UPDATED) https://t.co/SgiZ2vfxkH,0,3," Abstract: We consider the problem of clustering partially labeled data from a minimal number of randomly chosen pairwise comparisons between the items. We introduce an efficient local algorithm based on a power iteration of the non-backtracking operator and study its performance on a simple model. For the case of two clusters, we give bounds on the classification error and show that a small error can be achieved from $O(n)$ randomly chosen measurements, where $n$ is the number of items in the dataset. Our algorithm is therefore efficient both in terms of time and space complexities. We also investigate numerically the performance of the algorithm on synthetic and real world data. "
785640249158406144,2016-10-11 00:36:46,https://t.co/kvKH1YHrS5,Estimating mutual information in high dimensions via classification error. (arXiv:1606.05229v2 [stat.ML] UPDATED) https://t.co/kvKH1YHrS5,0,4," Abstract: Multivariate pattern analyses approaches in neuroimaging are fundamentally concerned with investigating the quantity and type of information processed by various regions of the human brain; typically, estimates of classification accuracy are used to quantify information. While a extensive and powerful library of methods can be applied to train and assess classifiers, it is not always clear how to use the resulting measures of classification performance to draw scientific conclusions: e.g. for the purpose of evaluating redundancy between brain regions. An additional confound for interpreting classification performance is the dependence of the error rate on the number and choice of distinct classes obtained for the classification task. In contrast, mutual information is a quantity defined independently of the experimental design, and has ideal properties for comparative analyses. Unfortunately, estimating the mutual information based on observations becomes statistically infeasible in high dimensions without some kind of assumption or prior. In this paper, we construct a novel classification-based estimator of mutual information based on high-dimensional asymptotics. We show that in a particular limiting regime, the mutual information is an invertible function of the expected $k$-class Bayes error. While the theory is based on a large-sample, high-dimensional limit, we demonstrate through simulations that our proposed estimator has superior performance to the alternatives in problems of moderate dimensionality. "
785640247531044865,2016-10-11 00:36:46,https://t.co/4m4jig32sv,Fast Algorithms for Demixing Sparse Signals from Nonlinear Observations. (arXiv:1608.01234v2 [stat.ML] UPDATED) https://t.co/4m4jig32sv,0,2," Abstract: We study the problem of demixing a pair of sparse signals from noisy, nonlinear observations of their superposition. Mathematically, we consider a nonlinear signal observation model, $y_i = g(a_i^Tx) + e_i, \ i=1,\ldots,m$, where $x = \Phi w+\Psi z$ denotes the superposition signal, $\Phi$ and $\Psi$ are orthonormal bases in $\mathbb{R}^n$, and $w, z\in\mathbb{R}^n$ are sparse coefficient vectors of the constituent signals, and $e_i$ represents the noise. Moreover, $g$ represents a nonlinear link function, and $a_i\in\mathbb{R}^n$ is the $i$-th row of the measurement matrix, $A\in\mathbb{R}^{m\times n}$. Problems of this nature arise in several applications ranging from astronomy, computer vision, and machine learning. In this paper, we make some concrete algorithmic progress for the above demixing problem. Specifically, we consider two scenarios: (i) the case when the demixing procedure has no knowledge of the link function, and (ii) the case when the demixing algorithm has perfect knowledge of the link function. In both cases, we provide fast algorithms for recovery of the constituents $w$ and $z$ from the observations. Moreover, we support these algorithms with a rigorous theoretical analysis, and derive (nearly) tight upper bounds on the sample complexity of the proposed algorithms for achieving stable recovery of the component signals. We also provide a range of numerical simulations to illustrate the performance of the proposed algorithms on both real and synthetic signals and images. "
785640245689720832,2016-10-11 00:36:45,https://t.co/v50mqCeW9j,Learning Bayesian Networks with Incomplete Data by Augmentation. (arXiv:1608.07734v2 [cs.AI] UPDATED) https://t.co/v50mqCeW9j,1,5," Abstract: We present new algorithms for learning Bayesian networks from data with missing values using a data augmentation approach. An exact Bayesian network learning algorithm is obtained by recasting the problem into a standard Bayesian network learning problem without missing data. To the best of our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large domains. We build on the exact method to create an approximate algorithm using a hill-climbing technique. This algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available. We perform a wide range of experiments to demonstrate the benefits of learning Bayesian networks with such new approach. "
785640243886260224,2016-10-11 00:36:45,https://t.co/jLoIWUQ2Fg,Theoretical Evaluation of Feature Selection Methods based on Mutual Information. (arXiv:1609.06575v2 [stat.ML] UPD… https://t.co/jLoIWUQ2Fg,1,9," Abstract: Feature selection methods are usually evaluated by wrapping specific classifiers and datasets in the evaluation process, resulting very often in unfair comparisons between methods. In this work, we develop a theoretical framework that allows obtaining the true feature ordering of two-dimensional sequential forward feature selection methods based on mutual information, which is independent of entropy or mutual information estimation methods, classifiers, or datasets, and leads to an undoubtful comparison of the methods. Moreover, the theoretical framework unveils problems intrinsic to some methods that are otherwise difficult to detect, namely inconsistencies in the construction of the objective function used to select the candidate features, due to various types of indeterminations and to the possibility of the entropy of continuous random variables taking null and negative values. "
785276798200324097,2016-10-10 00:32:33,https://t.co/XCKL4UWciu,High-dimensional inference in linear models: robustness and adaptivity to model sparsity. (arXiv:1610.02122v1 [sta… https://t.co/XCKL4UWciu,0,8," Abstract: In high-dimensional linear models, the sparsity assumption is typically made, stating that most of the model parameters have value equal to zero. Under the sparsity assumption, estimation and, recently, inference as well as the fundamental limits of detection have been well studied. However, in certain cases, sparsity assumption may be violated, and a large number of covariates can be expected to be associated with the response, indicating that possibly all, rather just a few, model parameters are different from zero. A natural example is a genome-wide gene expression profiling, where all genes are believed to affect a common disease marker. We show that the current inferential methods are sensitive to the sparsity assumption, and may in turn result in severe bias: lack of control of Type-I error is apparent once the model is not sparse. In this article, we propose a new inferential method, named CorrT, which is robust and adaptive to the sparsity assumption. CorrT is shown to have Type I error approaching the nominal level, regardless of how sparse or dense the model is. Specifically, the developed test is based on a moment condition induced by the hypothesis and the covariate structure of the model design. Such a construction circumvents the fundamental difficulty of accurately estimating non-sparse high-dimensional models. As a result, the proposed test guards against large estimation errors caused by potential absence of sparsity, and at the same time, adapts to the model sparsity. In fact, CorrT is also shown to be optimal whenever sparsity holds. Numerical experiments show favorable performance of CorrT compared to existing methods. We also apply CorrT to a real dataset and confirm some known discoveries related to HER2+ cancer patients and the gene-to-gene interaction. "
785276796338073600,2016-10-10 00:32:32,https://t.co/mc3FznrkDg,Stochastic Averaging for Constrained Optimization with Application to Online Resource Allocation. (arXiv:1610.0214… https://t.co/mc3FznrkDg,0,4," Abstract: Existing approaches to resource allocation for nowadays stochastic networks are challenged to meet fast convergence and tolerable delay requirements. In the era of data deluge and information explosion, the present paper leverages online learning advances to facilitate stochastic resource allocation tasks. By recognizing the central role of Lagrange multipliers, the underlying constrained optimization problem is formulated as a machine learning task involving both training and operational modes, with the goal of learning the sought multipliers in a fast and efficient manner. To this end, an order-optimal offline learning approach is developed first for batch training, and it is then generalized to the online setting with a procedure termed learn-and-adapt. The novel resource allocation protocol permeates benefits of stochastic approximation and statistical learning to obtain low-complexity online updates with learning errors close to the statistical accuracy limits, while still preserving adaptation performance, which in the stochastic network optimization context guarantees queue stability. Analysis and simulated tests demonstrate that the proposed data-driven approach improves the delay and convergence performance of existing resource allocation schemes. "
785276793834045440,2016-10-10 00:32:32,https://t.co/oxvLDGzMlC,Morphology Generation for Statistical Machine Translation using Deep Learning Techniques. (arXiv:1610.02209v1 [cs.… https://t.co/oxvLDGzMlC,0,3," Abstract: Morphology unbalanced languages remains a big challenge in the context of machine translation. In this paper, we propose to de-couple machine translation from morphology generation in order to better deal with the problem. We investigate the morphology simplification with a reasonable trade-off between expected gain and generation complexity. For the Chinese-Spanish task, optimum morphological simplification is in gender and number. For this purpose, we design a new classification architecture which, compared to other standard machine learning techniques, obtains the best results. This proposed neural-based architecture consists of several layers: an embedding, a convolutional followed by a recurrent neural network and, finally, ends with sigmoid and softmax layers. We obtain classification results over 98% accuracy in gender classification, over 93% in number classification, and an overall translation improvement of 0.7 METEOR. "
785276791820840960,2016-10-10 00:32:31,https://t.co/KLAKOFeHjE,Universal Clustering via Crowdsourcing. (arXiv:1610.02276v1 [cs.HC]) https://t.co/KLAKOFeHjE,0,2," Abstract: Consider unsupervised clustering of objects drawn from a discrete set, through the use of human intelligence available in crowdsourcing platforms. This paper defines and studies the problem of universal clustering using responses of crowd workers, without knowledge of worker reliability or task difficulty. We model stochastic worker response distributions by incorporating traits of memory for similar objects and traits of distance among differing objects. We are particularly interested in two limiting worker types---temporary workers who retain no memory of responses and long-term workers with memory. We first define clustering algorithms for these limiting cases and then integrate them into an algorithm for the unified worker model. We prove asymptotic consistency of the algorithms and establish sufficient conditions on the sample complexity of the algorithm. Converse arguments establish necessary conditions on sample complexity, proving that the defined algorithms are asymptotically order-optimal in cost. "
785276789354561537,2016-10-10 00:32:30,https://t.co/xHBVVOjlMD,The Generalized Reparameterization Gradient. (arXiv:1610.02287v1 [stat.ML]) https://t.co/xHBVVOjlMD,0,10," Abstract: The reparameterization gradient has become a widely used method to obtain Monte Carlo gradients to optimize the variational objective. However, this technique does not easily apply to commonly used distributions such as beta or gamma without further approximations, and most practical applications of the reparameterization gradient fit Gaussian distributions. In this paper, we introduce the generalized reparameterization gradient, a method that extends the reparameterization gradient to a wider class of variational distributions. Generalized reparameterizations use invertible transformations of the latent variables which lead to transformed distributions that weakly depend on the variational parameters. This results in new Monte Carlo gradients that combine reparameterization gradients and score function gradients. We demonstrate our approach on variational inference for two complex probabilistic models. The generalized reparameterization is effective: even a single sample from the variational distribution is enough to obtain a low-variance gradient. "
785276786791907328,2016-10-10 00:32:30,https://t.co/RqXxIzRPNE,Combining local and global smoothing in multivariate density estimation. (arXiv:1610.02372v1 [stat.ME]) https://t.co/RqXxIzRPNE,0,4, Abstract: Non-parametric estimation of a multivariate density estimation is tackled via a method which combines traditional local smoothing with a form of global smoothing but without imposing a rigid structure. Simulation work delivers encouraging indications on the effectiveness of the method. An application to density-based clustering illustrates a possible usage. 
785276783528665088,2016-10-10 00:32:29,https://t.co/2bbKAshoGg,Boltzmann-Machine Learning of Prior Distributions of Binarized Natural Images. (arXiv:1412.7012v3 [stat.ML] UPDATE… https://t.co/2bbKAshoGg,0,4," Abstract: Prior distributions of binarized natural images are learned by using a Boltzmann machine. According the results of this study, there emerges a structure with two sublattices in the interactions, and the nearest-neighbor and next-nearest-neighbor interactions correspondingly take two discriminative values, which reflects the individual characteristics of the three sets of pictures that we process. Meanwhile, in a longer spatial scale, a longer-range, although still rapidly decaying, ferromagnetic interaction commonly appears in all cases. The characteristic length scale of the interactions is universally up to approximately four lattice spacings $\xi \approx 4$. These results are derived by using the mean-field method, which effectively reduces the computational time required in a Boltzmann machine. An improved mean-field method called the Bethe approximation also gives the same results, as well as the Monte Carlo method does for small size images. These reinforce the validity of our analysis and findings. Relations to criticality, frustration, and simple-cell receptive fields are also discussed. "
785276779858685952,2016-10-10 00:32:28,https://t.co/pfY8eZLVAG,Fast Discrete Distribution Clustering Using Wasserstein Barycenter with Sparse Support. (arXiv:1510.00012v3 [stat.… https://t.co/pfY8eZLVAG,0,4," Abstract: In a variety of research areas, the weighted bag of vectors and the histogram are widely used descriptors for complex objects. Both can be expressed as discrete distributions. D2-clustering pursues the minimum total within-cluster variation for a set of discrete distributions subject to the Kantorovich-Wasserstein metric. D2-clustering has a severe scalability issue, the bottleneck being the computation of a centroid distribution, called Wasserstein barycenter, that minimizes its sum of squared distances to the cluster members. In this paper, we develop a modified Bregman ADMM approach for computing the approximate discrete Wasserstein barycenter of large clusters. In the case when the support points of the barycenters are unknown and of low cardinality, our method achieves high accuracy empirically at a much reduced computational cost. The strengths and weaknesses of our method and its alternatives are examined through experiments; and scenarios for their respective usage are recommended. Moreover, we develop both serial and parallelized versions of the algorithm. By experimenting with large-scale data, we demonstrate the computational efficiency of the new methods and investigate their convergence properties and numerical stability. The clustering results obtained on several datasets in different domains are highly competitive in comparison with some widely used methods' in the corresponding areas. "
785276775681126404,2016-10-10 00:32:27,https://t.co/hNH5he3dJD,Pseudo-Bayesian Robust PCA: Algorithms and Analyses. (arXiv:1512.02188v2 [cs.CV] UPDATED) https://t.co/hNH5he3dJD,2,7," Abstract: Commonly used in computer vision and other applications, robust PCA represents an algorithmic attempt to reduce the sensitivity of classical PCA to outliers. The basic idea is to learn a decomposition of some data matrix of interest into low rank and sparse components, the latter representing unwanted outliers. Although the resulting optimization problem is typically NP-hard, convex relaxations provide a computationally-expedient alternative with theoretical support. However, in practical regimes performance guarantees break down and a variety of non-convex alternatives, including Bayesian-inspired models, have been proposed to boost estimation quality. Unfortunately though, without additional a priori knowledge none of these methods can significantly expand the critical operational range such that exact principal subspace recovery is possible. Into this mix we propose a novel pseudo-Bayesian algorithm that explicitly compensates for design weaknesses in many existing non-convex approaches leading to state-of-the-art performance with a sound analytical foundation. Surprisingly, our algorithm can even outperform convex matrix completion despite the fact that the latter is provided with perfect knowledge of which entries are not corrupted. "
785276773722357760,2016-10-10 00:32:27,https://t.co/WoV8DMKU3w,Data-driven Rank Breaking for Efficient Rank Aggregation. (arXiv:1601.05495v2 [cs.LG] UPDATED) https://t.co/WoV8DMKU3w,0,2," Abstract: Rank aggregation systems collect ordinal preferences from individuals to produce a global ranking that represents the social preference. Rank-breaking is a common practice to reduce the computational complexity of learning the global ranking. The individual preferences are broken into pairwise comparisons and applied to efficient algorithms tailored for independent paired comparisons. However, due to the ignored dependencies in the data, naive rank-breaking approaches can result in inconsistent estimates. The key idea to produce accurate and consistent estimates is to treat the pairwise comparisons unequally, depending on the topology of the collected data. In this paper, we provide the optimal rank-breaking estimator, which not only achieves consistency but also achieves the best error bound. This allows us to characterize the fundamental tradeoff between accuracy and complexity. Further, the analysis identifies how the accuracy depends on the spectral gap of a corresponding comparison graph. "
785276771428171776,2016-10-10 00:32:26,https://t.co/1wdQ8uAi1Q,Online Isotonic Regression. (arXiv:1603.04190v2 [cs.LG] UPDATED) https://t.co/1wdQ8uAi1Q,0,2," Abstract: We consider the online version of the isotonic regression problem. Given a set of linearly ordered points (e.g., on the real line), the learner must predict labels sequentially at adversarially chosen positions and is evaluated by her total squared loss compared against the best isotonic (non-decreasing) function in hindsight. We survey several standard online learning algorithms and show that none of them achieve the optimal regret exponent; in fact, most of them (including Online Gradient Descent, Follow the Leader and Exponential Weights) incur linear regret. We then prove that the Exponential Weights algorithm played over a covering net of isotonic functions has a regret bounded by $O\big(T^{1/3} \log^{2/3}(T)\big)$ and present a matching $\Omega(T^{1/3})$ lower bound on regret. We provide a computationally efficient version of this algorithm. We also analyze the noise-free case, in which the revealed labels are isotonic, and show that the bound can be improved to $O(\log T)$ or even to $O(1)$ (when the labels are revealed in isotonic order). Finally, we extend the analysis beyond squared loss and give bounds for entropic loss and absolute loss. "
785276767317688325,2016-10-10 00:32:25,https://t.co/fFekva5gKs,Convolutional Neural Networks Analyzed via Convolutional Sparse Coding. (arXiv:1607.08194v3 [stat.ML] UPDATED) https://t.co/fFekva5gKs,2,9," Abstract: Convolutional neural networks (CNN) have led to many state-of-the-art results spanning through various fields. However, a clear and profound theoretical understanding of the forward pass, the core algorithm of CNN, is still lacking. In parallel, within the wide field of sparse approximation, Convolutional Sparse Coding (CSC) has gained increasing attention in recent years. A theoretical study of this model was recently conducted, establishing it as a reliable and stable alternative to the commonly practiced patch-based processing. Herein, we propose a novel multi-layer model, ML-CSC, in which signals are assumed to emerge from a cascade of CSC layers. This is shown to be tightly connected to CNN, so much so that the forward pass of the CNN is in fact the thresholding pursuit serving the ML-CSC model. This connection brings a fresh view to CNN, as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network, and their stable estimation, all guaranteed under simple local sparsity conditions. Lastly, identifying the weaknesses in the above pursuit scheme, we propose an alternative to the forward pass, which is connected to deconvolutional, recurrent and residual networks, and has better theoretical guarantees. "
785276762209062915,2016-10-10 00:32:24,https://t.co/fxiqMk81Ey,Clustering Mixed Datasets Using Homogeneity Analysis with Applications to Big Data. (arXiv:1608.04961v2 [stat.ML] … https://t.co/fxiqMk81Ey,0,4," Abstract: Clustering datasets with a mix of continuous and categorical attributes is encountered routinely by data analysts. This work presents a method for clustering such datasets using Homogeneity Analysis. An optimal Euclidean representation of mixed datasets is obtained using Homogeneity Analysis. This representation is then clustered. The clustering solutions from this method are compared to the clustering solutions obtained using the method based on the Gower distance that is popularly used with such datasets. This comparison is made on datasets that have been the subject of other research investigations. The Homogeneity Analysis solution is an eigenvalue based solution. The eigenvalues are used to produce the optimal Euclidean representation. Even with a single eigenvalue, the Homogeneity Analysis based solution performed better than the method based on the Gower distance. Extending the solution to use multiple eigenvalues from the Homogeneity Analysis solution is illustrated on real world datasets. This method can be used in conjunction with the mini-batch K-Means algorithm to cluster large datasets. This is illustrated on a real world dataset. The relevant theory from Homogeneity Analysis is presented. "
784189154645123072,2016-10-07 00:30:38,https://t.co/LXiHAiYv48,Binary classification of multi-channel EEG records based on the $\epsilon$-complexity of continuous vector functio… https://t.co/LXiHAiYv48,1,4, Abstract: A methodology for binary classification of EEG records which correspond to different mental states is proposed. This model-free methodology is based on our theory of the $\epsilon$-complexity of continuous functions which is extended here (see Appendix) to the case of vector functions. This extension permits us to handle multichannel EEG recordings. The essence of the methodology is to use the $\epsilon$-complexity coefficients as features to classify (using well known classifiers) different types of vector functions representing EEG-records corresponding to different types of mental states. We apply our methodology to the problem of classification of multichannel EEG-records related to a group of healthy adolescents and a group of adolescents with schizophrenia. We found that our methodology permits accurate classification of the data in the four-dimensional feather space of the $\epsilon$-complexity coefficients. 
784189151151329280,2016-10-07 00:30:37,https://t.co/WfDvZsQhxK,Learning Protein Dynamics with Metastable Switching Systems. (arXiv:1610.01642v1 [stat.ML]) https://t.co/WfDvZsQhxK,1,4," Abstract: We introduce a machine learning approach for extracting fine-grained representations of protein evolution from molecular dynamics datasets. Metastable switching linear dynamical systems extend standard switching models with a physically-inspired stability constraint. This constraint enables the learning of nuanced representations of protein dynamics that closely match physical reality. We derive an EM algorithm for learning, where the E-step extends the forward-backward algorithm for HMMs and the M-step requires the solution of large biconvex optimization problems. We construct an approximate semidefinite program solver based on the Frank-Wolfe algorithm and use it to solve the M-step. We apply our EM algorithm to learn accurate dynamics from large simulation datasets for the opioid peptide met-enkephalin and the proto-oncogene Src-kinase. Our learned models demonstrate significant improvements in temporal coherence over HMMs and standard switching models for met-enkephalin, and sample transition paths (possibly useful in rational drug design) for Src-kinase. "
784189149129674752,2016-10-07 00:30:37,https://t.co/wBszEwBCAe,Understanding intermediate layers using linear classifier probes. (arXiv:1610.01644v1 [stat.ML]) https://t.co/wBszEwBCAe,3,10," Abstract: Neural network models have a reputation for being black boxes. We propose a new method to understand better the roles and dynamics of the intermediate layers. This has direct consequences on the design of such models and it enables the expert to be able to justify certain heuristics (such as the auxiliary heads in the Inception model). Our method uses linear classifiers, referred to as ""probes"", where a probe can only use the hidden units of a given intermediate layer as discriminating features. Moreover, these probes cannot affect the training phase of a model, and they are generally added after training. They allow the user to visualize the state of the model at multiple steps of training. We demonstrate how this can be used to develop a better intuition about a known model and to diagnose potential problems. "
784189146516652032,2016-10-07 00:30:36,https://t.co/UqqVA9LFNo,Generalized Inverse Classification. (arXiv:1610.01675v1 [cs.LG]) https://t.co/UqqVA9LFNo,0,4," Abstract: Inverse classification is the process of perturbing an instance in a meaningful way such that it is more likely to conform to a specific class. Historical methods that address such a problem are often framed to leverage only a single classifier, or specific set of classifiers. These works are often accompanied by naive assumptions. In this work we propose generalized inverse classification (GIC), which avoids restricting the classification model that can be used. We incorporate this formulation into a refined framework in which GIC takes place. Under this framework, GIC operates on features that are immediately actionable. Each change incurs an individual cost, either linear or non-linear. Such changes are subjected to occur within a specified level of cumulative change (budget). Furthermore, our framework incorporates the estimation of features that change as a consequence of direct actions taken (indirectly changeable features). To solve such a problem, we propose three real-valued heuristic-based methods and two sensitivity analysis-based comparison methods, each of which is evaluated on two freely available real-world datasets. Our results demonstrate the validity and benefits of our formulation, framework, and methods. "
784189144452984833,2016-10-07 00:30:36,https://t.co/7KkkNomVMV,Automatic Sleep Stage Scoring with Single-Channel EEG Using Convolutional Neural Networks. (arXiv:1610.01683v1 [st… https://t.co/7KkkNomVMV,2,3," Abstract: We used convolutional neural networks (CNNs) for automatic sleep stage scoring based on single-channel electroencephalography (EEG) to learn task-specific filters for classification without using prior domain knowledge. We used an openly available dataset from 20 healthy young adults for evaluation and applied 20-fold cross-validation. We used class-balanced random sampling within the stochastic gradient descent (SGD) optimization of the CNN to avoid skewed performance in favor of the most represented sleep stages. We achieved high mean F1-score (81%, range 79-83%), mean accuracy across individual sleep stages (82%, range 80-84%) and overall accuracy (74%, range 71-76%) over all subjects. By analyzing and visualizing the filters that our CNN learns, we found that rules learned by the filters correspond to sleep scoring criteria in the American Academy of Sleep Medicine (AASM) manual that human experts follow. Our method's performance is balanced across classes and our results are comparable to state-of-the-art methods with hand-engineered features. We show that, without using prior domain knowledge, a CNN can automatically learn to distinguish among different normal sleep stages. "
784189142536249346,2016-10-07 00:30:35,https://t.co/RSVzO1UPCl,Sampled Fictitious Play is Hannan Consistent. (arXiv:1610.01687v1 [cs.GT]) https://t.co/RSVzO1UPCl,0,2," Abstract: Fictitious play is a simple and widely studied adaptive heuristic for playing repeated games. It is well known that fictitious play fails to be Hannan consistent. Several variants of fictitious play including regret matching, generalized regret matching and smooth fictitious play, are known to be Hannan consistent. In this note, we consider sampled fictitious play: at each round, the player samples past times and plays the best response to previous moves of other players at the sampled time points. We show that sampled fictitious play, using Bernoulli sampling, is Hannan consistent. Unlike several existing Hannan consistency proofs that rely on concentration of measure results, ours instead uses anti-concentration results from Littlewood-Offord theory. "
784189139965054976,2016-10-07 00:30:35,https://t.co/0ojMNf4Y9w,Human Decision-Making under Limited Time. (arXiv:1610.01698v1 [stat.ML]) https://t.co/0ojMNf4Y9w,1,5," Abstract: Subjective expected utility theory assumes that decision-makers possess unlimited computational resources to reason about their choices; however, virtually all decisions in everyday life are made under resource constraints - i.e. decision-makers are bounded in their rationality. Here we experimentally tested the predictions made by a formalization of bounded rationality based on ideas from statistical mechanics and information-theory. We systematically tested human subjects in their ability to solve combinatorial puzzles under different time limitations. We found that our bounded-rational model accounts well for the data. The decomposition of the fitted model parameter into the subjects' expected utility function and resource parameter provide interesting insight into the subjects' information capacity limits. Our results confirm that humans gradually fall back on their learned prior choice patterns when confronted with increasing resource limitations. "
784189137607884800,2016-10-07 00:30:34,https://t.co/7ship0EEhP,A Methodology for Customizing Clinical Tests for Esophageal Cancer based on Patient Preferences. (arXiv:1610.01712… https://t.co/7ship0EEhP,0,3," Abstract: Tests for Esophageal cancer can be expensive, uncomfortable and can have side effects. For many patients, we can predict non-existence of disease with 100% certainty, just using demographics, lifestyle, and medical history information. Our objective is to devise a general methodology for customizing tests using user preferences so that expensive or uncomfortable tests can be avoided. We propose to use classifiers trained from electronic health records (EHR) for selection of tests. The key idea is to design classifiers with 100% false normal rates, possibly at the cost higher false abnormals. We compare Naive Bayes classification (NB), Random Forests (RF), Support Vector Machines (SVM) and Logistic Regression (LR), and find kernel Logistic regression to be most suitable for the task. We propose an algorithm for finding the best probability threshold for kernel LR, based on test set accuracy. Using the proposed algorithm, we describe schemes for selecting tests, which appear as features in the automatic classification algorithm, using preferences on costs and discomfort of the users. We test our methodology with EHRs collected for more than 3000 patients, as a part of project carried out by a reputed hospital in Mumbai, India. Kernel SVM and kernel LR with a polynomial kernel of degree 3, yields an accuracy of 99.8% and sensitivity 100%, without the MP features, i.e. using only clinical tests. We demonstrate our test selection algorithm using two case studies, one using cost of clinical tests, and other using ""discomfort"" values for clinical tests. We compute the test sets corresponding to the lowest false abnormals for each criterion described above, using exhaustive enumeration of 15 clinical tests. The sets turn out to different, substantiating our claim that one can customize test sets based on user preferences. "
784189134785175553,2016-10-07 00:30:33,https://t.co/RAckpeimms,Constrained Maximum Correntropy Adaptive Filtering. (arXiv:1610.01766v1 [stat.ML]) https://t.co/RAckpeimms,0,4," Abstract: Constrained adaptive filtering algorithms inculding constrained least mean square (CLMS), constrained affine projection (CAP) and constrained recursive least squares (CRLS) have been extensively studied in many applications. Most existing constrained adaptive filtering algorithms are developed under mean square error (MSE) criterion, which is an ideal optimality criterion under Gaussian noises. This assumption however fails to model the behavior of non-Gaussian noises found in practice. Motivated by the robustness and simplicity of maximum correntropy criterion (MCC) in non-Gaussian impulsive noises, this paper proposes a new adaptive filtering algorithm called constrained maximum correntropy criterion (CMCC). Specifically, CMCC incorporates a linear constraint into a MCC filter to solve a constrained optimization problem explicitly. The proposed adaptive filtering algorithm is easy to implement and has low computational complexity, and in terms of convergence accuracy (say lower mean square deviation) and stability, can significantly outperform those MSE based constrained adaptive algorithms in presence of heavy-tailed impulsive noises. Additionally, the mean square convergence behaviors are studied under energy conservation relation, and a sufficient condition to ensure the mean square convergence and the steady-state mean square deviation (MSD) of the proposed algorithm are obtained. Simulation results confirm the theoretical predictions under both Gaussian and non- Gaussian noises, and demonstrate the excellent performance of the novel algorithm by comparing it with other conventional methods. "
784189131811323904,2016-10-07 00:30:33,https://t.co/H9rCHf67Uk,"Go With the Flow, on Jupiter and Snow. Coherence From Video Data without Trajectories. (arXiv:1610.01857v1 [physic… https://t.co/H9rCHf67Uk",0,4," Abstract: Viewing a data set such as the clouds of Jupiter, coherence is readily apparent to human observers, especially the Great Red Spot, but also other great storms and persistent structures. There are now many different definitions and perspectives mathematically describing coherent structures, but we will take an image processing perspective here. We describe an image processing perspective inference of coherent sets from a fluidic system directly from image data, without attempting to first model underlying flow fields, related to a concept in image processing called motion tracking. In contrast to standard spectral methods for image processing which are generally related to a symmetric affinity matrix, leading to standard spectral graph theory, we need a not symmetric affinity which arises naturally from the underlying arrow of time. We develop an anisotropic, directed diffusion operator corresponding to flow on a directed graph, from a directed affinity matrix developed with coherence in mind, and corresponding spectral graph theory from the graph Laplacian. Our examples will include partitioning the weather and cloud structures of Jupiter, and a local to Potsdam, N.Y. lake-effect snow event on Earth, as well as the the benchmark test double-gyre system. "
784189128866947072,2016-10-07 00:30:32,https://t.co/QMrLTH72M2,Connecting Generative Adversarial Networks and Actor-Critic Methods. (arXiv:1610.01945v1 [cs.LG]) https://t.co/QMrLTH72M2,6,35," Abstract: Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities. "
784189126480433152,2016-10-07 00:30:31,https://t.co/dCGYyVfTLN,Efficient L1-Norm Principal-Component Analysis via Bit Flipping. (arXiv:1610.01959v1 [cs.DS]) https://t.co/dCGYyVfTLN,0,4," Abstract: It was shown recently that the $K$ L1-norm principal components (L1-PCs) of a real-valued data matrix $\mathbf X \in \mathbb R^{D \times N}$ ($N$ data samples of $D$ dimensions) can be exactly calculated with cost $\mathcal{O}(2^{NK})$ or, when advantageous, $\mathcal{O}(N^{dK - K + 1})$ where $d=\mathrm{rank}(\mathbf X)$, $K<d$ [1],[2]. In applications where $\mathbf X$ is large (e.g., ""big"" data of large $N$ and/or ""heavy"" data of large $d$), these costs are prohibitive. In this work, we present a novel suboptimal algorithm for the calculation of the $K < d$ L1-PCs of $\mathbf X$ of cost $\mathcal O(ND \mathrm{min} \{ N,D\} + N^2(K^4 + dK^2) + dNK^3)$, which is comparable to that of standard (L2-norm) PC analysis. Our theoretical and experimental studies show that the proposed algorithm calculates the exact optimal L1-PCs with high frequency and achieves higher value in the L1-PC optimization metric than any known alternative algorithm of comparable computational cost. The superiority of the calculated L1-PCs over standard L2-PCs (singular vectors) in characterizing potentially faulty data/measurements is demonstrated with experiments on data dimensionality reduction and disease diagnosis from genomic data. "
784189123850543104,2016-10-07 00:30:31,https://t.co/Ijb030Tjdr,Regularized Dynamic Boltzmann Machine with Delay Pruning for Unsupervised Learning of Temporal Sequences. (arXiv:1… https://t.co/Ijb030Tjdr,2,10," Abstract: We introduce Delay Pruning, a simple yet powerful technique to regularize dynamic Boltzmann machines (DyBM). The recently introduced DyBM provides a particularly structured Boltzmann machine, as a generative model of a multi-dimensional time-series. This Boltzmann machine can have infinitely many layers of units but allows exact inference and learning based on its biologically motivated structure. DyBM uses the idea of conduction delays in the form of fixed length first-in first-out (FIFO) queues, with a neuron connected to another via this FIFO queue, and spikes from a pre-synaptic neuron travel along the queue to the post-synaptic neuron with a constant period of delay. Here, we present Delay Pruning as a mechanism to prune the lengths of the FIFO queues (making them zero) by setting some delay lengths to one with a fixed probability, and finally selecting the best performing model with fixed delays. The uniqueness of structure and a non-sampling based learning rule in DyBM, make the application of previously proposed regularization techniques like Dropout or DropConnect difficult, leading to poor generalization. First, we evaluate the performance of Delay Pruning to let DyBM learn a multidimensional temporal sequence generated by a Markov chain. Finally, we show the effectiveness of delay pruning in learning high dimensional sequences using the moving MNIST dataset, and compare it with Dropout and DropConnect methods. "
784189117911404544,2016-10-07 00:30:29,https://t.co/7VUjtmWNiu,How much does your data exploration overfit? Controlling bias via information usage. (arXiv:1511.05219v2 [stat.ML]… https://t.co/7VUjtmWNiu,1,10," Abstract: Modern data is messy and high-dimensional, and it is often not clear a priori what are the right questions to ask. Instead, the analyst typically needs to use the data to search for interesting analyses to perform and hypotheses to test. This is an adaptive process, where the choice of analysis to be performed next depends on the results of the previous analyses on the same data. Ultimately, which results are reported can be heavily influenced by the data. It is widely recognized that this process, even if well-intentioned, can lead to biases and false discoveries, contributing to the crisis of reproducibility in science. But while %the adaptive nature of exploration any data-exploration renders standard statistical theory invalid, experience suggests that different types of exploratory analysis can lead to disparate levels of bias, and the degree of bias also depends on the particulars of the data set. In this paper, we propose a general information usage framework to quantify and provably bound the bias and other error metrics of an arbitrary exploratory analysis. We prove that our mutual information based bound is tight in natural settings, and then use it to give rigorous insights into when commonly used procedures do or do not lead to substantially biased estimation. Through the lens of information usage, we analyze the bias of specific exploration procedures such as filtering, rank selection and clustering. Our general framework also naturally motivates randomization techniques that provably reduces exploration bias while preserving the utility of the data analysis. We discuss the connections between our approach and related ideas from differential privacy and blinded data analysis, and supplement our results with illustrative simulations. "
784189115721977856,2016-10-07 00:30:29,https://t.co/DFZHqhspTo,Gamma Belief Networks. (arXiv:1512.03081v2 [stat.ML] UPDATED) https://t.co/DFZHqhspTo,5,20," Abstract: To infer multilayer deep representations of high-dimensional discrete and nonnegative real vectors, we propose an augmentable gamma belief network (GBN) that factorizes each of its hidden layers into the product of a sparse connection weight matrix and the nonnegative real hidden units of the next layer. The GBN's hidden layers are jointly trained with an upward-downward Gibbs sampler that solves each layer with the same subroutine. The gamma-negative binomial process combined with a layer-wise training strategy allows inferring the width of each layer given a fixed budget on the width of the first layer. Example results illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the GBN can add more layers to improve its performance in both unsupervisedly extracting features and predicting heldout data. For exploratory data analysis, we extract trees and subnetworks from the learned deep network to visualize how the very specific factors discovered at the first hidden layer and the increasingly more general factors discovered at deeper hidden layers are related to each other, and we generate synthetic data by propagating random variables through the deep network from the top hidden layer back to the bottom data layer. "
784189111976558597,2016-10-07 00:30:28,https://t.co/ompTsHgsHZ,Global optimization of factor models using alternating minimization. (arXiv:1604.04942v2 [stat.ML] UPDATED) https://t.co/ompTsHgsHZ,0,4," Abstract: Learning new representations in machine learning is often tackled using a factorization of the data. For many such problems, including sparse coding and matrix completion, learning these factorizations can be difficult, in terms of efficiency and to guarantee that the solution is a global minimum. Recently, a general class of objectives have been introduced, called induced regularized factor models (RFMs), which have an induced convex form that enables global optimization. Though attractive theoretically, this induced form is impractical, particularly for large or growing datasets. In this work, we investigate the use of a practical alternating minimization algorithms for induced RFMs, that ensure convergence to global optima. We characterize the stationary points of these models, and, using these insights, highlight practical choices for the objectives. We then provide theoretical and empirical evidence that alternating minimization, from a random initialization, converges to global minima for a large subclass of induced RFMs. In particular, we prove that induced RFMs do not have degenerate saddlepoints and that local minima are actually global minima. Finally, we provide an extensive investigation into practical optimization choices for using alternating minimization for induced RFMs, for both batch and stochastic gradient descent. "
784189108692381696,2016-10-07 00:30:27,https://t.co/kMt8FhsgmJ,Unreasonable Effectiveness of Learning Neural Networks: From Accessible States and Robust Ensembles to Basic Algor… https://t.co/kMt8FhsgmJ,2,9," Abstract: In artificial neural networks, learning from data is a computationally demanding task in which a large number of connection weights are iteratively tuned through stochastic-gradient-based heuristic processes over a cost-function. It is not well understood how learning occurs in these systems, in particular how they avoid getting trapped in configurations with poor computational performance. Here we study the difficult case of networks with discrete weights, where the optimization landscape is very rough even for simple architectures, and provide theoretical and numerical evidence of the existence of rare - but extremely dense and accessible - regions of configurations in the network weight space. We define a novel measure, which we call the ""robust ensemble"" (RE), which suppresses trapping by isolated configurations and amplifies the role of these dense regions. We analytically compute the RE in some exactly solvable models, and also provide a general algorithmic scheme which is straightforward to implement: define a cost-function given by a sum of a finite number of replicas of the original cost-function, with a constraint centering the replicas around a driving assignment. To illustrate this, we derive several powerful new algorithms, ranging from Markov Chains to message passing to gradient descent processes, where the algorithms target the robust dense states, resulting in substantial improvements in performance. The weak dependence on the number of precision bits of the weights leads us to conjecture that very similar reasoning applies to more conventional neural networks. Analogous algorithmic schemes can also be applied to other optimization problems. "
784189106003771392,2016-10-07 00:30:27,https://t.co/JJH5WuDmyB,An Iterative Smoothing Algorithm for Regression with Structured Sparsity. (arXiv:1605.09658v2 [stat.ML] UPDATED) https://t.co/JJH5WuDmyB,1,5," Abstract: In the context high-dimensionnal predictive models, we consider the problem of optimizing the sum of a smooth convex loss and a non-smooth convex penalty, whose proximal operator is known, and a non-smooth convex structured penalties such as total variation, or overlapping group lasso. We propose to smooth the structured penalty, since it allows a generic framework in which a large range of non-smooth convex structured penalties can be minimized without computing their proximal operators that are either not known or expensive to compute. The problem can be minimized with an accelerated proximal gradient method to benefit of (non-smoothed) sparsity-inducing penalties. We propose an expression of the duality gap to control the convergence of the global non-smooth problem. This expression is applicable to a large range of structured penalties. However, smoothing methods have many limitations that the proposed solver aims to overcome. Therefore, we propose a continuation algorithm, called CONESTA, that dynamically generates a decreasing sequence of smoothing parameters in order to maintain the optimal convergence speed towards any globally desired precision. At each continuation step, the aforementioned duality gap provides the current error and thus the next smaller prescribed precision. Given this precision, we propose a expression to calculate the optimal smoothing parameter, that minimizes the number of iterations to reach such precision. We demonstrate that CONESTA achieves an improved convergence rate compared to classical (without continuation) proximal gradient smoothing. Moreover, experiments conducted on both simulated and high-dimensional neuroimaging (MRI) data, exhibit that CONESTA significantly outperforms the excessive gap method, ADMM, classical proximal gradient smoothing and inexact FISTA in terms of convergence speed and/or precision of the solution. "
784189103269183488,2016-10-07 00:30:26,https://t.co/zsM0HO5ehK,Direct Feedback Alignment Provides Learning in Deep Neural Networks. (arXiv:1609.01596v3 [stat.ML] UPDATED) https://t.co/zsM0HO5ehK,3,7," Abstract: Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task. "
783827993919643648,2016-10-06 00:35:31,https://t.co/aj9csTD3yi,"Ensemble Validation: Selectivity has a Price, but Variety is Free. (arXiv:1610.01234v1 [stat.ML]) https://t.co/aj9csTD3yi",1,5," Abstract: If classifiers are selected from a hypothesis class to form an ensemble, bounds on average error rate over the selected classifiers include a component for selectivity, which grows as the fraction of hypothesis classifiers selected for the ensemble shrinks, and a component for variety, which grows with the size of the hypothesis class or in-sample data set. We show that the component for selectivity asymptotically dominates the component for variety, meaning that variety is essentially free. "
783827992044789760,2016-10-06 00:35:30,https://t.co/pBlfvNtVGt,"On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products. (arXiv:1610.01256… https://t.co/pBlfvNtVGt",3,7," Abstract: Machine learning algorithms are increasingly influencing our decisions and interacting with us in all parts of our daily lives. Therefore, just like for power plants, highways, and a myriad of other engineered socio-technical systems, we must consider the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in the machine learning context; in this paper, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences and data products, finding that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. In particular, we note an emerging dichotomy of applications: ones in which safety is important and risk minimization is not the complete story (we name these Type A applications), and ones in which safety is not so critical and risk minimization is sufficient (we name these Type B applications). Finally, we discuss how four different strategies for achieving safety in engineering (inherently safe design, safety reserves, safe fail, and procedural safeguards) can be mapped to the machine learning context through interpretability and causality of predictive models, objectives beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data. "
783827989654016000,2016-10-06 00:35:30,https://t.co/Cn4kLI1LNZ,Solving Heterogeneous Estimating Equations with Gradient Forests. (arXiv:1610.01271v1 [stat.ME]) https://t.co/Cn4kLI1LNZ,1,3," Abstract: Forest-based methods are being used in an increasing variety of statistical tasks, including causal inference, survival analysis, and quantile regression. Extending forest-based methods to these new statistical settings requires specifying tree-growing algorithms that are targeted to the task at hand, and the ad-hoc design of such algorithms can require considerable effort. In this paper, we develop a unified framework for the design of fast tree-growing procedures for tasks that can be characterized by heterogeneous estimating equations. The resulting gradient forest consists of trees grown by recursively applying a pre-processing step where we label each observation with gradient-based pseudo-outcomes, followed by a regression step that runs a standard CART regression split on these pseudo-outcomes. We apply our framework to two important statistical problems, non-parametric quantile regression and heterogeneous treatment effect estimation via instrumental variables, and we show that the resulting procedures considerably outperform baseline forests whose splitting rules do not take into account the statistical question at hand. Finally, we prove the consistency of gradient forests, and establish a central limit theorem. Our method will be available as an R-package, gradientForest, which draws from the ranger package for random forests. "
783827987766607873,2016-10-06 00:35:29,https://t.co/dge28PK6zW,Decentralized Topic Modelling with Latent Dirichlet Allocation. (arXiv:1610.01417v1 [stat.ML]) https://t.co/dge28PK6zW,0,8," Abstract: Privacy preserving networks can be modelled as decentralized networks (e.g., sensors, connected objects, smartphones), where communication between nodes of the network is not controlled by an all-knowing, central node. For this type of networks, the main issue is to gather/learn global information on the network (e.g., by optimizing a global cost function) while keeping the (sensitive) information at each node. In this work, we focus on text information that agents do not want to share (e.g., text messages, emails, confidential reports). We use recent advances on decentralized optimization and topic models to infer topics from a graph with limited communication. We propose a method to adapt latent Dirichlet allocation (LDA) model to decentralized optimization and show on synthetic data that we still recover similar parameters and similar performance at each node than with stochastic methods accessing to the whole information in the graph. "
783827985921105920,2016-10-06 00:35:29,https://t.co/X6VbeZV8NH,Non-Parametric Cluster Significance Testing with Reference to a Unimodal Null Distribution. (arXiv:1610.01424v1 [s… https://t.co/X6VbeZV8NH,0,2," Abstract: Cluster analysis is an unsupervised learning strategy that can be employed to identify subgroups of observations in data sets of unknown structure. This strategy is particularly useful for analyzing high-dimensional data such as microarray gene expression data. Many clustering methods are available, but it is challenging to determine if the identified clusters represent distinct subgroups. We propose a novel strategy to investigate the significance of identified clusters by comparing the within- cluster sum of squares from the original data to that produced by clustering an appropriate unimodal null distribution. The null distribution we present for this problem uses kernel density estimation and thus does not require that the data follow any particular distribution. We find that our method can accurately test for the presence of clustering even when the number of features is high. "
783827983979216896,2016-10-06 00:35:28,https://t.co/D4MMRh3Dk4,Recovering Multiple Nonnegative Time Series From a Few Temporal Aggregates. (arXiv:1610.01492v1 [stat.ML]) https://t.co/D4MMRh3Dk4,0,3," Abstract: Motivated by electricity consumption metering, we extend existing nonnegative matrix factorization (NMF) algorithms to use linear measurements as observations, instead of matrix entries. The objective is to estimate multiple time series at a fine temporal scale from temporal aggregates measured on each individual series. Furthermore, our algorithm is extended to take into account individual autocorrelation to provide better estimation, using a recent convex relaxation of quadratically constrained quadratic program. Extensive experiments on synthetic and real-world electricity consumption datasets illustrate the effectiveness of our matrix recovery algorithms. "
783827982133628928,2016-10-06 00:35:28,https://t.co/ItgY5eITaz,A Novel Representation of Neural Networks. (arXiv:1610.01549v1 [stat.ML]) https://t.co/ItgY5eITaz,3,16," Abstract: Deep Neural Networks (DNNs) have become very popular for prediction in many areas. Their strength is in representation with a high number of parameters that are commonly learned via gradient descent or similar optimization methods. However, the representation is non-standardized, and the gradient calculation methods are often performed using component-based approaches that break parameters down into scalar units, instead of considering the parameters as whole entities. In this work, these problems are addressed. Standard notation is used to represent DNNs in a compact framework. Gradients of DNN loss functions are calculated directly over the inner product space on which the parameters are defined. This framework is general and is applied to two common network types: the Multilayer Perceptron and the Deep Autoencoder. "
783827979076009984,2016-10-06 00:35:27,https://t.co/zDUBrQF1aY,Projected Nesterov's Proximal-Gradient Algorithm for Sparse Signal Reconstruction with a Convex Constraint. (arXiv… https://t.co/zDUBrQF1aY,1,4," Abstract: We develop a projected Nesterov's proximal-gradient (PNPG) approach for sparse signal reconstruction that combines adaptive step size with Nesterov's momentum acceleration. The objective function that we wish to minimize is the sum of a convex differentiable data-fidelity (negative log-likelihood (NLL)) term and a convex regularization term. We apply sparse signal regularization where the signal belongs to a closed convex set within the closure of the domain of the NLL; the convex-set constraint facilitates flexible NLL domains and accurate signal recovery. Signal sparsity is imposed using the $\ell_1$-norm penalty on the signal's linear transform coefficients or gradient map, respectively. The PNPG approach employs projected Nesterov's acceleration step with restart and an inner iteration to compute the proximal mapping. We propose an adaptive step-size selection scheme to obtain a good local majorizing function of the NLL and reduce the time spent backtracking. Thanks to step-size adaptation, PNPG does not require Lipschitz continuity of the gradient of the NLL. We present an integrated derivation of the momentum acceleration and its $\mathcal{O}(k^{-2})$ convergence-rate and iterate convergence proofs, which account for adaptive step-size selection, inexactness of the iterative proximal mapping, and the convex-set constraint. The tuning of PNPG is largely application-independent. Tomographic and compressed-sensing reconstruction experiments with Poisson generalized linear and Gaussian linear measurement models demonstrate the performance of the proposed approach. "
783827977377353729,2016-10-06 00:35:27,https://t.co/MbiuPcldpu,A Low Complexity Algorithm with $O(\sqrt{T})$ Regret and Finite Constraint Violations for Online Convex Optimizati… https://t.co/MbiuPcldpu,0,4," Abstract: This paper considers online convex optimization over a complicated constraint set, which typically consists of multiple functional constraints and a set constraint. The conventional projection based online projection algorithm (Zinkevich, 2003) can be difficult to implement due to the potentially high computation complexity of the projection operation. In this paper, we relax the functional constraints by allowing them to be violated at each round but still requiring them to be satisfied in the long term. This type of relaxed online convex optimization (with long term constraints) was first considered in Mahdavi et al. (2012). That prior work proposes an algorithm to achieve $O(\sqrt{T})$ regret and $O(T^{3/4})$ constraint violations for general problems and another algorithm to achieve an $O(T^{2/3})$ bound for both regret and constraint violations when the constraint set can be described by a finite number of linear constraints. A recent extension in Jenatton et al. (2016) can achieve $O(T^{\max\{\beta,1-\beta\}})$ regret and $O(T^{1-\beta/2})$ constraint violations where $\beta\in (0,1)$. The current paper proposes a new simple algorithm that yields improved performance in comparison to prior works. The new algorithm achieves an $O(\sqrt{T})$ regret bound with finite constraint violations. "
783827975305359360,2016-10-06 00:35:26,https://t.co/9SD5ljxa9O,A Geometric Framework for Convolutional Neural Networks. (arXiv:1608.04374v2 [stat.ML] UPDATED) https://t.co/9SD5ljxa9O,1,15," Abstract: In this paper, a geometric framework for neural networks is proposed. This framework uses the inner product space structure underlying the parameter set to perform gradient descent not in a component-based form, but in a coordinate-free manner. Convolutional neural networks are described in this framework in a compact form, with the gradients of standard --- and higher-order --- loss functions calculated for each layer of the network. This approach can be applied to other network structures and provides a basis on which to create new networks. "
783827973229187072,2016-10-06 00:35:26,https://t.co/NSRhmv7efP,Active Sensing of Social Networks. (arXiv:1601.05834v2 [cs.SI] CROSS LISTED) https://t.co/NSRhmv7efP,0,3," Abstract: This paper develops an active sensing method to estimate the relative weight (or trust) agents place on their neighbors' information in a social network. The model used for the regression is based on the steady state equation in the linear DeGroot model under the influence of stubborn agents, i.e., agents whose opinions are not influenced by their neighbors. This method can be viewed as a \emph{social RADAR}, where the stubborn agents excite the system and the latter can be estimated through the reverberation observed from the analysis of the agents' opinions. The social network sensing problem can be interpreted as a blind compressed sensing problem with a sparse measurement matrix. We prove that the network structure will be revealed when a sufficient number of stubborn agents independently influence a number of ordinary (non-stubborn) agents. We investigate the scenario with a deterministic or randomized DeGroot model and propose a consistent estimator of the steady states for the latter scenario. Simulation results on synthetic and real world networks support our findings. "
783464371691323392,2016-10-05 00:30:36,https://t.co/XMYci6cd3u,Sequential Low-Rank Change Detection. (arXiv:1610.00732v1 [stat.ML]) https://t.co/XMYci6cd3u,1,3," Abstract: Detecting emergence of a low-rank signal from high-dimensional data is an important problem arising from many applications such as camera surveillance and swarm monitoring using sensors. We consider a procedure based on the largest eigenvalue of the sample covariance matrix over a sliding window to detect the change. To achieve dimensionality reduction, we present a sketching-based approach for rank change detection using the low-dimensional linear sketches of the original high-dimensional observations. The premise is that when the sketching matrix is a random Gaussian matrix, and the dimension of the sketching vector is sufficiently large, the rank of sample covariance matrix for these sketches equals the rank of the original sample covariance matrix with high probability. Hence, we may be able to detect the low-rank change using sample covariance matrices of the sketches without having to recover the original covariance matrix. We character the performance of the largest eigenvalue statistic in terms of the false-alarm-rate and the expected detection delay, and present an efficient online implementation via subspace tracking. "
783464368298156032,2016-10-05 00:30:36,https://t.co/jXcZbrkujV,cleverhans v0.1: an adversarial machine learning library. (arXiv:1610.00768v1 [cs.LG]) https://t.co/jXcZbrkujV,0,8," Abstract: cleverhans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the cleverhans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system. "
783464364359708672,2016-10-05 00:30:35,https://t.co/4EXlAZJ28D,The Search Problem in Mixture Models. (arXiv:1610.00843v1 [stat.ML]) https://t.co/4EXlAZJ28D,0,4," Abstract: We consider the task of learning the parameters of a {\em single} component of a mixture model, for the case when we are given {\em side information} about that component; we call this the ""search problem"" in mixture models. We would like to solve this with computational and sample complexity lower than solving the overall original problem, where one learns parameters of all components. Our main contributions are the development of a simple but general model for the notion of side information, and a corresponding simple matrix-based algorithm for solving the search problem in this general setting. We then specialize this model and algorithm to four common scenarios: Gaussian mixture models, LDA topic models, subspace clustering, and mixed linear regression. For each one of these we show that if (and only if) the side information is informative, we obtain better sample complexity than existing standard mixture model algorithms (e.g. tensor methods). We also illustrate several natural ways one can obtain such side information, for specific problem instances. Our experiments on real datasets (NY Times, Yelp, BSDS500) further demonstrate the practicality of our algorithms showing significant improvement in runtime and accuracy. "
783464361901842432,2016-10-05 00:30:34,https://t.co/Kig4DhHSLu,Revisiting Role Discovery in Networks: From Node to Edge Roles. (arXiv:1610.00844v1 [stat.ML]) https://t.co/Kig4DhHSLu,2,3," Abstract: Previous work in network analysis has focused on modeling the mixed-memberships of node roles in the graph, but not the roles of edges. We introduce the edge role discovery problem and present a generalizable framework for learning and extracting edge roles from arbitrary graphs automatically. Furthermore, while existing node-centric role models have mainly focused on simple degree and egonet features, this work also explores graphlet features for role discovery. In addition, we also develop an approach for automatically learning and extracting important and useful edge features from an arbitrary graph. The experimental results demonstrate the utility of edge roles for network analysis tasks on a variety of graphs from various problem domains. "
783464358273806337,2016-10-05 00:30:33,https://t.co/2qUulzfCBY,Model Selection for Gaussian Process Regression by Approximation Set Coding. (arXiv:1610.00907v1 [stat.ML]) https://t.co/2qUulzfCBY,1,4," Abstract: Gaussian processes are powerful, yet analytically tractable models for supervised learning. A Gaussian process is characterized by a mean function and a covariance function (kernel), which are determined by a model selection criterion. The functions to be compared do not just differ in their parametrization but in their fundamental structure. It is often not clear which function structure to choose, for instance to decide between a squared exponential and a rational quadratic kernel. Based on the principle of approximation set coding, we develop a framework for model selection to rank kernels for Gaussian process regression. In our experiments approximation set coding shows promise to become a model selection criterion competitive with maximum evidence (also called marginal likelihood) and leave-one-out cross-validation. "
783464354939281408,2016-10-05 00:30:32,https://t.co/iAAKzhHnQb,QuickeNing: A Generic Quasi-Newton Algorithm for Faster Gradient-Based Optimization *. (arXiv:1610.00960v1 [stat.M… https://t.co/iAAKzhHnQb,2,6," Abstract: We propose an approach to accelerate gradient-based optimization algorithms by giving them the ability to exploit curvature information using quasi-Newton update rules. The proposed scheme, called QuickeNing, is generic and can be applied to a large class of first-order methods such as incremental and block-coordinate algorithms; it is also compatible with composite objectives, meaning that it has the ability to provide exactly sparse solutions when the objective involves a sparsity-inducing regularization. QuickeNing relies on limited-memory BFGS rules, making it appropriate for solving high-dimensional optimization problems; with no line-search, it is also simple to use and to implement. Besides, it enjoys a worst-case linear convergence rate for strongly convex problems. We present experimental results where QuickeNing gives significant improvements over competing methods for solving large-scale high-dimensional machine learning problems. "
783464351286067200,2016-10-05 00:30:32,https://t.co/90kSwtmP68,Stochastic Optimization with Variance Reduction for Infinite Datasets with Finite-Sum Structure. (arXiv:1610.00970… https://t.co/90kSwtmP68,2,9," Abstract: Stochastic optimization algorithms with variance reduction have proven successful for minimizing large finite sums of functions. However, in the context of empirical risk minimization, it is often helpful to augment the training set by considering random perturbations of input examples. In this case, the objective is no longer a finite sum, and the main candidate for optimization is the stochastic gradient descent method (SGD). In this paper, we introduce a variance reduction approach for this setting when the objective is strongly convex. After an initial linearly convergent phase, the algorithm achieves a $O(1/t)$ convergence rate in expectation like SGD, but with a constant factor that is typically much smaller, depending on the variance of gradient estimates due to perturbations on a single example. "
783464348182249472,2016-10-05 00:30:31,https://t.co/69Tot8Tphk,Real-time wind power forecast. (arXiv:1610.01000v1 [stat.AP]) https://t.co/69Tot8Tphk,0,3," Abstract: We focus on short-term wind power forecast using machine learning techniques. We show on real data provided by the wind energy company Maia Eolis, that parametric models, even following closely the physical equation relating wind production to wind speed are out-performed by intelligent learning algorithms. In particular, the CART-Bagging algorithm gives very stable and promising results. Besides, we show on this application that the default methodology to select a subset of predictors provided in the standard random forest package can be refined, especially when there exists among the predictors one variable which has a major impact. "
783464345099464705,2016-10-05 00:30:30,https://t.co/w6zvyX5hkT,A SMART Stochastic Algorithm for Nonconvex Optimization with Applications to Robust Machine Learning. (arXiv:1610.… https://t.co/w6zvyX5hkT,0,5," Abstract: Machine learning theory typically assumes that training data is unbiased and not adversarially generated. When real training data deviates from these assumptions, trained models make erroneous predictions, sometimes with disastrous effects. Robust losses, such as the huber norm, were designed to mitigate the effects of such contaminated data, but they are limited to the regression context. In this paper, we show how to transform any optimization problem that arises from fitting a machine learning model into one that (1) detects and removes contaminated data from the training set while (2) simultaneously fitting the trimmed model on the uncontaminated data that remains. To solve the resulting nonconvex optimization problem, we introduce a fast stochastic proximal-gradient algorithm that incorporates prior knowledge through nonsmooth regularization. For datasets of size $n$, our approach requires $O(n^{2/3}/\varepsilon)$ gradient evaluations to reach $\varepsilon$-accuracy and, when a certain error bound holds, the complexity improves to $O(\kappa n^{2/3}\log(1/\varepsilon))$. These rates are $n^{1/3}$ times better than those achieved by typical, full gradient methods. "
783464341630808064,2016-10-05 00:30:29,https://t.co/FVpcYm641j,A Non-generative Framework and Convex Relaxations for Unsupervised Learning. (arXiv:1610.01132v1 [cs.LG]) https://t.co/FVpcYm641j,1,7," Abstract: We give a novel formal theoretical framework for unsupervised learning with two distinctive characteristics. First, it does not assume any generative model and based on a worst-case performance metric. Second, it is comparative, namely performance is measured with respect to a given hypothesis class. This allows to avoid known computational hardness results and improper algorithms based on convex relaxations. We show how several families of unsupervised learning models, which were previously only analyzed under probabilistic assumptions and are otherwise provably intractable, can be efficiently learned in our framework by convex optimization. "
783464338417917952,2016-10-05 00:30:28,https://t.co/sHN8QcG17q,Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. (arXiv:1506.02142v6 [stat.ML… https://t.co/sHN8QcG17q,8,30," Abstract: Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning. "
783464333766516737,2016-10-05 00:30:27,https://t.co/XR5ArFOM2Z,Efficient Distributed SGD with Variance Reduction. (arXiv:1512.02970v2 [cs.LG] UPDATED) https://t.co/XR5ArFOM2Z,0,8," Abstract: Stochastic Gradient Descent (SGD) has become one of the most popular optimization methods for training machine learning models on massive datasets. However, SGD suffers from two main drawbacks: (i) The noisy gradient updates have high variance, which slows down convergence as the iterates approach the optimum, and (ii) SGD scales poorly in distributed settings, typically experiencing rapidly decreasing marginal benefits as the number of workers increases. In this paper, we propose a highly parallel method, CentralVR, that uses error corrections to reduce the variance of SGD gradient updates, and scales linearly with the number of worker nodes. CentralVR enjoys low iteration complexity, provably linear convergence rates, and exhibits linear performance gains up to hundreds of cores for massive datasets. We compare CentralVR to state-of-the-art parallel stochastic optimization methods on a variety of models and datasets, and find that our proposed methods exhibit stronger scaling than other SGD variants. "
783464329337270272,2016-10-05 00:30:26,https://t.co/InT5tFpagj,A Theoretically Grounded Application of Dropout in Recurrent Neural Networks. (arXiv:1512.05287v4 [stat.ML] UPDATE… https://t.co/InT5tFpagj,2,21," Abstract: Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning. "
783464325210046464,2016-10-05 00:30:25,https://t.co/J5cFn2JKWU,"CompAdaGrad: A Compressed, Complementary, Computationally-Efficient Adaptive Gradient Method. (arXiv:1609.03319v2 … https://t.co/J5cFn2JKWU",1,9," Abstract: The adaptive gradient online learning method known as AdaGrad has seen widespread use in the machine learning community in stochastic and adversarial online learning problems and more recently in deep learning methods. The method's full-matrix incarnation offers much better theoretical guarantees and potentially better empirical performance than its diagonal version; however, this version is computationally prohibitive and so the simpler diagonal version often is used in practice. We introduce a new method, CompAdaGrad, that navigates the space between these two schemes and show that this method can yield results much better than diagonal AdaGrad while avoiding the (effectively intractable) $O(n^3)$ computational complexity of full-matrix AdaGrad for dimension $n$. CompAdaGrad essentially performs full-matrix regularization in a low-dimensional subspace while performing diagonal regularization in the complementary subspace. We derive CompAdaGrad's updates for composite mirror descent in case of the squared $\ell_2$ norm and the $\ell_1$ norm, demonstrate that its complexity per iteration is linear in the dimension, and establish guarantees for the method independent of the choice of composite regularizer. Finally, we show preliminary results on several datasets. "
783464317240958976,2016-10-05 00:30:23,https://t.co/OjfCnZaXMd,Coherent structure coloring: identification of coherent structures from sparse data using graph theory. (arXiv:161… https://t.co/OjfCnZaXMd,0,2," Abstract: We present a frame-invariant method for detecting coherent structures from Lagrangian flow trajectories that can be sparse in number, as is the case in many fluid mechanics applications of practical interest. The method, based on principles used in graph coloring and spectral graph drawing algorithms, examines a measure of the kinematic dissimilarity of all pairs of fluid trajectories, either measured experimentally, e.g. using particle tracking velocimetry; or numerically, by advecting fluid particles in the Eulerian velocity field. Coherence is assigned to groups of particles whose kinematics remain similar throughout the time interval for which trajectory data is available, regardless of their physical proximity to one another. Through the use of several analytical and experimental validation cases, this algorithm is shown to robustly detect coherent structures using significantly less flow data than is required by existing spectral graph theory methods. "
783102511980941312,2016-10-04 00:32:42,https://t.co/6hGYs7m4pn,A Primer on Coordinate Descent Algorithms. (arXiv:1610.00040v1 [math.OC]) https://t.co/6hGYs7m4pn,0,10," Abstract: This monograph presents a class of algorithms called coordinate descent algorithms for mathematicians, statisticians, and engineers outside the field of optimization. This particular class of algorithms has recently gained popularity due to their effectiveness in solving large-scale optimization problems in machine learning, compressed sensing, image processing, and computational statistics. Coordinate descent algorithms solve optimization problems by successively minimizing along each coordinate or coordinate hyperplane, which is ideal for parallelized and distributed computing. Avoiding detailed technicalities and proofs, this monograph gives relevant theory and examples for practitioners to effectively apply coordinate descent to modern problems in data science and engineering. To keep the primer up-to-date, we intend to publish this monograph only after no additional topics need to be added and we foresee no further major advances in the area. "
783102509724405761,2016-10-04 00:32:42,https://t.co/Ok78d4hjcX,Faster Kernels for Graphs with Continuous Attributes via Hashing. (arXiv:1610.00064v1 [cs.LG]) https://t.co/Ok78d4hjcX,0,7," Abstract: While state-of-the-art kernels for graphs with discrete labels scale well to graphs with thousands of nodes, the few existing kernels for graphs with continuous attributes, unfortunately, do not scale well. To overcome this limitation, we present hash graph kernels, a general framework to derive kernels for graphs with continuous attributes from discrete ones. The idea is to iteratively turn continuous attributes into discrete labels using randomized hash functions. We illustrate hash graph kernels for the Weisfeiler-Lehman subtree kernel and for the shortest-path kernel. The resulting novel graph kernels are shown to be, both, able to handle graphs with continuous attributes and scalable to large graphs and data sets. This is supported by our theoretical analysis and demonstrated by an extensive experimental evaluation. "
783102507027398656,2016-10-04 00:32:41,https://t.co/OLjKeM896N,X-CNN: Cross-modal Convolutional Neural Networks for Sparse Datasets. (arXiv:1610.00163v1 [stat.ML]) https://t.co/OLjKeM896N,3,9," Abstract: In this paper we propose cross-modal convolutional neural networks (X-CNNs), a novel biologically inspired type of CNN architectures, treating gradient descent-specialised CNNs as individual units of processing in a larger-scale network topology, while allowing for unconstrained information flow and/or weight sharing between analogous hidden layers of the network---thus generalising the already well-established concept of neural network ensembles (where information typically may flow only between the output layers of the individual networks). The constituent networks are individually designed to learn the output function on their own subset of the input data, after which cross-connections between them are introduced after each pooling operation to periodically allow for information exchange between them. This injection of knowledge into a model (by prior partition of the input data through domain knowledge or unsupervised methods) is expected to yield greatest returns in sparse data environments, which are typically less suitable for training CNNs. For evaluation purposes, we have compared a standard four-layer CNN as well as a sophisticated FitNet4 architecture against their cross-modal variants on the CIFAR-10 and CIFAR-100 datasets with differing percentages of the training data being removed, and find that at lower levels of data availability, the X-CNNs significantly outperform their baselines (typically providing a 2--6% benefit, depending on the dataset size and whether data augmentation is used), while still maintaining an edge on all of the full dataset tests. "
783102503923621888,2016-10-04 00:32:40,https://t.co/u8dVebHvys,Learning Optimized Risk Scores on Large-Scale Datasets. (arXiv:1610.00168v1 [stat.ML]) https://t.co/u8dVebHvys,0,2," Abstract: Risk scores are simple classification models that let users quickly assess risk by adding, subtracting and multiplying a few small numbers. These models are used for high-stakes applications in healthcare and criminology, but are difficult to learn from data because they need to be risk-calibrated, use small integer coefficients, and obey operational constraints. In this paper, we present a new approach to learn optimized risk scores from data by solving a discrete optimization problem. We formulate the risk score problem as a mixed integer nonlinear program, and present a new cutting plane algorithm to efficiently recover the optimal solution while avoiding the stalling behavior that occurs when we use existing cutting plane algorithms on non-convex problems. We pair our cutting plane algorithm with specialized procedures to generate feasible solutions, narrow the optimality gap, and reduce data-related computation. The resulting approach can learn optimized risk scores in a way that scales linearly in the number of samples, provides a proof of optimality, and accommodates complex operational constraints. We illustrate the benefits of our approach through extensive numerical experiments. "
783102501050519552,2016-10-04 00:32:40,https://t.co/GPnumU0R1C,A Birth and Death Process for Bayesian Network Structure Inference. (arXiv:1610.00189v1 [stat.ML]) https://t.co/GPnumU0R1C,0,3," Abstract: Bayesian networks (BNs) are graphical models that are useful for representing high-dimensional probability distributions. There has been a great deal of interest in recent years in the NP-hard problem of learning the structure of a BN from observed data. Typically, one assigns a score to various structures and the search becomes an optimization problem that can be approached with either deterministic or stochastic methods. In this paper, we walk through the space of graphs by modeling the appearance and disappearance of edges as a birth and death process and compare our novel approach to the popular Metropolis-Hastings search strategy. We give empirical evidence that the birth and death process has superior mixing properties. "
783102498420748289,2016-10-04 00:32:39,https://t.co/KTBSHr9qxw,Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation From Undersampled Data. (arXiv:16… https://t.co/KTBSHr9qxw,0,6," Abstract: Subspace learning and matrix factorization problems have a great many applications in science and engineering, and efficient algorithms are critical as dataset sizes continue to grow. Many relevant problem formulations are non-convex, and in a variety of contexts it has been observed that solving the non-convex problem directly is not only efficient but reliably accurate. We discuss convergence theory for a particular method: first order incremental gradient descent constrained to the Grassmannian. The output of the algorithm is an orthonormal basis for a $d$-dimensional subspace spanned by an input streaming data matrix. We study two sampling cases: where each data vector of the streaming matrix is fully sampled, or where it is undersampled by a sampling matrix $A_t\in \R^{m\times n}$ with $m\ll n$. We propose an adaptive stepsize scheme that depends only on the sampled data and algorithm outputs. We prove that with fully sampled data, the stepsize scheme maximizes the improvement of our convergence metric at each iteration, and this method converges from any random initialization to the true subspace, despite the non-convex formulation and orthogonality constraints. For the case of undersampled data, we establish monotonic improvement on the defined convergence metric for each iteration with high probability. "
783102496101326848,2016-10-04 00:32:39,https://t.co/o4xm1b6Wrr,Tuning Parameter Calibration in High-dimensional Logistic Regression With Theoretical Guarantees. (arXiv:1610.0020… https://t.co/o4xm1b6Wrr,0,1," Abstract: Feature selection is a standard approach to understanding and modeling high-dimensional classification data, but the corresponding statistical methods hinge on tuning parameters that are difficult to calibrate. In particular, existing calibration schemes in the logistic regression framework lack any finite sample guarantees. In this paper, we introduce a novel calibration scheme for penalized logistic regression. It is based on simple tests along the tuning parameter path and satisfies optimal finite sample bounds. It is also amenable to easy and efficient implementations, and it rivals or outmatches existing methods in simulations and real data applications. "
783102493639213056,2016-10-04 00:32:38,https://t.co/rse1v0DmBE,Deep unsupervised learning through spatial contrasting. (arXiv:1610.00243v1 [cs.LG]) https://t.co/rse1v0DmBE,1,2," Abstract: Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images. This criterion can be employed within conventional neural networks and trained using standard techniques such as SGD and back-propagation, thus complementing supervised methods. "
783102491605032961,2016-10-04 00:32:37,https://t.co/SfaFj3m1Ic,HNP3: A Hierarchical Nonparametric Point Process for Modeling Content Diffusion over Social Media. (arXiv:1610.002… https://t.co/SfaFj3m1Ic,0,1," Abstract: This paper introduces a novel framework for modeling temporal events with complex longitudinal dependency that are generated by dependent sources. This framework takes advantage of multidimensional point processes for modeling time of events. The intensity function of the proposed process is a mixture of intensities, and its complexity grows with the complexity of temporal patterns of data. Moreover, it utilizes a hierarchical dependent nonparametric approach to model marks of events. These capabilities allow the proposed model to adapt its temporal and topical complexity according to the complexity of data, which makes it a suitable candidate for real world scenarios. An online inference algorithm is also proposed that makes the framework applicable to a vast range of applications. The framework is applied to a real world application, modeling the diffusion of contents over networks. Extensive experiments reveal the effectiveness of the proposed framework in comparison with state-of-the-art methods. "
783102489457483776,2016-10-04 00:32:37,https://t.co/bJdWyjZVoV,Sparsity-driven weighted ensemble classifier. (arXiv:1610.00270v1 [stat.ML]) https://t.co/bJdWyjZVoV,0,2," Abstract: In this letter, a novel weighted ensemble classifier is proposed that improves classification accuracy and minimizes the number of classifiers. Ensemble weight finding problem is modeled as a cost function with following terms: (a) a data fidelity term aiming to decrease misclassification rate, (b) a sparsity term aiming to decrease the number of classifiers, and (c) a non-negativity constraint on the weights of the classifiers. The proposed cost function is a non-convex and hard to solve; thus, convex relaxation techniques and novel approximations are employed to obtain a numerically efficient solution. The proposed method achieves better or similar performance compared to state-of-the art classifier ensemble methods, while using lower number of classifiers. "
783102486492176384,2016-10-04 00:32:36,https://t.co/x7lWybidxl,Deep Learning Algorithms for Signal Recognition in Long Perimeter Monitoring Distributed Fiber Optic Sensors. (arX… https://t.co/x7lWybidxl,0,2," Abstract: In this paper, we show an approach to build deep learning algorithms for recognizing signals in distributed fiber optic monitoring and security systems for long perimeters. Synthesizing such detection algorithms poses a non-trivial research and development challenge, because these systems face stringent error (type I and II) requirements and operate in difficult signal-jamming environments, with intensive signal-like jamming and a variety of changing possible signal portraits of possible recognized events. To address these issues, we have developed a twolevel event detection architecture, where the primary classifier is based on an ensemble of deep convolutional networks, can recognize 7 classes of signals and receives time-space data frames as input. Using real-life data, we have shown that the applied methods result in efficient and robust multiclass detection algorithms that have a high degree of adaptability. "
783102484537610240,2016-10-04 00:32:36,https://t.co/J8ORU2DTKw,Density Estimation with Distribution Element Trees. (arXiv:1610.00345v1 [stat.ME]) https://t.co/J8ORU2DTKw,0,1," Abstract: The estimation of probability densities based on available data is a central task in many statistical applications. Especially in the case of large ensembles with many samples or high-dimensional sample spaces, computationally efficient methods are needed. We propose a new method that is based on a decomposition of the distribution to be estimated in terms of so-called distribution elements (DEs). These elements enable an adaptive and hierarchical discretization of the sample space with small or large elements in regions with high and variable or low densities, respectively. The refinement strategy that we propose is based on statistical goodness-of-fit and independence tests that evaluate the local approximation of the distribution in terms of DEs. The capabilities of our new method are inspected based on several low and high-dimensional examples. "
783102482629165057,2016-10-04 00:32:35,https://t.co/nzk1NB34Vy,An Optimal Treatment Assignment Strategy to Evaluate Demand Response Effect. (arXiv:1610.00362v1 [cs.SY]) https://t.co/nzk1NB34Vy,0,2," Abstract: Demand response is designed to motivate electricity customers to modify their loads at critical time periods. The accurate estimation of impact of demand response signals to customers' consumption is central to any successful program. In practice, learning these response is nontrivial because operators can only send a limited number of signals. In addition, customer behavior also depends on a large number of exogenous covariates. These two features lead to a high dimensional inference problem with limited number of observations. In this paper, we formulate this problem by using a multivariate linear model and adopt an experimental design approach to estimate the impact of demand response signals. We show that randomized assignment, which is widely used to estimate the average treatment effect, is not efficient in reducing the variance of the estimator when a large number of covariates is present. In contrast, we present a tractable algorithm that strategically assigns demand response signals to customers. This algorithm achieves the optimal reduction in estimation variance, independent of the number of covariates. The results are validated from simulations on synthetic data. "
783102479957422081,2016-10-04 00:32:35,https://t.co/GTQve3ef8t,A new kernel-based approach to system identification with quantized output data. (arXiv:1610.00470v1 [cs.SY]) https://t.co/GTQve3ef8t,0,1," Abstract: In this paper we introduce a novel method for linear system identification with quantized output data. We model the impulse response as a zero-mean Gaussian process whose covariance (kernel) is given by the recently proposed stable spline kernel, which encodes information on regularity and exponential stability. This serves as a starting point to cast our system identification problem into a Bayesian framework. We employ Markov Chain Monte Carlo methods to provide an estimate of the system. In particular, we design two methods based on the so-called Gibbs sampler that allow also to estimate the kernel hyperparameters by marginal likelihood maximization via the Expectation-Maximization method. Numerical simulations show the effectiveness of the proposed scheme, as compared to the state-of-the-art kernel-based methods when these are employed in system identification with quantized data. "
783102477759545344,2016-10-04 00:32:34,https://t.co/3PQNdmkLYA,The Blessing of Dimensionality: Separation Theorems in the Thermodynamic Limit. (arXiv:1610.00494v1 [stat.ML]) https://t.co/3PQNdmkLYA,0,9," Abstract: We consider the problem of efficient ""on the fly"" tuning of existing, or {\it legacy}, Artificial Intelligence (AI) systems. The legacy AI systems are allowed to be of arbitrary class, albeit the data they are using for computing interim or final decision responses should posses an underlying structure of a high-dimensional topological real vector space. The tuning method that we propose enables dealing with errors without the need to re-train the system. Instead of re-training a simple cascade of perceptron nodes is added to the legacy system. The added cascade modulates the AI legacy system's decisions. If applied repeatedly, the process results in a network of modulating rules ""dressing up"" and improving performance of existing AI systems. Mathematical rationale behind the method is based on the fundamental property of measure concentration in high dimensional spaces. The method is illustrated with an example of fine-tuning a deep convolutional network that has been pre-trained to detect pedestrians in images. "
783102474362163200,2016-10-04 00:32:33,https://t.co/goCxqEYx0F,Semi-supervised Learning with Sparse Autoencoders in Phone Classification. (arXiv:1610.00520v1 [stat.ML]) https://t.co/goCxqEYx0F,1,5," Abstract: We propose the application of a semi-supervised learning method to improve the performance of acoustic modelling for automatic speech recognition based on deep neural net- works. As opposed to unsupervised initialisation followed by supervised fine tuning, our method takes advantage of both unlabelled and labelled data simultaneously through mini- batch stochastic gradient descent. We tested the method with varying proportions of labelled vs unlabelled observations in frame-based phoneme classification on the TIMIT database. Our experiments show that the method outperforms standard supervised training for an equal amount of labelled data and provides competitive error rates compared to state-of-the-art graph-based semi-supervised learning techniques. "
783102471526813696,2016-10-04 00:32:33,https://t.co/hEgPHct7pQ,Data Integration with High Dimensionality. (arXiv:1610.00667v1 [stat.ML]) https://t.co/hEgPHct7pQ,0,4," Abstract: We consider a problem of data integration. Consider determining which genes affect a disease. The genes, which we call predictor objects, can be measured in different experiments on the same individual. We address the question of finding which genes are predictors of disease by any of the experiments. Our formulation is more general. In a given data set, there are a fixed number of responses for each individual, which may include a mix of discrete, binary and continuous variables. There is also a class of predictor objects, which may differ within a subject depending on how the predictor object is measured, i.e., depend on the experiment. The goal is to select which predictor objects affect any of the responses, where the number of such informative predictor objects or features tends to infinity as sample size increases. There are marginal likelihoods for each way the predictor object is measured, i.e., for each experiment. We specify a pseudolikelihood combining the marginal likelihoods, and propose a pseudolikelihood information criterion. Under regularity conditions, we establish selection consistency for the pseudolikelihood information criterion with unbounded true model size, which includes a Bayesian information criterion with appropriate penalty term as a special case. Simulations indicate that data integration improves upon, sometimes dramatically, using only one of the data sources. "
783102467399618560,2016-10-04 00:32:32,https://t.co/1qNTFqImkw,ECA: High Dimensional Elliptical Component Analysis in non-Gaussian Distributions. (arXiv:1310.3561v4 [stat.ML] UP… https://t.co/1qNTFqImkw,0,2," Abstract: We present a robust alternative to principal component analysis (PCA) --- called elliptical component analysis (ECA) --- for analyzing high dimensional, elliptically distributed data. ECA estimates the eigenspace of the covariance matrix of the elliptical data. To cope with heavy-tailed elliptical distributions, a multivariate rank statistic is exploited. At the model-level, we consider two settings: either that the leading eigenvectors of the covariance matrix are non-sparse or that they are sparse. Methodologically, we propose ECA procedures for both non-sparse and sparse settings. Theoretically, we provide both non-asymptotic and asymptotic analyses quantifying the theoretical performances of ECA. In the non-sparse setting, we show that ECA's performance is highly related to the effective rank of the covariance matrix. In the sparse setting, the results are twofold: (i) We show that the sparse ECA estimator based on a combinatoric program attains the optimal rate of convergence; (ii) Based on some recent developments in estimating sparse leading eigenvectors, we show that a computationally efficient sparse ECA estimator attains the optimal rate of convergence under a suboptimal scaling. "
783102464165806080,2016-10-04 00:32:31,https://t.co/mZWMDAh9Lp,"Two-stage Sampling, Prediction and Adaptive Regression via Correlation Screening (SPARCS). (arXiv:1502.06189v2 [st… https://t.co/mZWMDAh9Lp",0,2," Abstract: This paper proposes a general adaptive procedure for budget-limited predictor design in high dimensions called two-stage Sampling, Prediction and Adaptive Regression via Correlation Screening (SPARCS). SPARCS can be applied to high dimensional prediction problems in experimental science, medicine, finance, and engineering, as illustrated by the following. Suppose one wishes to run a sequence of experiments to learn a sparse multivariate predictor of a dependent variable $Y$ (disease prognosis for instance) based on a $p$ dimensional set of independent variables $\mathbf X=[X_1,\ldots, X_p]^T$ (assayed biomarkers). Assume that the cost of acquiring the full set of variables $\mathbf X$ increases linearly in its dimension. SPARCS breaks the data collection into two stages in order to achieve an optimal tradeoff between sampling cost and predictor performance. In the first stage we collect a few ($n$) expensive samples $\{y_i,\mathbf x_i\}_{i=1}^n$, at the full dimension $p\gg n$ of $\mathbf X$, winnowing the number of variables down to a smaller dimension $l < p$ using a type of cross-correlation or regression coefficient screening. In the second stage we collect a larger number $(t-n)$ of cheaper samples of the $l$ variables that passed the screening of the first stage. At the second stage, a low dimensional predictor is constructed by solving the standard regression problem using all $t$ samples of the selected variables. SPARCS is an adaptive online algorithm that implements false positive control on the selected variables, is well suited to small sample sizes, and is scalable to high dimensions. We establish asymptotic bounds for the Familywise Error Rate (FWER), specify high dimensional convergence rates for support recovery, and establish optimal sample allocation rules to the first and second stages. "
783102460575551488,2016-10-04 00:32:30,https://t.co/3CUNXPl1YZ,Data-Driven Learning of a Union of Sparsifying Transforms Model for Blind Compressed Sensing. (arXiv:1511.01289v2 … https://t.co/3CUNXPl1YZ,0,1," Abstract: Compressed sensing is a powerful tool in applications such as magnetic resonance imaging (MRI). It enables accurate recovery of images from highly undersampled measurements by exploiting the sparsity of the images or image patches in a transform domain or dictionary. In this work, we focus on blind compressed sensing (BCS), where the underlying sparse signal model is a priori unknown, and propose a framework to simultaneously reconstruct the underlying image as well as the unknown model from highly undersampled measurements. Specifically, our model is that the patches of the underlying image(s) are approximately sparse in a transform domain. We also extend this model to a union of transforms model that better captures the diversity of features in natural images. The proposed block coordinate descent type algorithms for blind compressed sensing are highly efficient, and are guaranteed to converge to at least the partial global and partial local minimizers of the highly non-convex BCS problems. Our numerical experiments show that the proposed framework usually leads to better quality of image reconstructions in MRI compared to several recent image reconstruction methods. Importantly, the learning of a union of sparsifying transforms leads to better image reconstructions than a single adaptive transform. "
783102455949193216,2016-10-04 00:32:29,https://t.co/mXdQprU24a,On the interplay of network structure and gradient convergence in deep learning. (arXiv:1511.05297v6 [cs.LG] UPDAT… https://t.co/mXdQprU24a,0,3," Abstract: The regularization and output consistency behavior of dropout and layer-wise pretraining for learning deep networks have been fairly well studied. However, our understanding of how the asymptotic convergence of backpropagation in deep architectures is related to the structural properties of the network and other design choices (like denoising and dropout rate) is less clear at this time. An interesting question one may ask is whether the network architecture and input data statistics may guide the choices of learning parameters and vice versa. In this work, we explore the association between such structural, distributional and learnability aspects vis-\`a-vis their interaction with parameter convergence rates. We present a framework to address these questions based on convergence of backpropagation for general nonconvex objectives using first-order information. This analysis suggests an interesting relationship between feature denoising and dropout. Building upon these results, we obtain a setup that provides systematic guidance regarding the choice of learning parameters and network sizes that achieve a certain level of convergence (in the optimization sense) often mediated by statistical attributes of the inputs. Our results are supported by a set of experimental evaluations as well as independent empirical observations reported by other groups. "
783102451725529089,2016-10-04 00:32:28,https://t.co/PdB6vmhFbs,Recovery of non-linear cause-effect relationships from linearly mixed neuroimaging data. (arXiv:1605.00391v2 [stat… https://t.co/PdB6vmhFbs,2,4," Abstract: Causal inference concerns the identification of cause-effect relationships between variables. However, often only linear combinations of variables constitute meaningful causal variables. For example, recovering the signal of a cortical source from electroencephalography requires a well-tuned combination of signals recorded at multiple electrodes. We recently introduced the MERLiN (Mixture Effect Recovery in Linear Networks) algorithm that is able to recover, from an observed linear mixture, a causal variable that is a linear effect of another given variable. Here we relax the assumption of this cause-effect relationship being linear and present an extended algorithm that can pick up non-linear cause-effect relationships. Thus, the main contribution is an algorithm (and ready to use code) that has broader applicability and allows for a richer model class. Furthermore, a comparative analysis indicates that the assumption of linear cause-effect relationships is not restrictive in analysing electroencephalographic data. "
783102448571473922,2016-10-04 00:32:27,https://t.co/3Tw8FASh9Q,Robust Large Margin Deep Neural Networks. (arXiv:1605.08254v2 [stat.ML] UPDATED) https://t.co/3Tw8FASh9Q,0,5," Abstract: The generalization error of deep neural networks via their classification margin is studied in this work. Our approach is based on the Jacobian matrix of a deep neural network and can be applied to networks with arbitrary non-linearities and pooling layers, and to networks with different architectures such as feed forward networks and residual networks. Our analysis leads to the conclusion that a bounded spectral norm of the network's Jacobian matrix in the neighbourhood of the training samples is crucial for a deep neural network of arbitrary depth and width to generalize well. This is a significant improvement over the current bounds in the literature, which imply that the generalization error grows with either the width or the depth of the network. Moreover, it shows that the the recently proposed batch normalization and weight normalization re-parametrizations enjoy good generalization properties, and leads to a novel network regularizer based on the network's Jacobian matrix. The analysis is supported with experimental results on the MNIST and CIFAR-10 datasets. "
783102445975138304,2016-10-04 00:32:27,https://t.co/N3zKHWoyaq,Provable Burer-Monteiro factorization for a class of norm-constrained matrix problems. (arXiv:1606.01316v3 [stat.M… https://t.co/N3zKHWoyaq,0,2," Abstract: We study the projected gradient descent method on low-rank matrix problems with a strongly convex objective. We use the Burer-Monteiro factorization approach to implicitly enforce low-rankness; such factorization introduces non-convexity in the objective. We focus on constraint sets that include both positive semi-definite (PSD) constraints and specific matrix norm-constraints. Such criteria appear in quantum state tomography and phase retrieval applications. We show that non-convex projected gradient descent favors local linear convergence in the factored space. We build our theory on a novel descent lemma, that non-trivially extends recent results on the unconstrained problem. The resulting algorithm is Projected Factored Gradient Descent, abbreviated as ProjFGD, and shows superior performance compared to state of the art on quantum state tomography and sparse phase retrieval applications. "
783102443773104128,2016-10-04 00:32:26,https://t.co/8htN2zeFGg,Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models. (arXiv:1606.05320v2 [stat… https://t.co/8htN2zeFGg,1,11," Abstract: As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks (RNNs), state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining an RNN with a hidden Markov model (HMM), a simpler and more transparent model. We explore various combinations of RNNs and HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained first, then a small LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text. "
783102440962945024,2016-10-04 00:32:25,https://t.co/9BEQ20nUkT,On the expressive power of deep neural networks. (arXiv:1606.05336v4 [stat.ML] UPDATED) https://t.co/9BEQ20nUkT,1,9," Abstract: We study the expressive power of deep neural networks before and after training. Considering neural nets after random initialization, we show that three natural measures of expressivity all display an exponential dependence on the depth of the network. We prove, theoretically and experimentally, that all of these measures are in fact related to a fourth quantity, trajectory length. This quantity grows exponentially in the depth of the network, and is responsible for the depth sensitivity observed. These results translate to consequences for networks during and after training. The connection of all expressivity measures to trajectory length suggests that parameters earlier in the network have greater influence on the expressive power -- in particular, given a layer, its influence on expressivity is determined by the remaining depth of the network after that layer. This is verified with experiments on MNIST and CIFAR-10. We also find that the training process decreases depth sensitivity for real and synthetic data, but at different rates. "
783102438605750273,2016-10-04 00:32:25,https://t.co/FKxj1LetAs,Comparing the Performance of Graphical Structure Learning Algorithms with TETRAD. (arXiv:1607.08110v2 [stat.ML] UP… https://t.co/FKxj1LetAs,1,2," Abstract: In this report we describe a tool for comparing the performance of causal structure learning algorithms implemented in the TETRAD freeware suite of causal analysis methods. Currently the tool is available as a package in the TETRAD source code (written in Java), which can be loaded up in an Integrated Development Environment (IDE) such as IntelliJ IDEA. Simulations can be done varying the number of runs, sample sizes, and data modalities. Performance on this simulated data can then be compared for a number of algorithms, with parameters varied and with performance statistics as selected, producing a publishable report. The order of the algorithms in the output can be adjusted to the user's preference using a utility function over the statistics. Data sets from simulation can be saved along with their graphs to a file and loaded back in for further analysis, or used for analysis by other tools. "
783102435611013120,2016-10-04 00:32:24,https://t.co/q0wPQYtKx9,"Uniform Generalization, Concentration, and Adaptive Learning. (arXiv:1608.06072v2 [cs.LG] UPDATED) https://t.co/q0wPQYtKx9",0,4," Abstract: One fundamental goal in any learning algorithm is to mitigate its risk for overfitting. Mathematically, this requires that the learning algorithm enjoys a small generalization risk, which is defined either in expectation or in probability. Both types of generalization are commonly used in the literature. For instance, generalization in expectation has been used to analyze algorithms, such as ridge regression and SGD, whereas generalization in probability is used in the VC theory, among others. Recently, a third notion of generalization has been studied, called uniform generalization, which requires that the generalization risk vanishes uniformly in expectation across all bounded parametric losses. It has been shown that uniform generalization is, in fact, equivalent to an information-theoretic stability constraint, and that it recovers classical results in learning theory. It is achievable under various settings, such as sample compression schemes, finite hypothesis spaces, finite domains, and differential privacy. However, the relationship between uniform generalization and concentration remained unknown. In this paper, we answer this question by proving that, while a generalization in expectation does not imply a generalization in probability, a uniform generalization in expectation does imply concentration. We establish a chain rule for the uniform generalization risk of the composition of hypotheses and use it to derive a large deviation bound. Finally, we prove that the bound is tight. "
783102431911677953,2016-10-04 00:32:23,https://t.co/OQRDqKpGks,Stealing Machine Learning Models via Prediction APIs. (arXiv:1609.02943v2 [cs.CR] UPDATED) https://t.co/OQRDqKpGks,2,6," Abstract: Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (""predictive analytics"") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis. The tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., ""steal"") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures. "
782740025125531648,2016-10-03 00:32:19,https://t.co/XDRKJUMiPR,Max-plus statistical leverage scores. (arXiv:1609.09519v1 [stat.ML]) https://t.co/XDRKJUMiPR,0,3, Abstract: The statistical leverage scores of a complex matrix $A\in\mathbb{C}^{n\times d}$ record the degree of alignment between col$(A)$ and the coordinate axes in $\mathbb{C}^n$. These score are used in random sampling algorithms for solving certain numerical linear algebra problems. In this paper we present a max-plus algebraic analogue for statistical leverage scores. We show that max-plus statistical leverage scores can be used to calculate the exact asymptotic behavior of the conventional statistical leverage scores of a generic matrices of Puiseux series and also provide a novel way to approximate the conventional statistical leverage scores of a fixed or complex matrix. The advantage of approximating a complex matrices scores with max-plus scores is that the max-plus scores can be computed very quickly. This approximation is typically accurate to within an order or magnitude and should be useful in practical problems where the true scores are known to vary widely. 
782740023187734529,2016-10-03 00:32:18,https://t.co/qamPH9sT3L,Social Computing for Mobile Big Data in Wireless Networks. (arXiv:1609.09597v1 [cs.SI]) https://t.co/qamPH9sT3L,0,2," Abstract: Mobile big data contains vast statistical features in various dimensions, including spatial, temporal, and the underlying social domain. Understanding and exploiting the features of mobile data from a social network perspective will be extremely beneficial to wireless networks, from planning, operation, and maintenance to optimization and marketing. In this paper, we categorize and analyze the big data collected from real wireless cellular networks. Then, we study the social characteristics of mobile big data and highlight several research directions for mobile big data in the social computing areas. "
782740021287718912,2016-10-03 00:32:18,https://t.co/xLPNQlgCDE,On Identification of Sparse Multivariable ARX Model: A Sparse Bayesian Learning Approach. (arXiv:1609.09660v1 [cs.… https://t.co/xLPNQlgCDE,0,5," Abstract: This paper begins with considering the identification of sparse linear time-invariant networks described by multivariable ARX models. Such models possess relatively simple structure thus used as a benchmark to promote further research. With identifiability of the network guaranteed, this paper presents an identification method that infers both the Boolean structure of the network and the internal dynamics between nodes. Identification is performed directly from data without any prior knowledge of the system, including its order. The proposed method solves the identification problem using Maximum a posteriori estimation (MAP) but with inseparable penalties for complexity, both in terms of element (order of nonzero connections) and group sparsity (network topology). Such an approach is widely applied in Compressive Sensing (CS) and known as Sparse Bayesian Learning (SBL). We then propose a novel scheme that combines sparse Bayesian and group sparse Bayesian to efficiently solve the problem. The resulted algorithm has a similar form of the standard Sparse Group Lasso (SGL) while with known noise variance, it simplifies to exact re-weighted SGL. The method and the developed toolbox can be applied to infer networks from a wide range of fields, including systems biology applications such as signaling and genetic regulatory networks. "
782740019480068097,2016-10-03 00:32:17,https://t.co/DOrheigj2z,Phase Unmixing : Multichannel Source Separation with Magnitude Constraints. (arXiv:1609.09744v1 [cs.SD]) https://t.co/DOrheigj2z,0,3," Abstract: We consider the problem of estimating the phases of K mixed complex signals from a multichannel observation, when the mixing matrix and signal magnitudes are known. This problem can be cast as a non-convex quadratically constrained quadratic program which is known to be NP-hard in general. We propose three approaches to tackle it: a heuristic method, an alternate minimization method, and a convex relaxation into a semi-definite program. These approaches are showed to outperform the oracle multichannel Wiener filter in under-determined informed source separation tasks, using simulated and speech signals. The convex relaxation approach yields best results, including the potential for exact source separation in under-determined settings. "
782740017118580736,2016-10-03 00:32:17,https://t.co/wbY9NImdBe,Optimal spectral transportation with application to music transcription. (arXiv:1609.09799v1 [stat.ML]) https://t.co/wbY9NImdBe,0,3," Abstract: Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data. "
782740015252201472,2016-10-03 00:32:16,https://t.co/2hX36qhyOG,Structured Inference Networks for Nonlinear State Space Models. (arXiv:1609.09869v1 [stat.ML]) https://t.co/2hX36qhyOG,1,8," Abstract: Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood. "
782740013117239296,2016-10-03 00:32:16,https://t.co/9mqC5qcKNR,Turing learning: a metric-free approach to inferring behavior and its application to swarms. (arXiv:1603.04904v2 [… https://t.co/9mqC5qcKNR,1,6," Abstract: We propose Turing Learning, a novel system identification method for inferring the behavior of natural or artificial systems. Turing Learning simultaneously optimizes two populations of computer programs, one representing models of the behavior of the system under investigation, and the other representing classifiers. By observing the behavior of the system as well as the behaviors produced by the models, two sets of data samples are obtained. The classifiers are rewarded for discriminating between these two sets, that is, for correctly categorizing data samples as either genuine or counterfeit. Conversely, the models are rewarded for 'tricking' the classifiers into categorizing their data samples as genuine. Unlike other methods for system identification, Turing Learning does not require predefined metrics to quantify the difference between the system and its models. We present two case studies with swarms of simulated robots and prove that the underlying behaviors cannot be inferred by a metric-based system identification method. By contrast, Turing Learning infers the behaviors with high accuracy. It also produces a useful by-product - the classifiers - that can be used to detect abnormal behavior in the swarm. Moreover, we show that Turing Learning also successfully infers the behavior of physical robot swarms. The results show that collective behaviors can be directly inferred from motion trajectories of individuals in the swarm, which may have significant implications for the study of animal collectives. Furthermore, Turing Learning could prove useful whenever a behavior is not easily characterizable using metrics, making it suitable for a wide range of applications. "
782740011242385408,2016-10-03 00:32:15,https://t.co/IIIF5ywllf,Scalable and Flexible Multiview MAX-VAR Canonical Correlation Analysis. (arXiv:1605.09459v3 [stat.ML] UPDATED) https://t.co/IIIF5ywllf,0,2," Abstract: Generalized canonical correlation analysis (GCCA) aims at finding latent low-dimensional common structure from multiple views (feature vectors in different domains) of the same entities. Unlike principal component analysis (PCA) that handles a single view, (G)CCA is able to integrate information from different feature spaces. Here we focus on MAX-VAR GCCA, a popular formulation which has recently gained renewed interest in multilingual processing and speech modeling. The classic MAX-VAR GCCA problem can be solved optimally via eigen-decomposition of a matrix that compounds the (whitened) correlation matrices of the views; but this solution has serious scalability issues, and is not directly amenable to incorporating pertinent structural constraints such as non-negativity and sparsity on the canonical components. We posit regularized MAX-VAR GCCA as a non-convex optimization problem and propose an alternating optimization (AO)-based algorithm to handle it. Our algorithm alternates between {\em inexact} solutions of a regularized least squares subproblem and a manifold-constrained non-convex subproblem, thereby achieving substantial memory and computational savings. An important benefit of our design is that it can easily handle structure-promoting regularization. We show that the algorithm globally converges to a critical point at a sublinear rate, and approaches a global optimal solution at a linear rate when no regularization is considered. Judiciously designed simulations and large-scale word embedding tasks are employed to showcase the effectiveness of the proposed algorithm. "
782740007693975552,2016-10-03 00:32:15,https://t.co/1kV5NGVs4b,StruClus: Structural Clustering of Large-Scale Graph Databases. (arXiv:1609.09000v1 [cs.DB] CROSS LISTED) https://t.co/1kV5NGVs4b,0,7," Abstract: We present a structural clustering algorithm for large-scale datasets of small labeled graphs, utilizing a frequent subgraph sampling strategy. A set of representatives provides an intuitive description of each cluster, supports the clustering process, and helps to interpret the clustering results. The projection-based nature of the clustering approach allows us to bypass dimensionality and feature extraction problems that arise in the context of graph datasets reduced to pairwise distances or feature vectors. While achieving high quality and (human) interpretable clusterings, the runtime of the algorithm only grows linearly with the number of graphs. Furthermore, the approach is easy to parallelize and therefore suitable for very large datasets. Our extensive experimental evaluation on synthetic and real world datasets demonstrates the superiority of our approach over existing structural and subspace clustering algorithms, both, from a runtime and quality point of view. "
781652758491521024,2016-09-30 00:31:54,https://t.co/5M2A9SDGvT,Recurrent Convolutional Networks for Pulmonary Nodule Detection in CT Imaging. (arXiv:1609.09143v1 [stat.ML]) https://t.co/5M2A9SDGvT,13,31," Abstract: Computed tomography (CT) generates a stack of cross-sectional images covering a region of the body. The visual assessment of these images for the identification of potential abnormalities is a challenging and time consuming task due to the large amount of information that needs to be processed. In this article we propose a deep artificial neural network architecture, ReCTnet, for the fully-automated detection of pulmonary nodules in CT scans. The architecture learns to distinguish nodules and normal structures at the pixel level and generates three-dimensional probability maps highlighting areas that are likely to harbour the objects of interest. Convolutional and recurrent layers are combined to learn expressive image representations exploiting the spatial dependencies across axial slices. We demonstrate that leveraging intra-slice dependencies substantially increases the sensitivity to detect pulmonary nodules without inflating the false positive rate. On the publicly available LIDC/IDRI dataset consisting of 1,018 annotated CT scans, ReCTnet reaches a detection sensitivity of 90.5% with an average of 4.5 false positives per scan. Comparisons with a competing multi-channel convolutional neural network for multi-slice segmentation and other published methodologies using the same dataset provide evidence that ReCTnet offers significant performance gains. "
781652756209799172,2016-09-30 00:31:54,https://t.co/mlSGzIJLKj,MPI-FAUN: An MPI-Based Framework for Alternating-Updating Nonnegative Matrix Factorization. (arXiv:1609.09154v1 [c… https://t.co/mlSGzIJLKj,1,6," Abstract: Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors $W$ and $H$, for the given input matrix $A$, such that $A \approx W H$. NMF is a useful tool for many applications in different domains such as topic modeling in text mining, background separation in video analysis, and community detection in social networks. Despite its popularity in the data mining community, there is a lack of efficient parallel algorithms to solve the problem for big data sets. The main contribution of this work is a new, high-performance parallel computational framework for a broad class of NMF algorithms that iteratively solves alternating non-negative least squares (NLS) subproblems for $W$ and $H$. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). The framework is flexible and able to leverage a variety of NMF and NLS algorithms, including Multiplicative Update, Hierarchical Alternating Least Squares, and Block Principal Pivoting. Our implementation allows us to benchmark and compare different algorithms on massive dense and sparse data matrices of size that spans for few hundreds of millions to billions. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements. The code and the datasets used for conducting the experiments are available online. "
781652752728547328,2016-09-30 00:31:53,https://t.co/KiGYyMSfQm,EXTRACT: Strong Examples from Weakly-Labeled Sensor Data. (arXiv:1609.09196v1 [stat.ML]) https://t.co/KiGYyMSfQm,0,3," Abstract: Thanks to the rise of wearable and connected devices, sensor-generated time series comprise a large and growing fraction of the world's data. Unfortunately, extracting value from this data can be challenging, since sensors report low-level signals (e.g., acceleration), not the high-level events that are typically of interest (e.g., gestures). We introduce a technique to bridge this gap by automatically extracting examples of real-world events in low-level data, given only a rough estimate of when these events have taken place. By identifying sets of features that repeat in the same temporal arrangement, we isolate examples of such diverse events as human actions, power consumption patterns, and spoken words with up to 96% precision and recall. Our method is fast enough to run in real time and assumes only minimal knowledge of which variables are relevant or the lengths of events. Our evaluation uses numerous publicly available datasets and over 1 million samples of manually labeled sensor data. "
781652750207705088,2016-09-30 00:31:52,https://t.co/wO7ionfj6j,Deep Multi-Species Embedding. (arXiv:1609.09353v1 [cs.LG]) https://t.co/wO7ionfj6j,0,3," Abstract: Understanding how species are distributed across landscapes over time is a fundamental question in biodiversity research. Unfortunately, most species distribution models only target a single species at a time, despite the fact that there is strong evidence that species are not independently distributed. We propose Deep Multi-Species Embedding (DMSE), which jointly embed vectors corresponding to multiple species as well as vectors representing environmental covariates into a common high dimensional feature space via a deep neural network. Applied to \textit{eBird} bird watching data, our single-species DMSE model outperforms commonly used random forest models in terms of accuracy. Our multi-species DMSE model further improves the single species version. Through this model, we are able to confirm quantitatively many species-species interactions, which are only understood qualitatively among ecologists. As an additional contribution, we provide a graphical embedding of hundreds of bird species in the Northeast US. "
781652747938570240,2016-09-30 00:31:52,https://t.co/5gwdqKvG7t,Cooperative Training of Descriptor and Generator Networks. (arXiv:1609.09408v1 [stat.ML]) https://t.co/5gwdqKvG7t,0,7," Abstract: This paper studies the cooperative training of two probabilistic models of signals such as images. Both models are parametrized by convolutional neural networks (ConvNets). The first network is a descriptor network, which is an exponential family model or an energy-based model, whose feature statistics or energy function are defined by a bottom-up ConvNet, which maps the observed signal to the feature statistics. The second network is a generator network, which is a non-linear version of factor analysis. It is defined by a top-down ConvNet, which maps the latent factors to the observed signal. The maximum likelihood training algorithms of both the descriptor net and the generator net are in the form of alternating back-propagation, and both algorithms involve Langevin sampling. %In the training of the descriptor net, the Langevin sampling is used to sample synthesized examples from the model. In the training of the generator net, the Langevin sampling is used to sample the latent factors from the posterior distribution. The Langevin sampling in both algorithms can be time consuming. We observe that the two training algorithms can cooperate with each other by jumpstarting each other's Langevin sampling, and they can be naturally and seamlessly interwoven into a CoopNets algorithm that can train both nets simultaneously. "
781652745703030784,2016-09-30 00:31:51,https://t.co/3N1q5QIMeB,CNN Architectures for Large-Scale Audio Classification. (arXiv:1609.09430v1 [cs.SD]) https://t.co/3N1q5QIMeB,0,6," Abstract: Convolutional Neural Networks (CNNs) have proven very effective in image classification and have shown promise for audio classification. We apply various CNN architectures to audio and investigate their ability to classify videos with a very large data set of 70M training videos (5.24 million hours) with 30,871 labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet, VGG, Inception, and ResNet. We explore the effects of training with different sized subsets of the training videos. Additionally we report the effect of training using different subsets of the labels. While our dataset contains video-level labels, we are also interested in Acoustic Event Detection (AED) and train a classifier on embeddings learned from the video-level task on Audio Set [5]. We find that derivatives of image classification networks do well on our audio classification task, that increasing the number of labels we train on provides some improved performance over subsets of labels, that performance of models improves as we increase training set size, and that a model using embeddings learned from the video-level task does much better than a baseline on the Audio Set classification task. "
781652742808936449,2016-09-30 00:31:50,https://t.co/KKWEHcEw0e,A Searchlight Factor Model Approach for Locating Shared Information in Multi-Subject fMRI Analysis. (arXiv:1609.09… https://t.co/KKWEHcEw0e,0,4," Abstract: There is a growing interest in joint multi-subject fMRI analysis. The challenge of such analysis comes from inherent anatomical and functional variability across subjects. One approach to resolving this is a shared response factor model. This assumes a shared and time synchronized stimulus across subjects. Such a model can often identify shared information, but it may not be able to pinpoint with high resolution the spatial location of this information. In this work, we examine a searchlight based shared response model to identify shared information in small contiguous regions (searchlights) across the whole brain. Validation using classification tasks demonstrates that we can pinpoint informative local regions. "
781652740124581888,2016-09-30 00:31:50,https://t.co/Td0zSt8qcK,Classifier comparison using precision. (arXiv:1609.09471v1 [cs.LG]) https://t.co/Td0zSt8qcK,0,5," Abstract: New proposed models are often compared to state-of-the-art using statistical significance testing. Literature is scarce for classifier comparison using metrics other than accuracy. We present a survey of statistical methods that can be used for classifier comparison using precision, accounting for inter-precision correlation arising from use of same dataset. Comparisons are made using per-class precision and methods presented to test global null hypothesis of an overall model comparison. Comparisons are extended to multiple multi-class classifiers and to models using cross validation or its variants. Partial Bayesian update to precision is introduced when population prevalence of a class is known. Applications to compare deep architectures are studied. "
781652737972957185,2016-09-30 00:31:49,https://t.co/NwJjphXXhk,Fast learning rates with heavy-tailed losses. (arXiv:1609.09481v1 [stat.ML]) https://t.co/NwJjphXXhk,0,4," Abstract: We study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails. To enable such analyses, we introduce two new conditions: (i) the envelope function $\sup_{f \in \mathcal{F}}|\ell \circ f|$, where $\ell$ is the loss function and $\mathcal{F}$ is the hypothesis class, exists and is $L^r$-integrable, and (ii) $\ell$ satisfies the multi-scale Bernstein's condition on $\mathcal{F}$. Under these assumptions, we prove that learning rate faster than $O(n^{-1/2})$ can be obtained and, depending on $r$ and the multi-scale Bernstein's powers, can be arbitrarily close to $O(n^{-1})$. We then verify these assumptions and derive fast learning rates for the problem of vector quantization by $k$-means clustering with heavy-tailed distributions. The analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints. "
781652734974038016,2016-09-30 00:31:49,https://t.co/oUidTPwxRI,Provably Correct Algorithms for Matrix Column Subset Selection with Selectively Sampled Data. (arXiv:1505.04343v2 … https://t.co/oUidTPwxRI,0,4," Abstract: We consider the problem of matrix column subset selection, which selects a subset of columns from an input matrix such that the input can be well approximated by the span of the selected columns. Column subset selection has been applied to numerous real-world data applications such as population genetics summarization, electronic circuits testing and recommendation systems. In many applications the complete data matrix is unavailable and one needs to select representative columns by inspecting only a small portion of the input matrix. In this paper we propose the first provably correct column subset selection algorithms for partially observed data matrices. Our proposed algorithms exhibit different merits and drawbacks in terms of statistical accuracy, computational efficiency, sample complexity and sampling schemes, which provides a nice exploration of the tradeoff between these desired properties for column subset selection. The proposed methods employ the idea of feedback driven sampling and are inspired by several sampling schemes previously introduced for low-rank matrix approximation tasks [DMM08, FKV04, DV06, KS14]. Our analysis shows that, under the assumption that the input data matrix has incoherent rows but possibly coherent columns, all algorithms provably converge to the best low-rank approximation of the original data as number of selected columns increases. Furthermore, two of the proposed algorithms enjoy a relative error bound, which is preferred for column subset selection and matrix approximation purposes. We also demonstrate through both theoretical and empirical analysis the power of feedback driven sampling compared to uniform random sampling on input matrices with highly correlated columns. "
781652732440616960,2016-09-30 00:31:48,https://t.co/emQMRCzN9z,Maximum Entropy Vector Kernels for MIMO system identification. (arXiv:1508.02865v2 [cs.SY] UPDATED) https://t.co/emQMRCzN9z,0,2," Abstract: Recent contributions have framed linear system identification as a nonparametric regularized inverse problem. Relying on $\ell_2$-type regularization which accounts for the stability and smoothness of the impulse response to be estimated, these approaches have been shown to be competitive w.r.t classical parametric methods. In this paper, adopting Maximum Entropy arguments, we derive a new $\ell_2$ penalty deriving from a vector-valued kernel; to do so we exploit the structure of the Hankel matrix, thus controlling at the same time complexity, measured by the McMillan degree, stability and smoothness of the identified models. As a special case we recover the nuclear norm penalty on the squared block Hankel matrix. In contrast with previous literature on reweighted nuclear norm penalties, our kernel is described by a small number of hyper-parameters, which are iteratively updated through marginal likelihood maximization; constraining the structure of the kernel acts as a (hyper)regularizer which helps controlling the effective degrees of freedom of our estimator. To optimize the marginal likelihood we adapt a Scaled Gradient Projection (SGP) algorithm which is proved to be significantly computationally cheaper than other first and second order off-the-shelf optimization methods. The paper also contains an extensive comparison with many state-of-the-art methods on several Monte-Carlo studies, which confirms the effectiveness of our procedure. "
781652730028945408,2016-09-30 00:31:47,https://t.co/wqQMcUSuie,Block-diagonal covariance selection for high-dimensional Gaussian graphical models. (arXiv:1511.04033v3 [math.ST] … https://t.co/wqQMcUSuie,0,4," Abstract: Gaussian graphical models are widely utilized to infer and visualize networks of dependencies between continuous variables. However, inferring the graph is difficult when the sample size is small compared to the number of variables. To reduce the number of parameters to estimate in the model, we propose a non-asymptotic model selection procedure supported by strong theoretical guarantees based on an oracle inequality and a minimax lower bound. The covariance matrix of the model is approximated by a block-diagonal matrix. The structure of this matrix is detected by thresholding the sample covariance matrix, where the threshold is selected using the slope heuristic. Based on the block-diagonal structure of the covariance matrix, the estimation problem is divided into several independent problems: subsequently, the network of dependencies between variables is inferred using the graphical lasso algorithm in each block. The performance of the procedure is illustrated on simulated data. An application to a real gene expression dataset with a limited sample size is also presented: the dimension reduction allows attention to be objectively focused on interactions among smaller subsets of genes, leading to a more parsimonious and interpretable modular network. "
781652727365500928,2016-09-30 00:31:47,https://t.co/MmhH0SIkcI,Online Optimization with Costly and Noisy Measurements using Random Fourier Expansions. (arXiv:1603.09620v3 [cs.LG… https://t.co/MmhH0SIkcI,0,4," Abstract: This paper analyzes DONE, an online optimization algorithm that iteratively minimizes an unknown function based on costly and noisy measurements. The algorithm maintains a surrogate of the unknown function in the form of a random Fourier expansion (RFE). The surrogate is updated whenever a new measurement is available, and then used to determine the next measurement point. The algorithm is comparable to Bayesian optimization algorithms, but its computational complexity per iteration does not depend on the number of measurements. We derive several theoretical results that provide insight on how the hyper-parameters of the algorithm should be chosen. The algorithm is compared to a Bayesian optimization algorithm for a benchmark problem and three applications, namely, optical coherence tomography, optical beam-forming network tuning, and robot arm control. It is found that the DONE algorithm is significantly faster than Bayesian optimization in the discussed problems, while achieving a similar or better performance. "
781652724412809216,2016-09-30 00:31:46,https://t.co/XBF0ALF3ra,"Network structure, metadata and the prediction of missing nodes and annotations. (arXiv:1604.00255v2 [physics.soc-… https://t.co/XBF0ALF3ra",0,2," Abstract: The empirical validation of community detection methods is often based on available annotations on the nodes that serve as putative indicators of the large-scale network structure. Most often, the suitability of the annotations as topological descriptors itself is not assessed, and without this it is not possible to ultimately distinguish between actual shortcomings of the community detection algorithms on one hand, and the incompleteness, inaccuracy or structured nature of the data annotations themselves on the other. In this work we present a principled method to access both aspects simultaneously. We construct a joint generative model for the data and metadata, and a nonparametric Bayesian framework to infer its parameters from annotated datasets. We assess the quality of the metadata not according to its direct alignment with the network communities, but rather in its capacity to predict the placement of edges in the network. We also show how this feature can be used to predict the connections to missing nodes when only the metadata is available, as well as missing metadata. By investigating a wide range of datasets, we show that while there are seldom exact agreements between metadata tokens and the inferred data groups, the metadata is often informative of the network structure nevertheless, and can improve the prediction of missing nodes. This shows that the method uncovers meaningful patterns in both the data and metadata, without requiring or expecting a perfect agreement between the two. "
781290289357021184,2016-09-29 00:31:35,https://t.co/ckrKljRjvP,A Fast Factorization-based Approach to Robust PCA. (arXiv:1609.08677v1 [cs.CV]) https://t.co/ckrKljRjvP,1,8," Abstract: Robust principal component analysis (RPCA) has been widely used for recovering low-rank matrices in many data mining and machine learning problems. It separates a data matrix into a low-rank part and a sparse part. The convex approach has been well studied in the literature. However, state-of-the-art algorithms for the convex approach usually have relatively high complexity due to the need of solving (partial) singular value decompositions of large matrices. A non-convex approach, AltProj, has also been proposed with lighter complexity and better scalability. Given the true rank $r$ of the underlying low rank matrix, AltProj has a complexity of $O(r^2dn)$, where $d\times n$ is the size of data matrix. In this paper, we propose a novel factorization-based model of RPCA, which has a complexity of $O(kdn)$, where $k$ is an upper bound of the true rank. Our method does not need the precise value of the true rank. From extensive experiments, we observe that AltProj can work only when $r$ is precisely known in advance; however, when the needed rank parameter $r$ is specified to a value different from the true rank, AltProj cannot fully separate the two parts while our method succeeds. Even when both work, our method is about 4 times faster than AltProj. Our method can be used as a light-weight, scalable tool for RPCA in the absence of the precise value of the true rank. "
781290287872217089,2016-09-29 00:31:34,https://t.co/9GEPeMECD1,Optimizing Neural Network Hyperparameters with Gaussian Processes for Dialog Act Classification. (arXiv:1609.08703… https://t.co/9GEPeMECD1,2,12," Abstract: Systems based on artificial neural networks (ANNs) have achieved state-of-the-art results in many natural language processing tasks. Although ANNs do not require manually engineered features, ANNs have many hyperparameters to be optimized. The choice of hyperparameters significantly impacts models' performances. However, the ANN hyperparameters are typically chosen by manual, grid, or random search, which either requires expert experiences or is computationally expensive. Recent approaches based on Bayesian optimization using Gaussian processes (GPs) is a more systematic way to automatically pinpoint optimal or near-optimal machine learning hyperparameters. Using a previously published ANN model yielding state-of-the-art results for dialog act classification, we demonstrate that optimizing hyperparameters using GP further improves the results, and reduces the computational time by a factor of 4 compared to a random search. Therefore it is a useful technique for tuning ANN models to yield the best performances for natural language processing tasks. "
781290286286856193,2016-09-29 00:31:34,https://t.co/cS3xe2ZWZm,Stabilizing Linear Prediction Models using Autoencoder. (arXiv:1609.08752v1 [stat.ML]) https://t.co/cS3xe2ZWZm,1,4," Abstract: To date, the instability of prognostic predictors in a sparse high dimensional model, which hinders their clinical adoption, has received little attention. Stable prediction is often overlooked in favour of performance. Yet, stability prevails as key when adopting models in critical areas as healthcare. Our study proposes a stabilization scheme by detecting higher order feature correlations. Using a linear model as basis for prediction, we achieve feature stability by regularising latent correlation in features. Latent higher order correlation among features is modelled using an autoencoder network. Stability is enhanced by combining a recent technique that uses a feature graph, and augmenting external unlabelled data for training the autoencoder network. Our experiments are conducted on a heart failure cohort from an Australian hospital. Stability was measured using Consistency index for feature subsets and signal-to-noise ratio for model parameters. Our methods demonstrated significant improvement in feature stability and model estimation stability when compared to baselines. "
781290284130922496,2016-09-29 00:31:34,https://t.co/cblcNQgBQk,Approachability of convex sets in generalized quitting games. (arXiv:1609.08870v1 [cs.GT]) https://t.co/cblcNQgBQk,0,2," Abstract: We consider Blackwell approachability, a very powerful and geometric tool in game theory, used for example to design strategies of the uninformed player in repeated games with incomplete information. We extend this theory to ""generalized quitting games"" , a class of repeated stochastic games in which each player may have quitting actions, such as the Big-Match. We provide three simple geometric and strongly related conditions for the weak approachability of a convex target set. The first is sufficient: it guarantees that, for any fixed horizon, a player has a strategy ensuring that the expected time-average payoff vector converges to the target set as horizon goes to infinity. The third is necessary: if it is not satisfied, the opponent can weakly exclude the target set. In the special case where only the approaching player can quit the game (Big-Match of type I), the three conditions are equivalent and coincide with Blackwell's condition. Consequently, we obtain a full characterization and prove that the game is weakly determined-every convex set is either weakly approachable or weakly excludable. In games where only the opponent can quit (Big-Match of type II), none of our conditions is both sufficient and necessary for weak approachability. We provide a continuous time sufficient condition using techniques coming from differential games, and show its usefulness in practice, in the spirit of Vieille's seminal work for weak approachability.Finally, we study uniform approachability where the strategy should not depend on the horizon and demonstrate that, in contrast with classical Blackwell approacha-bility for convex sets, weak approachability does not imply uniform approachability. "
781290282717446144,2016-09-29 00:31:33,https://t.co/AegaSr2kx4,Sparse principal component regression for generalized linear models. (arXiv:1609.08886v1 [stat.ML]) https://t.co/AegaSr2kx4,2,10," Abstract: Principal component regression (PCR) is a widely used two-stage procedure: principal component analysis (PCA), followed by regression in which the selected principal components are regarded as new explanatory variables in the model. Note that PCA is based only on the explanatory variables, so the principal components are not selected using the information on the response variable. In this paper, we propose a one-stage procedure for PCR in the framework of generalized linear models. The basic loss function is based on a combination of the regression loss and PCA loss. An estimate of the regression parameter is obtained as the minimizer of the basic loss function with a sparse penalty. We call the proposed method sparse principal component regression for generalized linear models (SPCR-glm). Taking the two loss function into consideration simultaneously, SPCR-glm enables us to obtain sparse principal component loadings that are related to a response variable. However, a combination of loss functions may cause a parameter identification problem, but this potential problem is avoided by virtue of the sparse penalty. Thus, the sparse penalty plays two roles in this method. The parameter estimation procedure is proposed using various update algorithms with the coordinate descent algorithm. We apply SPCR-glm to two real datasets, doctor visits data and mouse consomic strain data. SPCR-glm provides more easily interpretable principal component (PC) scores and clearer classification on PC plots than the usual PCA. "
781290281207463936,2016-09-29 00:31:33,https://t.co/dxsyuclBXD,Statistical comparison of classifiers through Bayesian hierarchical modelling. (arXiv:1609.08905v1 [cs.LG]) https://t.co/dxsyuclBXD,0,6," Abstract: Usually one compares the accuracy of two competing classifiers via null hypothesis significance tests (nhst). Yet the nhst tests suffer from important shortcomings, which can be overcome by switching to Bayesian hypothesis testing. We propose a Bayesian hierarchical model which jointly analyzes the cross-validation results obtained by two classifiers on multiple data sets. It returns the posterior probability of the accuracies of the two classifiers being practically equivalent or significantly different. A further strength of the hierarchical model is that, by jointly analyzing the results obtained on all data sets, it reduces the estimation error compared to the usual approach of averaging the cross-validation results obtained on a given data set. "
781290279617822724,2016-09-29 00:31:32,https://t.co/7PPeOIhXHc,The Famine of Forte: Few Search Problems Greatly Favor Your Algorithm. (arXiv:1609.08913v1 [stat.ML]) https://t.co/7PPeOIhXHc,0,2," Abstract: No Free Lunch theorems show that the average performance across any closed-under-permutation set of problems is fixed for all algorithms, under appropriate conditions. Extending these results, we demonstrate that the proportion of favorable problems is itself strictly bounded, such that no single algorithm can perform well over a large fraction of possible problems. Our results explain why we must either continue to develop new learning methods year after year or move towards highly parameterized models that are both flexible and sensitive to their hyperparameters. "
781290277764030465,2016-09-29 00:31:32,https://t.co/A6K5LXsGHx,A Discriminative Framework for Anomaly Detection in Large Videos. (arXiv:1609.08938v1 [cs.CV]) https://t.co/A6K5LXsGHx,0,4," Abstract: We address an anomaly detection setting in which training sequences are unavailable and anomalies are scored independently of temporal ordering. Current algorithms in anomaly detection are based on the classical density estimation approach of learning high-dimensional models and finding low-probability events. These algorithms are sensitive to the order in which anomalies appear and require either training data or early context assumptions that do not hold for longer, more complex videos. By defining anomalies as examples that can be distinguished from other examples in the same video, our definition inspires a shift in approaches from classical density estimation to simple discriminative learning. Our contributions include a novel framework for anomaly detection that is (1) independent of temporal ordering of anomalies, and (2) unsupervised, requiring no separate training sequences. We show that our algorithm can achieve state-of-the-art results even when we adjust the setting by removing training sequences from standard datasets. "
781290276086226944,2016-09-29 00:31:32,https://t.co/WF4R6RZBhT,"Variational Autoencoder for Deep Learning of Images, Labels and Captions. (arXiv:1609.08976v1 [stat.ML]) https://t.co/WF4R6RZBhT",5,17," Abstract: A novel variational autoencoder is developed to model images, as well as associated labels or captions. The Deep Generative Deconvolutional Network (DGDN) is used as a decoder of the latent image features, and a deep Convolutional Neural Network (CNN) is used as an image encoder; the CNN is used to approximate a distribution for the latent DGDN features/code. The latent code is also linked to generative models for labels (Bayesian support vector machine) or captions (recurrent neural network). When predicting a label/caption for a new image at test, averaging is performed across the distribution of latent codes; this is computationally efficient as a consequence of the learned CNN-based encoder. Since the framework is capable of modeling the image in the presence/absence of associated labels/captions, a new semi-supervised setting is manifested for CNN learning with images; the framework even allows unsupervised CNN learning, based on images alone. "
781290273699667968,2016-09-29 00:31:31,https://t.co/EawrLg6mK7,Statistical analysis of latent generalized correlation matrix estimation in transelliptical distribution. (arXiv:1… https://t.co/EawrLg6mK7,0,3," Abstract: Correlation matrices play a key role in many multivariate methods (e.g., graphical model estimation and factor analysis). The current state-of-the-art in estimating large correlation matrices focuses on the use of Pearson's sample correlation matrix. Although Pearson's sample correlation matrix enjoys various good properties under Gaussian models, it is not an effective estimator when facing heavy-tailed distributions. As a robust alternative, Han and Liu [J. Am. Stat. Assoc. 109 (2015) 275-287] advocated the use of a transformed version of the Kendall's tau sample correlation matrix in estimating high dimensional latent generalized correlation matrix under the transelliptical distribution family (or elliptical copula). The transelliptical family assumes that after unspecified marginal monotone transformations, the data follow an elliptical distribution. In this paper, we study the theoretical properties of the Kendall's tau sample correlation matrix and its transformed version proposed in Han and Liu [J. Am. Stat. Assoc. 109 (2015) 275-287] for estimating the population Kendall's tau correlation matrix and the latent Pearson's correlation matrix under both spectral and restricted spectral norms. With regard to the spectral norm, we highlight the role of ""effective rank"" in quantifying the rate of convergence. With regard to the restricted spectral norm, we for the first time present a ""sign sub-Gaussian condition"" which is sufficient to guarantee that the rank-based correlation matrix estimator attains the fast rate of convergence. In both cases, we do not need any moment condition. "
781290271967436800,2016-09-29 00:31:31,https://t.co/hNrrGJUBiw,Asymptotic and finite-sample properties of estimators based on stochastic gradients. (arXiv:1408.2923v6 [stat.ME] … https://t.co/hNrrGJUBiw,0,4," Abstract: Stochastic gradient descent procedures have gained popularity for parameter estimation from large data sets. However, their statistical properties are not well understood, in theory. And in practice, avoiding numerical instability requires careful tuning of key parameters. Here, we introduce implicit stochastic gradient descent procedures, which involve parameter updates that are implicitly defined. Intuitively, implicit updates shrink standard stochastic gradient descent updates. The amount of shrinkage depends on the observed Fisher information matrix, which does not need to be explicitly computed; thus, implicit procedures increase stability without increasing the computational burden. Our theoretical analysis provides the first full characterization of the asymptotic behavior of both standard and implicit stochastic gradient descent-based estimators, including finite-sample error bounds. Importantly, analytical expressions for the variances of these stochastic gradient-based estimators reveal their exact loss of efficiency. We also develop new algorithms to compute implicit stochastic gradient descent-based estimators for generalized linear models, Cox proportional hazards, M-estimators, in practice, and perform extensive experiments. Our results suggest that implicit stochastic gradient descent procedures are poised to become a workhorse for approximate inference from large data sets "
781290269820026880,2016-09-29 00:31:30,https://t.co/Wgy0QZ2Pbz,Statistical and computational trade-offs in estimation of sparse principal components. (arXiv:1408.5369v2 [math.ST… https://t.co/Wgy0QZ2Pbz,0,3," Abstract: In recent years, sparse principal component analysis has emerged as an extremely popular dimension reduction technique for high-dimensional data. The theoretical challenge, in the simplest case, is to estimate the leading eigenvector of a population covariance matrix under the assumption that this eigenvector is sparse. An impressive range of estimators have been proposed; some of these are fast to compute, while others are known to achieve the minimax optimal rate over certain Gaussian or sub-Gaussian classes. In this paper, we show that, under a widely-believed assumption from computational complexity theory, there is a fundamental trade-off between statistical and computational performance in this problem. More precisely, working with new, larger classes satisfying a restricted covariance concentration condition, we show that there is an effective sample size regime in which no randomised polynomial time algorithm can achieve the minimax optimal rate. We also study the theoretical performance of a (polynomial time) variant of the well-known semidefinite relaxation estimator, revealing a subtle interplay between statistical and computational efficiency. "
781290267685117953,2016-09-29 00:31:30,https://t.co/XcM6UWbgt0,Modeling Recovery Curves With Application to Prostatectomy. (arXiv:1504.06964v2 [stat.ME] UPDATED) https://t.co/XcM6UWbgt0,0,2," Abstract: We propose a Bayesian model that predicts recovery curves based on information available before the disruptive event. A recovery curve of interest is the quantified sexual function of prostate cancer patients after prostatectomy surgery. We illustrate the utility of our model as a pre-treatment medical decision aid, producing personalized predictions that are both interpretable and accurate. We uncover covariate relationships that agree with and supplement that in existing medical literature. "
781290265218805761,2016-09-29 00:31:29,https://t.co/V3XRvVkTMx,WordRank: Learning Word Embeddings via Robust Ranking. (arXiv:1506.02761v4 [cs.CL] UPDATED) https://t.co/V3XRvVkTMx,0,4," Abstract: Embedding words in a vector space has gained a lot of attention in recent years. While state-of-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics. Then, based on this insight, we propose a novel framework WordRank that efficiently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm is very competitive to the state-of-the- arts on large corpora, while outperforms them by a significant margin when the training set is limited (i.e., sparse and noisy). With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark. Our multi-node distributed implementation of WordRank is publicly available for general usage. "
781290263578828800,2016-09-29 00:31:29,https://t.co/ZrjQj3vGQh,Minimum Density Hyperplanes. (arXiv:1507.04201v3 [stat.ML] UPDATED) https://t.co/ZrjQj3vGQh,0,2," Abstract: Associating distinct groups of objects (clusters) with contiguous regions of high probability density (high-density clusters), is central to many statistical and machine learning approaches to the classification of unlabelled data. We propose a novel hyperplane classifier for clustering and semi-supervised classification which is motivated by this objective. The proposed minimum density hyperplane minimises the integral of the empirical probability density function along it, thereby avoiding intersection with high density clusters. We show that the minimum density and the maximum margin hyperplanes are asymptotically equivalent, thus linking this approach to maximum margin clustering and semi-supervised support vector classifiers. We propose a projection pursuit formulation of the associated optimisation problem which allows us to find minimum density hyperplanes efficiently in practice, and evaluate its performance on a range of benchmark datasets. The proposed approach is found to be very competitive with state of the art methods for clustering and semi-supervised classification. "
781290261871730688,2016-09-29 00:31:28,https://t.co/YNTIgnkiM2,Stochastically Transitive Models for Pairwise Comparisons: Statistical and Computational Issues. (arXiv:1510.05610… https://t.co/YNTIgnkiM2,0,2," Abstract: There are various parametric models for analyzing pairwise comparison data, including the Bradley-Terry-Luce (BTL) and Thurstone models, but their reliance on strong parametric assumptions is limiting. In this work, we study a flexible model for pairwise comparisons, under which the probabilities of outcomes are required only to satisfy a natural form of stochastic transitivity. This class includes parametric models including the BTL and Thurstone models as special cases, but is considerably more general. We provide various examples of models in this broader stochastically transitive class for which classical parametric models provide poor fits. Despite this greater flexibility, we show that the matrix of probabilities can be estimated at the same rate as in standard parametric models. On the other hand, unlike in the BTL and Thurstone models, computing the minimax-optimal estimator in the stochastically transitive model is non-trivial, and we explore various computationally tractable alternatives. We show that a simple singular value thresholding algorithm is statistically consistent but does not achieve the minimax rate. We then propose and study algorithms that achieve the minimax rate over interesting sub-classes of the full stochastically transitive class. We complement our theoretical results with thorough numerical simulations. "
781290260307247104,2016-09-29 00:31:28,https://t.co/llOcY64Qtv,MERLiN: Mixture Effect Recovery in Linear Networks. (arXiv:1512.01255v3 [stat.ME] UPDATED) https://t.co/llOcY64Qtv,0,4," Abstract: Causal inference concerns the identification of cause-effect relationships between variables, e.g. establishing whether a stimulus affects activity in a certain brain region. The observed variables themselves often do not constitute meaningful causal variables, however, and linear combinations need to be considered. In electroencephalographic studies, for example, one is not interested in establishing cause-effect relationships between electrode signals (the observed variables), but rather between cortical signals (the causal variables) which can be recovered as linear combinations of electrode signals. We introduce MERLiN (Mixture Effect Recovery in Linear Networks), a family of causal inference algorithms that implement a novel means of constructing causal variables from non-causal variables. We demonstrate through application to EEG data how the basic MERLiN algorithm can be extended for application to different (neuroimaging) data modalities. Given an observed linear mixture, the algorithms can recover a causal variable that is a linear effect of another given variable. That is, MERLiN allows us to recover a cortical signal that is affected by activity in a certain brain region, while not being a direct effect of the stimulus. The Python/Matlab implementation for all presented algorithms is available on this https URL "
781290257920688129,2016-09-29 00:31:27,https://t.co/O1zMCeRMGU,Predictive Coarse-Graining. (arXiv:1605.08301v2 [stat.ML] UPDATED) https://t.co/O1zMCeRMGU,1,3," Abstract: We propose a data-driven, coarse-graining formulation in the context of equilibrium statistical mechanics. In contrast to existing techniques which are based on a fine-to-coarse map, we adopt the opposite strategy by prescribing a probabilistic coarse-to-fine map. This corresponds to a directed probabilistic model where the coarse variables play the role of latent generators of the fine scale (all-atom) data. From an information-theoretic perspective, the framework proposed provides an improvement upon the relative entropy method and is capable of quantifying the uncertainty due to the information loss that unavoidably takes place during the CG process. Furthermore, it can be readily extended to a fully Bayesian model where various sources of uncertainties are reflected in the posterior of the model parameters. The latter can be used to produce not only point estimates of fine-scale reconstructions or macroscopic observables, but more importantly, predictive posterior distributions on these quantities. Predictive posterior distributions reflect the confidence of the model as a function of the amount of data and the level of coarse-graining. The issues of model complexity and model selection are seamlessly addressed by employing a hierarchical prior that favors the discovery of sparse solutions, revealing the most prominent features in the coarse-grained model. A flexible and parallelizable Monte Carlo - Expectation-Maximization (MC-EM) scheme is proposed for carrying out inference and learning tasks. A comparative assessment of the proposed methodology is presented for a lattice spin system and the SPC/E water model. "
781290256469491712,2016-09-29 00:31:27,https://t.co/uI9K7tMLES,Whole-brain substitute CT generation using Markov random field mixture models. (arXiv:1607.02188v2 [stat.AP] UPDAT… https://t.co/uI9K7tMLES,0,2," Abstract: Computed tomography (CT) equivalent information is needed for attenuation correction in PET imaging and for dose planning in radiotherapy. Prior work has shown that Gaussian mixture models can be used to generate a substitute CT (s-CT) image from a specific set of MRI modalities. This work introduces a more flexible class of mixture models for s-CT generation, that incorporates spatial dependency in the data through a Markov random field prior on the latent field of class memberships associated with a mixture model. Furthermore, the mixture distributions are extended from Gaussian to normal inverse Gaussian (NIG), allowing heavier tails and skewness. The amount of data needed to train a model for s-CT generation is of the order of 100 million voxels. The computational efficiency of the parameter estimation and prediction methods are hence paramount, especially when spatial dependency is included in the models. A stochastic Expectation Maximization (EM) gradient algorithm is proposed in order to tackle this challenge. The advantages of the spatial model and NIG distributions are evaluated with a cross-validation study based on data from 14 patients. The study show that the proposed model enhances the predictive quality of the s-CT images by reducing the mean absolute error with 17.9%. Also, the distribution of CT values conditioned on the MR images are better explained by the proposed model as evaluated using continuous ranked probability scores. "
781290254812798976,2016-09-29 00:31:27,https://t.co/AOxUnkK4Zm,Sequence Graph Transform (SGT): A Feature Extraction Function for Sequence Data Mining. (arXiv:1608.03533v4 [stat.… https://t.co/AOxUnkK4Zm,0,6," Abstract: The ubiquitous presence of sequence data across fields such as the web, healthcare, bioinformatics, and text mining has made sequence mining a vital research area. However, sequence mining is particularly challenging because of difficulty in finding (dis)similarity/distance between sequences. This is because a distance measure between sequences is not obvious due to their unstructuredness---arbitrary strings of arbitrary length. Feature representations, such as n-grams, are often used but they either compromise on extracting both short- and long-term sequence patterns or have a high computation. We propose a new function, Sequence Graph Transform (SGT), that extracts the short- and long-term sequence features and embeds them in a finite-dimensional feature space. Importantly, SGT has low computation and can extract any amount of short- to long-term patterns without any increase in the computation, also proved theoretically in this paper. Due to this, SGT yields superior result with significantly higher accuracy and lower computation compared to the existing methods. We show it via several experimentation and SGT's real world application for clustering, classification, search and visualization as examples. "
781290252472377344,2016-09-29 00:31:26,https://t.co/sJvjdeYTfo,Why does deep and cheap learning work so well?. (arXiv:1608.08225v2 [cond-mat.dis-nn] UPDATED) https://t.co/sJvjdeYTfo,2,13," Abstract: We show how the success of deep learning depends not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can be approximated through ""cheap learning"" with exponentially fewer parameters than generic ones, because they have simplifying properties tracing back to the laws of physics. The exceptional simplicity of physics-based functions hinges on properties such as symmetry, locality, compositionality and polynomial log-probability, and we explore how these properties translate into exceptionally simple neural networks approximating both natural phenomena such as images and abstract representations thereof such as drawings. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to renormalization group procedures. We prove various ""no-flattening theorems"" showing when such efficient deep networks cannot be accurately approximated by shallow ones without efficiency loss: flattening even linear functions can be costly, and flattening polynomials is exponentially expensive; we use group theoretic techniques to show that n variables cannot be multiplied using fewer than 2^n neurons in a single hidden layer. "
780928858086309888,2016-09-28 00:35:23,https://t.co/vbhxc55qga,Robust Time-Series Retrieval Using Probabilistic Adaptive Segmental Alignment. (arXiv:1609.08201v1 [cs.DB]) https://t.co/vbhxc55qga,1,4," Abstract: Traditional pairwise sequence alignment is based on matching individual samples from two sequences, under time monotonicity constraints. However, in many application settings matching subsequences (segments) instead of individual samples may bring in additional robustness to noise or local non-causal perturbations. This paper presents an approach to segmental sequence alignment that jointly segments and aligns two sequences, generalizing the traditional per-sample alignment. To accomplish this task, we introduce a distance metric between segments based on average pairwise distances and then present a modified pair-HMM (PHMM) that incorporates the proposed distance metric to solve the joint segmentation and alignment task. We also propose a relaxation to our model that improves the computational efficiency of the generic segmental PHMM. Our results demonstrate that this new measure of sequence similarity can lead to improved classification performance, while being resilient to noise, on a variety of sequence retrieval problems, from EEG to motion sequence classification. "
780928850859614208,2016-09-28 00:35:21,https://t.co/hJ9oPuCz7e,Variational Inference with Hamiltonian Monte Carlo. (arXiv:1609.08203v1 [stat.ML]) https://t.co/hJ9oPuCz7e,3,15," Abstract: Variational inference lies at the core of many state-of-the-art algorithms. To improve the approximation of the posterior beyond parametric families, it was proposed to include MCMC steps into the variational lower bound. In this work we explore this idea using steps of the Hamiltonian Monte Carlo (HMC) algorithm, an efficient MCMC method. In particular, we incorporate the acceptance step of the HMC algorithm, guaranteeing asymptotic convergence to the true posterior. Additionally, we introduce some extensions to the HMC algorithm geared towards faster convergence. The theoretical advantages of these modifications are reflected by performance improvements in our experimental results. "
780928840579293184,2016-09-28 00:35:19,https://t.co/fKyuim4T6I,Automatic Construction of a Recurrent Neural Network based Classifier for Vehicle Passage Detection. (arXiv:1609.0… https://t.co/fKyuim4T6I,1,3," Abstract: Recurrent Neural Networks (RNNs) are extensively used for time-series modeling and prediction. We propose an approach for automatic construction of a binary classifier based on Long Short-Term Memory RNNs (LSTM-RNNs) for detection of a vehicle passage through a checkpoint. As an input to the classifier we use multidimensional signals of various sensors that are installed on the checkpoint. Obtained results demonstrate that the previous approach to handcrafting a classifier, consisting of a set of deterministic rules, can be successfully replaced by an automatic RNN training on an appropriately labelled data. "
780928829351137280,2016-09-28 00:35:16,https://t.co/O82DMTkdSN,Online Categorical Subspace Learning for Sketching Big Data with Misses. (arXiv:1609.08235v1 [stat.ML]) https://t.co/O82DMTkdSN,0,5," Abstract: With the scale of data growing every day, reducing the dimensionality (a.k.a. sketching) of high-dimensional data has emerged as a task of paramount importance. Relevant issues to address in this context include the sheer volume of data that may consist of categorical samples, the typically streaming format of acquisition, and the possibly missing entries. To cope with these challenges, the present paper develops a novel categorical subspace learning approach to unravel the latent structure for three prominent categorical (bilinear) models, namely, Probit, Tobit, and Logit. The deterministic Probit and Tobit models treat data as quantized values of an analog-valued process lying in a low-dimensional subspace, while the probabilistic Logit model relies on low dimensionality of the data log-likelihood ratios. Leveraging the low intrinsic dimensionality of the sought models, a rank regularized maximum-likelihood estimator is devised, which is then solved recursively via alternating majorization-minimization to sketch high-dimensional categorical data `on the fly.' The resultant procedure alternates between sketching the new incomplete datum and refining the latent subspace, leading to lightweight first-order algorithms with highly parallelizable tasks per iteration. As an extra degree of freedom, the quantization thresholds are also learned jointly along with the subspace to enhance the predictive power of the sought models. Performance of the subspace iterates is analyzed for both infinite and finite data streams, where for the former asymptotic convergence to the stationary point set of the batch estimator is established, while for the latter sublinear regret bounds are derived for the empirical cost. Simulated tests with both synthetic and real-world datasets corroborate the merits of the novel schemes for real-time movie recommendation and chess-game classification. "
780928818827649024,2016-09-28 00:35:14,https://t.co/R5ORqTYfIT,Multi-label Methods for Prediction with Sequential Data. (arXiv:1609.08349v1 [cs.LG]) https://t.co/R5ORqTYfIT,0,10," Abstract: The number of methods available for classification of multi-label data has increased rapidly over recent years, yet relatively few links have been made with the related task of classification of sequential data. If labels indices are considered as time indices, the problems can often be seen as equivalent. In this paper we detect and elaborate on connections between multi-label methods and Markovian models, and study the suitability of multi-label methods for prediction in sequential data. From this study we draw upon the most suitable techniques from the area and develop two novel competitive approaches which can be applied to either kind of data. We carry out an empirical evaluation investigating performance on real-world sequential-prediction tasks: electricity demand, and route prediction. As well as showing that several popular multi-label algorithms are in fact easily applicable to sequencing tasks, our novel approaches, which benefit from a unified view of these areas, prove very competitive against established methods. "
780928809046568960,2016-09-28 00:35:11,https://t.co/RLFis8NtSd,Multiple protein feature prediction with statistical relational learning. (arXiv:1609.08391v1 [stat.ML]) https://t.co/RLFis8NtSd,0,5," Abstract: High throughput sequencing techniques have highly impactedon modern biology, widening the gap between sequenced andannotated data. Automatic annotation tools are thereforeof the foremost importance to guide biologists' experiments. However, most of the state-of-the-art methods rely on annotation transfer, offering reliable predictions only in homology settings. In this work we present a novel appraoch to protein feature prediction, which exploits the Semanti Based Regularization to inject prior knowledge in the learning process. The experimental results conducted on the yeast genome show that the introduction of the constraints positively impacts on the overall prediction quality. "
780928799085035520,2016-09-28 00:35:09,https://t.co/whAS8zJMBw,Generalization Error Bounds for Optimization Algorithms via Stability. (arXiv:1609.08397v1 [stat.ML]) https://t.co/whAS8zJMBw,0,4," Abstract: Many machine learning tasks can be formulated as Regularized Empirical Risk Minimization (R-ERM), and solved by optimization algorithms such as gradient descent (GD), stochastic gradient descent (SGD), and stochastic variance reduction (SVRG). Conventional analysis on these optimization algorithms focuses on their convergence rates during the training process, however, people in the machine learning community may care more about the generalization performance of the learned model on unseen test data. In this paper, we investigate on this issue, by using stability as a tool. In particular, we decompose the generalization error for R-ERM, and derive its upper bound for both convex and non-convex cases. In convex cases, we prove that the generalization error can be bounded by the convergence rate of the optimization algorithm and the stability of the R-ERM process, both in expectation (in the order of $\mathcal{O}((1/n)+\mathbb{E}\rho(T))$, where $\rho(T)$ is the convergence error and $T$ is the number of iterations) and in high probability (in the order of $\mathcal{O}\left(\frac{\log{1/\delta}}{\sqrt{n}}+\rho(T)\right)$ with probability $1-\delta$). For non-convex cases, we can also obtain a similar expected generalization error bound. Our theorems indicate that 1) along with the training process, the generalization error will decrease for all the optimization algorithms under our investigation; 2) Comparatively speaking, SVRG has better generalization ability than GD and SGD. We have conducted experiments on both convex and non-convex problems, and the experimental results verify our theoretical findings. "
780928789039702017,2016-09-28 00:35:06,https://t.co/eeuD5DUZHC,Modelling Radiological Language with Bidirectional Long Short-Term Memory Networks. (arXiv:1609.08409v1 [cs.CL]) https://t.co/eeuD5DUZHC,2,5," Abstract: Motivated by the need to automate medical information extraction from free-text radiological reports, we present a bi-directional long short-term memory (BiLSTM) neural network architecture for modelling radiological language. The model has been used to address two NLP tasks: medical named-entity recognition (NER) and negation detection. We investigate whether learning several types of word embeddings improves BiLSTM's performance on those tasks. Using a large dataset of chest x-ray reports, we compare the proposed model to a baseline dictionary-based NER system and a negation detection system that leverages the hand-crafted rules of the NegEx algorithm and the grammatical relations obtained from the Stanford Dependency Parser. Compared to these more traditional rule-based systems, we argue that BiLSTM offers a strong alternative for both our tasks. "
780928777538920448,2016-09-28 00:35:04,https://t.co/AKIPhNQiT0,Exact and Inexact Subsampled Newton Methods for Optimization. (arXiv:1609.08502v1 [math.OC]) https://t.co/AKIPhNQiT0,0,4," Abstract: The paper studies the solution of stochastic optimization problems in which approximations to the gradient and Hessian are obtained through subsampling. We first consider Newton-like methods that employ these approximations and discuss how to coordinate the accuracy in the gradient and Hessian to yield a superlinear rate of convergence in expectation. The second part of the paper analyzes an inexact Newton method that solves linear systems approximately using the conjugate gradient (CG) method, and that samples the Hessian and not the gradient (the gradient is assumed to be exact). We provide a complexity analysis for this method based on the properties of the CG iteration and the quality of the Hessian approximation, and compare it with a method that employs a stochastic gradient iteration instead of the CG method. We report preliminary numerical results that illustrate the performance of inexact subsampled Newton methods on machine learning applications based on logistic regression. "
780928767644532737,2016-09-28 00:35:01,https://t.co/wSYNj774z0,A Wild Bootstrap for Degenerate Kernel Tests. (arXiv:1408.5404v2 [stat.ML] UPDATED) https://t.co/wSYNj774z0,0,3," Abstract: A wild bootstrap method for nonparametric hypothesis tests based on kernel distribution embeddings is proposed. This bootstrap method is used to construct provably consistent tests that apply to random processes, for which the naive permutation-based bootstrap fails. It applies to a large group of kernel tests based on V-statistics, which are degenerate under the null hypothesis, and non-degenerate elsewhere. To illustrate this approach, we construct a two-sample test, an instantaneous independence test and a multiple lag independence test for time series. In experiments, the wild bootstrap gives strong performance on synthetic examples, on audio data, and in performance benchmarking for the Gibbs sampler. "
780928756181504000,2016-09-28 00:34:59,https://t.co/IWogNFiYFY,A Generalization of the Borkar-Meyn Theorem for Stochastic Recursive Inclusions. (arXiv:1502.01953v3 [cs.SY] UPDAT… https://t.co/IWogNFiYFY,0,2," Abstract: In this paper the stability theorem of Borkar and Meyn is extended to include the case when the mean field is a differential inclusion. Two different sets of sufficient conditions are presented that guarantee the stability and convergence of stochastic recursive inclusions. Our work builds on the works of Benaim, Hofbauer and Sorin as well as Borkar and Meyn. As a corollary to one of the main theorems, a natural generalization of the Borkar and Meyn Theorem follows. In addition, the original theorem of Borkar and Meyn is shown to hold under slightly relaxed assumptions. Finally, as an application to one of the main theorems we discuss a solution to the approximate drift problem. "
780928745343442944,2016-09-28 00:34:56,https://t.co/qI3waC9vKM,A Kernel Test of Goodness of Fit. (arXiv:1602.02964v4 [stat.ML] UPDATED) https://t.co/qI3waC9vKM,0,5," Abstract: We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein's method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation. "
780928734861885440,2016-09-28 00:34:54,https://t.co/M2BYbq5pV4,Multi-task Recurrent Model for Speech and Speaker Recognition. (arXiv:1603.09643v4 [cs.CL] UPDATED) https://t.co/M2BYbq5pV4,1,14," Abstract: Although highly correlated, speech and speaker recognition have been regarded as two independent tasks and studied by two communities. This is certainly not the way that people behave: we decipher both speech content and speaker traits at the same time. This paper presents a unified model to perform speech and speaker recognition simultaneously and altogether. The model is based on a unified neural network where the output of one task is fed to the input of the other, leading to a multi-task recurrent network. Experiments show that the joint model outperforms the task-specific models on both the two tasks. "
780928726401941504,2016-09-28 00:34:51,https://t.co/DGckHIzgzV,"Analysis of gradient descent methods with non-diminishing, bounded errors. (arXiv:1604.00151v2 [cs.SY] UPDATED) https://t.co/DGckHIzgzV",0,7," Abstract: In this paper, we present easily verifiable, sufficient conditions for both stability and convergence (to the minimum set) of gradient descent ($GD$) algorithms with bounded, non-diminishing errors. These errors often arise from using gradient estimators or because the objective function is noisy to begin with. Our work extends the contributions of Mangasarian \& Solodov and Bertsekas \& Tsitsiklis. Our framework improves over the aforementioned ones in that both stability (almost sure boundedness) and convergence are guaranteed even in the case of $GD$ with non-diminishing errors. We present a simplified, yet effective implementation of $GD$ using $SPSA$ with constant sensitivity parameters. Further, unlike other papers no `additional' restrictions are imposed on the step-size and so also on the learning rate when used to implement machine learning algorithms. Finally, we present the results of some experiments to validate the theory. "
780928716138504192,2016-09-28 00:34:49,https://t.co/LdUJdtC3vn,Dense Associative Memory for Pattern Recognition. (arXiv:1606.01164v2 [cs.NE] UPDATED) https://t.co/LdUJdtC3vn,0,5," Abstract: A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set. "
780928705916985344,2016-09-28 00:34:47,https://t.co/lk1W8FmIun,Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach. (arXiv:1609.03240v2 [stat… https://t.co/lk1W8FmIun,0,2," Abstract: We consider the non-square matrix sensing problem, under restricted isometry property (RIP) assumptions. We focus on the non-convex formulation, where any rank-$r$ matrix $X \in \mathbb{R}^{m \times n}$ is represented as $UV^\top$, where $U \in \mathbb{R}^{m \times r}$ and $V \in \mathbb{R}^{n \times r}$. In this paper, we complement recent findings on the non-convex geometry of the analogous PSD setting [5], and show that matrix factorization does not introduce any spurious local minima, under RIP. "
780928691211792384,2016-09-28 00:34:43,https://t.co/joljeGS7ZG,TristouNet: Triplet Loss for Speaker Turn Embedding. (arXiv:1609.04301v2 [cs.SD] UPDATED) https://t.co/joljeGS7ZG,0,2," Abstract: TristouNet is a neural network architecture based on Long Short-Term Memory recurrent networks, meant to project speech sequences into a fixed-dimensional euclidean space. Thanks to the triplet loss paradigm used for training, the resulting sequence embeddings can be compared directly with the euclidean distance, for speaker comparison purposes. Experiments on short (between 500ms and 5s) speech turn comparison and speaker change detection show that TristouNet brings significant improvements over the current state-of-the-art techniques for both tasks. "
780571013746745344,2016-09-27 00:53:26,https://t.co/wRBPRbdjU5,Predictive modelling of football injuries. (arXiv:1609.07480v1 [stat.AP]) https://t.co/wRBPRbdjU5,0,4," Abstract: The goal of this thesis is to investigate the potential of predictive modelling for football injuries. This work was conducted in close collaboration with Tottenham Hotspurs FC (THFC), the PGA European tour and the participation of Wolverhampton Wanderers (WW). Three investigations were conducted: 1. Predicting the recovery time of football injuries using the UEFA injury recordings: The UEFA recordings is a common standard for recording injuries in professional football. For this investigation, three datasets of UEFA injury recordings were available. Different machine learning algorithms were used in order to build a predictive model. The performance of the machine learning models is then improved by using feature selection conducted through correlation-based subset feature selection and random forests. 2. Predicting injuries in professional football using exposure records: The relationship between exposure (in training hours and match hours) in professional football athletes and injury incidence was studied. A common problem in football is understanding how the training schedule of an athlete can affect the chance of him getting injured. The task was to predict the number of days a player can train before he gets injured. 3. Predicting intrinsic injury incidence using in-training GPS measurements: A significant percentage of football injuries can be attributed to overtraining and fatigue. GPS data collected during training sessions might provide indicators of fatigue, or might be used to detect very intense training sessions which can lead to overtraining. This research used GPS data gathered during training sessions of the first team of THFC, in order to predict whether an injury would take place during a week. "
780571010894536704,2016-09-27 00:53:25,https://t.co/zLkbU8eIdf,Fast Learning of Clusters and Topics via Sparse Posteriors. (arXiv:1609.07521v1 [stat.ML]) https://t.co/zLkbU8eIdf,0,3," Abstract: Mixture models and topic models generate each observation from a single cluster, but standard variational posteriors for each observation assign positive probability to all possible clusters. This requires dense storage and runtime costs that scale with the total number of clusters, even though typically only a few clusters have significant posterior mass for any data point. We propose a constrained family of sparse variational distributions that allow at most $L$ non-zero entries, where the tunable threshold $L$ trades off speed for accuracy. Previous sparse approximations have used hard assignments ($L=1$), but we find that moderate values of $L>1$ provide superior performance. Our approach easily integrates with stochastic or incremental optimization algorithms to scale to millions of examples. Experiments training mixture models of image patches and topic models for news articles show that our approach produces better-quality models in far less time than baseline methods. "
780571008629633025,2016-09-27 00:53:25,https://t.co/cn4LQetdAL,"A Tutorial on Distributed (Non-Bayesian) Learning: Problem, Algorithms and Results. (arXiv:1609.07537v1 [math.OC]) https://t.co/cn4LQetdAL",1,8," Abstract: We overview some results on distributed learning with focus on a family of recently proposed algorithms known as non-Bayesian social learning. We consider different approaches to the distributed learning problem and its algorithmic solutions for the case of finitely many hypotheses. The original centralized problem is discussed at first, and then followed by a generalization to the distributed setting. The results on convergence and convergence rate are presented for both asymptotic and finite time regimes. Various extensions are discussed such as those dealing with directed time-varying networks, Nesterov's acceleration technique and a continuum sets of hypothesis. "
780571006415081476,2016-09-27 00:53:24,https://t.co/vT53aHf4Yw,Informative Planning and Online Learning with Sparse Gaussian Processes. (arXiv:1609.07560v1 [cs.RO]) https://t.co/vT53aHf4Yw,0,6," Abstract: A big challenge in environmental monitoring is the spatiotemporal variation of the phenomena to be observed. To enable persistent sensing and estimation in such a setting, it is beneficial to have a time-varying underlying environmental model. Here we present a planning and learning method that enables an autonomous marine vehicle to perform persistent ocean monitoring tasks by learning and refining an environmental model. To alleviate the computational bottleneck caused by large-scale data accumulated, we propose a framework that iterates between a planning component aimed at collecting the most information-rich data, and a sparse Gaussian Process learning component where the environmental model and hyperparameters are learned online by taking advantage of only a subset of data that provides the greatest contribution. Our simulations with ground-truth ocean data shows that the proposed method is both accurate and efficient. "
780571004393418752,2016-09-27 00:53:24,https://t.co/2hDybyr3HQ,Dynamic Pricing in High-dimensions. (arXiv:1609.07574v1 [stat.ML]) https://t.co/2hDybyr3HQ,0,5," Abstract: We study the pricing problem faced by a firm that sells a large number of products, described via a wide range of features, to customers that arrive over time. This is motivated in part by the prevalence of online marketplaces that allow for real-time pricing. We propose a dynamic policy, called Regularized Maximum Likelihood Pricing (RMLP), that obtains asymptotically optimal revenue. Our policy leverages the structure (sparsity) of a high-dimensional demand space in order to obtain a logarithmic regret compared to the clairvoyant policy that knows the parameters of the demand in advance. More specifically, the regret of our algorithm is of $O(s_0 \log T (\log d + \log T))$, where $d$ and $s_0$ correspond to the dimension of the demand space and its sparsity. Furthermore, we show that no policy can obtain regret better than $O(s_0 (\log d + \log T))$. "
780571002287878144,2016-09-27 00:53:23,https://t.co/0M12e3qDwT,Max-Norm Optimization for Robust Matrix Recovery. (arXiv:1609.07664v1 [stat.ML]) https://t.co/0M12e3qDwT,0,2," Abstract: This paper studies the matrix completion problem under arbitrary sampling schemes. We propose a new estimator incorporating both max-norm and nuclear-norm regularization, based on which we can conduct efficient low-rank matrix recovery using a random subset of entries observed with additive noise under general non-uniform and unknown sampling distributions. This method significantly relaxes the uniform sampling assumption imposed for the widely used nuclear-norm penalized approach, and makes low-rank matrix recovery feasible in more practical settings. Theoretically, we prove that the proposed estimator achieves fast rates of convergence under different settings. Computationally, we propose an alternating direction method of multipliers algorithm to efficiently compute the estimator, which bridges a gap between theory and practice of machine learning methods with max-norm regularization. Further, we provide thorough numerical studies to evaluate the proposed method using both simulated and real datasets. "
780570999519584258,2016-09-27 00:53:23,https://t.co/XSy8KuTOzN,Construction Safety Risk Modeling and Simulation. (arXiv:1609.07912v1 [stat.AP]) https://t.co/XSy8KuTOzN,0,2," Abstract: By building on a recently introduced genetic-inspired attribute-based conceptual framework for safety risk analysis, we propose a novel methodology to compute construction univariate and bivariate construction safety risk at a situational level. Our fully data-driven approach provides construction practitioners and academicians with an easy and automated way of extracting valuable empirical insights from databases of unstructured textual injury reports. By applying our methodology on an attribute and outcome dataset directly obtained from 814 injury reports, we show that the frequency-magnitude distribution of construction safety risk is very similar to that of natural phenomena such as precipitation or earthquakes. Motivated by this observation, and drawing on state-of-the-art techniques in hydroclimatology and insurance, we introduce univariate and bivariate nonparametric stochastic safety risk generators, based on Kernel Density Estimators and Copulas. These generators enable the user to produce large numbers of synthetic safety risk values faithfully to the original data, allowing safetyrelated decision-making under uncertainty to be grounded on extensive empirical evidence. Just like the accurate modeling and simulation of natural phenomena such as wind or streamflow is indispensable to successful structure dimensioning or water reservoir management, we posit that improving construction safety calls for the accurate modeling, simulation, and assessment of safety risk. The underlying assumption is that like natural phenomena, construction safety may benefit from being studied in an empirical and quantitative way rather than qualitatively which is the current industry standard. Finally, a side but interesting finding is that attributes related to high energy levels and to human error emerge as strong risk shapers on the dataset we used to illustrate our methodology. "
780570997527236612,2016-09-27 00:53:22,https://t.co/M4nE6QG5RG,Multiplicative LSTM for sequence modelling. (arXiv:1609.07959v1 [cs.NE]) https://t.co/M4nE6QG5RG,2,5," Abstract: This paper introduces multiplicative LSTM, a novel hybrid recurrent neural network architecture for sequence modelling that combines the long short-term memory (LSTM) and multiplicative recurrent neural network architectures. Multiplicative LSTM is motivated by its flexibility to have very different recurrent transition functions for each possible input, which we argue helps make it more expressive in autoregressive density estimation. We show empirically that multiplicative LSTM outperforms standard LSTM and its deep variants for a range of character level modelling tasks. We also found that this improvement increases as the complexity of the task scales up. This model achieves a test error of 1.19 bits/character on the last 4 million characters of the Hutter prize dataset when combined with dynamic evaluation. "
780570993693757440,2016-09-27 00:53:21,https://t.co/W6Be0VAbnu,Dropout with Expectation-linear Regularization. (arXiv:1609.08017v1 [cs.LG]) https://t.co/W6Be0VAbnu,0,8," Abstract: Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout's training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an \emph{explicit} control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently. "
780570989969235972,2016-09-27 00:53:20,https://t.co/W0mdeZHVpY,One-Class SVM with Privileged Information and its Application to Malware Detection. (arXiv:1609.08039v1 [stat.ML]) https://t.co/W0mdeZHVpY,0,3," Abstract: A number of important applied problems in engineering, finance and medicine can be formulated as a problem of anomaly detection. A classical approach to the problem is to describe a normal state using a one-class support vector machine. Then to detect anomalies we quantify a distance from a new observation to the constructed description of the normal class. In this paper we present a new approach to the one-class classification. We formulate a new problem statement and a corresponding algorithm that allow taking into account a privileged information during the training phase. We evaluate performance of the proposed approach using a synthetic dataset, as well as the publicly available Microsoft Malware Classification Challenge dataset. "
780566039390224384,2016-09-27 00:33:40,https://t.co/BMst1V90Wj,Fast Gaussian Process Regression for Big Data. (arXiv:1509.05142v5 [cs.LG] UPDATED) https://t.co/BMst1V90Wj,5,15," Abstract: Gaussian Processes are widely used for regression tasks. A known limitation in the application of Gaussian Processes to regression tasks is that the computation of the solution requires performing a matrix inversion. The solution also requires the storage of a large matrix in memory. These factors restrict the application of Gaussian Process regression to small and moderate size data sets. We present an algorithm based on empirically determined subset selection. The algorithm is based on applying model averaging to Gaussian Process estimators developed on bootstrapped datasets. We compare the performance of this algorithm with two other methods that are used to apply Gaussian Processes regression to large datasets. In the proposed method, hyper-parameter learning is performed over small datasets and requires very little tuning effort. Methods currently used to apply Gaussian Process regression to large datasets are typically associated with more hyper-parameters than the proposed method and can require a significant tuning effort. The results of the experiments reported in this work are consistent with results from Mini-max theory for non-parametric regression. The key benefit of this algorithm is the simplicity associated with implementation.. "
780566037628653568,2016-09-27 00:33:40,https://t.co/wzNULPvAPw,Uniform {\varepsilon}-Stability of Distributed Nonlinear Filtering over DNAs: Gaussian-Finite HMMs. (arXiv:1602.04… https://t.co/wzNULPvAPw,1,3," Abstract: In this work, we study stability of distributed filtering of Markov chains with finite state space, partially observed in conditionally Gaussian noise. We consider a nonlinear filtering scheme over a Distributed Network of Agents (DNA), which relies on the distributed evaluation of the likelihood part of the centralized nonlinear filter and is based on a particular specialization of the Alternating Direction Method of Multipliers (ADMM) for fast average consensus. Assuming the same number of consensus steps between any two consecutive noisy measurements for each sensor in the network, we fully characterize a minimal number of such steps, such that the distributed filter remains uniformly stable with a prescribed accuracy level, {\varepsilon} \in (0,1], within a finite operational horizon, T, and across all sensors. Stability is in the sense of the \ell_1-norm between the centralized and distributed versions of the posterior at each sensor, and at each time within T. Roughly speaking, our main result shows that uniform {\varepsilon}-stability of the distributed filtering process depends only loglinearly on T and (roughly) the size of the network, and only logarithmically on 1/{\varepsilon}. If this total loglinear bound is fulfilled, any additional consensus iterations will incur a fully quantified further exponential decay in the consensus error. Our bounds are universal, in the sense that they are independent of the particular structure of the Gaussian Hidden Markov Model (HMM) under consideration. "
780566035120414720,2016-09-27 00:33:39,https://t.co/6K6Z6MHpt6,Modeling Short Over-Dispersed Spike Count Data: A Hierarchical Parametric Empirical Bayes Framework. (arXiv:1605.0… https://t.co/6K6Z6MHpt6,0,3," Abstract: In this letter, a Hierarchical Parametric Empirical Bayes model is proposed to model spike count data. We have integrated Generalized Linear Models (GLMs) and empirical Bayes theory to simultaneously provide three advantages: (1) a model of over-dispersion of spike count values; (2) reduced MSE in estimation when compared to using the maximum likelihood method for GLMs; and (3) an efficient alternative to inference with fully Bayes estimators. We apply the model to study both simulated data and experimental neural data from the retina. The simulation results indicate that the new model can estimate both the weights of connections among neural populations and the output firing rates (mean spike count) efficiently and accurately. The results from the retinal datasets show that the proposed model outperforms both standard Poisson and Negative Binomial GLMs in terms of the prediction log-likelihood of held-out datasets. "
780566033379815424,2016-09-27 00:33:39,https://t.co/L6pjhYHsoC,Sampling Method for Fast Training of Support Vector Data Description. (arXiv:1606.05382v3 [cs.LG] UPDATED) https://t.co/L6pjhYHsoC,0,2, Abstract: Support Vector Data Description (SVDD) is a popular outlier detection technique which constructs a flexible description of the input data. SVDD computation time is high for large training datasets which limits its use in big-data process-monitoring applications. We propose a new iterative sampling-based method for SVDD training. The method incrementally learns the training data description at each iteration by computing SVDD on an independent random sample selected with replacement from the training data set. The experimental results indicate that the proposed method is extremely fast and provides a good data description . 
780566031714705408,2016-09-27 00:33:38,https://t.co/99pZenRM02,Mixtures of Bivariate von Mises Distributions with Applications to Modelling of Protein Dihedral Angles. (arXiv:16… https://t.co/99pZenRM02,0,2," Abstract: The modelling of empirically observed data is commonly done using mixtures of probability distributions. In order to model angular data, directional probability distributions such as the bivariate von Mises (BVM) is typically used. The critical task involved in mixture modelling is to determine the optimal number of component probability distributions. We employ the Bayesian information-theoretic principle of minimum message length (MML) to distinguish mixture models by balancing the trade-off between the model's complexity and its goodness-of-fit to the data. We consider the problem of modelling angular data resulting from the spatial arrangement of protein structures using BVM distributions. The main contributions of the paper include the development of the mixture modelling apparatus along with the MML estimation of the parameters of the BVM distribution. We demonstrate that statistical inference using the MML framework supersedes the traditional methods and offers a mechanism to objectively determine models that are of practical significance. "
780566029663600640,2016-09-27 00:33:38,https://t.co/qj4bcJTpKl,Fast Global Convergence of Online PCA. (arXiv:1607.07837v2 [math.OC] UPDATED) https://t.co/qj4bcJTpKl,0,2," Abstract: We study streaming principal component analysis (PCA), that is to find the top $k$ eigenvectors of a $d\times d$ hidden matrix $\bf \Sigma$ with online vectors drawn from covariance matrix $\bf \Sigma$. We provide GLOBAL convergence for Oja's algorithm which is popularly used in practice but lacks theoretical understanding for $k>1$. We also provide a modified variant $\mathsf{Oja}^{++}$ that runs EVEN FASTER than Oja's. Our results match the information theoretic lower bound in terms of dependency on error, on eigengap, on rank $k$, and on dimension $d$, up to poly-log factors. In addition, our convergence rate can be made gap-free, that is proportional to the approximation error and independent of the eigengap. In contrast, for general rank $k$, before our work (1) it was open to design any algorithm with efficient global convergence rate; and (2) it was open to design any algorithm with (even local) gap-free convergence rate. "
780566026106834944,2016-09-27 00:33:37,https://t.co/McFLQJlwuA,Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints. (arXiv:1609.01051v2 [stat.ML… https://t.co/McFLQJlwuA,4,8," Abstract: This work presents PESMOC, Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints, an information-based strategy for the simultaneous optimization of multiple expensive-to-evaluate black-box functions under the presence of several constraints. PESMOC can hence be used to solve a wide range of optimization problems. Iteratively, PESMOC chooses an input location on which to evaluate the objective functions and the constraints so as to maximally reduce the entropy of the Pareto set of the corresponding optimization problem. The constraints considered in PESMOC are assumed to have similar properties to those of the objective functions in typical Bayesian optimization problems. That is, they do not have a known expression (which prevents gradient computation), their evaluation is considered to be very expensive, and the resulting observations may be corrupted by noise. These constraints arise in a plethora of expensive black-box optimization problems. We carry out synthetic experiments to illustrate the effectiveness of PESMOC, where we sample both the objectives and the constraints from a Gaussian process prior. The results obtained show that PESMOC is able to provide better recommendations with a smaller number of evaluations than a strategy based on random search. "
780566023699300353,2016-09-27 00:33:36,https://t.co/zsM0HO5ehK,Direct Feedback Alignment Provides Learning in Deep Neural Networks. (arXiv:1609.01596v2 [stat.ML] UPDATED) https://t.co/zsM0HO5ehK,2,7," Abstract: Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task. "
780204822050070528,2016-09-26 00:38:19,https://t.co/qXl1HT884l,Efficient Feature Selection With Large and High-dimensional Data. (arXiv:1609.07195v1 [stat.ME]) https://t.co/qXl1HT884l,6,19," Abstract: Driven by the advances in technology, large and high-dimensional data have become the rule rather than the exception. Approaches that allow for feature selection with such data are thus highly sought after, in particular, since standard methods, like cross-validated Lasso, can be computationally intractable and, in any case, lack theoretical guarantees. In this paper, we propose a novel approach to feature selection in regression. Consisting of simple optimization steps and tests, it is computationally more efficient than existing methods and, therefore, suited even for very large data sets. Moreover, in contrast to standard methods, it is equipped with sharp statistical guarantees. We thus expect that our algorithm can help to leverage the increasing volume of data in Biology, Public Health, Astronomy, Economics, and other fields. "
780204820007354369,2016-09-26 00:38:19,https://t.co/fznvfjn67Y,Multilayer Spectral Graph Clustering via Convex Layer Aggregation. (arXiv:1609.07200v1 [cs.LG]) https://t.co/fznvfjn67Y,0,5," Abstract: Multilayer graphs are commonly used for representing different relations between entities and handling heterogeneous data processing tasks. New challenges arise in multilayer graph clustering for assigning clusters to a common multilayer node set and for combining information from each layer. This paper presents a theoretical framework for multilayer spectral graph clustering of the nodes via convex layer aggregation. Under a novel multilayer signal plus noise model, we provide a phase transition analysis that establishes the existence of a critical value on the noise level that permits reliable cluster separation. The analysis also specifies analytical upper and lower bounds on the critical value, where the bounds become exact when the clusters have identical sizes. Numerical experiments on synthetic multilayer graphs are conducted to validate the phase transition analysis and study the effect of layer weights and noise levels on clustering reliability. "
780204818426126336,2016-09-26 00:38:18,https://t.co/7oXO9Wqbbo,On the (im)possibility of fairness. (arXiv:1609.07236v1 [cs.CY]) https://t.co/7oXO9Wqbbo,0,4," Abstract: What does it mean for an algorithm to be fair? Different papers use different notions of algorithmic fairness, and although these appear internally consistent, they also seem mutually incompatible. We present a mathematical setting in which the distinctions in previous papers can be made formal. In addition to characterizing the spaces of inputs (the ""observed"" space) and outputs (the ""decision"" space), we introduce the notion of a construct space: a space that captures unobservable, but meaningful variables for the prediction. We show that in order to prove desirable properties of the entire decision-making process, different mechanisms for fairness require different assumptions about the nature of the mapping from construct space to decision space. The results in this paper imply that future treatments of algorithmic fairness should more explicitly state assumptions about the relationship between constructs and observations. "
780204816568086528,2016-09-26 00:38:18,https://t.co/XCY9Y6Gbwe,Using Neural Network Formalism to Solve Multiple-Instance Problems. (arXiv:1609.07257v1 [cs.LG]) https://t.co/XCY9Y6Gbwe,0,5," Abstract: Many objects in the real world are difficult to describe by a single numerical vector of a fixed length, whereas describing them by a set of vectors is more natural. Therefore, Multiple instance learning (MIL) techniques have been constantly gaining on importance throughout last years. MIL formalism represents each object (sample) by a set (bag) of feature vectors (instances) of fixed length where knowledge about objects (e.g., class label) is available on bag level but not necessarily on instance level. Many standard tools including supervised classifiers have been already adapted to MIL setting since the problem got formalized in late nineties. In this work we propose a neural network (NN) based formalism that intuitively bridges the gap between MIL problem definition and the vast existing knowledge-base of standard models and classifiers. We show that the proposed NN formalism is effectively optimizable by a modified back-propagation algorithm and can reveal unknown patterns inside bags. Comparison to eight types of classifiers from the prior art on a set of 14 publicly available benchmark datasets confirms the advantages and accuracy of the proposed solution. "
780204814215028737,2016-09-26 00:38:17,https://t.co/Lka14mV18d,Constraint-Based Clustering Selection. (arXiv:1609.07272v1 [stat.ML]) https://t.co/Lka14mV18d,0,7," Abstract: Semi-supervised clustering methods incorporate a limited amount of supervision into the clustering process. Typically, this supervision is provided by the user in the form of pairwise constraints. Existing methods use such constraints in one of the following ways: they adapt their clustering procedure, their similarity metric, or both. All of these approaches operate within the scope of individual clustering algorithms. In contrast, we propose to use constraints to choose between clusterings generated by very different unsupervised clustering algorithms, run with different parameter settings. We empirically show that this simple approach often outperforms existing semi-supervised clustering methods. "
780204811375505408,2016-09-26 00:38:17,https://t.co/W2wFA8Udrs,"Estimating Probability Distributions using ""Dirac"" Kernels (via Rademacher-Walsh Polynomial Basis Functions). (arX… https://t.co/W2wFA8Udrs",0,4," Abstract: In many applications (in particular information systems, such as pattern recognition, machine learning, cheminformatics, bioinformatics to name but a few) the assessment of uncertainty is essential - i.e., the estimation of the underlying probability distribution function. More often than not, the form of this function is unknown and it becomes necessary to non-parametrically construct/estimate it from a given sample. One of the methods of choice to non-parametrically estimate the unknown probability distribution function for a given random variable (defined on binary space) has been the expansion of the estimation function in Rademacher-Walsh Polynomial basis functions. In this paper we demonstrate that the expansion of the probability distribution function estimation in Rademacher-Walsh Polynomial basis functions is equivalent to the expansion of the function estimation in a set of ""Dirac kernel"" functions. The latter approach can ameliorate the computational bottleneck and notational awkwardness often associated with the Rademacher-Walsh Polynomial basis functions approach, in particular when the binary input space is large. "
780204809584517120,2016-09-26 00:38:16,https://t.co/8npH7SQorD,Changepoint Detection in the Presence of Outliers. (arXiv:1609.07363v1 [stat.ME]) https://t.co/8npH7SQorD,0,3," Abstract: Many traditional methods for identifying changepoints can struggle in the presence of outliers, or when the noise is heavy-tailed. Often they will infer additional changepoints in order to fit the outliers. To overcome this problem, data often needs to be pre-processed to remove outliers, though this is not feasible in applications where the data needs to be analysed online. We present an approach to changepoint detection that is robust to the presence of outliers. The idea is to adapt existing penalised cost approaches for detecting changes so that they use cost functions that are less sensitive to outliers. We argue that cost functions that are bounded, such as the classical biweight cost, are particularly suitable -- as we show that only bounded cost functions are robust to arbitrarily extreme outliers. We present a novel and efficient dynamic programming algorithm that can then find the optimal segmentation under our penalised cost criteria. Importantly, this algorithm can be used in settings where the data needs to be analysed online. We present theoretical bounds on the worst-case complexity of this algorithm, and show empirically that its average computational cost is linear in the amount of the data. We show the usefulness of our approach for applications such as analysing well-log data, detecting copy number variation, and detecting tampering of wireless devices. "
780204807751602177,2016-09-26 00:38:16,https://t.co/AWW5isLHyb,A penalized likelihood method for classification with matrix-valued predictors. (arXiv:1609.07386v1 [stat.ML]) https://t.co/AWW5isLHyb,0,4," Abstract: We propose a penalized likelihood method to fit the linear discriminant analysis model when the predictor is matrix valued. We simultaneously estimate the means and the precision matrix, which we assume has a Kronecker product decomposition. Our penalties encourage pairs of response category mean matrices to have equal entries and also encourage zeros in the precision matrix. To compute our estimators, we use a blockwise coordinate descent algorithm. To update the optimization variables corresponding to response category mean matrices, we use an alternating minimization algorithm that takes advantage of the Kronecker structure of the precision matrix. We show that our method can outperform relevant competitors in classification, even when our modeling assumptions are violated. We analyze an EEG dataset to demonstrate our method's interpretability and classification accuracy. "
780204805482512384,2016-09-26 00:38:15,https://t.co/iTJzZWHn6N,One-vs-Each Approximation to Softmax for Scalable Estimation of Probabilities. (arXiv:1609.07410v1 [stat.ML]) https://t.co/iTJzZWHn6N,0,4," Abstract: The softmax representation of probabilities for categorical variables plays a prominent role in modern machine learning with numerous applications in areas such as large scale classification, neural language modeling and recommendation systems. However, softmax estimation is very expensive for large scale inference because of the high cost associated with computing the normalizing constant. Here, we introduce an efficient approximation to softmax probabilities which takes the form of a rigorous lower bound on the exact probability. This bound is expressed as a product over pairwise probabilities and it leads to scalable estimation based on stochastic optimization. It allows us to perform doubly stochastic estimation by subsampling both training instances and class labels. We show that the new bound has interesting theoretical properties and we demonstrate its use in classification problems. "
780204803838320640,2016-09-26 00:38:15,https://t.co/rhSTlKurfM,Screening Rules for Convex Problems. (arXiv:1609.07478v1 [math.OC]) https://t.co/rhSTlKurfM,0,3," Abstract: We propose a new framework for deriving screening rules for convex optimization problems. Our approach covers a large class of constrained and penalized optimization formulations, and works in two steps. First, given any approximate point, the structure of the objective function and the duality gap is used to gather information on the optimal solution. In the second step, this information is used to produce screening rules, i.e. safely identifying unimportant weight variables of the optimal solution. Our general framework leads to a large variety of useful existing as well as new screening rules for many applications. For example, we provide new screening rules for general simplex and $L_1$-constrained problems, Elastic Net, squared-loss Support Vector Machines, minimum enclosing ball, as well as structured norm regularized problems, such as group lasso. "
780204800117989376,2016-09-26 00:38:14,https://t.co/2pMJmvJ722,Active Ranking from Pairwise Comparisons and when Parametric Assumptions Don't Help. (arXiv:1606.08842v2 [cs.LG] U… https://t.co/2pMJmvJ722,0,5," Abstract: We consider sequential or active ranking of a set of n items based on noisy pairwise comparisons. Items are ranked according to the probability that a given item beats a randomly chosen item, and ranking refers to partitioning the items into sets of pre-specified sizes according to their scores. This notion of ranking includes as special cases the identification of the top-k items and the total ordering of the items. We first analyze a sequential ranking algorithm that counts the number of comparisons won, and uses these counts to decide whether to stop, or to compare another pair of items, chosen based on confidence intervals specified by the data collected up to that point. We prove that this algorithm succeeds in recovering the ranking using a number of comparisons that is optimal up to logarithmic factors. This guarantee does not require any structural properties of the underlying pairwise probability matrix, unlike a significant body of past work on pairwise ranking based on parametric models such as the Thurstone or Bradley-Terry-Luce models. It has been a long-standing open question as to whether or not imposing these parametric assumptions allows for improved ranking algorithms. For stochastic comparison models, in which the pairwise probabilities are bounded away from zero, our second contribution is to resolve this issue by proving a lower bound for parametric models. This shows, perhaps surprisingly, that these popular parametric modeling choices offer at most logarithmic gains for stochastic comparisons. "
779117963844718592,2016-09-23 00:39:32,https://t.co/BFgVO6daMw,Saturating Splines and Feature Selection. (arXiv:1609.06764v1 [stat.ML]) https://t.co/BFgVO6daMw,0,4," Abstract: We extend the adaptive regression spline model by incorporating saturation, the natural requirement that a function extend as a constant outside a certain range. We fit saturating splines to data using a convex optimization problem over a space of measures, which we solve using an efficient algorithm based on the conditional gradient method. Unlike many existing approaches, our algorithm solves the original infinite-dimensional (for splines of degree at least two) optimization problem without pre-specified knot locations. We then adapt our algorithm to fit generalized additive models with saturating splines as coordinate functions and show that the saturation requirement allows our model to simultaneously perform feature selection and nonlinear function fitting. Finally, we briefly sketch how the method can be extended to higher order splines and to different requirements on the extension outside the data range. "
779117959860129793,2016-09-23 00:39:31,https://t.co/NUFT1JJUep,Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor Processes. (arXiv:1609.06783v1 [stat.ML]) https://t.co/NUFT1JJUep,1,9," Abstract: The Dirichlet process and its extension, the Pitman-Yor process, are stochastic processes that take probability distributions as a parameter. These processes can be stacked up to form a hierarchical nonparametric Bayesian model. In this article, we present efficient methods for the use of these processes in this hierarchical context, and apply them to latent variable models for text analytics. In particular, we propose a general framework for designing these Bayesian models, which are called topic models in the computer science community. We then propose a specific nonparametric Bayesian topic model for modelling text from social media. We focus on tweets (posts on Twitter) in this article due to their ease of access. We find that our nonparametric model performs better than existing parametric models in both goodness of fit and real world applications. "
779117956655640577,2016-09-23 00:39:30,https://t.co/sgcpksQDeL,Bibliographic Analysis with the Citation Network Topic Model. (arXiv:1609.06826v1 [cs.DL]) https://t.co/sgcpksQDeL,1,4," Abstract: Bibliographic analysis considers author's research areas, the citation network and paper content among other things. In this paper, we combine these three in a topic model that produces a bibliographic model of authors, topics and documents using a non-parametric extension of a combination of the Poisson mixed-topic link model and the author-topic model. We propose a novel and efficient inference algorithm for the model to explore subsets of research publications from CiteSeerX. Our model demonstrates improved performance in both model fitting and a clustering task compared to several baselines. "
779117953082097664,2016-09-23 00:39:29,https://t.co/VtdYS32Q0G,Hawkes Processes with Stochastic Excitations. (arXiv:1609.06831v1 [cs.LG]) https://t.co/VtdYS32Q0G,0,3," Abstract: We propose an extension to Hawkes processes by treating the levels of self-excitation as a stochastic differential equation. Our new point process allows better approximation in application domains where events and intensities accelerate each other with correlated levels of contagion. We generalize a recent algorithm for simulating draws from Hawkes processes whose levels of excitation are stochastic processes, and propose a hybrid Markov chain Monte Carlo approach for model fitting. Our sampling procedure scales linearly with the number of required events and does not require stationarity of the point process. A modular inference procedure consisting of a combination between Gibbs and Metropolis Hastings steps is put forward. We recover expectation maximization as a special case. Our general approach is illustrated for contagion following geometric Brownian motion and exponential Langevin dynamics. "
779117948963291136,2016-09-23 00:39:29,https://t.co/FNSim8uOVB,Exact Sampling from Determinantal Point Processes. (arXiv:1609.06840v1 [cs.LG]) https://t.co/FNSim8uOVB,0,3," Abstract: Determinantal point processes (DPPs) are an important concept in random matrix theory and combinatorics. They have also recently attracted interest in the study of numerical methods for machine learning, as they offer an elegant ""missing link"" between independent Monte Carlo sampling and deterministic evaluation on regular grids, applicable to a general set of spaces. This is helpful whenever an algorithm *explores* to reduce uncertainty, such as in active learning, Bayesian optimization, reinforcement learning, and marginalization in graphical models. To draw samples from a DPP in practice, existing literature focuses on approximate schemes of low cost, or comparably inefficient exact algorithms like rejection sampling. We point out that, for many settings of relevance to machine learning, it is also possible to draw *exact* samples from DPPs on continuous domains. We start from an intuitive example on the real line, which is then generalized to multivariate real vector spaces. We also compare to previously studied approximations, showing that exact sampling, despite higher cost, can be preferable where precision is needed. "
779117945544904704,2016-09-23 00:39:28,https://t.co/53zoTtPIiG,A probabilistic network for the diagnosis of acute cardiopulmonary diseases. (arXiv:1609.06864v1 [stat.ML]) https://t.co/53zoTtPIiG,1,4," Abstract: We describe our experience in the development of a probabilistic network for the diagnosis of acute cardiopulmonary diseases. A panel of expert physicians collaborated to specify the qualitative part, that is a directed acyclic graph defining a factorization of the joint probability distribution of domain variables. The quantitative part, that is the set of all conditional probability distributions defined by each factor, was estimated in the Bayesian paradigm: we applied a special formal representation, characterized by a low number of parameters and a parameterization intelligible for physicians, elicited the joint prior distribution of parameters from medical experts, and updated it by conditioning on a dataset of hospital patient records using Markov Chain Monte Carlo simulation. Refinement was cyclically performed until the probabilistic network provided satisfactory Concordance Index values for a selection of acute diseases and reasonable inference on six fictitious patient cases. The probabilistic network can be employed to perform medical diagnosis on a total of 63 diseases (38 acute and 25 chronic) on the basis of up to 167 patient findings. "
779117942969606149,2016-09-23 00:39:27,https://t.co/OY9v1RcBN4,Randomized Independent Component Analysis. (arXiv:1609.06942v1 [stat.ML]) https://t.co/OY9v1RcBN4,0,8," Abstract: Independent component analysis (ICA) is a method for recovering statistically independent signals from observations of unknown linear combinations of the sources. Some of the most accurate ICA decomposition methods require searching for the inverse transformation which minimizes different approximations of the Mutual Information, a measure of statistical independence of random vectors. Two such approximations are the Kernel Generalized Variance or the Kernel Canonical Correlation which has been shown to reach the highest performance of ICA methods. However, the computational effort necessary just for computing these measures is cubic in the sample size. Hence, optimizing them becomes even more computationally demanding, in terms of both space and time. Here, we propose a couple of alternative novel measures based on randomized features of the samples - the Randomized Generalized Variance and the Randomized Canonical Correlation. The computational complexity of calculating the proposed alternatives is linear in the sample size and provide a controllable approximation of their Kernel-based non-random versions. We also show that optimization of the proposed statistical properties yields a comparable separation error at an order of magnitude faster compared to Kernel-based measures. "
779117940025110528,2016-09-23 00:39:26,https://t.co/X8Kn8anfRj,Early Warning System for Seismic Events in Coal Mines Using Machine Learning. (arXiv:1609.06957v1 [cs.LG]) https://t.co/X8Kn8anfRj,1,4, Abstract: This document describes an approach to the problem of predicting dangerous seismic events in active coal mines up to 8 hours in advance. It was developed as a part of the AAIA'16 Data Mining Challenge: Predicting Dangerous Seismic Events in Active Coal Mines. The solutions presented consist of ensembles of various predictive models trained on different sets of features. The best one achieved a winning score of 0.939 AUC. 
779117936820772864,2016-09-23 00:39:26,https://t.co/tjOqxWhSp3,An equivalence between high dimensional Bayes optimal inference and M-estimation. (arXiv:1609.07060v1 [stat.ML]) https://t.co/tjOqxWhSp3,0,5," Abstract: When recovering an unknown signal from noisy measurements, the computational difficulty of performing optimal Bayesian MMSE (minimum mean squared error) inference often necessitates the use of maximum a posteriori (MAP) inference, a special case of regularized M-estimation, as a surrogate. However, MAP is suboptimal in high dimensions, when the number of unknown signal components is similar to the number of measurements. In this work we demonstrate, when the signal distribution and the likelihood function associated with the noise are both log-concave, that optimal MMSE performance is asymptotically achievable via another M-estimation procedure. This procedure involves minimizing convex loss and regularizer functions that are nonlinearly smoothed versions of the widely applied MAP optimization problem. Our findings provide a new heuristic derivation and interpretation for recent optimal M-estimators found in the setting of linear measurements and additive noise, and further extend these results to nonlinear measurements with non-additive noise. We numerically demonstrate superior performance of our optimal M-estimators relative to MAP. Overall, at the heart of our work is the revelation of a remarkable equivalence between two seemingly very different computational problems: namely that of high dimensional Bayesian integration underlying MMSE inference, and high dimensional convex optimization underlying M-estimation. In essence we show that the former difficult integral may be computed by solving the latter, simpler optimization problem. "
779117934602022914,2016-09-23 00:39:25,https://t.co/wuF6WwjPFk,The Many-Body Expansion Combined with Neural Networks. (arXiv:1609.07072v1 [physics.chem-ph]) https://t.co/wuF6WwjPFk,0,6," Abstract: Fragmentation methods such as the many-body expansion (MBE) are a common strategy to model large systems by partitioning energies into a hierarchy of decreasingly significant contributions. The number of fragments required for chemical accuracy is still prohibitively expensive for ab-initio MBE to compete with force field approximations for applications beyond single-point energies. Alongside the MBE, empirical models of ab-initio potential energy surfaces have improved, especially non-linear models based on neural networks (NN) which can reproduce ab-initio potential energy surfaces rapidly and accurately. Although they are fast, NNs suffer from their own curse of dimensionality; they must be trained on a representative sample of chemical space. In this paper we examine the synergy of the MBE and NN's, and explore their complementarity. The MBE offers a systematic way to treat systems of arbitrary size and intelligently sample chemical space. NN's reduce, by a factor in excess of $10^6$ the computational overhead of the MBE and reproduce the accuracy of ab-initio calculations without specialized force fields. We show they are remarkably general, providing comparable accuracy with drastically different chemical embeddings. To assess this we test a new chemical embedding which can be inverted to predict molecules with desired properties. "
779117931565285376,2016-09-23 00:39:24,https://t.co/ShUOvxK4D7,(Bandit) Convex Optimization with Biased Noisy Gradient Oracles. (arXiv:1609.07087v1 [cs.LG]) https://t.co/ShUOvxK4D7,1,7," Abstract: Algorithms for bandit convex optimization and online learning often rely on constructing noisy gradient estimates, which are then used in appropriately adjusted first-order algorithms, replacing actual gradients. Depending on the properties of the function to be optimized and the nature of ""noise"" in the bandit feedback, the bias and variance of gradient estimates exhibit various tradeoffs. In this paper we propose a novel framework that replaces the specific gradient estimation methods with an abstract oracle. With the help of the new framework we unify previous works, reproducing their results in a clean and concise fashion, while, perhaps more importantly, the framework also allows us to formally show that to achieve the optimal root-$n$ rate either the algorithms that use existing gradient estimators, or the proof techniques used to analyze them have to go beyond what exists today. "
779117928625106944,2016-09-23 00:39:24,https://t.co/O9wE5vwpN5,Neural Photo Editing with Introspective Adversarial Networks. (arXiv:1609.07093v1 [cs.LG]) https://t.co/O9wE5vwpN5,4,6," Abstract: We present the Neural Photo Editor, an interface for exploring the latent space of generative image models and making large, semantically coherent changes to existing images. Our interface is powered by the Introspective Adversarial Network, a hybridization of the Generative Adversarial Network and the Variational Autoencoder designed for use in the editor. Our model makes use of a novel computational block based on dilated convolutions, and Orthogonal Regularization, a novel weight regularization method. We validate our model on CelebA, SVHN, and ImageNet, and produce samples and reconstructions with high visual fidelity. "
779117926376960000,2016-09-23 00:39:23,https://t.co/bnGg2oTuAj,Identification of relevant subtypes via preweighted sparse clustering. (arXiv:1304.3760v3 [stat.ME] UPDATED) https://t.co/bnGg2oTuAj,1,4," Abstract: Cluster analysis methods are used to identify homogeneous subgroups in a data set. In biomedical applications, one frequently applies cluster analysis in order to identify biologically interesting subgroups. In particular, one may wish to identify subgroups that are associated with a particular outcome of interest. Conventional clustering methods generally do not identify such subgroups, particularly when there are a large number of high-variance features in the data set. Conventional methods may identify clusters associated with these high-variance features when one wishes to obtain secondary clusters that are more interesting biologically or more strongly associated with a particular outcome of interest. A modification of sparse clustering can be used to identify such secondary clusters or clusters associated with an outcome of interest. This method correctly identifies such clusters of interest in several simulation scenarios. The method is also applied to a large prospective cohort study of temporomandibular disorders and a leukemia microarray data set. "
779117924447576064,2016-09-23 00:39:23,https://t.co/iH80WlzSUf,Fast Kronecker product kernel methods via sampled vec trick. (arXiv:1601.01507v2 [stat.ML] UPDATED) https://t.co/iH80WlzSUf,0,4," Abstract: Kronecker product kernel provides the standard approach in the kernel methods literature for learning from pair-input data, where both data points and prediction tasks have their own feature representations. The methods allow simultaneous generalization to both new tasks and data unobserved in the training sample, a setting known as zero-shot or zero-data learning. Such a setting occurs in numerous applications, including drug-target interaction prediction, collaborative filtering and information retrieval. Efficient training algorithms based on the so-called vec trick, that makes use of the special structure of the Kronecker product, are known for the case where the output matrix for the training sample is fully observed, i.e. the correct output for each data point-task combination is available. In this work we generalize these results, proposing an efficient algorithm for sampled Kronecker product multiplication, where only a subset of the full Kronecker product, that corresponds to the training sample, is computed. This allows us to derive a general framework for training Kronecker kernel methods, as specific examples we implement Kronecker ridge regression and support vector machine algorithms. Experimental results demonstrate that the proposed approach leads to accurate models, while allowing order of magnitude improvements in training and prediction time. "
779117919598964736,2016-09-23 00:39:22,https://t.co/fpvIFZXxwg,Clinical Tagging with Joint Probabilistic Models. (arXiv:1608.00686v3 [stat.ML] UPDATED) https://t.co/fpvIFZXxwg,0,3," Abstract: We describe a method for parameter estimation in bipartite probabilistic graphical models for joint prediction of clinical conditions from the electronic medical record. The method does not rely on the availability of gold-standard labels, but rather uses noisy labels, called anchors, for learning. We provide a likelihood-based objective and a moments-based initialization that are effective at learning the model parameters. The learned model is evaluated in a task of assigning a heldout clinical condition to patients based on retrospective analysis of the records, and outperforms baselines which do not account for the noisiness in the labels or do not model the conditions jointly. "
778753901633810433,2016-09-22 00:32:53,https://t.co/ZTBx9wNIR3,Generalized Kalman Smoothing: Modeling and Algorithms. (arXiv:1609.06369v1 [math.OC]) https://t.co/ZTBx9wNIR3,4,21," Abstract: State-space smoothing has found many applications in science and engineering. Under linear and Gaussian assumptions, smoothed estimates can be obtained using efficient recursions, for example Rauch-Tung-Striebel and Mayne-Fraser algorithms. Such schemes are equivalent to linear algebraic techniques that minimize a convex quadratic objective function with structure induced by the dynamic model. These classical formulations fall short in many important circumstances. For instance, smoothers obtained using quadratic penalties can fail when outliers are present in the data, and cannot track impulsive inputs and abrupt state changes. Motivated by these shortcomings, generalized Kalman smoothing formulations have been proposed in the last few years, replacing quadratic models with more suitable, often nonsmooth, convex functions. In contrast to classical models, these general estimators require use of iterated algorithms, and these have received increased attention from control, signal processing, machine learning, and optimization communities. In this survey we show that the optimization viewpoint provides the control and signal processing community great freedom in the development of novel modeling and inference frameworks for dynamical systems. We discuss general statistical models for dynamic systems, making full use of nonsmooth convex penalties and constraints, and providing links to important models in signal processing and machine learning. We also survey optimization techniques for these formulations, paying close attention to dynamic problem structure. Modeling concepts and algorithms are illustrated with numerical examples. "
778753895812198400,2016-09-22 00:32:51,https://t.co/NM5pE7TMha,Multiclass Classification Calibration Functions. (arXiv:1609.06385v1 [stat.ML]) https://t.co/NM5pE7TMha,0,3," Abstract: In this paper we refine the process of computing calibration functions for a number of multiclass classification surrogate losses. Calibration functions are a powerful tool for easily converting bounds for the surrogate risk (which can be computed through well-known methods) into bounds for the true risk, the probability of making a mistake. They are particularly suitable in non-parametric settings, where the approximation error can be controlled, and provide tighter bounds than the common technique of upper-bounding the 0-1 loss by the surrogate loss. The abstract nature of the more sophisticated existing calibration function results requires calibration functions to be explicitly derived on a case-by-case basis, requiring repeated efforts whenever bounds for a new surrogate loss are required. We devise a streamlined analysis that simplifies the process of deriving calibration functions for a large number of surrogate losses that have been proposed in the literature. The effort of deriving calibration functions is then surmised in verifying, for a chosen surrogate loss, a small number of conditions that we introduce. As case studies, we recover existing calibration functions for the well-known loss of Lee et al. (2004), and also provide novel calibration functions for well-known losses, including the one-versus-all loss and the logistic regression loss, plus a number of other losses that have been shown to be classification-calibrated in the past, but for which no calibration function had been derived. "
778753892389617664,2016-09-22 00:32:51,https://t.co/ODXnKwHwo4,Learning HMMs with Nonparametric Emissions via Spectral Decompositions of Continuous Matrices. (arXiv:1609.06390v1… https://t.co/ODXnKwHwo4,3,8," Abstract: Recently, there has been a surge of interest in using spectral methods for estimating latent variable models. However, it is usually assumed that the distribution of the observations conditioned on the latent variables is either discrete or belongs to a parametric family. In this paper, we study the estimation of an $m$-state hidden Markov model (HMM) with only smoothness assumptions, such as H\""olderian conditions, on the emission densities. By leveraging some recent advances in continuous linear algebra and numerical analysis, we develop a computationally efficient spectral algorithm for learning nonparametric HMMs. Our technique is based on computing an SVD on nonparametric estimates of density functions by viewing them as \emph{continuous matrices}. We derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices. We implement our method using Chebyshev polynomial approximations. Our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient. "
778753888505647104,2016-09-22 00:32:50,https://t.co/BlBDdVILqI,AMOS: An Automated Model Order Selection Algorithm for Spectral Graph Clustering. (arXiv:1609.06457v1 [cs.SI]) https://t.co/BlBDdVILqI,0,3," Abstract: One of the longstanding problems in spectral graph clustering (SGC) is the so-called model order selection problem: automated selection of the correct number of clusters. This is equivalent to the problem of finding the number of connected components or communities in an undirected graph. In this paper, we propose AMOS, an automated model order selection algorithm for SGC. Based on a recent analysis of clustering reliability for SGC under the random interconnection model, AMOS works by incrementally increasing the number of clusters, estimating the quality of identified clusters, and providing a series of clustering reliability tests. Consequently, AMOS outputs clusters of minimal model order with statistical clustering reliability guarantees. Comparing to three other automated graph clustering methods on real-world datasets, AMOS shows superior performance in terms of multiple external and internal clustering metrics. "
778753884344811520,2016-09-22 00:32:49,https://t.co/9isLV1ovlW,Network-regularized Sparse Logistic Regression Models for Clinical Risk Prediction and Biomarker Discovery. (arXiv… https://t.co/9isLV1ovlW,0,4," Abstract: Molecular profiling data (e.g., gene expression) has been used for clinical risk prediction and biomarker discovery. However, it is necessary to integrate other prior knowledge like biological pathways or gene interaction networks to improve the predictive ability and biological interpretability of biomarkers. Here, we first introduce a general regularized Logistic Regression (LR) framework with regularized term $\lambda \|\bm{w}\|_1 + \eta\bm{w}^T\bm{M}\bm{w}$, which can reduce to different penalties, including Lasso, elastic net, and network-regularized terms with different $\bm{M}$. This framework can be easily solved in a unified manner by a cyclic coordinate descent algorithm which can avoid inverse matrix operation and accelerate the computing speed. However, if those estimated $\bm{w}_i$ and $\bm{w}_j$ have opposite signs, then the traditional network-regularized penalty may not perform well. To address it, we introduce a novel network-regularized sparse LR model with a new penalty $\lambda \|\bm{w}\|_1 + \eta|\bm{w}|^T\bm{M}|\bm{w}|$ to consider the difference between the absolute values of the coefficients. And we develop two efficient algorithms to solve it. Finally, we test our methods and compare them with the related ones using simulated and real data to show their efficiency. "
778753880347643904,2016-09-22 00:32:48,https://t.co/XSJ6Ol9GFn,"Bibliographic Analysis on Research Publications using Authors, Categorical Labels and the Citation Network. (arXiv… https://t.co/XSJ6Ol9GFn",0,3," Abstract: Bibliographic analysis considers the author's research areas, the citation network and the paper content among other things. In this paper, we combine these three in a topic model that produces a bibliographic model of authors, topics and documents, using a nonparametric extension of a combination of the Poisson mixed-topic link model and the author-topic model. This gives rise to the Citation Network Topic Model (CNTM). We propose a novel and efficient inference algorithm for the CNTM to explore subsets of research publications from CiteSeerX. The publication datasets are organised into three corpora, totalling to about 168k publications with about 62k authors. The queried datasets are made available online. In three publicly available corpora in addition to the queried datasets, our proposed model demonstrates an improved performance in both model fitting and document clustering, compared to several baselines. Moreover, our model allows extraction of additional useful knowledge from the corpora, such as the visualisation of the author-topics network. Additionally, we propose a simple method to incorporate supervision into topic modelling to achieve further improvement on the clustering task. "
778753875717259264,2016-09-22 00:32:47,https://t.co/EnQ3RqNYAL,On Data-Independent Properties for Density-Based Dissimilarity Measures in Hybrid Clustering. (arXiv:1609.06533v1 … https://t.co/EnQ3RqNYAL,1,4," Abstract: Hybrid clustering combines partitional and hierarchical clustering for computational effectiveness and versatility in cluster shape. In such clustering, a dissimilarity measure plays a crucial role in the hierarchical merging. The dissimilarity measure has great impact on the final clustering, and data-independent properties are needed to choose the right dissimilarity measure for the problem at hand. Properties for distance-based dissimilarity measures have been studied for decades, but properties for density-based dissimilarity measures have so far received little attention. Here, we propose six data-independent properties to evaluate density-based dissimilarity measures associated with hybrid clustering, regarding equality, orthogonality, symmetry, outlier and noise observations, and light-tailed models for heavy-tailed clusters. The significance of the properties is investigated, and we study some well-known dissimilarity measures based on Shannon entropy, misclassification rate, Bhattacharyya distance and Kullback-Leibler divergence with respect to the proposed properties. As none of them satisfy all the proposed properties, we introduce a new dissimilarity measure based on the Kullback-Leibler information and show that it satisfies all proposed properties. The effect of the proposed properties is also illustrated on several real and simulated data sets. "
778753870503763968,2016-09-22 00:32:45,https://t.co/jLoIWUQ2Fg,Theoretical Evaluation of Feature Selection Methods based on Mutual Information. (arXiv:1609.06575v1 [stat.ML]) https://t.co/jLoIWUQ2Fg,0,13," Abstract: Feature selection methods are usually evaluated by wrapping specific classifiers and datasets in the evaluation process, resulting very often in unfair comparisons between methods. In this work, we develop a theoretical framework that allows obtaining the true feature ordering of two-dimensional sequential forward feature selection methods based on mutual information, which is independent of entropy or mutual information estimation methods, classifiers, or datasets, and leads to an undoubtful comparison of the methods. Moreover, the theoretical framework unveils problems intrinsic to some methods that are otherwise difficult to detect, namely inconsistencies in the construction of the objective function used to select the candidate features, due to various types of indeterminations and to the possibility of the entropy of continuous random variables taking null and negative values. "
778753866787532801,2016-09-22 00:32:45,https://t.co/hCHXGRFr5G,Gaussian Process Pseudo-Likelihood Models for Sequence Labeling. (arXiv:1412.7868v2 [cs.LG] UPDATED) https://t.co/hCHXGRFr5G,1,6, Abstract: Several machine learning problems arising in natural language processing can be modeled as a sequence labeling problem. We provide Gaussian process models based on pseudo-likelihood approximation to perform sequence labeling. Gaussian processes (GPs) provide a Bayesian approach to learning in a kernel based framework. The pseudo-likelihood model enables one to capture long range dependencies among the output components of the sequence without becoming computationally intractable. We use an efficient variational Gaussian approximation method to perform inference in the proposed model. We also provide an iterative algorithm which can effectively make use of the information from the neighboring labels to perform prediction. The ability to capture long range dependencies makes the proposed approach useful for a wide range of sequence labeling problems. Numerical experiments on some sequence labeling data sets demonstrate the usefulness of the proposed approach. 
778753864078155777,2016-09-22 00:32:44,https://t.co/2siF85sS3R,Classification and Regression Using a Constrained Convex Splitting Method. (arXiv:1506.02196v3 [math.ST] UPDATED) https://t.co/2siF85sS3R,1,4," Abstract: This paper deals with sparse feature selection and grouping for classification and regression. The classification or regression problems under consideration consists in minimizing a convex empirical risk function subject to an $\ell^1$ constraint, a pairwise $\ell^\infty$ constraint, or a pairwise $\ell^1$ constraint. Existing work, such as the Lasso formulation, has focused mainly on Lagrangian penalty approximations, which often require ad hoc or computationally expensive procedures to determine the relaxation parameter. We depart from this approach and address the constrained problem directly via a splitting method. The structure of the method is that of the classical gradient projection algorithm, which alternates a gradient step on the objective and a projection step onto the lower level set modeling the constraint. The novelty of our approach is that the projection step is implemented via an outer approximation scheme in which the constraint set is approximated by a sequence of simple convex sets consisting of the intersection of two half-spaces. Convergence of the iterates generated by the algorithm is established. Experiments on both synthetic and biological data show that the formulation using the pairwise $\ell^1$ constraint outperforms those using the pairwise $\ell^\infty$ and the $\ell^1$ constraint. "
778753861381070850,2016-09-22 00:32:43,https://t.co/uFsa4f4ahI],Statistically efficient thinning of a Markov chain sampler. (arXiv:1510.07727v5 [https://t.co/uFsa4f4ahI] UPDATED) https://t.co/KuDkDux1PQ,1,3,INDEXERROR
778753857987813376,2016-09-22 00:32:42,https://t.co/OzzdZa470M,Unified Statistical Theory of Spectral Graph Analysis. (arXiv:1602.03861v4 [math.ST] UPDATED) https://t.co/OzzdZa470M,3,7," Abstract: The goal of this paper is to show that there exists a simple, yet universal statistical logic of spectral graph analysis by recasting it into a nonparametric function estimation problem. The prescribed viewpoint appears to be good enough to accommodate most of the existing spectral graph techniques as a consequence of just one single formalism and algorithm. "
778753854657564672,2016-09-22 00:32:42,https://t.co/IxW4ykjM1Q,Autoregressive Moving Average Graph Filtering. (arXiv:1602.04436v2 [cs.LG] UPDATED) https://t.co/IxW4ykjM1Q,1,3," Abstract: One of the cornerstones of the field of signal processing on graphs are graph filters, direct analogues of classical filters, but intended for signals defined on graphs. This work brings forth new insights on the distributed graph filtering problem. We design a family of autoregressive moving average (ARMA) recursions, which (i) are able to approximate any desired graph frequency response, and (ii) give exact solutions for tasks such as graph signal denoising and interpolation. The design philosophy, which allows us to design the ARMA coefficients independently from the underlying graph, renders the ARMA graph filters suitable in static and, particularly, time-varying settings. The latter occur when the graph signal and/or graph are changing over time. We show that in case of a time-varying graph signal our approach extends naturally to a two-dimensional filter, operating concurrently in the graph and regular time domains. We also derive sufficient conditions for filter stability when the graph and signal are time-varying. The analytical and numerical results presented in this paper illustrate that ARMA graph filters are practically appealing for static and time-varying settings, as predicted by theoretical derivations. "
778753851411329024,2016-09-22 00:32:41,https://t.co/e7FraoEjpw,Policy Error Bounds for Model-Based Reinforcement Learning with Factored Linear Models. (arXiv:1602.06346v2 [stat.… https://t.co/e7FraoEjpw,0,4," Abstract: In this paper we study a model-based approach to calculating approximately optimal policies in Markovian Decision Processes. In particular, we derive novel bounds on the loss of using a policy derived from a factored linear model, a class of models which generalize numerous previous models out of those that come with strong computational guarantees. For the first time in the literature, we derive performance bounds for model-based techniques where the model inaccuracy is measured in weighted norms. Moreover, our bounds show a decreased sensitivity to the discount factor and, unlike similar bounds derived for other approaches, they are insensitive to measure mismatch. Similarly to previous works, our proofs are also based on contraction arguments, but with the main differences that we use carefully constructed norms building on Banach lattices, and the contraction property is only assumed for operators acting on ""compressed"" spaces, thus weakening previous assumptions, while strengthening previous results. "
778753847535730688,2016-09-22 00:32:40,https://t.co/yQLbmqydev,Non-convex Global Minimization and False Discovery Rate Control for the TREX. (arXiv:1604.06815v2 [stat.ML] UPDATE… https://t.co/yQLbmqydev,1,2," Abstract: The TREX is a recently introduced method for performing sparse high-dimensional regression. Despite its statistical promise as an alternative to the lasso, square-root lasso, and scaled lasso, the TREX is computationally challenging in that it requires solving a non-convex optimization problem. This paper shows a remarkable result: despite the non-convexity of the TREX problem, there exists a polynomial-time algorithm that is guaranteed to find the global minimum. This result adds the TREX to a very short list of non-convex optimization problems that can be globally optimized (principal components analysis being a famous example). After deriving and developing this new approach, we demonstrate that (i) the ability of the preexisting TREX heuristic to reach the global minimum is strongly dependent on the difficulty of the underlying statistical problem, (ii) the new polynomial-time algorithm for TREX permits a novel variable ranking and selection scheme, (iii) this scheme can be incorporated into a rule that controls the false discovery rate (FDR) of included features in the model. To achieve this last aim, we provide an extension of the results of Barber & Candes (2015) to establish that the knockoff filter framework can be applied to the TREX. This investigation thus provides both a rare case study of a heuristic for non-convex optimization and a novel way of exploiting non-convexity for statistical inference. "
778753840506101760,2016-09-22 00:32:38,https://t.co/ferr2Jfb3y,Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization. (arXiv:1607.01231v3 [math.OC] UPDATED) https://t.co/ferr2Jfb3y,1,9," Abstract: In this paper we study stochastic quasi-Newton methods for nonconvex stochastic optimization, where we assume that noisy information about the gradients of the objective function is available via a stochastic first-order oracle ($\SFO$). We propose a general framework for such methods, for which we prove almost sure convergence to stationary points and analyze its worst-case iteration complexity. When a randomly chosen iterate is returned as the output of such an algorithm, we prove that in the worst-case, the $\SFO$-calls complexity is $O(\epsilon^{-2})$ to ensure that the expectation of the squared norm of the gradient is smaller than the given accuracy tolerance $\epsilon$. We also propose a specific algorithm, namely a stochastic damped L-BFGS (SdLBFGS) method, that falls under the proposed framework. Moreover, we incorporate the SVRG variance reduction technique into the proposed SdLBFGS method, and analyze its $\SFO$-calls complexity. Numerical results on a nonconvex binary classification problem using SVM, and a multiclass classification problem using neural networks are reported. "
778391430188371968,2016-09-21 00:32:33,https://t.co/W10kBMGAA5,Conformalized Kernel Ridge Regression. (arXiv:1609.05959v1 [stat.ML]) https://t.co/W10kBMGAA5,1,4," Abstract: General predictive models do not provide a measure of confidence in predictions without Bayesian assumptions. A way to circumvent potential restrictions is to use conformal methods for constructing non-parametric confidence regions, that offer guarantees regarding validity. In this paper we provide a detailed description of a computationally efficient conformal procedure for Kernel Ridge Regression (KRR), and conduct a comparative numerical study to see how well conformal regions perform against the Bayesian confidence sets. The results suggest that conformalized KRR can yield predictive confidence regions with specified coverage rate, which is essential in constructing anomaly detection systems based on predictive models. "
778391425859846144,2016-09-21 00:32:32,https://t.co/hmaBbQKLeR,Boosting Factor-Specific Functional Historical Models for the Detection of Synchronisation in Bioelectrical Signal… https://t.co/hmaBbQKLeR,0,4," Abstract: The link between different psychophysiological measures during emotion episodes is not well understood. To analyse the functional relationship between electroencephalography (EEG) and facial electromyography (EMG), we apply historical function-on-function regression models to EEG and EMG data that were simultaneously recorded from 24 participants while they were playing a computerised gambling task. Given the complexity of the data structure for this application, we extend simple functional historical models to models including random historical effects, factor-specific historical effects, and factor-specific random historical effects. Estimation is conducted by a component-wise gradient boosting algorithm, which scales well to large data sets and complex models. "
778391422097555456,2016-09-21 00:32:31,https://t.co/q8wYzBR9p9,Distributed Adaptive Learning of Graph Signals. (arXiv:1609.06100v1 [cs.LG]) https://t.co/q8wYzBR9p9,2,6," Abstract: The aim of this paper is to propose distributed strategies for adaptive learning of signals defined over graphs. Assuming the graph signal to be band-limited, the method enables distributed reconstruction, with guaranteed performance in terms of mean-square error, and tracking from a limited number of sampled observations taken from a subset of vertices. A detailed mean square analysis is carried out and illustrates the role played by the sampling strategy on the performance of the proposed method. Finally, some useful strategies for distributed selection of the sampling set are provided. Several numerical results validate our theoretical findings, and illustrate the performance of the proposed method for distributed adaptive learning of signals defined over graphs. "
778391417869701120,2016-09-21 00:32:30,https://t.co/Qrmnm2yHBD,Multilevel Monte Carlo for Scalable Bayesian Computations. (arXiv:1609.06144v1 [stat.ML]) https://t.co/Qrmnm2yHBD,2,6," Abstract: Markov chain Monte Carlo (MCMC) algorithms are ubiquitous in Bayesian computations. However, they need to access the full data set in order to evaluate the posterior density at every step of the algorithm. This results in a great computational burden in big data applications. In contrast to MCMC methods, Stochastic Gradient MCMC (SGMCMC) algorithms such as the Stochastic Gradient Langevin Dynamics (SGLD) only require access to a batch of the data set at every step. This drastically improves the computational performance and scales well to large data sets. However, the difficulty with SGMCMC algorithms comes from the sensitivity to its parameters which are notoriously difficult to tune. Moreover, the Root Mean Square Error (RMSE) scales as $\mathcal{O}(c^{-\frac{1}{3}})$ as opposed to standard MCMC $\mathcal{O}(c^{-\frac{1}{2}})$ where $c$ is the computational cost. We introduce a new class of Multilevel Stochastic Gradient Markov chain Monte Carlo algorithms that are able to mitigate the problem of tuning the step size and more importantly of recovering the $\mathcal{O}(c^{-\frac{1}{2}})$ convergence of standard Markov Chain Monte Carlo methods without the need to introduce Metropolis-Hasting steps. A further advantage of this new class of algorithms is that it can easily be parallelised over a heterogeneous computer architecture. We illustrate our methodology using Bayesian logistic regression and provide numerical evidence that for a prescribed relative RMSE the computational cost is sublinear in the number of data items. "
778391414724038656,2016-09-21 00:32:29,https://t.co/T1np46Zbmh,QUDA: A Direct Approach for Sparse Quadratic Discriminant Analysis. (arXiv:1510.00084v3 [stat.ME] UPDATED) https://t.co/T1np46Zbmh,2,5," Abstract: Quadratic discriminant analysis (QDA) is a standard tool for classification due to its simplicity and flexibility. Because the number of its parameters scales quadratically with the number of the variables, QDA is not practical, however, when the dimensionality is relatively large. To address this, we propose a novel procedure named QUDA for QDA in analyzing high-dimensional data. Formulated in a simple and coherent framework, QUDA aims to directly estimate the key quantities in the Bayes discriminant function including quadratic interactions and a linear index of the variables for classification. Under appropriate sparsity assumptions, we establish consistency results for estimating the interactions and the linear index, and further demonstrate that the misclassification rate of our procedure converges to the optimal Bayes risk, even when the dimensionality is exponentially high with respect to the sample size. An efficient algorithm based on the alternating direction method of multipliers (ADMM) is developed for finding interactions, which is much faster than its competitor in the literature. The promising performance of QUDA is illustrated via extensive simulation studies and the analysis of two datasets. "
778391410592641024,2016-09-21 00:32:28,https://t.co/sSWQnYcq7B,Preconditioned Data Sparsification for Big Data with Applications to PCA and K-means. (arXiv:1511.00152v3 [stat.ML… https://t.co/sSWQnYcq7B,1,6," Abstract: We analyze a compression scheme for large data sets that randomly keeps a small percentage of the components of each data sample. The benefit is that the output is a sparse matrix and therefore subsequent processing, such as PCA or K-means, is significantly faster, especially in a distributed-data setting. Furthermore, the sampling is single-pass and applicable to streaming data. The sampling mechanism is a variant of previous methods proposed in the literature combined with a randomized preconditioning to smooth the data. We provide guarantees for PCA in terms of the covariance matrix, and guarantees for K-means in terms of the error in the center estimators at a given step. We present numerical evidence to show both that our bounds are nearly tight and that our algorithms provide a real benefit when applied to standard test data sets, as well as providing certain benefits over related sampling approaches. "
778391406553554944,2016-09-21 00:32:27,https://t.co/Hkd6T63ACL,Why (and How) Avoid Orthogonal Procrustes in Regularized Multivariate Analysis. (arXiv:1605.02674v2 [stat.ML] UPDA… https://t.co/Hkd6T63ACL,0,3," Abstract: Multivariate Analysis (MVA) comprises a family of well-known methods for feature extraction that exploit correlations among input variables of the data representation. One important property that is enjoyed by most such methods is uncorrelation among the extracted features. Recently, regularized versions of MVA methods have appeared in the literature, mainly with the goal to gain interpretability of the solution. In these cases, the solutions can no longer be obtained in a closed manner, and it is frequent to recur to the iteration of two steps, one of them being an orthogonal Procrustes problem. This letter shows that the Procrustes solution is not optimal from the perspective of the overall MVA method, and proposes an alternative approach based on the solution of an eigenvalue problem. Our method ensures the preservation of several properties of the original methods, most notably the uncorrelation of the extracted features, as demonstrated theoretically and through a collection of selected experiments. "
778391402971590656,2016-09-21 00:32:26,https://t.co/0kX4Q1tJjo,Bayesian Variable Selection for Globally Sparse Probabilistic PCA. (arXiv:1605.05918v2 [stat.ML] UPDATED) https://t.co/0kX4Q1tJjo,2,4," Abstract: Sparse versions of principal component analysis (PCA) have imposed themselves as simple, yet powerful ways of selecting relevant features of high-dimensional data in an unsupervised manner. However, when several sparse principal components are computed, the interpretation of the selected variables is difficult since each axis has its own sparsity pattern and has to be interpreted separately. To overcome this drawback, we propose a Bayesian procedure called globally sparse probabilistic PCA (GSPPCA) that allows to obtain several sparse components with the same sparsity pattern. This allows the practitioner to identify the original variables which are relevant to describe the data. To this end, using Roweis' probabilistic interpretation of PCA and a Gaussian prior on the loading matrix, we provide the first exact computation of the marginal likelihood of a Bayesian PCA model. To avoid the drawbacks of discrete model selection, a simple relaxation of this framework is presented. It allows to find a path of models using a variational expectation-maximization algorithm. The exact marginal likelihood is then maximized over this path. This approach is illustrated on real and synthetic data sets. In particular, using unlabeled microarray data, GSPPCA infers much more relevant gene subsets than traditional sparse PCA algorithms. "
778391399217631232,2016-09-21 00:32:26,https://t.co/mjZ1kJkBWe,Alternating Optimisation and Quadrature for Robust Reinforcement Learning. (arXiv:1605.07496v2 [cs.LG] UPDATED) https://t.co/mjZ1kJkBWe,1,6," Abstract: Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables - state features that are randomly determined by the environment in a physical setting but are controllable in a simulator. This paper considers the problem of finding an optimal policy while taking into account the impact of environment variables. We present alternating optimisation and quadrature (ALOQ), which uses Bayesian optimisation and Bayesian quadrature to address such settings. ALOQ is robust to the presence of significant rare events, which may not be observable under random sampling, but have a considerable impact on determining the optimal policy. We provide experimental results demonstrating our approach learning more efficiently than existing methods. "
778391396004818944,2016-09-21 00:32:25,https://t.co/zIabsQLnm3,Modeling Missing Data in Clinical Time Series with RNNs. (arXiv:1606.04130v4 [cs.LG] UPDATED) https://t.co/zIabsQLnm3,2,5," Abstract: We demonstrate a simple strategy to cope with missing data in sequential inputs, addressing the task of multilabel classification of diagnoses given clinical time series. Collected from the pediatric intensive care unit (PICU) at Children's Hospital Los Angeles, our data consists of multivariate time series of observations. The measurements are irregularly spaced, leading to missingness patterns in temporally discretized sequences. While these artifacts are typically handled by imputation, we achieve superior predictive performance by treating the artifacts as features. Unlike linear models, recurrent neural networks can realize this improvement using only simple binary indicators of missingness. For linear models, we show an alternative strategy to capture this signal. Training models on missingness patterns only, we show that for some diseases, what tests are run can be as predictive as the results themselves. "
778391391340822528,2016-09-21 00:32:24,https://t.co/DtvPS7KOGG,Identifiable Phenotyping using Constrained Non-Negative Matrix Factorization. (arXiv:1608.00704v3 [stat.ML] UPDATE… https://t.co/DtvPS7KOGG,3,5," Abstract: This work proposes a new algorithm for automated and simultaneous phenotyping of multiple co-occurring medical conditions, also referred as comorbidities, using clinical notes from the electronic health records (EHRs). A basic latent factor estimation technique of non-negative matrix factorization (NMF) is augmented with domain specific constraints to obtain sparse latent factors that are anchored to a fixed set of chronic conditions. The proposed anchoring mechanism ensures a one-to-one identifiable and interpretable mapping between the latent factors and the target comorbidities. Qualitative assessment of the empirical results by clinical experts suggests that the proposed model learns clinically interpretable phenotypes while being predictive of 30 day mortality. The proposed method can be readily adapted to any non-negative EHR data across various healthcare institutions. "
778391385632366592,2016-09-21 00:32:22,https://t.co/SfR07dhSMM,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. (arXiv:1609.04802v2 [cs.CV] … https://t.co/SfR07dhSMM,3,8," Abstract: Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method. "
778029296900268033,2016-09-20 00:33:34,https://t.co/NBRtoy3Gq5,ADAGIO: Fast Data-aware Near-Isometric Linear Embeddings. (arXiv:1609.05388v1 [stat.ML]) https://t.co/NBRtoy3Gq5,0,3," Abstract: Many important applications, including signal reconstruction, parameter estimation, and signal processing in a compressed domain, rely on a low-dimensional representation of the dataset that preserves {\em all} pairwise distances between the data points and leverages the inherent geometric structure that is typically present. Recently Hedge, Sankaranarayanan, Yin and Baraniuk \cite{hedge2015} proposed the first data-aware near-isometric linear embedding which achieves the best of both worlds. However, their method NuMax does not scale to large-scale datasets. Our main contribution is a simple, data-aware, near-isometric linear dimensionality reduction method which significantly outperforms a state-of-the-art method \cite{hedge2015} with respect to scalability while achieving high quality near-isometries. Furthermore, our method comes with strong worst-case theoretical guarantees that allow us to guarantee the quality of the obtained near-isometry. We verify experimentally the efficiency of our method on numerous real-world datasets, where we find that our method ($<$10 secs) is more than 3\,000$\times$ faster than the state-of-the-art method \cite{hedge2015} ($>$9 hours) on medium scale datasets with 60\,000 data points in 784 dimensions. Finally, we use our method as a preprocessing step to increase the computational efficiency of a classification application and for speeding up approximate nearest neighbor queries. "
778029294845067264,2016-09-20 00:33:33,https://t.co/O0i5kbWCiW,Sequential Ensemble Learning for Outlier Detection: A Bias-Variance Perspective. (arXiv:1609.05528v1 [cs.LG]) https://t.co/O0i5kbWCiW,0,4," Abstract: Ensemble methods for classification and clustering have been effectively used for decades, while ensemble learning for outlier detection has only been studied recently. In this work, we design a new ensemble approach for outlier detection in multi-dimensional point data, which provides improved accuracy by reducing error through both bias and variance. Although classification and outlier detection appear as different problems, their theoretical underpinnings are quite similar in terms of the bias-variance trade-off [1], where outlier detection is considered as a binary classification task with unobserved labels but a similar bias-variance decomposition of error. In this paper, we propose a sequential ensemble approach called CARE that employs a two-phase aggregation of the intermediate results in each iteration to reach the final outcome. Unlike existing outlier ensembles which solely incorporate a parallel framework by aggregating the outcomes of independent base detectors to reduce variance, our ensemble incorporates both the parallel and sequential building blocks to reduce bias as well as variance by ($i$) successively eliminating outliers from the original dataset to build a better data model on which outlierness is estimated (sequentially), and ($ii$) combining the results from individual base detectors and across iterations (parallelly). Through extensive experiments on sixteen real-world datasets mainly from the UCI machine learning repository [2], we show that CARE performs significantly better than or at least similar to the individual baselines. We also compare CARE with the state-of-the-art outlier ensembles where it also provides significant improvement when it is the winner and remains close otherwise. "
778029292953501696,2016-09-20 00:33:33,https://t.co/8vLhNHj0E3,On Randomized Distributed Coordinate Descent with Quantized Updates. (arXiv:1609.05539v1 [stat.ML]) https://t.co/8vLhNHj0E3,0,4," Abstract: In this paper, we study the randomized distributed coordinate descent algorithm with quantized updates. In the literature, the iteration complexity of the randomized distributed coordinate descent algorithm has been characterized under the assumption that machines can exchange updates with an infinite precision. We consider a practical scenario in which the messages exchange occurs over channels with finite capacity, and hence the updates have to be quantized. We derive sufficient conditions on the quantization error such that the algorithm with quantized update still converge. We further compare the convergence rates of the randomized distributed coordinate descent algorithm with and without quantization errors through numerical results. "
778029291112202240,2016-09-20 00:33:32,https://t.co/2sSuYhHBWX,Optimality and Sub-optimality of PCA for Spiked Random Matrices and Synchronization. (arXiv:1609.05573v1 [math.ST]) https://t.co/2sSuYhHBWX,0,3," Abstract: A central problem of random matrix theory is to understand the eigenvalues of spiked random matrix models, in which a prominent eigenvector is planted into a random matrix. These distributions form natural statistical models for principal component analysis (PCA) problems throughout the sciences. Baik, Ben Arous and P\'ech\'e showed that the spiked Wishart ensemble exhibits a sharp phase transition asymptotically: when the signal strength is above a critical threshold, it is possible to detect the presence of a spike based on the top eigenvalue, and below the threshold the top eigenvalue provides no information. Such results form the basis of our understanding of when PCA can detect a low-rank signal in the presence of noise. However, not all the information about the spike is necessarily contained in the spectrum. We study the fundamental limitations of statistical methods, including non-spectral ones. Our results include: i) For the Gaussian Wigner ensemble, we show that PCA achieves the optimal detection threshold for a variety of benign priors for the spike. We extend previous work on the spherically symmetric and i.i.d. Rademacher priors through an elementary, unified analysis. ii) For any non-Gaussian Wigner ensemble, we show that PCA is always suboptimal for detection. However, a variant of PCA achieves the optimal threshold (for benign priors) by pre-transforming the matrix entries according to a carefully designed function. This approach has been stated before, and we give a rigorous and general analysis. iii) For both the Gaussian Wishart ensemble and various synchronization problems over groups, we show that inefficient procedures can work below the threshold where PCA succeeds, whereas no known efficient algorithm achieves this. This conjectural gap between what is statistically possible and what can be done efficiently remains open. "
778029288960458753,2016-09-20 00:33:32,https://t.co/gS0IrR97Px,Stochastic Matrix Factorization. (arXiv:1609.05772v1 [stat.ML]) https://t.co/gS0IrR97Px,0,6," Abstract: This paper considers a restriction to non-negative matrix factorization in which at least one matrix factor is stochastic. That is, the elements of the matrix factors are non-negative and the columns of one matrix factor sum to 1. This restriction includes topic models, a popular method for analyzing unstructured data. It also includes a method for storing and finding pictures. The paper presents necessary and sufficient conditions on the observed data such that the factorization is unique. In addition, the paper characterizes natural bounds on the parameters for any observed data and presents a consistent least squares estimator. The results are illustrated using a topic model analysis of PhD abstracts in economics and the problem of storing and retrieving a set of pictures of faces. "
778029287156944896,2016-09-20 00:33:31,https://t.co/C7MBihPWiB,Enabling Dark Energy Science with Deep Generative Models of Galaxy Images. (arXiv:1609.05796v1 [stat.ML]) https://t.co/C7MBihPWiB,0,4," Abstract: Understanding the nature of dark energy, the mysterious force driving the accelerated expansion of the Universe, is a major challenge of modern cosmology. The next generation of cosmological surveys, specifically designed to address this issue, rely on accurate measurements of the apparent shapes of distant galaxies. However, shape measurement methods suffer from various unavoidable biases and therefore will rely on a precise calibration to meet the accuracy requirements of the science analysis. This calibration process remains an open challenge as it requires large sets of high quality galaxy images. To this end, we study the application of deep conditional generative models in generating realistic galaxy images. In particular we consider variations on conditional variational autoencoder and introduce a new adversarial objective for training of conditional generative networks. Our results suggest a reliable alternative to the acquisition of expensive high quality observations for generating the calibration data needed by the next generation of cosmological surveys. "
778029285235908608,2016-09-20 00:33:31,https://t.co/oVhU90TaMV,Inherent Trade-Offs in the Fair Determination of Risk Scores. (arXiv:1609.05807v1 [cs.LG]) https://t.co/oVhU90TaMV,0,3," Abstract: Recent discussion in the public sphere about algorithmic classification has involved tension between competing notions of what it means for a probabilistic classification to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously. Moreover, even satisfying all three conditions approximately requires that the data lie in an approximate version of one of the constrained special cases identified by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them. "
778029283210125312,2016-09-20 00:33:30,https://t.co/Ets5DVat9w,The Projected Power Method: An Efficient Algorithm for Joint Alignment from Pairwise Differences. (arXiv:1609.0582… https://t.co/Ets5DVat9w,1,5," Abstract: Various applications involve assigning discrete label values to a collection of objects based on some pairwise noisy data. Due to the discrete---and hence nonconvex---structure of the problem, computing the optimal assignment (e.g.~maximum likelihood assignment) becomes intractable at first sight. This paper makes progress towards efficient computation by focusing on a concrete joint alignment problem---that is, the problem of recovering $n$ discrete variables $x_i \in \{1,\cdots, m\}$, $1\leq i\leq n$ given noisy observations of their modulo differences $\{x_i - x_j~\mathsf{mod}~m\}$. We propose a low-complexity and model-free procedure, which operates in a lifted space by representing distinct label values in orthogonal directions, and which attempts to optimize quadratic functions over hypercubes. Starting with a first guess computed via a spectral method, the algorithm successively refines the iterates via projected power iterations. We prove that for a broad class of statistical models, the proposed projected power method makes no error---and hence converges to the maximum likelihood estimate---in a suitable regime. Numerical experiments have been carried out on both synthetic and real data to demonstrate the practicality of our algorithm. We expect this algorithmic framework to be effective for a broad range of discrete assignment problems. "
778029281331015682,2016-09-20 00:33:30,https://t.co/T7QtuQYech,A Cheap Linear Attention Mechanism with Fast Lookups and Fixed-Size Representations. (arXiv:1609.05866v1 [cs.LG]) https://t.co/T7QtuQYech,2,7," Abstract: The softmax content-based attention mechanism has proven to be very beneficial in many applications of recurrent neural networks. Nevertheless it suffers from two major computational limitations. First, its computations for an attention lookup scale linearly in the size of the attended sequence. Second, it does not encode the sequence into a fixed-size representation but instead requires to memorize all the hidden states. These two limitations restrict the use of the softmax attention mechanism to relatively small-scale applications with short sequences and few lookups per sequence. In this work we introduce a family of linear attention mechanisms designed to overcome the two limitations listed above. We show that removing the softmax non-linearity from the traditional attention formulation yields constant-time attention lookups and fixed-size representations of the attended sequences. These properties make these linear attention mechanisms particularly suitable for large-scale applications with extreme query loads, real-time requirements and memory constraints. Early experiments on a question answering task show that these linear mechanisms yield significantly better accuracy results than no attention, but obviously worse than their softmax alternative. "
778029279095492609,2016-09-20 00:33:29,https://t.co/IqMjFneOme,Geometrically Convergent Distributed Optimization with Uncoordinated Step-Sizes. (arXiv:1609.05877v1 [math.OC]) https://t.co/IqMjFneOme,0,3," Abstract: A recent algorithmic family for distributed optimization, DIGing's, have been shown to have geometric convergence over time-varying undirected/directed graphs. Nevertheless, an identical step-size for all agents is needed. In this paper, we study the convergence rates of the Adapt-Then-Combine (ATC) variation of the DIGing algorithm under uncoordinated step-sizes. We show that the ATC variation of DIGing algorithm converges geometrically fast even if the step-sizes are different among the agents. In addition, our analysis implies that the ATC structure can accelerate convergence compared to the distributed gradient descent (DGD) structure which has been used in the original DIGing algorithm. "
778029277224853504,2016-09-20 00:33:29,https://t.co/7hzyr47wSy,Online and Distributed learning of Gaussian mixture models by Bayesian Moment Matching. (arXiv:1609.05881v1 [cs.AI… https://t.co/7hzyr47wSy,0,5," Abstract: The Gaussian mixture model is a classic technique for clustering and data modeling that is used in numerous applications. With the rise of big data, there is a need for parameter estimation techniques that can handle streaming data and distribute the computation over several processors. While online variants of the Expectation Maximization (EM) algorithm exist, their data efficiency is reduced by a stochastic approximation of the E-step and it is not clear how to distribute the computation over multiple processors. We propose a Bayesian learning technique that lends itself naturally to online and distributed computation. Since the Bayesian posterior is not tractable, we project it onto a family of tractable distributions after each observation by matching a set of sufficient moments. This Bayesian moment matching technique compares favorably to online EM in terms of time and accuracy on a set of data modeling benchmarks. "
778029274884341760,2016-09-20 00:33:28,https://t.co/uFsa4flLGi],On the Geometric Ergodicity of Hamiltonian Monte Carlo. (arXiv:1601.08057v2 [https://t.co/uFsa4flLGi] UPDATED) https://t.co/rBLceK7fNP,0,6,INDEXERROR
778029272699207680,2016-09-20 00:33:28,https://t.co/5wWQtbuQ7T,Detecting weak changes in dynamic events over networks. (arXiv:1603.08981v2 [cs.LG] UPDATED) https://t.co/5wWQtbuQ7T,0,5," Abstract: Large volume of networked streaming event data are becoming increasingly available in a wide variety of applications, such as social network analysis, Internet traffic monitoring and healthcare analytics. Streaming event data are discrete observation occurred in continuous time, and the precise time interval between two events carries a great deal of information about the dynamics of the underlying systems. How to promptly detect changes in these dynamic systems using these streaming event data? In this paper, we propose a novel change-point detection framework for multi-dimensional event data over networks. We cast the problem into sequential hypothesis test, and derive the likelihood ratios for point processes, which are computed efficiently via an EM-like algorithm that is parameter-free and can be computed in a distributed fashion. We derive a highly accurate theoretical characterization of the false-alarm-rate, and show that it can achieve weak signal detection by aggregating local statistics over time and networks. Finally, we demonstrate the good performance of our algorithm on numerical examples and real-world datasets from twitter and Memetracker. "
778029270874656778,2016-09-20 00:33:27,https://t.co/QABuk2a9Ma,Fast Algorithms for Robust PCA via Gradient Descent. (arXiv:1605.07784v2 [cs.IT] UPDATED) https://t.co/QABuk2a9Ma,0,11," Abstract: We consider the problem of Robust PCA in the fully and partially observed settings. Without corruptions, this is the well-known matrix completion problem. From a statistical standpoint this problem has been recently well-studied, and conditions on when recovery is possible (how many observations do we need, how many corruptions can we tolerate) via polynomial-time algorithms is by now understood. This paper presents and analyzes a non-convex optimization approach that greatly reduces the computational complexity of the above problems, compared to the best available algorithms. In particular, in the fully observed case, with $r$ denoting rank and $d$ dimension, we reduce the complexity from $\mathcal{O}(r^2d^2\log(1/\varepsilon))$ to $\mathcal{O}(rd^2\log(1/\varepsilon))$ -- a big savings when the rank is big. For the partially observed case, we show the complexity of our algorithm is no more than $\mathcal{O}(r^4d \log d \log(1/\varepsilon))$. Not only is this the best-known run-time for a provable algorithm under partial observation, but in the setting where $r$ is small compared to $d$, it also allows for near-linear-in-$d$ run-time that can be exploited in the fully-observed case as well, by simply running our algorithm on a subset of the observations. "
778029268660019200,2016-09-20 00:33:27,https://t.co/fnSX5ZqeMY,A Locally Adaptive Normal Distribution. (arXiv:1606.02518v2 [stat.ML] UPDATED) https://t.co/fnSX5ZqeMY,0,3," Abstract: The multivariate normal density is a monotonic function of the distance to the mean, and its ellipsoidal shape is due to the underlying Euclidean metric. We suggest to replace this metric with a locally adaptive, smoothly changing (Riemannian) metric that favors regions of high local density. The resulting locally adaptive normal distribution (LAND) is a generalization of the normal distribution to the ""manifold"" setting, where data is assumed to lie near a potentially low-dimensional manifold embedded in $\mathbb{R}^D$. The LAND is parametric, depending only on a mean and a covariance, and is the maximum entropy distribution under the given metric. The underlying metric is, however, non-parametric. We develop a maximum likelihood algorithm to infer the distribution parameters that relies on a combination of gradient descent and Monte Carlo integration. We further extend the LAND to mixture models, and provide the corresponding EM algorithm. We demonstrate the efficiency of the LAND to fit non-trivial probability distributions over both synthetic data, and EEG measurements of human sleep. "
778029266575450113,2016-09-20 00:33:26,https://t.co/WHTX9Z1e6t,Early Visual Concept Learning with Unsupervised Deep Learning. (arXiv:1606.05579v2 [stat.ML] UPDATED) https://t.co/WHTX9Z1e6t,0,4," Abstract: Automated discovery of early visual concepts from raw image data is a major open challenge in AI research. Addressing this problem, we propose an unsupervised approach for learning disentangled representations of the underlying factors of variation. We draw inspiration from neuroscience, and show how this can be achieved in an unsupervised generative model by applying the same learning pressures as have been suggested to act in the ventral visual stream in the brain. By enforcing redundancy reduction, encouraging statistical independence, and exposure to data with transform continuities analogous to those to which human infants are exposed, we obtain a variational autoencoder (VAE) framework capable of learning disentangled factors. Our approach makes few assumptions and works well across a wide variety of datasets. Furthermore, our solution has useful emergent properties, such as zero-shot inference and an intuitive understanding of ""objectness"". "
778029264625082368,2016-09-20 00:33:26,https://t.co/3qXKU4Lonk,Energy-Economic Multiple Incremental/Decremental Kernel Ridge Regression for Green Clouds. (arXiv:1608.00621v2 [cs… https://t.co/3qXKU4Lonk,0,2," Abstract: This study presents an energy-economic approach for incremental/decremental learning based on kernel ridge regression, a frequently used regressor on clouds. To avoid reanalyzing the entire dataset when data change every time, the proposed mechanism supports incremental/decremental processing for both single and multiple samples (i.e., batch processing). Moreover, incremental/decremental analyses in empirical and intrinsic space are also introduced to handle with data matrices with a large number of samples or feature dimensions. At the end of this study, we further the proposed mechanism to statistical Kernelized Bayesian Regression, so that incremental/decremental analyses become applicable. Experimental results showed that the performance in accuracy of the proposed method remained as well as the original nonincremental design. Furthermore, training time and power consumption were significantly reduced. These findings thereby demonstrate the effectiveness of the proposed method. "
778029262586740736,2016-09-20 00:33:25,https://t.co/StxIRRutmY,Deep Survival Analysis. (arXiv:1608.02158v2 [stat.ML] UPDATED) https://t.co/StxIRRutmY,0,5," Abstract: The electronic health record (EHR) provides an unprecedented opportunity to build actionable tools to support physicians at the point of care. In this paper, we investigate survival analysis in the context of EHR data. We introduce deep survival analysis, a hierarchical generative approach to survival analysis. It departs from previous approaches in two primary ways: (1) all observations, including covariates, are modeled jointly conditioned on a rich latent structure; and (2) the observations are aligned by their failure time, rather than by an arbitrary time zero as in traditional survival analysis. Further, it (3) scalably handles heterogeneous (continuous and discrete) data types that occur in the EHR. We validate deep survival analysis model by stratifying patients according to risk of developing coronary heart disease (CHD). Specifically, we study a dataset of 313,000 patients corresponding to 5.5 million months of observations. When compared to the clinically validated Framingham CHD risk score, deep survival analysis is significantly superior in stratifying patients according to their risk. "
778029260040761344,2016-09-20 00:33:25,https://t.co/4KKUwdZ6iS,Medical image denoising using convolutional denoising autoencoders. (arXiv:1608.04667v2 [cs.CV] UPDATED) https://t.co/4KKUwdZ6iS,0,4," Abstract: Image denoising is an important pre-processing step in medical image analysis. Different algorithms have been proposed in past three decades with varying denoising performances. More recently, having outperformed all conventional methods, deep learning based models have shown a great promise. These methods are however limited for requirement of large training sample size and high computational costs. In this paper we show that using small sample size, denoising autoencoders constructed using convolutional layers can be used for efficient denoising of medical images. Heterogeneous images can be combined to boost sample size for increased denoising performance. Simplest of networks can reconstruct images with corruption levels so high that noise and signal are not differentiable to human eye. "
778029257855537153,2016-09-20 00:33:24,https://t.co/dLmtPT90Fw,Efficient Exploration for Dialogue Policy Learning with BBQ Networks &amp; Replay Buffer Spiking. (arXiv:1608.05081v2 … https://t.co/dLmtPT90Fw,1,4," Abstract: When rewards are sparse and action spaces large, Q-learning with $\epsilon$-greedy exploration can be inefficient. This poses problems for otherwise promising applications such as task-oriented dialogue systems, where the primary reward signal, indicating successful completion of a task, requires a complex sequence of appropriate actions. Under these circumstances, a randomly exploring agent might never stumble upon a successful outcome in reasonable time. We present two techniques that significantly improve the efficiency of exploration for deep Q-learning agents in dialogue systems. First, we introduce an exploration technique based on Thompson sampling, drawing Monte Carlo samples from a Bayes-by-backprop neural network, demonstrating marked improvement over common approaches such as $\epsilon$-greedy and Boltzmann exploration. Second, we show that spiking the replay buffer with experiences from a small number of successful episodes, as are easy to harvest for dialogue tasks, can make Q-learning feasible when it might otherwise fail. "
778029250536497152,2016-09-20 00:33:23,https://t.co/lAXCzSvMu6,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network. (arXi… https://t.co/lAXCzSvMu6,1,4," Abstract: Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods. "
777668088519065600,2016-09-19 00:38:15,https://t.co/MZvyjFK1GU,Predicting Shot Making in Basketball using Convolutional Neural Networks Learnt from Adversarial Multiagent Trajec… https://t.co/MZvyjFK1GU,5,11," Abstract: In this paper, we predict the likelihood of a player making a shot in basketball from multiagent trajectories. Previous approaches to similar problems center on hand-crafting features to capture domain specific knowledge. Although intuitive, recent work in deep learning has shown this approach is prone to missing important predictive features. To circumvent this issue, we present a convolutional neural network (CNN) approach where we initially represent the multiagent behavior as an image. To encode the adversarial nature of basketball, we use a multi-channel image which we then feed into a CNN. Additionally, to capture the temporal aspect of the trajectories we use ""fading."" By using gradient ascent, we were able to discover what the CNN filters look for during training. Last, we find that a combined FNN+CNN is the best performing network with an error rate of 26%. "
777668087034310656,2016-09-19 00:38:14,https://t.co/IYvrAcFbXe,Unbiased Sparse Subspace Clustering By Selective Pursuit. (arXiv:1609.05057v1 [stat.ML]) https://t.co/IYvrAcFbXe,0,11," Abstract: Sparse subspace clustering (SSC) is an elegant approach for unsupervised segmentation if the data points of each cluster are located in linear subspaces. This model applies, for instance, in motion segmentation if some restrictions on the camera model hold. SSC requires that problems based on the $l_1$-norm are solved to infer which points belong to the same subspace. If these unknown subspaces are well-separated this algorithm is guaranteed to succeed. The algorithm rests upon the assumption that points on the same subspace are well spread. The question what happens if this condition is violated has not yet been investigated. In this work, the effect of particular distributions on the same subspace will be analyzed. It will be shown that SSC fails to infer correct labels if points on the same subspace fall into more than one cluster. "
777668085448896512,2016-09-19 00:38:14,https://t.co/exEhQIoiSF,Discovering Relationships Across Disparate Data Modalities. (arXiv:1609.05148v1 [stat.ML]) https://t.co/exEhQIoiSF,0,5," Abstract: Discovering whether certain properties are associated with other properties is fundamental to all science. As the amount of data increases, it is becoming increasingly difficult and important to determine whether one property of the data (e.g., cloud density) is related to another (e.g., grass wetness). Only If they are related does it make sense to further investigate the nature of the relationship. Unfortunately, reliably identifying relationships can be challenging, especially when the properties themselves are complex and the relationship is nonlinear and high-dimensional. Here, we describe a procedure, Multiscale Generalized Correlation (MGC), that addresses these challenges. Our key insight is that if two properties are related, comparisons between measurements of similar pairs of the first property (e.g., two clouds of similar density) should be correlated with the comparisons between corresponding measurements of the second property (grass wetness under those clouds). We demonstrate the statistical and computational efficiency of MGC in both simulations and theory. We then apply it to detect the presence and nature of the relationships between brain activity and personality, brain shape and disorder, and brain connectivity and creativity. Finally, we demonstrate that MGC does not suffer from the false positives that have plagued parametric methods. Our open source implementation of MGC is applicable to fundamental questions confronting science, government, finance, and many other disciplines. "
777668083754340352,2016-09-19 00:38:14,https://t.co/YF050ng4U0,Gradient Descent Learns Linear Dynamical Systems. (arXiv:1609.05191v1 [cs.LG]) https://t.co/YF050ng4U0,1,15," Abstract: We prove that gradient descent efficiently converges to the global optimizer of the maximum likelihood objective of an unknown linear time-invariant dynamical system from a sequence of noisy observations generated by the system. Even though the objective function is non-convex, we provide polynomial running time and sample complexity bounds under strong but natural assumptions. Linear systems identification has been studied for many decades, yet, to the best of our knowledge, these are the first polynomial guarantees for the problem we consider. "
777668082223476740,2016-09-19 00:38:13,https://t.co/e7A6oKkh2z,Likelihood-free Model Choice. (arXiv:1503.07689v3 [stat.ME] UPDATED) https://t.co/e7A6oKkh2z,1,6," Abstract: This document is an invited chapter covering the specificities of ABC model choice, intended for the incoming Handbook of ABC by Sisson, Fan, and Beaumont (2017). Beyond exposing the potential pitfalls of ABC based posterior probabilities, the review emphasizes mostly the solution proposed by Pudlo et al. (2016) on the use of random forests for aggregating summary statistics and and for estimating the posterior probability of the most likely model via a secondary random fores. "
777668080210247680,2016-09-19 00:38:13,https://t.co/vZbWShX0HO,Matrix Completion has No Spurious Local Minimum. (arXiv:1605.07272v2 [cs.LG] UPDATED) https://t.co/vZbWShX0HO,0,6," Abstract: Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for \textit{positive semidefinite} matrix completion has no spurious local minima \--- all local minima must also be global. Therefore, many popular optimization algorithms such as (stochastic) gradient descent can provably solve positive semidefinite matrix completion with \textit{arbitrary} initialization in polynomial time. The result can be generalized to the setting when the observed entries contain noise. We believe that our main proof strategy can be useful for understanding geometric properties of other statistical problems involving partial or noisy observations. "
776579965974347776,2016-09-16 00:34:26,https://t.co/EW6Bk8qykY,Bayesian Reinforcement Learning: A Survey. (arXiv:1609.04436v1 [cs.AI]) https://t.co/EW6Bk8qykY,7,44," Abstract: Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties. "
776579963587887108,2016-09-16 00:34:26,https://t.co/bHqr6ktFyO,Sampling Generative Networks: Notes on a Few Effective Techniques. (arXiv:1609.04468v1 [cs.NE]) https://t.co/bHqr6ktFyO,1,12, Abstract: We introduce several techniques for sampling and visualizing the latent spaces of generative models. Replacing linear interpolation with spherical linear interpolation prevents diverging from a model's prior distribution and produces sharper samples. J-Diagrams and MINE grids are introduced as visualizations of manifolds created by analogies and nearest neighbors. We demonstrate two new techniques for deriving attribute vectors: bias-corrected vectors with data replication and synthetic vectors with data augmentation. Most techniques are intended to be independent of model type and examples are shown on both Variational Autoencoders and Generative Adversarial Networks. 
776579960874135552,2016-09-16 00:34:25,https://t.co/hyoA8YTV4w,Column Networks for Collective Classification. (arXiv:1609.04508v1 [cs.AI]) https://t.co/hyoA8YTV4w,0,6," Abstract: Relational learning deals with data that are characterized by relational structures. An important task is collective classification, which is to jointly classify networked objects. While it holds a great promise to produce a better accuracy than non-collective classifiers, collective classification is computational challenging and has not leveraged on the recent breakthroughs of deep learning. We present Column Network (CLN), a novel deep learning model for collective classification in multi-relational domains. CLN has many desirable theoretical properties: (i) it encodes multi-relations between any two instances; (ii) it is deep and compact, allowing complex functions to be approximated at the network level with a small set of free parameters; (iii) local and relational features are learned simultaneously; (iv) long-range, higher-order dependencies between instances are supported naturally; and (v) crucially, learning and inference are efficient, linear in the size of the network and the number of relations. We evaluate CLN on multiple real-world applications: (a) delay prediction in software projects, (b) PubMed Diabetes publication classification and (c) film genre classification. In all applications, CLN demonstrates a higher accuracy than state-of-the-art rivals. "
776579958101708800,2016-09-16 00:34:24,https://t.co/S2Gy7R6vEk,Sparse Tensor Graphical Model: Non-convex Optimization and Statistical Inference. (arXiv:1609.04522v1 [stat.ML]) https://t.co/S2Gy7R6vEk,2,18," Abstract: We consider the estimation and inference of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data. To facilitate the estimation of the precision matrix corresponding to each way of the tensor, we assume the data follow a tensor normal distribution whose covariance has a Kronecker product structure. A critical challenge in the estimation and inference of this model is the fact that its penalized maximum likelihood estimation involves minimizing a non-convex objective function. To address it, this paper makes two contributions: (i) In spite of the non-convexity of this estimation problem, we prove that an alternating minimization algorithm, which iteratively estimates each sparse precision matrix while fixing the others, attains an estimator with the optimal statistical rate of convergence. Notably, such an estimator achieves estimation consistency with only one tensor sample, which was not observed in the previous work. (ii) We propose a de-biased statistical inference procedure for testing hypotheses on the true support of the sparse precision matrices, and employ it for testing a growing number of hypothesis with false discovery rate (FDR) control. The asymptotic normality of our test statistic and the consistency of FDR control procedure are established. Our theoretical results are further backed up by thorough numerical studies. We implement the methods into a publicly available R package Tlasso. "
776579955153104896,2016-09-16 00:34:24,https://t.co/fb3lw1paaH,Sparse Low-rank Tensor Response Regression. (arXiv:1609.04523v1 [stat.ML]) https://t.co/fb3lw1paaH,0,4," Abstract: Motivated by structural and functional neuroimaging analysis, we propose a new class of tensor response regression models. The model embeds two key low-dimensional structures: sparsity and low- rankness, and can handle both a general and a symmetric tensor response. We formulate parameter estimation of our model as a non- convex optimization problem, and develop an efficient alternating updating algorithm. Theoretically, we establish a non-asymptotic estimation error bound for the actual estimator obtained from our algorithm. This error bound reveals an interesting interaction between the computational efficiency and the statistical rate of convergence. Based on this general error bound, we further obtain an optimal estimation error rate when the distribution of the error tensor is Gaussian. Our technical analysis is based upon exploitation of the bi-convex structure of the objective function and a careful characterization of the impact of an intermediate sparse tensor decomposition step. The resulting new technical tools are also of independent interest to the theoretical analysis of general non-convex optimization problems. Simulations and an analysis of the autism spectrum disorder imaging data further illustrate the efficacy of our method. "
776579952552636416,2016-09-16 00:34:23,https://t.co/JygFabLo4N,Matrix Product State for Higher-Order Tensor Compression and Classification. (arXiv:1609.04541v1 [stat.ML]) https://t.co/JygFabLo4N,0,7," Abstract: This paper introduces matrix product state (MPS) decomposition as a new and systematic method to compress multidimensional data represented by higher-order tensors. It solves two major bottlenecks in tensor compression: computation and compression quality. Regardless of tensor order, MPS compresses tensors to matrices of moderate dimension which can be used for classification. Mainly based on a successive sequence of singular value decompositions (SVD), MPS is quite simple to implement and arrives at the global optimal matrix, bypassing local alternating optimization, which is not only computationally expensive but cannot yield the global solution. Benchmark results show that MPS can achieve better classification performance with favorable computation cost compared to other tensor compression methods. "
776579949918613505,2016-09-16 00:34:22,https://t.co/qSqilAm7SE,Recursive nearest agglomeration (ReNA): fast clustering for approximation of structured signals. (arXiv:1609.04608… https://t.co/qSqilAm7SE,0,4," Abstract: -In this work, we revisit fast dimension reduction approaches, as with random projections and random sampling. Our goal is to summarize the data to decrease computational costs and memory footprint of subsequent analysis. Such dimension reduction can be very efficient when the signals of interest have a strong structure, such as with images. We focus on this setting and investigate feature clustering schemes for data reductions that capture this structure. An impediment to fast dimension reduction is that good clustering comes with large algorithmic costs. We address it by contributing a linear-time agglomerative clustering scheme, Recursive Nearest Agglomeration (ReNA). Unlike existing fast agglomerative schemes, it avoids the creation of giant clusters. We empirically validate that it approximates the data as well as traditional variance-minimizing clustering schemes that have a quadratic complexity. In addition, we analyze signal approximation with feature clustering and show that it can remove noise, improving subsequent analysis steps. As a consequence, data reduction by clustering features with ReNA yields very fast and accurate models, enabling to process large datasets on budget. Our theoretical analysis is backed by extensive experiments on publicly-available data that illustrate the computation efficiency and the denoising properties of the resulting dimension reduction scheme. "
776579946978344960,2016-09-16 00:34:22,https://t.co/m5I2qtrHVa,Distributed Estimation of the Operating State of a Single-Bus DC MicroGrid without an External Communication Inter… https://t.co/m5I2qtrHVa,0,2," Abstract: We propose a decentralized Maximum Likelihood solution for estimating the stochastic renewable power generation and demand in single bus Direct Current (DC) MicroGrids (MGs), with high penetration of droop controlled power electronic converters. The solution relies on the fact that the primary control parameters are set in accordance with the local power generation status of the generators. Therefore, the steady state voltage is inherently dependent on the generation capacities and the load, through a non-linear parametric model, which can be estimated. To have a well conditioned estimation problem, our solution avoids the use of an external communication interface and utilizes controlled voltage disturbances to perform distributed training. Using this tool, we develop an efficient, decentralized Maximum Likelihood Estimator (MLE) and formulate the sufficient condition for the existence of the globally optimal solution. The numerical results illustrate the promising performance of our MLE algorithm. "
776579944117919744,2016-09-16 00:34:21,https://t.co/6QgSqJE2Rt,Learning Schizophrenia Imaging Genetics Data Via Multiple Kernel Canonical Correlation Analysis. (arXiv:1609.04699… https://t.co/6QgSqJE2Rt,2,6," Abstract: Kernel and Multiple Kernel Canonical Correlation Analysis (CCA) are employed to classify schizophrenic and healthy patients based on their SNPs, DNA Methylation and fMRI data. Kernel and Multiple Kernel CCA are popular methods for finding nonlinear correlations between high-dimensional datasets. Data was gathered from 183 patients, 79 with schizophrenia and 104 healthy controls. Kernel and Multiple Kernel CCA represent new avenues for studying schizophrenia, because, to our knowledge, these methods have not been used on these data before. Classification is performed via k-means clustering on the kernel matrix outputs of the Kernel and Multiple Kernel CCA algorithm. Accuracies of the Kernel and Multiple Kernel CCA classification are compared to that of the regularized linear CCA algorithm classification, and are found to be significantly more accurate. Both algorithms demonstrate maximal accuracies when the combination of DNA methylation and fMRI data are used, and experience lower accuracies when the SNP data are incorporated. "
776579939046920192,2016-09-16 00:34:20,https://t.co/UA4wZrp5yH,Mixture model modal clustering. (arXiv:1609.04721v1 [stat.ML]) https://t.co/UA4wZrp5yH,0,6," Abstract: The two most extended density-based approaches to clustering are surely mixture model clustering and modal clustering. In the mixture model approach, the density is represented as a mixture and clusters are associated to the different mixture components. In modal clustering, clusters are understood as regions of high density separated from each other by zones of lower density, so that they are closely related to certain regions around the density modes. If the true density is indeed in the assumed class of mixture densities, then mixture model clustering allows to scrutinize more subtle situations than modal clustering. However, when mixture modeling is used in a nonparametric way, taking advantage of the denseness of the sieve of mixture densities to approximate any density, then the correspondence between clusters and mixture components may become questionable. In this paper we introduce two methods to adopt a modal clustering point of view after a mixture model fit. Numerous examples are provided to illustrate that mixture modeling can also be used for clustering in a nonparametric sense, as long as clusters are understood as the domains of attraction of the density modes. "
776579936085745664,2016-09-16 00:34:19,https://t.co/XRXYxdPxmY,"Coherence Pursuit: Fast, Simple, and Robust Principal Component Analysis. (arXiv:1609.04789v1 [cs.LG]) https://t.co/XRXYxdPxmY",0,8," Abstract: This paper presents a remarkably simple, yet powerful, algorithm for robust Principal Component Analysis (PCA). In the proposed approach, an outlier is set apart from an inlier by comparing their coherence with the rest of the data points. As inliers lie on a low dimensional subspace, they are likely to have strong mutual coherence provided there are enough inliers. By contrast, outliers do not typically admit low dimensional structures, wherefore an outlier is unlikely to bear strong resemblance with a large number of data points. The mutual coherences are computed by forming the Gram matrix of normalized data points. Subsequently, the subspace is recovered from the span of a small subset of the data points that exhibit strong coherence with the rest of the data. As coherence pursuit only involves one simple matrix multiplication, it is significantly faster than the state of-the-art robust PCA algorithms. We provide a mathematical analysis of the proposed algorithm under a random model for the distribution of the inliers and outliers. It is shown that the proposed method can recover the correct subspace even if the data is predominantly outliers. To the best of our knowledge, this is the first provable robust PCA algorithm that is simultaneously non-iterative, can tolerate a large number of outliers and is robust to linearly dependent outliers. "
776579932902260736,2016-09-16 00:34:18,https://t.co/fEePjxbE1P,Learning from networked examples. (arXiv:1405.2600v2 [cs.AI] UPDATED) https://t.co/fEePjxbE1P,2,8," Abstract: Many machine learning algorithms are based on the assumption that training examples are drawn identically and independently. However, this assumption does not hold anymore when learning from a networked sample because two or more training examples may share some common objects, and hence share the features of these shared objects. We first show that the classic approach of ignoring this problem potentially can have a disastrous effect on the accuracy of statistics, and then consider alternatives. One of these is to only use independent examples, discarding other information. However, this is clearly suboptimal. We analyze sample error bounds in a networked setting, providing both improved and new results. Next, we propose an efficient weighting method which achieves a better sample error bound than those of previous methods. Our approach is based on novel concentration inequalities for networked variables. "
776579929676873729,2016-09-16 00:34:18,https://t.co/oKT0UwTLPF,False Discoveries Occur Early on the Lasso Path. (arXiv:1511.01957v4 [math.ST] UPDATED) https://t.co/oKT0UwTLPF,1,4," Abstract: In regression settings where explanatory variables have very low correlations and there are relatively few effects, each of large magnitude, we expect the Lasso to find the important variables with few errors, if any. This paper shows that in a regime of linear sparsity---meaning that the fraction of variables with a non-vanishing effect tends to a constant, however small---this cannot really be the case, even when the design variables are stochastically independent. We demonstrate that true features and null features are always interspersed on the Lasso path, and that this phenomenon occurs no matter how strong the effect sizes are. We derive a sharp asymptotic trade-off between false and true positive rates or, equivalently, between measures of type I and type II errors along the Lasso path. This trade-off states that if we ever want to achieve a type II error (false negative rate) under a critical value, then anywhere on the Lasso path the type I error (false positive rate) will need to exceed a given threshold so that we can never have both errors at a low level at the same time. Our analysis uses tools from approximate message passing (AMP) theory as well as novel elements to deal with a possibly adaptive selection of the Lasso regularizing parameter. "
776579925558095872,2016-09-16 00:34:17,https://t.co/eG2kZKZad3,Preconditioned Stochastic Gradient Descent. (arXiv:1512.04202v2 [stat.ML] UPDATED) https://t.co/eG2kZKZad3,1,10," Abstract: Stochastic gradient descent (SGD) still is the workhorse for many practical problems. However, it converges slow, and can be difficult to tune. It is possible to precondition SGD to accelerate its convergence remarkably. But many attempts in this direction either aim at solving specialized problems, or result in significantly more complicated methods than SGD. This paper proposes a new method to estimate a preconditioner such that the amplitudes of perturbations of preconditioned stochastic gradient match that of the perturbations of parameters to be optimized in a way comparable to Newton method for deterministic optimization. Unlike the preconditioners based on secant equation fitting as done in deterministic quasi-Newton methods, which assume positive definite Hessian and approximate its inverse, the new preconditioner works equally well for both convex and non-convex optimizations with exact or noisy gradients. When stochastic gradient is used, it can naturally damp the gradient noise to stabilize SGD. Efficient preconditioner estimation methods are developed, and with reasonable simplifications, they are applicable to large scaled problems. Experimental results demonstrate that equipped with the new preconditioner, without any tuning effort, preconditioned SGD can efficiently solve many challenging problems like the training of a deep neural network or a recurrent neural network requiring extremely long term memories. "
776579922995339264,2016-09-16 00:34:16,https://t.co/NeXwUHQAWi,CLEAR: Covariant LEAst-square Re-fitting with applications to image restoration. (arXiv:1606.05158v2 [math.ST] UPD… https://t.co/NeXwUHQAWi,0,3," Abstract: In this paper, we propose a new framework to remove parts of the systematic errors affecting popular restoration algorithms, with a special focus for image processing tasks. Generalizing ideas that emerged for $\ell_1$ regularization, we develop an approach re-fitting the results of standard methods towards the input data. Total variation regularizations and non-local means are special cases of interest. We identify important covariant information that should be preserved by the re-fitting method, and emphasize the importance of preserving the Jacobian (w.r.t. the observed signal) of the original estimator. Then, we provide an approach that has a ""twicing"" flavor and allows re-fitting the restored signal by adding back a local affine transformation of the residual term. We illustrate the benefits of our method on numerical simulations for image restoration tasks. "
776579919837102082,2016-09-16 00:34:15,https://t.co/4Ml9fEbrlx,Alternating Back-Propagation for Generator Network. (arXiv:1606.08571v3 [stat.ML] UPDATED) https://t.co/4Ml9fEbrlx,2,8," Abstract: This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a non-linear generalization of factor analysis. In this model, the mapping from the latent factors to the observed vector is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates between the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dynamics or gradient descent. (2) Learning back-propagation, which updates the parameters given the inferred latent factors by gradient descent. The gradient computations in both steps are powered by back-propagation, and they share most of their code in common. We show that the alternating back-propagation algorithm can learn realistic generator models of natural images, video sequences, and sounds. Moreover, it can also be used to learn from incomplete or indirect training data. "
776579917031043072,2016-09-16 00:34:15,https://t.co/e0cith5877,Ten Steps of EM Suffice for Mixtures of Two Gaussians. (arXiv:1609.00368v2 [stat.ML] UPDATED) https://t.co/e0cith5877,0,5," Abstract: We provide global convergence guarantees for the expectation-maximization (EM) algorithm applied to mixtures of two Gaussians with known covariance matrices. We show that EM converges geometrically to the correct mean vectors, and provide simple, closed-form expressions for the convergence rate. As a simple illustration, we show that in one dimension ten steps of the EM algorithm initialized at $+ \infty$ result in less than 1% error estimation of the means. "
776579911440039937,2016-09-16 00:34:13,https://t.co/AbEoB9pXG2,Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model. (arXiv:1609.00680v3 [q-bio.BM] UP… https://t.co/AbEoB9pXG2,0,3," Abstract: Recently exciting progress has been made on protein contact prediction, but the predicted contacts for proteins without many sequence homologs is still of low quality and not very useful for de novo structure prediction. This paper presents a new deep learning method that predicts contacts by integrating both evolutionary coupling (EC) and sequence conservation information through an ultra-deep neural network formed by two deep residual networks. This deep neural network allows us to model very complex sequence-contact relationship as well as long-range inter-contact correlation. Our method greatly outperforms existing contact prediction methods and leads to much more accurate contact-assisted protein folding. Tested on three datasets of 579 proteins, the average top L long-range prediction accuracy obtained our method, the representative EC method CCMpred and the CASP11 winner MetaPSICOV is 0.47, 0.21 and 0.30, respectively; the average top L/10 long-range accuracy of our method, CCMpred and MetaPSICOV is 0.77, 0.47 and 0.59, respectively. Ab initio folding using our predicted contacts as restraints can yield correct folds (i.e., TMscore>0.6) for 203 test proteins, while that using MetaPSICOV- and CCMpred-predicted contacts can do so for only 79 and 62 proteins, respectively. Further, our contact-assisted models have much better quality than template-based models. Using our predicted contacts as restraints, we can (ab initio) fold 208 of the 398 membrane proteins with TMscore>0.5. By contrast, when the training proteins of our method are used as templates, homology modeling can only do so for 10 of them. One interesting finding is that even if we do not train our prediction models with any membrane proteins, our method works very well on membrane protein prediction. Finally, in recent blind CAMEO benchmark our method successfully folded 5 test proteins with a novel fold. "
776217592562409472,2016-09-15 00:34:30,https://t.co/TYlpn1nR9z,Private Topic Modeling. (arXiv:1609.04120v1 [stat.ML]) https://t.co/TYlpn1nR9z,1,11," Abstract: We develop a privatised stochastic variational inference method for Latent Dirichlet Allocation (LDA). The iterative nature of stochastic variational inference presents challenges: multiple iterations are required to obtain accurate posterior distributions, yet each iteration increases the amount of noise that must be added to achieve a reasonable degree of privacy. We propose a practical algorithm that overcomes this challenge by combining: (1) A relaxed notion of the differential privacy, called concentrated differential privacy, which provides high probability bounds for cumulative privacy loss, which is well suited for iterative algorithms, rather than focusing on single-query loss; and (2) Privacy amplification resulting from subsampling of large-scale data. Focusing on conjugate exponential family models, in our private variational inference, all the posterior distributions will be privatised by simply perturbing expected sufficient statistics. Using Wikipedia data, we illustrate the effectiveness of our algorithm for large-scale data. "
776217590326845440,2016-09-15 00:34:29,https://t.co/vqMOC78Pl6,Stochastic Heavy Ball. (arXiv:1609.04228v1 [math.ST]) https://t.co/vqMOC78Pl6,0,6," Abstract: This paper deals with a natural stochastic optimization procedure derived from the so-called Heavy-ball method differential equation, which was introduced by Polyak in the 1960s with his seminal contribution [Pol64]. The Heavy-ball method is a second-order dynamics that was investigated to minimize convex functions f . The family of second-order methods recently received a large amount of attention, until the famous contribution of Nesterov [Nes83], leading to the explosion of large-scale optimization problems. This work provides an in-depth description of the stochastic heavy-ball method, which is an adaptation of the deterministic one when only unbiased evalutions of the gradient are available and used throughout the iterations of the algorithm. We first describe some almost sure convergence results in the case of general non-convex coercive functions f . We then examine the situation of convex and strongly convex potentials and derive some non-asymptotic results about the stochastic heavy-ball method. We end our study with limit theorems on several rescaled algorithms. "
776217588103802886,2016-09-15 00:34:29,https://t.co/zi6xWhgEPN,Gray-box inference for structured Gaussian process models. (arXiv:1609.04289v1 [stat.ML]) https://t.co/zi6xWhgEPN,1,5," Abstract: We develop an automated variational inference method for Bayesian structured prediction problems with Gaussian process (GP) priors and linear-chain likelihoods. Our approach does not need to know the details of the structured likelihood model and can scale up to a large number of observations. Furthermore, we show that the required expected likelihood term and its gradients in the variational objective (ELBO) can be estimated efficiently by using expectations over very low-dimensional Gaussian distributions. Optimization of the ELBO is fully parallelizable over sequences and amenable to stochastic optimization, which we use along with control variate techniques and state-of-the-art incremental optimization to make our framework useful in practice. Results on a set of natural language processing tasks show that our method can be as good as (and sometimes better than) hard-coded approaches including SVM-struct and CRFs, and overcomes the scalability limitations of previous inference algorithms based on sampling. Overall, this is a fundamental step to developing automated inference methods for Bayesian structured prediction. "
776217585671086080,2016-09-15 00:34:28,https://t.co/joljeGS7ZG,TristouNet: Triplet Loss for Speaker Turn Embedding. (arXiv:1609.04301v1 [cs.SD]) https://t.co/joljeGS7ZG,0,4," Abstract: TristouNet is a neural network architecture based on Long Short-Term Memory recurrent networks, meant to project speech sequences into a fixed-dimensional euclidean space. Thanks to the triplet loss paradigm used for training, the resulting sequence embeddings can be compared directly with the euclidean distance, for speaker comparison purposes. Experiments on short (between 500ms and 5s) speech turn comparison and speaker change detection show that TristouNet brings significant improvements over the current state-of-the-art techniques for both tasks. "
776217582491803648,2016-09-15 00:34:27,https://t.co/2sUaoPAVbr,Very Simple Classifier: a Concept Binary Classifier toInvestigate Features Based on Subsampling and Localility. (a… https://t.co/2sUaoPAVbr,0,4, Abstract: We propose Very Simple Classifier (VSC) a novel method designed to incorporate the concepts of subsampling and locality in the definition of features to be used as the input of a perceptron. The rationale is that locality theoretically guarantees a bound on the generalization error. Each feature in VSC is a max-margin classifier built on randomly-selected pairs of samples. The locality in VSC is achieved by multiplying the value of the feature by a confidence measure that can be characterized in terms of the Chebichev inequality. The output of the layer is then fed in a output layer of neurons. The weights of the output layer are then determined by a regularized pseudoinverse. Extensive comparison of VSC against 9 competitors in the task of binary classification is carried out. Results on 22 benchmark datasets with fixed parameters show that VSC is competitive with the Multi Layer Perceptron (MLP) and outperforms the other competitors. An exploration of the parameter space shows VSC can outperform MLP. 
776217579778170885,2016-09-15 00:34:27,https://t.co/smYfLLKF7x,Relativistic Monte Carlo. (arXiv:1609.04388v1 [stat.ML]) https://t.co/smYfLLKF7x,6,12," Abstract: Hamiltonian Monte Carlo (HMC) is a popular Markov chain Monte Carlo (MCMC) algorithm that generates proposals for a Metropolis-Hastings algorithm by simulating the dynamics of a Hamiltonian system. However, HMC is sensitive to large time discretizations and performs poorly if there is a mismatch between the spatial geometry of the target distribution and the scales of the momentum distribution. In particular the mass matrix of HMC is hard to tune well. In order to alleviate these problems we propose relativistic Hamiltonian Monte Carlo, a version of HMC based on relativistic dynamics that introduce a maximum velocity on particles. We also derive stochastic gradient versions of the algorithm and show that the resulting algorithms bear interesting relationships to gradient clipping, RMSprop, Adagrad and Adam, popular optimisation methods in deep learning. Based on this, we develop relativistic stochastic gradient descent by taking the zero-temperature limit of relativistic stochastic gradient Hamiltonian Monte Carlo. In experiments we show that the relativistic algorithms perform better than classical Newtonian variants and Adam. "
776217577186070529,2016-09-15 00:34:26,https://t.co/jIwrrnSLi4,The LICORS Cabinet: Nonparametric Algorithms for Spatio-temporal Prediction. (arXiv:1506.02686v2 [stat.ML] UPDATED) https://t.co/jIwrrnSLi4,0,5," Abstract: Spatio-temporal data is intrinsically high dimensional, so unsupervised modeling is only feasible if we can exploit structure in the process. When the dynamics are local in both space and time, this structure can be exploited by splitting the global field into many lower-dimensional ""light cones"". We review light cone decompositions for predictive state reconstruction, introducing three simple light cone algorithms. These methods allow for tractable inference of spatio-temporal data, such as full-frame video. The algorithms make few assumptions on the underlying process yet have good predictive performance and can provide distributions over spatio-temporal data, enabling sophisticated probabilistic inference. "
776217575055364096,2016-09-15 00:34:25,https://t.co/Kb3DwTuOxS,Oracle performance for visual captioning. (arXiv:1511.04590v5 [cs.CV] UPDATED) https://t.co/Kb3DwTuOxS,0,2," Abstract: The task of associating images and videos with a natural language description has attracted a great amount of attention recently. Rapid progress has been made in terms of both developing novel algorithms and releasing new datasets. Indeed, the state-of-the-art results on some of the standard datasets have been pushed into the regime where it has become more and more difficult to make significant improvements. Instead of proposing new models, this work investigates the possibility of empirically establishing performance upper bounds on various visual captioning datasets without extra data labelling effort or human evaluation. In particular, it is assumed that visual captioning is decomposed into two steps: from visual inputs to visual concepts, and from visual concepts to natural language descriptions. One would be able to obtain an upper bound when assuming the first step is perfect and only requiring training a conditional language model for the second step. We demonstrate the construction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination of M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used for visual concept extraction in the first step and the simplicity of the language model for the second step, we show that current state-of-the-art models fall short when being compared with the learned upper bounds. Furthermore, with such a bound, we quantify several important factors concerning image and video captioning: the number of visual concepts captured by different models, the trade-off between the amount of visual elements captured and their accuracy, and the intrinsic difficulty and blessing of different datasets. "
776217571372793856,2016-09-15 00:34:25,https://t.co/JzUbao1tWb,Variational Latent Gaussian Process for Recovering Single-Trial Dynamics from Population Spike Trains. (arXiv:1604… https://t.co/JzUbao1tWb,1,8," Abstract: When governed by underlying low-dimensional dynamics, the interdependence of simultaneously recorded population of neurons can be explained by a small number of shared factors, or a low-dimensional trajectory. Recovering these latent trajectories, particularly from single-trial population recordings, may help us understand the dynamics that drive neural computation. However, due to the biophysical constraints and noise in the spike trains, inferring trajectories from data is a challenging statistical problem in general. Here, we propose a practical and efficient inference method, called the variational latent Gaussian process (vLGP). The vLGP combines a generative model with a history-dependent point process observation together with a smoothness prior on the latent trajectories. The vLGP improves upon earlier methods for recovering latent trajectories, which assume either observation models inappropriate for point processes or linear dynamics. We compare and validate vLGP on both simulated datasets and population recordings from the primary visual cortex. In the V1 dataset, we find that vLGP achieves substantially higher performance than previous methods for predicting omitted spike trains, as well as capturing both the toroidal topology of visual stimuli space, and the noise-correlation. These results show that vLGP is a robust method with a potential to reveal hidden neural dynamics from large-scale neural recordings. "
776217568944283652,2016-09-15 00:34:24,https://t.co/ejIF4XsjAz,Clustering Time Series and the Surprising Robustness of HMMs. (arXiv:1605.02531v2 [cs.IT] UPDATED) https://t.co/ejIF4XsjAz,3,9," Abstract: Suppose that we are given a time series where consecutive samples are believed to come from a probabilistic source, that the source changes from time to time and that the total number of sources is fixed. Our objective is to estimate the distributions of the sources. A standard approach to this problem is to model the data as a hidden Markov model (HMM). However, since the data often lacks the Markov or the stationarity properties of an HMM, one can ask whether this approach is still suitable or perhaps another approach is required. In this paper we show that a maximum likelihood HMM estimator can be used to approximate the source distributions in a much larger class of models than HMMs. Specifically, we propose a natural and fairly general non-stationary model of the data, where the only restriction is that the sources do not change too often. Our main result shows that for this model, a maximum-likelihood HMM estimator produces the correct second moment of the data, and the results can be extended to higher moments. "
776217566956093444,2016-09-15 00:34:24,https://t.co/XpT8weP5KD,Human Pose Estimation in Space and Time using 3D CNN. (arXiv:1609.00036v2 [cs.CV] UPDATED) https://t.co/XpT8weP5KD,0,9," Abstract: This paper explores the capabilities of convolutional neural networks to deal with a task that is easily manageable for humans: perceiving 3D pose of a human body from varying angles. However, in our approach, we are restricted to using a monocular vision system. For this purpose, we apply a convolutional neural network approach on RGB videos and extend it to three dimensional convolutions. This is done via encoding the time dimension in videos as the 3\ts{rd} dimension in convolutional space, and directly regressing to human body joint positions in 3D coordinate space. This research shows the ability of such a network to achieve state-of-the-art performance on the selected Human3.6M dataset, thus demonstrating the possibility of successfully representing temporal data with an additional dimension in the convolutional operation. "
776217562120069121,2016-09-15 00:34:22,https://t.co/cbrOPKF0x2,Energy-based Generative Adversarial Network. (arXiv:1609.03126v2 [cs.LG] UPDATED) https://t.co/cbrOPKF0x2,5,17," Abstract: We introduce the ""Energy-based Generative Adversarial Network"" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images. "
775855410091810817,2016-09-14 00:35:19,https://t.co/xp4ATWloCU,A Greedy Algorithm to Cluster Specialists. (arXiv:1609.03666v1 [cs.LG]) https://t.co/xp4ATWloCU,0,7," Abstract: Several recent deep neural networks experiments leverage the generalist-specialist paradigm for classification. However, no formal study compared the performance of different clustering algorithms for class assignment. In this paper we perform such a study, suggest slight modifications to the clustering procedures, and propose a novel algorithm designed to optimize the performance of of the specialist-generalist classification system. Our experiments on the CIFAR-10 and CIFAR-100 datasets allow us to investigate situations for varying number of classes on similar data. We find that our \emph{greedy pairs} clustering algorithm consistently outperforms other alternatives, while the choice of the confusion matrix has little impact on the final performance. "
775855407969337344,2016-09-14 00:35:18,https://t.co/MJZjC6Io4F,Making Neural Networks Robust to Label Noise: a Loss Correction Approach. (arXiv:1609.03683v1 [stat.ML]) https://t.co/MJZjC6Io4F,6,18," Abstract: We present a theoretically grounded approach to train deep neural networks, including recurrent networks, subject to class-dependent label noise. Our method only performs a correction on the loss function, and is agnostic to both the application domain and network architecture. We propose two procedures for loss correction: they simply amount to at most a matrix inversion and multiplication, provided that we know the probability of each class being corrupted into another. We further show how one can estimate these probabilities, adapting a recent technique for noise estimation to the multi-class setting, and thus providing an end-to-end framework. Extensive experiments on MNIST, IMDB, CIFAR-10, CIFAR-100 employing a diversity of architectures --- stacking dense, convolutional, pooling, dropout, batch normalization, word embedding, LSTM and residual layers --- demonstrate the noise robustness of our proposals. Incidentally, we also prove that, when ReLU is the only non-linearity, the loss curvature is immune to class-dependent label noise. "
775855406342078464,2016-09-14 00:35:18,https://t.co/GvhmDXTX5q,Analysis of Kelner and Levin graph sparsification algorithm for a streaming setting. (arXiv:1609.03769v1 [stat.ML]) https://t.co/GvhmDXTX5q,0,5," Abstract: We derive a new proof to show that the incremental resparsification algorithm proposed by Kelner and Levin (2013) produces a spectral sparsifier in high probability. We rigorously take into account the dependencies across subsequent resparsifications using martingale inequalities, fixing a flaw in the original analysis. "
775855404840615936,2016-09-14 00:35:17,https://t.co/WC0qBAOBHb,Learning conditional independence structure for high-dimensional uncorrelated vector processes. (arXiv:1609.03772v… https://t.co/WC0qBAOBHb,2,6," Abstract: We formulate and analyze a graphical model selection method for inferring the conditional independence graph of a high-dimensional nonstationary Gaussian random process (time series) from a finite-length observation. The observed process samples are assumed uncorrelated over time and having a time-varying marginal distribution. The selection method is based on testing conditional variances obtained for small subsets of process components. This allows to cope with the high-dimensional regime, where the sample size can be (drastically) smaller than the process dimension. We characterize the required sample size such that the proposed selection method is successful with high probability. "
775855403162800129,2016-09-14 00:35:17,https://t.co/HtkbqPdMYg,Information Theoretic Structure Learning with Confidence. (arXiv:1609.03912v1 [cs.IT]) https://t.co/HtkbqPdMYg,1,8," Abstract: Information theoretic measures (e.g. the Kullback Liebler divergence and Shannon mutual information) have been used for exploring possibly nonlinear multivariate dependencies in high dimension. If these dependencies are assumed to follow a Markov factor graph model, this exploration process is called structure discovery. For discrete-valued samples, estimates of the information divergence over the parametric class of multinomial models lead to structure discovery methods whose mean squared error achieves parametric convergence rates as the sample size grows. However, a naive application of this method to continuous nonparametric multivariate models converges much more slowly. In this paper we introduce a new method for nonparametric structure discovery that uses weighted ensemble divergence estimators that achieve parametric convergence rates and obey an asymptotic central limit theorem that facilitates hypothesis testing and other types of statistical validation. "
775855401610993664,2016-09-14 00:35:17,https://t.co/Y2RppPfxZm,Mapping the Similarities of Spectra: Global and Locally-biased Approaches to SDSS Galaxy Data. (arXiv:1609.03932v1… https://t.co/Y2RppPfxZm,0,4," Abstract: We apply a novel spectral graph technique, that of locally-biased semi-supervised eigenvectors, to study the diversity of galaxies. This technique permits us to characterize empirically the natural variations in observed spectra data, and we illustrate how this approach can be used in an exploratory manner to highlight both large-scale global as well as small-scale local structure in Sloan Digital Sky Survey (SDSS) data. We use this method in a way that simultaneously takes into account the measurements of spectral lines as well as the continuum shape. Unlike Principal Component Analysis, this method does not assume that the Euclidean distance between galaxy spectra is a good global measure of similarity between all spectra, but instead it only assumes that local difference information between similar spectra is reliable. Moreover, unlike other nonlinear dimensionality methods, this method can be used to characterize very finely both small-scale local as well as large-scale global properties of realistic noisy data. The power of the method is demonstrated on the SDSS Main Galaxy Sample by illustrating that the derived embeddings of spectra carry an unprecedented amount of information. By using a straightforward global or unsupervised variant, we observe that the main features correlate strongly with star formation rate and that they clearly separate active galactic nuclei. Computed parameters of the method can be used to describe line strengths and their interdependencies. By using a locally-biased or semi-supervised variant, we are able to focus on typical variations around specific objects of astronomical interest. We present several examples illustrating that this approach can enable new discoveries in the data as well as a detailed understanding of very fine local structure that would otherwise be overwhelmed by large-scale noise and global trends in the data. "
775855400151310336,2016-09-14 00:35:16,https://t.co/Q9g3jUmjvV,Noisy Inductive Matrix Completion Under Sparse Factor Models. (arXiv:1609.03958v1 [stat.ML]) https://t.co/Q9g3jUmjvV,0,5," Abstract: Inductive Matrix Completion (IMC) is an important class of matrix completion problems that allows direct inclusion of available features to enhance estimation capabilities. These models have found applications in personalized recommendation systems, multilabel learning, dictionary learning, etc. This paper examines a general class of noisy matrix completion tasks where the underlying matrix is following an IMC model i.e., it is formed by a mixing matrix (a priori unknown) sandwiched between two known feature matrices. The mixing matrix here is assumed to be well approximated by the product of two sparse matrices---referred here to as ""sparse factor models."" We leverage the main theorem of Soni:2016:NMC and extend it to provide theoretical error bounds for the sparsity-regularized maximum likelihood estimators for the class of problems discussed in this paper. The main result is general in the sense that it can be used to derive error bounds for various noise models. In this paper, we instantiate our main result for the case of Gaussian noise and provide corresponding error bounds in terms of squared loss. "
775855398721089536,2016-09-14 00:35:16,https://t.co/FFhluFa42L,Self-Sustaining Iterated Learning. (arXiv:1609.03960v1 [math.OC]) https://t.co/FFhluFa42L,2,6," Abstract: An important result from psycholinguistics (Griffiths & Kalish, 2005) states that no language can be learned iteratively by rational agents in a self-sustaining manner. We show how to modify the learning process slightly in order to achieve self-sustainability. Our work is in two parts. First, we characterize iterated learnability in geometric terms and show how a slight, steady increase in the lengths of the training sessions ensures self-sustainability for any discrete language class. In the second part, we tackle the nondiscrete case and investigate self-sustainability for iterated linear regression. We discuss the implications of our findings to issues of non-equilibrium dynamics in natural algorithms. "
775855397156556800,2016-09-14 00:35:16,https://t.co/0ZV06JUEod,Optimal learning with Bernstein Online Aggregation. (arXiv:1404.1356v5 [stat.ML] UPDATED) https://t.co/0ZV06JUEod,0,3," Abstract: We introduce a new recursive aggregation procedure called Bernstein Online Aggregation (BOA). The exponential weights include an accuracy term and a second order term that is a proxy of the quadratic variation as in Hazan and Kale (2010). This second term stabilizes the procedure that is optimal in different senses. We first obtain optimal regret bounds in the deterministic context. Then, an adaptive version is the first exponential weights algorithm that exhibits a second order bound with excess losses that appears first in Gaillard et al. (2014). The second order bounds in the deterministic context are extended to a general stochastic context using the cumulative predictive risk. Such conversion provides the main result of the paper, an inequality of a novel type comparing the procedure with any deterministic aggregation procedure for an integrated criteria. Then we obtain an observable estimate of the excess of risk of the BOA procedure. To assert the optimality, we consider finally the iid case for strongly convex and Lipschitz continuous losses and we prove that the optimal rate of aggregation of Tsybakov (2003) is achieved. The batch version of the BOA procedure is then the first adaptive explicit algorithm that satisfies an optimal oracle inequality with high probability. "
775855395722125317,2016-09-14 00:35:15,https://t.co/A1VbWALbOW,On Lipschitz Continuity and Smoothness of Loss Functions in Learning to Rank. (arXiv:1405.0586v3 [cs.LG] UPDATED) https://t.co/A1VbWALbOW,2,7," Abstract: In binary classification and regression problems, it is well understood that Lipschitz continuity and smoothness of the loss function play key roles in governing generalization error bounds for empirical risk minimization algorithms. In this paper, we show how these two properties affect generalization error bounds in the learning to rank problem. The learning to rank problem involves vector valued predictions and therefore the choice of the norm with respect to which Lipschitz continuity and smoothness are defined becomes crucial. Choosing the $\ell_\infty$ norm in our definition of Lipschitz continuity allows us to improve existing bounds. Furthermore, under smoothness assumptions, our choice enables us to prove rates that interpolate between $1/\sqrt{n}$ and $1/n$ rates. Application of our results to ListNet, a popular learning to rank method, gives state-of-the-art performance guarantees. "
775855393729867776,2016-09-14 00:35:15,https://t.co/5Co6JYLGyn,Signal Processing on Graphs: Causal Modeling of Unstructured Data. (arXiv:1503.00173v3 [cs.IT] UPDATED) https://t.co/5Co6JYLGyn,2,7," Abstract: Many applications collect a large number of time series, for example, the financial data of companies quoted in a stock exchange, the health care data of all patients that visit the emergency room of a hospital, or the temperature sequences continuously measured by weather stations across the US. These data are often referred to as unstructured. A first task in its analytics is to derive a low dimensional representation, a graph or discrete manifold, that describes well the interrelations among the time series and their intrarelations across time. This paper presents a computationally tractable algorithm for estimating this graph that structures the data. The resulting graph is directed and weighted, possibly capturing causal relations, not just reciprocal correlations as in many existing approaches in the literature. A convergence analysis is carried out. The algorithm is demonstrated on random graph datasets and real network time series datasets, and its performance is compared to that of related methods. The adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested. "
775855392375136256,2016-09-14 00:35:14,https://t.co/focHcFda2j,On Bayesian index policies for sequential resource allocation. (arXiv:1601.01190v2 [stat.ML] UPDATED) https://t.co/focHcFda2j,0,4," Abstract: This paper is about index policies for minimizing (frequentist) regret in a stochastic multi-armed bandit model, that are inspired by a Bayesian view on the problem. Our main contribution is to prove that the Bayes-UCB algorithm, which relies on quantiles of posterior distributions, is asymptotically optimal when the rewards distributions belong to a one-dimensional exponential family, for a large class of prior distributions. We also show that the Bayesian literature gives new insight on what kind of exploration rates could be used in frequentist, UCB-type algorithms. Indeed, approximations of the Bayesian optimal solution or the Finite Horizon Gittins indices provide a justification for the kl-UCB+ and kl-UCB-H+ algorithms, whose asymptotic optimality is also established. "
775855391074820096,2016-09-14 00:35:14,https://t.co/UZ6pPlB5ZV,Nested Mini-Batch K-Means. (arXiv:1602.02934v5 [stat.ML] UPDATED) https://t.co/UZ6pPlB5ZV,0,5," Abstract: A new algorithm is proposed which accelerates the mini-batch k-means algorithm of Sculley (2010) by using the distance bounding approach of Elkan (2003). We argue that, when incorporating distance bounds into a mini-batch algorithm, already used data should preferentially be reused. To this end we propose using nested mini-batches, whereby data in a mini-batch at iteration t is automatically reused at iteration t+1. Using nested mini-batches presents two difficulties. The first is that unbalanced use of data can bias estimates, which we resolve by ensuring that each data sample contributes exactly once to centroids. The second is in choosing mini-batch sizes, which we address by balancing premature fine-tuning of centroids with redundancy induced slow-down. Experiments show that the resulting nmbatch algorithm is very effective, often arriving within 1% of the empirical minimum 100 times earlier than the standard mini-batch algorithm. "
775855389694976000,2016-09-14 00:35:14,https://t.co/yFU2OHpKb5,Challenges in Bayesian Adaptive Data Analysis. (arXiv:1604.02492v2 [cs.LG] UPDATED) https://t.co/yFU2OHpKb5,1,5," Abstract: Traditional statistical analysis requires that the analysis process and data are independent. By contrast, the new field of adaptive data analysis hopes to understand and provide algorithms and accuracy guarantees for research as it is commonly performed in practice, as an iterative process of interacting repeatedly with the same data set, such as repeated tests against a holdout set. Previous work has defined a model with a rather strong lower bound on sample complexity in terms of the number of queries, $n\sim\sqrt q$, arguing that adaptive data analysis is much harder than static data analysis, where $n\sim\log q$ is possible. Instead, we argue that those strong lower bounds point to a limitation of the previous model in that it must consider wildly asymmetric scenarios which do not hold in typical applications. To better understand other difficulties of adaptivity, we propose a new Bayesian version of the problem that mandates symmetry. Since the other lower bound techniques are ruled out, we can more effectively see difficulties that might otherwise be overshadowed. As a first contribution to this model, we produce a new problem using error-correcting codes on which a large family of methods, including all previously proposed algorithms, require roughly $n\sim\sqrt[4]q$. These early results illustrate new difficulties in adaptive data analysis regarding slightly correlated queries on problems with concentrated uncertainty. "
775855388201717760,2016-09-14 00:35:13,https://t.co/TyZBw4AMpj,An Evolutionary Algorithm to Learn SPARQL Queries for Source-Target-Pairs: Finding Patterns for Human Associations… https://t.co/TyZBw4AMpj,0,3," Abstract: Efficient usage of the knowledge provided by the Linked Data community is often hindered by the need for domain experts to formulate the right SPARQL queries to answer questions. For new questions they have to decide which datasets are suitable and in which terminology and modelling style to phrase the SPARQL query. In this work we present an evolutionary algorithm to help with this challenging task. Given a training list of source-target node-pair examples our algorithm can learn patterns (SPARQL queries) from a SPARQL endpoint. The learned patterns can be visualised to form the basis for further investigation, or they can be used to predict target nodes for new source nodes. Amongst others, we apply our algorithm to a dataset of several hundred human associations (such as ""circle - square"") to find patterns for them in DBpedia. We show the scalability of the algorithm by running it against a SPARQL endpoint loaded with > 7.9 billion triples. Further, we use the resulting SPARQL queries to mimic human associations with a Mean Average Precision (MAP) of 39.9 % and a Recall@10 of 63.9 %. "
775491794213666816,2016-09-13 00:30:26,https://t.co/wql5KeDkJw,Extract fetal ECG from single-lead abdominal ECG by de-shape short time Fourier transform and nonlocal median. (ar… https://t.co/wql5KeDkJw,1,2," Abstract: The multiple fundamental frequency detection problem and the source separation problem from a single-channel signal containing multiple oscillatory components and a nonstationary noise are both challenging tasks. To extract the fetal electrocardiogram (ECG) from a single-lead maternal abdominal ECG, we face both challenges. In this paper, we propose a novel method to extract the fetal ECG signal from the single channel maternal abdominal ECG signal, without any additional measurement. The algorithm is composed of three main ingredients. First, the maternal and fetal heart rates are estimated by the de-shape short time Fourier transform, which is a recently proposed nonlinear time-frequency analysis technique; second, the beat tracking technique is applied to accurately obtain the maternal and fetal R peaks; third, the maternal and fetal ECG waveforms are established by the nonlocal median. The algorithm is evaluated on a simulated fetal ECG signal database ({\em fecgsyn} database), and tested on two real databases with the annotation provided by experts ({\em adfecgdb} database and {\em CinC2013} database). In general, the algorithm could be applied to solve other detection and source separation problems, and reconstruct the time-varying wave-shape function of each oscillatory component. "
775491790736658433,2016-09-13 00:30:25,https://t.co/OQRDqKpGks,Stealing Machine Learning Models via Prediction APIs. (arXiv:1609.02943v1 [cs.CR]) https://t.co/OQRDqKpGks,3,10," Abstract: Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (""predictive analytics"") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis. The tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., ""steal"") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures. "
775491787708325889,2016-09-13 00:30:24,https://t.co/BbqPO10DKd,Iteratively Reweighted Least Squares Algorithms for L1-Norm Principal Component Analysis. (arXiv:1609.02997v1 [sta… https://t.co/BbqPO10DKd,0,9," Abstract: Principal component analysis (PCA) is often used to reduce the dimension of data by selecting a few orthonormal vectors that explain most of the variance structure of the data. L1 PCA uses the L1 norm to measure error, whereas the conventional PCA uses the L2 norm. For the L1 PCA problem minimizing the fitting error of the reconstructed data, we propose an exact reweighted and an approximate algorithm based on iteratively reweighted least squares. We provide convergence analyses, and compare their performance against benchmark algorithms in the literature. The computational experiment shows that the proposed algorithms consistently perform best. "
775491784617168896,2016-09-13 00:30:24,https://t.co/dhegC9bK64,On the Relationship between Online Gaussian Process Regression and Kernel Least Mean Squares Algorithms. (arXiv:16… https://t.co/dhegC9bK64,2,6," Abstract: We study the relationship between online Gaussian process (GP) regression and kernel least mean squares (KLMS) algorithms. While the latter have no capacity of storing the entire posterior distribution during online learning, we discover that their operation corresponds to the assumption of a fixed posterior covariance that follows a simple parametric model. Interestingly, several well-known KLMS algorithms correspond to specific cases of this model. The probabilistic perspective allows us to understand how each of them handles uncertainty, which could explain some of their performance differences. "
775491781492346881,2016-09-13 00:30:23,https://t.co/0NQcIRgh1d,Sharing Hash Codes for Multiple Purposes. (arXiv:1609.03219v1 [stat.ML]) https://t.co/0NQcIRgh1d,0,2," Abstract: Locality sensitive hashing (LSH) is a powerful tool for sublinear-time approximate nearest neighbor search, and a variety of hashing schemes have been proposed for different similarity measures. However, hash codes significantly depend on the similarity, which prohibits users from adjusting the similarity at query time. In this paper, we propose multiple purpose LSH (mp-LSH) which shares the hash codes for different similarities. By using vector/code augmentation and cover tree techniques, our mp-LSH supports L2, cosine, and inner product similarities, and their corresponding weighted sums, where the weights can be adjusted at query time. It also allows us to modify the importance of pre-defined groups of features. Thus, mp-LSH enables us, for example, to retrieve similar items to a query with the user preference taken into account, to find a similar material to a query with some properties (stability, utility, etc.) optimized, and to turn on or off a part of multi-modal information (brightness, color, audio, text, etc.) in image/video retrieval. We theoretically and empirically analyze the performance of three variants of mp-LSH, and demonstrate their usefulness on several real-world data sets. "
775491777910435840,2016-09-13 00:30:22,https://t.co/l3F9BK87iO,Supervised multiway factorization. (arXiv:1609.03228v1 [stat.ME]) https://t.co/l3F9BK87iO,0,3," Abstract: We describe a probabilistic PARAFAC/CANDECOMP (CP) factorization for multiway (i.e., tensor) data that incorporates auxiliary covariates, SupCP. SupCP generalizes the supervised singular value decomposition (SupSVD) for vector-valued observations, to allow for observations that have the form of a matrix or higher-order array. Such data are increasingly encountered in biomedical research and other fields. We describe a likelihood-based latent variable representation of the CP factorization, in which the latent variables are informed by additional covariates. We give conditions for identifiability, and develop an EM algorithm for simultaneous estimation of all model parameters. SupCP can be used for dimension reduction, capturing latent structures that are more accurate and interpretable due to covariate supervision. Moreover, SupCP specifies a full probability distribution for a multiway data observation with given covariate values, which can be used for predictive modeling. We conduct comprehensive simulations to evaluate the SupCP algorithm, and we apply it to a facial image database with facial descriptors (e.g., smiling / not smiling) as covariates. Software is available at this https URL . "
775491774072643585,2016-09-13 00:30:21,https://t.co/lk1W8FmIun,Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach. (arXiv:1609.03240v1 [stat… https://t.co/lk1W8FmIun,0,4," Abstract: We consider the non-square matrix sensing problem, under restricted isometry property (RIP) assumptions. We focus on the non-convex formulation, where any rank-$r$ matrix $X \in \mathbb{R}^{m \times n}$ is represented as $UV^\top$, where $U \in \mathbb{R}^{m \times r}$ and $V \in \mathbb{R}^{n \times r}$. In this paper, we complement recent findings on the non-convex geometry of the analogous PSD setting [5], and show that matrix factorization does not introduce any spurious local minima, under RIP. "
775491770050289664,2016-09-13 00:30:20,https://t.co/SGhg55YxpC,Less than a Single Pass: Stochastically Controlled Stochastic Gradient Method. (arXiv:1609.03261v1 [math.OC]) https://t.co/SGhg55YxpC,2,7," Abstract: We develop and analyze a procedure for gradient-based optimization that we refer to as stochastically controlled stochastic gradient (SCSG). As a member of the SVRG family of algorithms, SCSG makes use of gradient estimates at two scales. Unlike most existing algorithms in this family, both the computation cost and the communication cost of SCSG do not necessarily scale linearly with the sample size n; indeed, these costs are independent of n when the target accuracy is low. An experimental evaluation of SCSG on the MNIST dataset shows that it can yield accurate results on this dataset on a single commodity machine with a memory footprint of only 2.6MB and only eight disk accesses. "
775491767013638144,2016-09-13 00:30:19,https://t.co/J5cFn2JKWU,"CompAdaGrad: A Compressed, Complementary, Computationally-Efficient Adaptive Gradient Method. (arXiv:1609.03319v1 … https://t.co/J5cFn2JKWU",0,3," Abstract: The adaptive gradient online learning method known as AdaGrad has seen widespread use in the machine learning community in stochastic and adversarial online learning problems and more recently in deep learning methods. The method's full-matrix incarnation offers much better theoretical guarantees and potentially better empirical performance than its diagonal version; however, this version is computationally prohibitive and so the simpler diagonal version often is used in practice. We introduce a new method, CompAdaGrad, that navigates the space between these two schemes and show that this method can yield results much better than diagonal AdaGrad while avoiding the (effectively intractable) $O(n^3)$ computational complexity of full-matrix AdaGrad for dimension $n$. CompAdaGrad essentially performs full-matrix regularization in a low-dimensional subspace while performing diagonal regularization in the complementary subspace. We derive CompAdaGrad's updates for composite mirror descent in case of the squared $\ell_2$ norm and the $\ell_1$ norm, demonstrate that its complexity per iteration is linear in the dimension, and establish guarantees for the method independent of the choice of composite regularizer. Finally, we show preliminary results on several datasets. "
775491763851198464,2016-09-13 00:30:19,https://t.co/pp8mHPiTQg,On Generation of Time-based Label Refinements. (arXiv:1609.03333v1 [stat.ME]) https://t.co/pp8mHPiTQg,0,2," Abstract: Process mining is a research field focused on the analysis of event data with the aim of extracting insights in processes. Applying process mining techniques on data from smart home environments has the potential to provide valuable insights in (un)healthy habits and to contribute to ambient assisted living solutions. Finding the right event labels to enable application of process mining techniques is however far from trivial, as simply using the triggering sensor as the label for sensor events results in uninformative models that allow for too much behavior (overgeneralizing). Refinements of sensor level event labels suggested by domain experts have shown to enable discovery of more precise and insightful process models. However, there exist no automated approach to generate refinements of event labels in the context of process mining. In this paper we propose a framework for automated generation of label refinements based on the time attribute of events. We show on a case study with real life smart home event data that behaviorally more specific, and therefore more insightful, process models can be found by using automatically generated refined labels in process discovery. "
775491761074470912,2016-09-13 00:30:18,https://t.co/OUMqyN4N5U,Finite-sample and asymptotic analysis of generalization ability with an application to penalized regression. (arXi… https://t.co/OUMqyN4N5U,0,3," Abstract: In this paper, we study the performance of extremum estimators from the perspective of generalization ability (GA): the ability of a model to predict outcomes in new samples from the same population. By adapting the classical concentration inequalities, we derive upper bounds on the empirical out-of-sample prediction errors as a function of the in-sample errors, in-sample data size, heaviness in the tails of the error distribution, and model complexity. We show that the error bounds may be used for tuning key estimation hyper-parameters, such as the number of folds $K$ in cross-validation. We also show how $K$ affects the bias-variance trade-off for cross-validation. We demonstrate that the $\mathcal{L}_2$-norm difference between penalized and the corresponding un-penalized regression estimates is directly explained by the GA of the estimates and the GA of empirical moment conditions. Lastly, we prove that all penalized regression estimates are $L_2$-consistent for both the $n \geqslant p$ and the $n < p$ cases. Simulations are used to demonstrate key results. Keywords: generalization ability, upper bound of generalization error, penalized regression, cross-validation, bias-variance trade-off, $\mathcal{L}_2$ difference between penalized and unpenalized regression, lasso, high-dimensional data. "
775491757761003524,2016-09-13 00:30:17,https://t.co/yhwVCRjL8r,Optimal Encoding and Decoding for Point Process Observations: an Approximate Closed-Form Filter. (arXiv:1609.03519… https://t.co/yhwVCRjL8r,0,3," Abstract: The process of dynamic state estimation (filtering) based on point process observations is in general intractable. Numerical sampling techniques are often practically useful, but lead to limited conceptual insight about optimal encoding/decoding strategies, which are of significant relevance to Computational Neuroscience. We develop an analytically tractable Bayesian approximation to optimal filtering based on point process observations, which allows us to introduce distributional assumptions about sensor properties, that greatly facilitate the analysis of optimal encoding in situations deviating from common assumptions of uniform coding. Numerical comparison with particle filtering demonstrate the quality of the approximation. The analytic framework leads to insights which are difficult to obtain from numerical algorithms, and is consistent with biological observations about the distribution of sensory cells' tuning curve centers. "
775491754988560384,2016-09-13 00:30:16,https://t.co/nLByHyPF0d,"Comment on ""Why does deep and cheap learning work so well?"" [arXiv:1608.08225]. (arXiv:1609.03541v1 [cond-mat.dis-… https://t.co/nLByHyPF0d",0,8," Abstract: In a recent paper, ""Why does deep and cheap learning work so well?"", Lin and Tegmark claim to show that the mapping between deep belief networks and the variational renormalization group derived in [arXiv:1410.3831] is invalid, and present a ""counterexample"" that claims to show that this mapping does not hold. In this comment, we show that these claims are incorrect and stem from a misunderstanding of the variational RG procedure proposed by Kadanoff. We also explain why the ""counterexample"" of Lin and Tegmark is compatible with the mapping proposed in [arXiv:1410.3831]. "
775491751758991360,2016-09-13 00:30:16,https://t.co/SfCPqAXVct,Online Data Thinning via Multi-Subspace Tracking. (arXiv:1609.03544v1 [stat.ML]) https://t.co/SfCPqAXVct,0,3," Abstract: In an era of ubiquitous large-scale streaming data, the availability of data far exceeds the capacity of expert human analysts. In many settings, such data is either discarded or stored unprocessed in datacenters. This paper proposes a method of online data thinning, in which large-scale streaming datasets are winnowed to preserve unique, anomalous, or salient elements for timely expert analysis. At the heart of this proposed approach is an online anomaly detection method based on dynamic, low-rank Gaussian mixture models. Specifically, the high-dimensional covariances matrices associated with the Gaussian components are associated with low-rank models. According to this model, most observations lie near a union of subspaces. The low-rank modeling mitigates the curse of dimensionality associated with anomaly detection for high-dimensional data, and recent advances in subspace clustering and subspace tracking allow the proposed method to adapt to dynamic environments. Furthermore, the proposed method allows subsampling, is robust to missing data, and uses a mini-batch online optimization approach. The resulting algorithms are scalable, efficient, and are capable of operating in real time. Experiments on wide-area motion imagery and e-mail databases illustrate the efficacy of the proposed approach. "
775491747275300866,2016-09-13 00:30:15,https://t.co/8bfkTDC49F,Nonparametric risk bounds for time-series forecasting. (arXiv:1212.0463v2 [math.ST] UPDATED) https://t.co/8bfkTDC49F,0,4," Abstract: We derive generalization error bounds for traditional time-series forecasting models. Our results hold for many standard forecasting tools including autoregressive models, moving average models, and, more generally, linear state-space models. These non-asymptotic bounds need only weak assumptions on the data-generating process, yet allow forecasters to select among competing models and to guarantee, with high probability, that their chosen model will perform well. We motivate our techniques with and apply them to standard economic and financial forecasting tools---a GARCH model for predicting equity volatility and a dynamic stochastic general equilibrium model (DSGE), the standard tool in macroeconomic forecasting. We demonstrate in particular how our techniques can aid forecasters and policy makers in choosing models which behave well under uncertainty and mis-specification. "
775491744116908032,2016-09-13 00:30:14,https://t.co/fUBPmcNxec,Refined Error Bounds for Several Learning Algorithms. (arXiv:1512.07146v2 [cs.LG] UPDATED) https://t.co/fUBPmcNxec,0,4," Abstract: This article studies the achievable guarantees on the error rates of certain learning algorithms, with particular focus on refining logarithmic factors. Many of the results are based on a general technique for obtaining bounds on the error rates of sample-consistent classifiers with monotonic error regions, in the realizable case. We prove bounds of this type expressed in terms of either the VC dimension or the sample compression size. This general technique also enables us to derive several new bounds on the error rates of general sample-consistent learning algorithms, as well as refined bounds on the label complexity of the CAL active learning algorithm. Additionally, we establish a simple necessary and sufficient condition for the existence of a distribution-free bound on the error rates of all sample-consistent learning rules, converging at a rate inversely proportional to the sample size. We also study learning in the presence of classification noise, deriving a new excess error rate guarantee for general VC classes under Tsybakov's noise condition, and establishing a simple and general necessary and sufficient condition for the minimax excess risk under bounded noise to converge at a rate inversely proportional to the sample size. "
775491740874801152,2016-09-13 00:30:13,https://t.co/KKmgEtXaBr,Fast K-Means with Accurate Bounds. (arXiv:1602.02514v6 [stat.ML] UPDATED) https://t.co/KKmgEtXaBr,3,6," Abstract: We propose a novel accelerated exact k-means algorithm, which performs better than the current state-of-the-art low-dimensional algorithm in 18 of 22 experiments, running up to 3 times faster. We also propose a general improvement of existing state-of-the-art accelerated exact k-means algorithms through better estimates of the distance bounds used to reduce the number of distance calculations, and get a speedup in 36 of 44 experiments, up to 1.8 times faster. We have conducted experiments with our own implementations of existing methods to ensure homogeneous evaluation of performance, and we show that our implementations perform as well or better than existing available implementations. Finally, we propose simplified variants of standard approaches and show that they are faster than their fully-fledged counterparts in 59 of 62 experiments. "
775491737829699584,2016-09-13 00:30:12,https://t.co/IHCUOXKJj0,A Simple Approach to Sparse Clustering. (arXiv:1602.07277v2 [stat.ML] UPDATED) https://t.co/IHCUOXKJj0,0,7," Abstract: Consider the problem of sparse clustering, where it is assumed that only a subset of the features are useful for clustering purposes. In the framework of the COSA method of Friedman and Meulman, subsequently improved in the form of the Sparse K-means method of Witten and Tibshirani, a natural and simpler hill-climbing approach is introduced. The new method is shown to be competitive with these two methods and others. "
775491734331617280,2016-09-13 00:30:12,https://t.co/BHO2v0VwQ2,Variance-Reduced Proximal Stochastic Gradient Descent for Non-convex Composite optimization. (arXiv:1606.00602v2 [… https://t.co/BHO2v0VwQ2,0,7," Abstract: Here we study non-convex composite optimization: first, a finite-sum of smooth but non-convex functions, and second, a general function that admits a simple proximal mapping. Most research on stochastic methods for composite optimization assumes convexity or strong convexity of each function. In this paper, we extend this problem into the non-convex setting using variance reduction techniques, such as prox-SVRG and prox-SAGA. We prove that, with a constant step size, both prox-SVRG and prox-SAGA are suitable for non-convex composite optimization, and help the problem converge to a stationary point within $O(1/\epsilon)$ iterations. That is similar to the convergence rate seen with the state-of-the-art RSAG method and faster than stochastic gradient descent. Our analysis is also extended into the min-batch setting, which linearly accelerates the convergence. To the best of our knowledge, this is the first analysis of convergence rate of variance-reduced proximal stochastic gradient for non-convex composite optimization. "
775491729277485056,2016-09-13 00:30:10,https://t.co/OXLRLb1JGA,Random projections of random manifolds. (arXiv:1607.04331v2 [stat.ML] UPDATED) https://t.co/OXLRLb1JGA,1,5," Abstract: Interesting data often concentrate on low dimensional smooth manifolds inside a high dimensional ambient space. Random projections are a simple, powerful tool for dimensionality reduction of such data. Previous works have studied bounds on how many projections are needed to accurately preserve the geometry of these manifolds, given their intrinsic dimensionality, volume and curvature. However, such works employ definitions of volume and curvature that are inherently difficult to compute. Therefore such theory cannot be easily tested against numerical simulations to understand the tightness of the proven bounds. We instead study typical distortions arising in random projections of an ensemble of smooth Gaussian random manifolds. We find explicitly computable, approximate theoretical bounds on the number of projections required to accurately preserve the geometry of these manifolds. Our bounds, while approximate, can only be violated with a probability that is exponentially small in the ambient dimension, and therefore they hold with high probability in cases of practical interest. Moreover, unlike previous work, we test our theoretical bounds against numerical experiments on the actual geometric distortions that typically occur for random projections of random smooth manifolds. We find our bounds are tighter than previous results by several orders of magnitude. "
775129368096018432,2016-09-12 00:30:17,https://t.co/glXSiw4fTx,On Sequential Elimination Algorithms for Best-Arm Identification in Multi-Armed Bandits. (arXiv:1609.02606v1 [stat… https://t.co/glXSiw4fTx,1,7," Abstract: We consider the best-arm identification problem in multi-armed bandits, which focuses purely on exploration. A player is given a fixed budget to explore a finite set of arms, and the rewards of each arm are drawn independently from a fixed, unknown distribution. The player aims to identify the arm with the largest expected reward. We propose a general framework to unify sequential elimination algorithms, where the arms are dismissed iteratively until a unique arm is left. Our analysis reveals a novel performance measure expressed in terms of the sampling mechanism and number of eliminated arms at each round. Based on this result, we develop an algorithm that divides the budget according to a nonlinear function of remaining arms at each round. We provide theoretical guarantees for the algorithm, characterizing the suitable nonlinearity for different problem environments. Matching the theoretical results, our experiments show that the nonlinear algorithm outperforms the state-of-the-art. We finally study the side-observation model, where pulling an arm reveals the rewards of its related arms, and we establish improved theoretical guarantees in the pure-exploration setting. "
775129366330241024,2016-09-12 00:30:16,https://t.co/g3alTceAw4,Why is Differential Evolution Better than Grid Search for Tuning Defect Predictors?. (arXiv:1609.02613v1 [cs.SE]) https://t.co/g3alTceAw4,0,5," Abstract: Context: One of the black arts of data mining is learning the magic parameters that control the learners. In software analytics, at least for defect prediction, several methods, like grid search and differential evolution(DE), have been proposed to learn those parameters. They've been proved to be able to improve learner performance. Objective: We want to evaluate which method that can find better parameters in terms of performance score and runtime. Methods: This paper compares grid search to differential evolution, which is an evolutionary algorithm that makes extensive use of stochastic jumps around the search space. Results: We find that the seemingly complete approach of grid search does no better, and sometimes worse, than the stochastic search. Yet, when repeated 20 times to check for conclusion validity, DE was over 210 times faster (6.2 hours vs 54 days for grid search when both tuning Random Forest over 17 test data sets with F-measure as optimzation objective). Conclusions: These results are puzzling: why does a quick partial search be just as effective as a much slower, and much more,extensive search? To answer that question, we turned to the theoretical optimization literature. Bergstra and Bengio conjecture that grid search is not more effective than more randomized searchers if the underlying search space is inherently low dimensional.This is significant since recent results show that defect prediction exhibits very low intrinsic dimensionality-an observation that explains why a fast method like DE may work as well as a seemingly more thorough grid search. This suggests, as a future research direction, that it might be possible to peek at data sets before doing any optimization in order to match the optimization algorithm to the problem at hand. "
775129364614750209,2016-09-12 00:30:16,https://t.co/nWT7dFRZrM,Distributed Processing of Biosignal-Database for Emotion Recognition with Mahout. (arXiv:1609.02631v1 [stat.ML]) https://t.co/nWT7dFRZrM,1,4," Abstract: This paper investigates the use of distributed processing on the problem of emotion recognition from physiological sensors using a popular machine learning library on distributed mode. Specifically, we run a random forests classifier on the biosignal-data, which have been pre-processed to form exclusive groups in an unsupervised fashion, on a Cloudera cluster using Mahout. The use of distributed processing significantly reduces the time required for the offline training of the classifier, enabling processing of large physiological datasets through many iterations. "
775129362530242561,2016-09-12 00:30:15,https://t.co/76WwawcwX4,Singularity structures and impacts on parameter estimation in finite mixtures of distributions. (arXiv:1609.02655v… https://t.co/76WwawcwX4,4,5," Abstract: Singularities of a statistical model are the elements of the model's parameter space which make the corresponding Fisher information matrix degenerate. These are the points for which estimation techniques such as the maximum likelihood estimator and standard Bayesian procedures do not admit the root-$n$ parametric rate of convergence. We propose a general framework for the identification of singularity structures of the parameter space of finite mixtures, and study the impacts of the singularity levels on minimax lower bounds and rates of convergence for the maximum likelihood estimator over a compact parameter space. Our study makes explicit the deep links between model singularities, parameter estimation convergence rates and minimax lower bounds, and the algebraic geometry of the parameter space for mixtures of continuous distributions. The theory is applied to establish concrete convergence rates of parameter estimation for finite mixture of skewnormal distributions. This rich and increasingly popular mixture model is shown to exhibit a remarkably complex range of asymptotic behaviors which have not been hitherto reported in the literature. "
775129360714047488,2016-09-12 00:30:15,https://t.co/9P9tnDuzqr,Boosting Joint Models for Longitudinal and Time-to-Event Data. (arXiv:1609.02686v1 [stat.ML]) https://t.co/9P9tnDuzqr,0,5," Abstract: Joint Models for longitudinal and time-to-event data have gained a lot of attention in the last few years as they are a helpful technique to approach a data structure very common in life sciences: in many clinical studies or registries longitudinal outcomes are recorded alongside event times. The Danish cystic fibrosis registry collects lung functions of cystic fibrosis patients together with the onset of pulmonary infections. Those two processes are linked to each other and the two outcomes should hence be modeled jointly in order to prevent the bias introduced by the independent modelling. Commonly, joint models are estimated in likelihood based expectation maximization or Bayesian approaches using frameworks were variable selection is problematic and which do not work for high-dimensional data. In this paper, we propose a boosting algorithm tackling these challenges by being able to simultaneously estimate predictors for joint models and automatically select the most influential variables in potentially high-dimensional data situations. We analyse the performance of the new algorithm in a simulation study before we apply it to the Danish cystic fibrosis registry. This is the first approach to combine state-of-the art algorithms from the field of machine-learning with the model class of joint models, providing a fully data-driven mechanism to select variables and predictor effects in an unified framework of boosting joint models. "
775129358788853760,2016-09-12 00:30:14,https://t.co/4xwMsoKGdN,Efficient batch-sequential Bayesian optimization with moments of truncated Gaussian vectors. (arXiv:1609.02700v1 [… https://t.co/4xwMsoKGdN,7,9," Abstract: We deal with the efficient parallelization of Bayesian global optimization algorithms, and more specifically of those based on the expected improvement criterion and its variants. A closed form formula relying on multivariate Gaussian cumulative distribution functions is established for a generalized version of the multipoint expected improvement criterion. In turn, the latter relies on intermediate results that could be of independent interest concerning moments of truncated Gaussian vectors. The obtained expansion of the criterion enables studying its differentiability with respect to point batches and calculating the corresponding gradient in closed form. Furthermore , we derive fast numerical approximations of this gradient and propose efficient batch optimization strategies. Numerical experiments illustrate that the proposed approaches enable computational savings of between one and two order of magnitudes, hence enabling derivative-based batch-sequential acquisition function maximization to become a practically implementable and efficient standard. "
775129357195087873,2016-09-12 00:30:14,https://t.co/QpF36SIFTh,By-passing the Kohn-Sham equations with machine learning. (arXiv:1609.02815v1 [physics.comp-ph]) https://t.co/QpF36SIFTh,3,8," Abstract: Last year, at least 30,000 scientific papers used the Kohn-Sham scheme of density functional theory to solve electronic structure problems in a wide variety of scientific fields, ranging from materials science to biochemistry to astrophysics. Machine learning holds the promise of learning the kinetic energy functional via examples, by-passing the need to solve these equations. This should yield substantial savings in computer time, allowing either larger systems or longer time-scales to be tackled. Attempts to machine-learn this functional have been limited by the need to find its derivative. The present work overcomes this difficulty, by learning the density-potential map directly. Both the improved accuracy and lower computational cost is demonstrated on DFT calculations of small molecules. "
775129355597012992,2016-09-12 00:30:14,https://t.co/rguPI8PEKY,Distributed Online Optimization in Dynamic Environments Using Mirror Descent. (arXiv:1609.02845v1 [math.OC]) https://t.co/rguPI8PEKY,1,4," Abstract: This work addresses decentralized online optimization in non-stationary environments. A network of agents aim to track the minimizer of a global time-varying convex function. The minimizer evolves according to a known dynamics corrupted by an unknown, unstructured noise. At each time, the global function can be cast as a sum of a finite number of local functions, each of which is assigned to one agent in the network. Moreover, the local functions become available to agents sequentially, and agents do not have a prior knowledge of the future cost functions. Therefore, agents must communicate with each other to build an online approximation of the global function. We propose a decentralized variation of the celebrated Mirror Descent, developed by Nemirovksi and Yudin. Using the notion of Bregman divergence in lieu of Euclidean distance for projection, Mirror Descent has been shown to be a powerful tool in large-scale optimization. Our algorithm builds on Mirror Descent, while ensuring that agents perform a consensus step to follow the global function and take into account the dynamics of the global minimizer. To measure the performance of the proposed online algorithm, we compare it to its offline counterpart, where the global functions are available a priori. The gap between the two is called dynamic regret. We establish a regret bound that scales inversely in the spectral gap of the network, and more notably it represents the deviation of minimizer sequence with respect to the given dynamics. We then show that our results subsume a number of results in distributed optimization. We demonstrate the application of our method to decentralized tracking of dynamic parameters and verify the results via numerical experiments. "
775129354112303104,2016-09-12 00:30:13,https://t.co/VUdIPwsMXO,Robust Spectral Detection of Global Structures in the Data by Learning a Regularization. (arXiv:1609.02906v1 [stat… https://t.co/VUdIPwsMXO,1,5," Abstract: Spectral methods are popular in detecting global structures in the given data that can be represented as a matrix. However when the data matrix is sparse or noisy, classic spectral methods usually fail to work, due to localization of eigenvectors (or singular vectors) induced by the sparsity or noise. In this work, we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors. Using matrix perturbation analysis, we demonstrate that the learned regularizations suppress down the eigenvalues associated with localized eigenvectors and enable us to recover the informative eigenvectors representing the global structure. We show applications of our method in several inference problems: community detection in networks, clustering from pairwise similarities, rank estimation and matrix completion problems. Using extensive experiments, we illustrate that our method solves the localization problem and works down to the theoretical detectability limits in different kinds of synthetic data. This is in contrast with existing spectral algorithms based on data matrix, non-backtracking matrix, Laplacians and those with rank-one regularizations, which perform poorly in the sparse case with noise. "
775129352430317568,2016-09-12 00:30:13,https://t.co/YAu3RbNPDK,Semi-Supervised Classification with Graph Convolutional Networks. (arXiv:1609.02907v1 [cs.LG]) https://t.co/YAu3RbNPDK,2,21, Abstract: We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin. 
775129350215794689,2016-09-12 00:30:12,https://t.co/jjlVVptYkz,A Mathematical Formalization of Hierarchical Temporal Memory's Spatial Pooler. (arXiv:1601.06116v3 [stat.ML] UPDAT… https://t.co/jjlVVptYkz,8,23," Abstract: Hierarchical temporal memory (HTM) is an emerging machine learning algorithm, with the potential to provide a means to perform predictions on spatiotemporal data. The algorithm, inspired by the neocortex, currently does not have a comprehensive mathematical framework. This work brings together all aspects of the spatial pooler (SP), a critical learning component in HTM, under a single unifying framework. The primary learning mechanism is explored, where a maximum likelihood estimator for determining the degree of permanence update is proposed. The boosting mechanisms are studied and found to be only relevant during the initial few iterations of the network. Observations are made relating HTM to well-known algorithms such as competitive learning and attribute bagging. Methods are provided for using the SP for classification as well as dimensionality reduction. Empirical evidence verifies that given the proper parameterizations, the SP may be used for feature learning. "
775129348319870976,2016-09-12 00:30:12,https://t.co/GqMGMGpj2c,A Non-convex One-Pass Framework for Generalized Factorization Machines and Rank-One Matrix Sensing. (arXiv:1608.05… https://t.co/GqMGMGpj2c,1,6," Abstract: We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from $d$ dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank $k$, our algorithm converges linearly, achieves $O(\epsilon)$ recovery error after retrieving $O(k^{3}d\log(1/\epsilon))$ training instances, consumes $O(kd)$ memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval. "
774044904892825601,2016-09-09 00:41:01,https://t.co/yyHEb9ta9h,Discrete Variational Autoencoders. (arXiv:1609.02200v1 [stat.ML]) https://t.co/yyHEb9ta9h,2,15," Abstract: Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We introduce a novel class of probabilistic models, comprising an undirected discrete component and a directed hierarchical continuous component, that can be trained efficiently using the variational autoencoder framework. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, OMNIGLOT, and Caltech-101 Silhouettes datasets. "
774044902443274242,2016-09-09 00:41:00,https://t.co/CZ5eSvAoC1,Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation. (arXiv:1609.02208v1 [cs.IT]) https://t.co/CZ5eSvAoC1,0,3," Abstract: Estimators of information theoretic measures such as entropy and mutual information are a basic workhorse for many downstream applications in modern data science. State of the art approaches have been either geometric (nearest neighbor (NN) based) or kernel based (with a globally chosen bandwidth). In this paper, we combine both these approaches to design new estimators of entropy and mutual information that outperform state of the art methods. Our estimator uses local bandwidth choices of $k$-NN distances with a finite $k$, independent of the sample size. Such a local and data dependent choice improves performance in practice, but the bandwidth is vanishing at a fast rate, leading to a non-vanishing bias. We show that the asymptotic bias of the proposed estimator is universal; it is independent of the underlying distribution. Hence, it can be pre-computed and subtracted from the estimate. As a byproduct, we obtain a unified way of obtaining both kernel and NN estimators. The corresponding theoretical contribution relating the asymptotic geometry of nearest neighbors to order statistics is of independent mathematical interest. "
774044900530720768,2016-09-09 00:41:00,https://t.co/mKLhuj3ZYu,Functorial Hierarchical Clustering with Overlaps. (arXiv:1609.02513v1 [cs.LG]) https://t.co/mKLhuj3ZYu,0,4," Abstract: This work draws its inspiration from three important sources of research on dissimilarity-based clustering and intertwines those three threads into a consistent principled functorial theory of clustering. Those three are the overlapping clustering of Jardine and Sibson, the functorial approach of Carlsson and Memoli to partition-based clustering, and the Isbell/Dress school's study of injective envelopes. Carlsson and Memoli introduce the idea of viewing clustering methods as functors from a category of metric spaces to a category of clusters, with functoriality subsuming many desirable properties. Our first series of results extends their theory of functorial clustering schemes to methods that allow overlapping clusters in the spirit of Jardine and Sibson. This obviates some of the unpleasant effects of chaining that occur, for example with single-linkage clustering. We prove an equivalence between these general overlapping clustering functors and projections of weight spaces to what we term clustering domains, by focusing on the order structure determined by the morphisms. As a specific application of this machinery, we are able to prove that there are no functorial projections to cut metrics, or even to tree metrics. Finally, although we focus less on the construction of clustering methods (clustering domains) derived from injective envelopes, we lay out some preliminary results, that hopefully will give a feel for how the third leg of the stool comes into play. "
774044898450284545,2016-09-09 00:40:59,https://t.co/HKeXYjkflY,DiSMEC - Distributed Sparse Machines for Extreme Multi-label Classification. (arXiv:1609.02521v1 [stat.ML]) https://t.co/HKeXYjkflY,0,3," Abstract: Extreme multi-label classification refers to supervised multi-label learning involving hundreds of thousands or even millions of labels. Datasets in extreme classification exhibit fit to power-law distribution, i.e. a large fraction of labels have very few positive instances in the data distribution. Most state-of-the-art approaches for extreme multi-label classification attempt to capture correlation among labels by embedding the label matrix to a low-dimensional linear sub-space. However, in the presence of power-law distributed extremely large and diverse label spaces, structural assumptions such as low rank can be easily violated. In this work, we present DiSMEC, which is a large-scale distributed framework for learning one-versus-rest linear classifiers coupled with explicit capacity control to control model size. Unlike most state-of-the-art methods, DiSMEC does not make any low rank assumptions on the label matrix. Using double layer of parallelization, DiSMEC can learn classifiers for datasets consisting hundreds of thousands labels within few hours. The explicit capacity control mechanism filters out spurious parameters which keep the model compact in size, without losing prediction accuracy. We conduct extensive empirical evaluation on publicly available real-world datasets consisting upto 670,000 labels. We compare DiSMEC with recent state-of-the-art approaches, including - SLEEC which is a leading approach for learning sparse local embeddings, and FastXML which is a tree-based approach optimizing ranking based loss function. On some of the datasets, DiSMEC can significantly boost prediction accuracies - 10% better compared to SLECC and 15% better compared to FastXML, in absolute terms. "
774044896629919745,2016-09-09 00:40:59,https://t.co/qXnEijpEfH,Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic Differentiation. (arXiv:1603.03236v4 [cs.… https://t.co/qXnEijpEfH,22,45," Abstract: Optimization on manifolds is a class of methods for optimization of an objective function, subject to constraints which are smooth, in the sense that the set of points which satisfy the constraints admits the structure of a differentiable manifold. While many optimization problems are of the described form, technicalities of differential geometry and the laborious calculation of derivatives pose a significant barrier for experimenting with these methods. We introduce Pymanopt (available at this https URL), a toolbox for optimization on manifolds, implemented in Python, that---similarly to the Manopt Matlab toolbox---implements several manifold geometries and optimization algorithms. Moreover, we lower the barriers to users further by using automated differentiation for calculating derivative information, saving users time and saving them from potential calculation and implementation errors. "
773681420279644160,2016-09-08 00:36:39,https://t.co/SsgEeBHBrT,Learning Boltzmann Machine with EM-like Method. (arXiv:1609.01840v1 [cs.LG]) https://t.co/SsgEeBHBrT,0,11," Abstract: We propose an expectation-maximization-like(EMlike) method to train Boltzmann machine with unconstrained connectivity. It adopts Monte Carlo approximation in the E-step, and replaces the intractable likelihood objective with efficiently computed objectives or directly approximates the gradient of likelihood objective in the M-step. The EM-like method is a modification of alternating minimization. We prove that EM-like method will be the exactly same with contrastive divergence in restricted Boltzmann machine if the M-step of this method adopts special approximation. We also propose a new measure to assess the performance of Boltzmann machine as generative models of data, and its computational complexity is O(Rmn). Finally, we demonstrate the performance of EM-like method using numerical experiments. "
773681415661711364,2016-09-08 00:36:38,https://t.co/DBlE5w4flA,Chaining Bounds for Empirical Risk Minimization. (arXiv:1609.01872v1 [stat.ML]) https://t.co/DBlE5w4flA,0,6," Abstract: This paper extends the standard chaining technique to prove excess risk upper bounds for empirical risk minimization with random design settings even if the magnitude of the noise and the estimates is unbounded. The bound applies to many loss functions besides the squared loss, and scales only with the sub-Gaussian or subexponential parameters without further statistical assumptions such as the bounded kurtosis condition over the hypothesis class. A detailed analysis is provided for slope constrained and penalized linear least squares regression with a sub-Gaussian setting, which often proves tight sample complexity bounds up to logartihmic factors. "
773681410578219008,2016-09-08 00:36:37,https://t.co/2SdwS7NhCr,Random matrices meet machine learning: a large dimensional analysis of LS-SVM. (arXiv:1609.02020v1 [stat.ML]) https://t.co/2SdwS7NhCr,1,13," Abstract: This article proposes a performance analysis of kernel least squares support vector machines (LS-SVMs) based on a random matrix approach, in the regime where both the dimension of data $p$ and their number $n$ grow large at the same rate. Under a two-class Gaussian mixture model for the input data, we prove that the LS-SVM decision function is asymptotically normal with means and covariances shown to depend explicitly on the derivatives of the kernel function. This provides improved understanding along with new insights into the internal workings of SVM-type methods for large datasets. "
773681404047683584,2016-09-08 00:36:35,https://t.co/kotDMjMeoz,Ask the GRU: Multi-task Learning for Deep Text Recommendations. (arXiv:1609.02116v1 [stat.ML]) https://t.co/kotDMjMeoz,0,2," Abstract: In a variety of application domains the content to be recommended to users is associated with text. This includes research papers, movies with associated plot summaries, news articles, blog posts, etc. Recommendation approaches based on latent factor models can be extended naturally to leverage text by employing an explicit mapping from text to factors. This enables recommendations for new, unseen content, and may generalize better, since the factors for all items are produced by a compactly-parametrized model. Previous work has used topic models or averages of word embeddings for this mapping. In this paper we present a method leveraging deep recurrent neural networks to encode the text sequence into a latent vector, specifically gated recurrent units (GRUs) trained end-to-end on the collaborative filtering task. For the task of scientific paper recommendation, this yields models with significantly higher accuracy. In cold-start scenarios, we beat the previous state-of-the-art, all of which ignore word order. Performance is further improved by multi-task learning, where the text encoder network is trained for a combination of content recommendation and item metadata prediction. This regularizes the collaborative filtering model, ameliorating the problem of sparsity of the observed rating matrix. "
773681398527954944,2016-09-08 00:36:34,https://t.co/Y9pQUiFZIq,Multivariate Spearman's rho for aggregating ranks using copulas. (arXiv:1410.4391v3 [stat.ML] UPDATED) https://t.co/Y9pQUiFZIq,0,3," Abstract: We study the problem of rank aggregation: given a set of ranked lists, we want to form a consensus ranking. Furthermore, we consider the case of extreme lists: i.e., only the rank of the best or worst elements are known. We impute missing ranks by the average value and generalise Spearman's \rho to extreme ranks. Our main contribution is the derivation of a non-parametric estimator for rank aggregation based on multivariate extensions of Spearman's \rho, which measures correlation between a set of ranked lists. Multivariate Spearman's \rho is defined using copulas, and we show that the geometric mean of normalised ranks maximises multivariate correlation. Motivated by this, we propose a weighted geometric mean approach for learning to rank which has a closed form least squares solution. When only the best or worst elements of a ranked list are known, we impute the missing ranks by the average value, allowing us to apply Spearman's \rho. Finally, we demonstrate good performance on the rank aggregation benchmarks MQ2007 and MQ2008. "
773681393004056576,2016-09-08 00:36:33,https://t.co/NqAQxEumkX,Sketching for Sequential Change-Point Detection. (arXiv:1505.06770v4 [cs.LG] UPDATED) https://t.co/NqAQxEumkX,1,7," Abstract: We study sequential change-point detection using sketches (linear projections) of high-dimensional signal vectors, by presenting the sketching procedures that are derived based on the generalized likelihood ratio statistic. We consider both fixed and time-varying projections, and derive theoretical approximations to two fundamental performance metrics: the average run length (ARL) and the expected detection delay (EDD); these approximations are shown to be highly accurate by numerical simulations. We also characterize the performance of the procedure when the projection is a Gaussian random projection or a sparse 0-1 matrix (in particular, an expander graph). Finally, we demonstrate the good performance of the sketching performance using simulation and real-data examples on solar flare detection and failure detection in power networks. "
773681388180627456,2016-09-08 00:36:31,https://t.co/sOQUKBBccA,Structured Prediction Energy Networks. (arXiv:1511.06350v3 [cs.LG] CROSS LISTED) https://t.co/sOQUKBBccA,0,10," Abstract: We introduce structured prediction energy networks (SPENs), a flexible framework for structured prediction. A deep architecture is used to define an energy function of candidate labels, and then predictions are produced by using back-propagation to iteratively optimize the energy with respect to the labels. This deep architecture captures dependencies between labels that would lead to intractable graphical models, and performs structure learning by automatically learning discriminative features of the structured output. One natural application of our technique is multi-label classification, which traditionally has required strict prior assumptions about the interactions between labels to ensure tractable learning and prediction. We are able to apply SPENs to multi-label problems with substantially larger label sets than previous applications of structured prediction, while modeling high-order interactions using minimal structural assumptions. Overall, deep learning provides remarkable tools for learning features of the inputs to a prediction problem, and this work extends these techniques to learning features of structured outputs. Our experiments provide impressive performance on a variety of benchmark multi-label classification tasks, demonstrate that our technique can be used to provide interpretable structure learning, and illuminate fundamental trade-offs between feed-forward and iterative structured prediction. "
773319151007965184,2016-09-07 00:37:07,https://t.co/VrK9S2Jh8F,Evolutionary Synthesis of Deep Neural Networks via Synaptic Cluster-driven Genetic Encoding. (arXiv:1609.01360v1 [… https://t.co/VrK9S2Jh8F,3,5," Abstract: There has been significant recent interest towards achieving highly efficient deep neural network architectures. A promising paradigm for achieving this is the concept of evolutionary deep intelligence, which attempts to mimic biological evolution processes to synthesize highly-efficient deep neural networks over successive generations. An important aspect of evolutionary deep intelligence is the genetic encoding scheme used to mimic heredity, which can have a significant impact on the quality of offspring deep neural networks. Motivated by the neurobiological phenomenon of synaptic clustering, we introduce a new genetic encoding scheme where synaptic probability is driven towards the formation of a highly sparse set of synaptic clusters. Experimental results for the task of image classification demonstrated that the synthesized offspring networks using this synaptic cluster-driven genetic encoding scheme can achieve state-of-the-art performance while having network architectures that are not only significantly more efficient (with a ~125-fold decrease in synapses for MNIST) compared to the original ancestor network, but also tailored for GPU-accelerated machine learning applications. "
773319149741338625,2016-09-07 00:37:07,https://t.co/4uhhbMtz4f,Structured Sparse Principal Components Analysis with the TV-Elastic Net penalty. (arXiv:1609.01423v1 [stat.ML]) https://t.co/4uhhbMtz4f,0,5," Abstract: Principal component analysis (PCA) is an exploratory tool widely used in data analysis to uncover dominant patterns of variability within a population. Despite its ability to represent a data set in a low-dimensional space, the interpretability of PCA remains limited. However, in neuroimaging, it is essential to uncover clinically interpretable phenotypic markers that would account for the main variability in the brain images of a population. Recently, some alternatives to the standard PCA approach, such as Sparse PCA, have been proposed, their aim being to limit the density of the components. Nonetheless, sparsity alone does not entirely solve the interpretability problem, since it may yield scattered and unstable components. We hypothesized that the incorporation of prior information regarding the structure of the data may lead to improved relevance and interpretability of brain patterns. We therefore present a simple extension of the popular PCA framework that adds structured sparsity penalties on the loading vectors in order to identify the few stable regions in the brain images accounting for most of the variability. Such structured sparsity can be obtained by combining l1 and total variation (TV) penalties, where the TV regularization encodes higher order information about the structure of the data. This paper presents the structured sparse PCA (denoted SPCA-TV) optimization framework and its resolution. We demonstrate the efficiency and versatility of SPCA-TV on three different data sets. The gains of SPCA-TV over unstructured approaches are significant,since SPCA-TV reveals the variability within a data set in the form of intelligible brain patterns that are easy to interpret, and are more stable across different samples. "
773319148084535296,2016-09-07 00:37:07,https://t.co/KVdCRLuFRF,Q-Learning with Basic Emotions. (arXiv:1609.01468v1 [cs.AI]) https://t.co/KVdCRLuFRF,0,5," Abstract: Q-learning is a simple and powerful tool in solving dynamic problems where environments are unknown. It uses a balance of exploration and exploitation to find an optimal solution to the problem. In this paper, we propose using four basic emotions: joy, sadness, fear, and anger to influence a Qlearning agent. Simulations show that the proposed affective agent requires lesser number of steps to find the optimal path. We found when affective agent finds the optimal path, the ratio between exploration to exploitation gradually decreases, indicating lower total step count in the long run "
773319146243252224,2016-09-07 00:37:06,https://t.co/zsM0HO5ehK,Direct Feedback Alignment Provides Learning in Deep Neural Networks. (arXiv:1609.01596v1 [stat.ML]) https://t.co/zsM0HO5ehK,4,7," Abstract: Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task. "
773319144838139905,2016-09-07 00:37:06,https://t.co/gnRorULHiu,Law of Large Graphs. (arXiv:1609.01672v1 [stat.ME]) https://t.co/gnRorULHiu,0,6," Abstract: Estimating the mean of a population of graphs based on a sample is a core problem in network science. Often, this problem is especially difficult because the sample or cohort size is relatively small as compared to the number of parameters to estimate. While using the element-wise sample mean of the adjacency matrices is a common approach, this method does not exploit any underlying graph structure. We propose using a low-rank method together with tools for dimension selection and diagonal augmentation to improve performance over the naive methodology for small sample sizes. Theoretical results for the stochastic blockmodel show that this method will offer major improvements when there are many vertices. Similarly, in analyzing human connectome data, we demonstrate that the low-rank methods outperform the standard sample mean for many settings. These results indicate that low-rank methods should be a key part of the tool box for researchers studying populations of graphs. "
773319143235936256,2016-09-07 00:37:05,https://t.co/helND6mDmt,A General Method for Robust Bayesian Modeling. (arXiv:1510.05078v2 [stat.ML] UPDATED) https://t.co/helND6mDmt,0,6," Abstract: Robust Bayesian models are appealing alternatives to standard models, providing protection from data that contains outliers or other departures from the model assumptions. Historically, robust models were mostly developed on a case-by-case basis; examples include robust linear regression, robust mixture models, and bursty topic models. In this paper we develop a general approach to robust Bayesian modeling. We show how to turn an existing Bayesian model into a robust model, and then develop a generic strategy for computing with it. We use our method to study robust variants of several models, including linear regression, Poisson regression, logistic regression, and probabilistic topic models. We discuss the connections between our methods and existing approaches, especially empirical Bayes and James-Stein estimation. "
773319141604360194,2016-09-07 00:37:05,https://t.co/2aFizz0d79,A Probabilistic Modeling Approach to Hearing Loss Compensation. (arXiv:1602.01345v2 [stat.ML] UPDATED) https://t.co/2aFizz0d79,0,3," Abstract: Hearing Aid (HA) algorithms need to be tuned (""fitted"") to match the impairment of each specific patient. The lack of a fundamental HA fitting theory is a strong contributing factor to an unsatisfying sound experience for about 20% of hearing aid patients. This paper proposes a probabilistic modeling approach to the design of HA algorithms. The proposed method relies on a generative probabilistic model for the hearing loss problem and provides for automated inference of the corresponding (1) signal processing algorithm, (2) the fitting solution as well as a principled (3) performance evaluation metric. All three tasks are realized as message passing algorithms in a factor graph representation of the generative model, which in principle allows for fast implementation on hearing aid or mobile device hardware. The methods are theoretically worked out and simulated with a custom-built factor graph toolbox for a specific hearing loss model. "
773319140081856512,2016-09-07 00:37:05,https://t.co/FX0Gy68EX9,Should one minimize the Bellman residual or maximize the mean value?. (arXiv:1606.07636v2 [cs.LG] UPDATED) https://t.co/FX0Gy68EX9,0,3," Abstract: This paper aims at theoretically and empirically comparing two standard optimization criterion for Reinforcement Learning: i) maximization of the mean value (predominant approach in policy search algorithms) and ii) minimization of the Bellman residual (mainly used in approximate dynamic programming). For doing so, we introduce a new policy search algorithm based on the minimization of the residual $\|T_* v_\pi - v_\pi\|_{1,\nu}$. We prove that it enjoys a performance bound that is better than the sole known bound for maximizing the mean value and that matches the best known bounds in approximate dynamic programming. We also conduct experiments on randomly generated generic Markov decision processes to compare both approaches empirically. It turns out that the Bellman residual is a good optimization proxy only if there is a good match between the sampling distribution and the discounted state occupancy distribution induced by the optimal policy, while maximizing the mean value is rather insensitive to this issue. "
773319138752200704,2016-09-07 00:37:04,https://t.co/SZWsEvTUeX,Variational Gaussian Process Auto-Encoder for Ordinal Prediction of Facial Action Units. (arXiv:1608.04664v2 [stat… https://t.co/SZWsEvTUeX,2,5," Abstract: We address the task of simultaneous feature fusion and modeling of discrete ordinal outputs. We propose a novel Gaussian process(GP) auto-encoder modeling approach. In particular, we introduce GP encoders to project multiple observed features onto a latent space, while GP decoders are responsible for reconstructing the original features. Inference is performed in a novel variational framework, where the recovered latent representations are further constrained by the ordinal output labels. In this way, we seamlessly integrate the ordinal structure in the learned manifold, while attaining robust fusion of the input features. We demonstrate the representation abilities of our model on benchmark datasets from machine learning and affect analysis. We further evaluate the model on the tasks of feature fusion and joint ordinal prediction of facial action units. Our experiments demonstrate the benefits of the proposed approach compared to the state of the art. "
772956268763947008,2016-09-06 00:35:09,https://t.co/rYXrXuoKZS,Peacock Bundles: Bundle Coloring for Graphs with Globality-Locality Trade-off. (arXiv:1609.00719v1 [cs.CG]) https://t.co/rYXrXuoKZS,0,2," Abstract: Bundling of graph edges (node-to-node connections) is a common technique to enhance visibility of overall trends in the edge structure of a large graph layout, and a large variety of bundling algorithms have been proposed. However, with strong bundling, it becomes hard to identify origins and destinations of individual edges. We propose a solution: we optimize edge coloring to differentiate bundled edges. We quantify strength of bundling in a flexible pairwise fashion between edges, and among bundled edges, we quantify how dissimilar their colors should be by dissimilarity of their origins and destinations. We solve the resulting nonlinear optimization, which is also interpretable as a novel dimensionality reduction task. In large graphs the necessary compromise is whether to differentiate colors sharply between locally occurring strongly bundled edges (""local bundles""), or also between the weakly bundled edges occurring globally over the graph (""global bundles""); we allow a user-set global-local tradeoff. We call the technique ""peacock bundles"". Experiments show the coloring clearly enhances comprehensibility of graph layouts with edge bundling. "
772956265739878400,2016-09-06 00:35:09,https://t.co/uFsa4flLGi]),Stochastic Bouncy Particle Sampler. (arXiv:1609.00770v1 [https://t.co/uFsa4flLGi]) https://t.co/BpjyCMsUMV,0,3,INDEXERROR
772956263277821953,2016-09-06 00:35:08,https://t.co/17HUR9ZlL2,Graph-Based Active Learning: A New Look at Expected Error Minimization. (arXiv:1609.00845v1 [stat.ML]) https://t.co/17HUR9ZlL2,1,9," Abstract: In graph-based active learning, algorithms based on expected error minimization (EEM) have been popular and yield good empirical performance. The exact computation of EEM optimally balances exploration and exploitation. In practice, however, EEM-based algorithms employ various approximations due to the computational hardness of exact EEM. This can result in a lack of either exploration or exploitation, which can negatively impact the effectiveness of active learning. We propose a new algorithm TSA (Two-Step Approximation) that balances between exploration and exploitation efficiently while enjoying the same computational complexity as existing approximations. Finally, we empirically show the value of balancing between exploration and exploitation in both toy and real-world datasets where our method outperforms several state-of-the-art methods. "
772956260828405760,2016-09-06 00:35:08,https://t.co/qt7o0UMn0G,A Probabilistic Optimum-Path Forest Classifier for Binary Classification Problems. (arXiv:1609.00878v1 [cs.CV]) https://t.co/qt7o0UMn0G,1,2," Abstract: Probabilistic-driven classification techniques extend the role of traditional approaches that output labels (usually integer numbers) only. Such techniques are more fruitful when dealing with problems where one is not interested in recognition/identification only, but also into monitoring the behavior of consumers and/or machines, for instance. Therefore, by means of probability estimates, one can take decisions to work better in a number of scenarios. In this paper, we propose a probabilistic-based Optimum Path Forest (OPF) classifier to handle with binary classification problems, and we show it can be more accurate than naive OPF in a number of datasets. In addition to being just more accurate or not, probabilistic OPF turns to be another useful tool to the scientific community. "
772956258898997252,2016-09-06 00:35:07,https://t.co/NcEyk1awar,High Dimensional Human Guided Machine Learning. (arXiv:1609.00904v1 [cs.AI]) https://t.co/NcEyk1awar,1,6," Abstract: Have you ever looked at a machine learning classification model and thought, I could have made that? Well, that is what we test in this project, comparing XGBoost trained on human engineered features to training directly on data. The human engineered features do not outperform XGBoost trained di- rectly on the data, but they are comparable. This project con- tributes a novel method for utilizing human created classifi- cation models on high dimensional datasets. "
772956256327852032,2016-09-06 00:35:06,https://t.co/G7lStwRkUb,Decoding visual stimuli in human brain by using Anatomical Pattern Analysis on fMRI images. (arXiv:1609.00921v1 [s… https://t.co/G7lStwRkUb,2,4," Abstract: A universal unanswered question in neuroscience and machine learning is whether computers can decode the patterns of the human brain. Multi-Voxels Pattern Analysis (MVPA) is a critical tool for addressing this question. However, there are two challenges in the previous MVPA methods, which include decreasing sparsity and noises in the extracted features and increasing the performance of prediction. In overcoming mentioned challenges, this paper proposes Anatomical Pattern Analysis (APA) for decoding visual stimuli in the human brain. This framework develops a novel anatomical feature extraction method and a new imbalance AdaBoost algorithm for binary classification. Further, it utilizes an Error-Correcting Output Codes (ECOC) method for multi-class prediction. APA can automatically detect active regions for each category of the visual stimuli. Moreover, it enables us to combine homogeneous datasets for applying advanced classification. Experimental studies on 4 visual categories (words, consonants, objects and scrambled photos) demonstrate that the proposed approach achieves superior performance to state-of-the-art methods. "
772956254188793856,2016-09-06 00:35:06,https://t.co/hMWn97O0yb,A Unified Convergence Analysis of the Multiplicative Update Algorithm for Regularized NMF with General Divergences… https://t.co/hMWn97O0yb,1,5," Abstract: The multiplicative update (MU) algorithm has been used extensively to estimate the basis and coefficient matrices in nonnegative matrix factorization (NMF) problems under a wide range of divergences and regularizations. However, theoretical convergence guarantees have only been derived for a few special divergences. In this work, we provide a conceptually simple, self-contained, and unified proof for the convergence of the MU algorithm applied on NMF with a wide range of divergences and regularizations. Our result shows the sequence of iterates (i.e., pairs of basis and coefficient matrices) produced by the MU algorithm converges to the set of stationary points of the NMF (optimization) problem. Our proof strategy has the potential to open up new avenues for analyzing similar problems. "
772956252280328193,2016-09-06 00:35:06,https://t.co/eNja72JVzd,Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences. (arXiv… https://t.co/eNja72JVzd,1,4," Abstract: We provide two fundamental results on the population (infinite-sample) likelihood function of Gaussian mixture models with $M \geq 3$ components. Our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical Gaussians. We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro (2007). Our second main result shows that the EM algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least $1-e^{-\Omega(M)}$. We further establish that a first-order variant of EM will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points. Overall, our results highlight the necessity of careful initialization when using the EM algorithm in practice, even when applied in highly favorable settings. "
772956250216824832,2016-09-06 00:35:05,https://t.co/CybVqYWVLf,Distribution-Specific Hardness of Learning Neural Networks. (arXiv:1609.01037v1 [cs.LG]) https://t.co/CybVqYWVLf,2,3," Abstract: Although neural networks are routinely and successfully trained in practice using simple gradient-based methods, most existing theoretical results are negative, showing that learning such networks is difficult, in a worst-case sense over all data distributions. In this paper, we take a more nuanced view, and consider whether specific assumptions on the ""niceness"" of the input distribution, or ""niceness"" of the target function (e.g. in terms of smoothness, non-degeneracy, incoherence, random choice of parameters etc.), are sufficient to guarantee learnability using gradient-based methods. We provide evidence that neither class of assumptions alone is sufficient: For any member of a class of ""nice"" simple target functions, there are difficult input distributions, and on the other hand, for any member of a general class of ""nice"" input distributions, there are simple target functions which are difficult to learn. Thus, to formally explain the practical success of neural network learning, it seems that one would need to employ a careful combination of assumptions on both the input distribution and the target function. To prove our results, we develop some tools which may be of independent interest, such as extension of Fourier-based techniques for proving hardness in the statistical queries framework \cite{blum1994weakly}, from the Boolean cube to Euclidean space. "
772956247670788096,2016-09-06 00:35:04,https://t.co/McFLQJlwuA,Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints. (arXiv:1609.01051v1 [stat.ML… https://t.co/McFLQJlwuA,2,5," Abstract: This work presents PESMOC, Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints, an information-based strategy for the simultaneous optimization of multiple expensive-to-evaluate black-box functions under the presence of several constraints. PESMOC can hence be used to solve a wide range of optimization problems. Iteratively, PESMOC chooses an input location on which to evaluate the objective functions and the constraints so as to maximally reduce the entropy of the Pareto set of the corresponding optimization problem. The constraints considered in PESMOC are assumed to have similar properties to those of the objective functions in typical Bayesian optimization problems. That is, they do not have a known expression (which prevents gradient computation), their evaluation is considered to be very expensive, and the resulting observations may be corrupted by noise. These constraints arise in a plethora of expensive black-box optimization problems. We carry out synthetic experiments to illustrate the effectiveness of PESMOC, where we sample both the objectives and the constraints from a Gaussian process prior. The results obtained show that PESMOC is able to provide better recommendations with a smaller number of evaluations than a strategy based on random search. "
772956244680273920,2016-09-06 00:35:04,https://t.co/9yjJBiTcbc,GTApprox: surrogate modeling for industrial design. (arXiv:1609.01088v1 [cs.MS]) https://t.co/9yjJBiTcbc,0,2," Abstract: We describe GTApprox - a new tool for medium-scale surrogate modeling in industrial design. Compared to existing software, GTApprox brings several innovations: a few novel approximation algorithms, several advanced methods of automated model selection, novel options in the form of hints. We demonstrate the efficiency of GTApprox on a large collection of test problems. In addition, we describe several applications of GTApprox to real engineering problems. "
772956242331459584,2016-09-06 00:35:03,https://t.co/GVm72SESRx,The Robustness of Estimator Composition. (arXiv:1609.01226v1 [cs.LG]) https://t.co/GVm72SESRx,0,3," Abstract: We formalize notions of robustness for composite estimators via the notion of a breakdown point. A composite estimator successively applies two (or more) estimators: on data decomposed into disjoint parts, it applies the first estimator on each part, then the second estimator on the outputs of the first estimator. And so on, if the composition is of more than two estimators. Informally, the breakdown point is the minimum fraction of data points which if significantly modified will also significantly modify the output of the estimator, so it is typically desirable to have a large breakdown point. Our main result shows that, under mild conditions on the individual estimators, the breakdown point of the composite estimator is the product of the breakdown points of the individual estimators. We also demonstrate several scenarios, ranging from regression to statistical testing, where this analysis is easy to apply, useful in understanding worst case robustness, and sheds powerful insights onto the associated data analysis. "
772956239580041216,2016-09-06 00:35:03,https://t.co/pbyvg4JG7s,Multivariate Dependence Beyond Shannon Information. (arXiv:1609.01233v1 [cs.IT]) https://t.co/pbyvg4JG7s,1,3," Abstract: Accurately determining dependency structure is critical to discovering a system's causal organization. We recently showed that the transfer entropy fails in a key aspect of this---measuring information flow---due to its conflation of dyadic and polyadic relationships. We extend this observation to demonstrate that this is true of all such Shannon information measures when used to analyze multivariate dependencies. This has broad implications, particularly when employing information to express the organization and mechanisms embedded in complex systems, including the burgeoning efforts to combine complex network theory with information theory. Here, we do not suggest that any aspect of information theory is wrong. Rather, the vast majority of its informational measures are simply inadequate for determining the meaningful dependency structure within joint probability distributions. Therefore, such information measures are inadequate for discovering intrinsic causal relations. We close by demonstrating that such distributions exist across an arbitrary set of variables. "
772956237839360000,2016-09-06 00:35:02,https://t.co/41CItjkKRT,A General Framework for Constrained Bayesian Optimization using Information-based Search. (arXiv:1511.09422v2 [sta… https://t.co/41CItjkKRT,3,12," Abstract: We present an information-theoretic framework for solving global black-box optimization problems that also have black-box constraints. Of particular interest to us is to efficiently solve problems with decoupled constraints, in which subsets of the objective and constraint functions may be evaluated independently. For example, when the objective is evaluated on a CPU and the constraints are evaluated independently on a GPU. These problems require an acquisition function that can be separated into the contributions of the individual function evaluations. We develop one such acquisition function and call it Predictive Entropy Search with Constraints (PESC). PESC is an approximation to the expected information gain criterion and it compares favorably to alternative approaches based on improvement in several synthetic and real-world problems. In addition to this, we consider problems with a mix of functions that are fast and slow to evaluate. These problems require balancing the amount of time spent in the meta-computation of PESC and in the actual evaluation of the target objective. We take a bounded rationality approach and develop partial update for PESC which trades off accuracy against speed. We then propose a method for adaptively switching between the partial and full updates for PESC. This allows us to interpolate between versions of PESC that are efficient in terms of function evaluations and those that are efficient in terms of wall-clock time. Overall, we demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization. "
772956236283273220,2016-09-06 00:35:02,https://t.co/4h3oc9ZxLn,Real-Time Community Detection in Large Social Networks on a Laptop. (arXiv:1601.03958v2 [cs.SI] UPDATED) https://t.co/4h3oc9ZxLn,1,3," Abstract: For a broad range of research, governmental and commercial applications it is important to understand the allegiances, communities and structure of key players in society. One promising direction towards extracting this information is to exploit the rich relational data in digital social networks (the social graph). As social media data sets are very large, most approaches make use of distributed computing systems for this purpose. Distributing graph processing requires solving many difficult engineering problems, which has lead some researchers to look at single-machine solutions that are faster and easier to maintain. In this article, we present a single-machine real-time system for large-scale graph processing that allows analysts to interactively explore graph structures. The key idea is that the aggregate actions of large numbers of users can be compressed into a data structure that encapsulates user similarities while being robust to noise and queryable in real-time. We achieve single machine real-time performance by compressing the neighbourhood of each vertex using minhash signatures and facilitate rapid queries through Locality Sensitive Hashing. These techniques reduce query times from hours using industrial desktop machines operating on the full graph to milliseconds on standard laptops. Our method allows exploration of strongly associated regions (i.e. communities) of large graphs in real-time on a laptop. It has been deployed in software that is actively used by social network analysts and offers another channel for media owners to monetise their data, helping them to continue to provide free services that are valued by billions of people globally. "
772956234383237121,2016-09-06 00:35:01,https://t.co/8GSw7HfQYE,Calculus of the exponent of Kurdyka-{\L}ojasiewicz inequality and its applications to linear convergence of first-… https://t.co/8GSw7HfQYE,0,2," Abstract: In this paper, we study the Kurdyka-{\L}ojasiewicz (KL) exponent, an important quantity for analyzing the convergence rate of first-order methods. Specifically, we develop various calculus rules to deduce the KL exponent of new (possibly nonconvex and nonsmooth) functions formed from functions with known KL exponents. In addition, we show that the well-studied Luo-Tseng error bound together with a mild assumption on the separation of stationary values implies that the KL exponent is $\frac12$. The Luo-Tseng error bound is known to hold for a large class of concrete structured optimization problems, and thus we deduce the KL exponent of a large class of functions whose exponents were previously unknown. Building upon this and the calculus rules, we are then able to show that for many convex or nonconvex optimization models for applications such as sparse recovery, their objective function's KL exponent is $\frac12$. This includes the least squares problem with smoothly clipped absolute deviation (SCAD) regularization or minimax concave penalty (MCP) regularization and the logistic regression problem with $\ell_1$ regularization. Since many existing local convergence rate analysis for first-order methods in the nonconvex scenario relies on the KL exponent, our results enable us to obtain explicit convergence rate for various first-order methods when they are applied to a large variety of practical optimization models. Finally, we further illustrate how our results can be applied to analyzing the local linear convergence rate of the proximal gradient algorithm and the inertial proximal algorithm for some specific models that arise in sparse recovery. "
772956232114110465,2016-09-06 00:35:01,https://t.co/vK1RAuoOSc,Towards optimal nonlinearities for sparse recovery using higher-order statistics. (arXiv:1605.08201v2 [cs.IT] UPDA… https://t.co/vK1RAuoOSc,0,2," Abstract: We consider machine learning techniques to develop low-latency approximate solutions to a class of inverse problems. More precisely, we use a probabilistic approach for the problem of recovering sparse stochastic signals that are members of the $\ell_p$-balls. In this context, we analyze the Bayesian mean-square-error (MSE) for two types of estimators: (i) a linear estimator and (ii) a structured estimator composed of a linear operator followed by a Cartesian product of univariate nonlinear mappings. By construction, the complexity of the proposed nonlinear estimator is comparable to that of its linear counterpart since the nonlinear mapping can be implemented efficiently in hardware by means of look-up tables (LUTs). The proposed structure lends itself to neural networks and iterative shrinkage/thresholding-type algorithms restricted to a single iterate (e.g. due to imposed hardware or latency constraints). By resorting to an alternating minimization technique, we obtain a sequence of optimized linear operators and nonlinear mappings that converge in the MSE objective. The result is attractive for real-time applications where general iterative and convex optimization methods are infeasible. "
772956229555646469,2016-09-06 00:35:00,https://t.co/0IZO6DskNI,Difference of Convex Functions Programming Applied to Control with Expert Data. (arXiv:1606.01128v2 [math.OC] UPDA… https://t.co/0IZO6DskNI,0,5," Abstract: This paper reports applications of Difference of Convex functions (DC) programming to Learning from Demonstrations (LfD) and Reinforcement Learning (RL) with expert data. This is made possible because the norm of the Optimal Bellman Residual (OBR), which is at the heart of many RL and LfD algorithms, is DC. Improvement in performance is demonstrated on two specific algorithms, namely Reward-regularized Classification for Apprenticeship Learning (RCAL) and Reinforcement Learning with Expert Demonstrations (RLED), through experiments on generic Markov Decision Processes (MDP), called Garnets. "
772956228129546241,2016-09-06 00:35:00,https://t.co/hiDIC299JZ,Using Kernel Methods and Model Selection for Prediction of Preterm Birth. (arXiv:1607.07959v2 [cs.LG] UPDATED) https://t.co/hiDIC299JZ,0,2," Abstract: We describe an application of machine learning to the problem of predicting preterm birth. We conduct a secondary analysis on a clinical trial dataset collected by the National In- stitute of Child Health and Human Development (NICHD) while focusing our attention on predicting different classes of preterm birth. We compare three approaches for deriving predictive models: a support vector machine (SVM) approach with linear and non-linear kernels, logistic regression with different model selection along with a model based on decision rules prescribed by physician experts for prediction of preterm birth. Our approach highlights the pre-processing methods applied to handle the inherent dynamics, noise and gaps in the data and describe techniques used to handle skewed class distributions. Empirical experiments demonstrate significant improvement in predicting preterm birth compared to past work. "
772956225382277120,2016-09-06 00:34:59,https://t.co/4abFrWL13F,Probabilistic Knowledge Graph Construction: Compositional and Incremental Approaches. (arXiv:1608.05921v2 [stat.ML… https://t.co/4abFrWL13F,2,4," Abstract: Knowledge graph construction consists of two tasks: extracting information from external resources (knowledge population) and inferring missing information through a statistical analysis on the extracted information (knowledge completion). In many cases, insufficient external resources in the knowledge population hinder the subsequent statistical inference. The gap between these two processes can be reduced by an incremental population approach. We propose a new probabilistic knowledge graph factorisation method that benefits from the path structure of existing knowledge (e.g. syllogism) and enables a common modelling approach to be used for both incremental population and knowledge completion tasks. More specifically, the probabilistic formulation allows us to develop an incremental population algorithm that trades off exploitation-exploration. Experiments on three benchmark datasets show that the balanced exploitation-exploration helps the incremental population, and the additional path structure helps to predict missing information in knowledge completion. "
772594931181428736,2016-09-05 00:39:20,https://t.co/zklZiZFg50,Least Ambiguous Set-Valued Classifiers with Bounded Error Levels. (arXiv:1609.00451v1 [stat.ME]) https://t.co/zklZiZFg50,0,3," Abstract: In most classification tasks there are observations that are ambiguous and therefore difficult to correctly label. Set-valued classification allows the classifiers to output a set of plausible labels rather than a single label, thereby giving a more appropriate and informative treatment to the labeling of ambiguous instances. We introduce a framework for multiclass set-valued classification, where the classifiers guarantee user-defined levels of coverage or confidence (the probability that the true label is contained in the set) while minimizing the ambiguity (the expected size of the output). We first derive oracle classifiers assuming the true distribution to be known. We show that the oracle classifiers are obtained from level sets of the functions that define the conditional probability of each class. Then we develop estimators with good asymptotic and finite sample properties. The proposed classifiers build on and refine many existing single-label classifiers. The optimal classifier can sometimes output the empty set. We provide two solutions to fix this issue that are suitable for various practical needs. "
772594929692446721,2016-09-05 00:39:20,https://t.co/ccaZLh6DpI,A deep learning model for estimating story points. (arXiv:1609.00489v1 [cs.SE]) https://t.co/ccaZLh6DpI,3,7," Abstract: Although there has been substantial research in software analytics for effort estimation in traditional software projects, little work has been done for estimation in agile projects, especially estimating user stories or issues. Story points are the most common unit of measure used for estimating the effort involved in implementing a user story or resolving an issue. In this paper, we offer for the \emph{first} time a comprehensive dataset for story points-based estimation that contains 23,313 issues from 16 open source projects. We also propose a prediction model for estimating story points based on a novel combination of two powerful deep learning architectures: long short-term memory and recurrent highway network. Our prediction system is \emph{end-to-end} trainable from raw input data to prediction outcomes without any manual feature engineering. An empirical evaluation demonstrates that our approach consistently outperforms three common effort estimation baselines and two alternatives in both Mean Absolute Error and the Standardized Accuracy. "
772594928316809216,2016-09-05 00:39:19,https://t.co/j9kVbXweB4,Generic Inference in Latent Gaussian Process Models. (arXiv:1609.00577v1 [stat.ML]) https://t.co/j9kVbXweB4,0,11," Abstract: We develop an automated variational method for inference in models with Gaussian process (GP) priors and general likelihoods. The method supports multiple outputs and multiple latent functions and does not require detailed knowledge of the conditional likelihood, only needing its evaluation as a black-box function. Using a mixture of Gaussians as the variational distribution, we show that the evidence lower bound and its gradients can be estimated efficiently using empirical expectations over univariate Gaussian distributions. Furthermore, the method is scalable to large datasets which is achieved by using an augmented prior via the inducing-variable approach underpinning most sparse GP approximations, along with parallel computation and stochastic optimization. We evaluate our method with experiments on small datasets, medium-scale datasets and a large dataset, showing its competitiveness under different likelihood models and sparsity levels. Moreover, we analyze learning in our model under batch and stochastic settings, and study the effect of optimizing the inducing inputs. Finally, in the large-scale experiment, we investigate the problem of predicting airline delays and show that our method is on par with the state-of-the-art hard-coded approach for scalable GP regression. "
772594926924300288,2016-09-05 00:39:19,https://t.co/1csRG7H8yh,SEBOOST - Boosting Stochastic Learning Using Subspace Optimization Techniques. (arXiv:1609.00629v1 [cs.CV]) https://t.co/1csRG7H8yh,0,4," Abstract: We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method for large-scale problems, and has been adapted for the stochastic learning framework. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce two hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating promising results. "
772594924814565376,2016-09-05 00:39:18,https://t.co/2RLHcgLqyd,Localization by Fusing a Group of Fingerprints via Multiple Antennas in Indoor Environment. (arXiv:1609.00661v1 [s… https://t.co/2RLHcgLqyd,0,3," Abstract: Most existing fingerprints-based indoor localization approaches are based on some single fingerprints, such as received signal strength (RSS), channel impulse response (CIR), and signal subspace. However, the localization accuracy obtained by the single fingerprint approach is rather susceptible to the changing environment, multi-path, and non-line-of-sight (NLOS) propagation. Furthermore, building the fingerprints is a very time consuming process. In this paper, we propose a novel localization framework by Fusing A Group Of fingerprinTs (FAGOT) via multiple antennas for the indoor environment. We first build a GrOup Of Fingerprints (GOOF), which includes five different fingerprints, namely, RSS, covariance matrix, signal subspace, fractional low order moment, and fourth-order cumulant, which are obtained by different transformations of the received signals from multiple antennas in the offline stage. Then, we design a parallel GOOF multiple classifiers based on AdaBoost (GOOF-AdaBoost) to train each of these fingerprints in parallel as five strong multiple classifiers. In the online stage, we input the corresponding transformations of the real measurements into these strong classifiers to obtain independent decisions. Finally, we propose an efficient combination fusion algorithm, namely, MUltiple Classifiers mUltiple Samples (MUCUS) fusion algorithm to improve the accuracy of localization by combining the predictions of multiple classifiers with different samples. As compared with the single fingerprint approaches, the prediction probability of our proposed approach is improved significantly. The process for building fingerprints can also be reduced drastically. We demonstrate the feasibility and performance of the proposed algorithm through extensive simulations as well as via real experimental data using a Universal Software Radio Peripheral (USRP) platform with four antennas. "
772594923455602688,2016-09-05 00:39:18,https://t.co/u1o9ag7Sjw,The Inflation Technique for Causal Inference with Latent Variables. (arXiv:1609.00672v1 [quant-ph]) https://t.co/u1o9ag7Sjw,0,7," Abstract: The problem of causal inference is to determine if a given probability distribution on observed variables is compatible with some causal structure. The difficult case is when the structure includes latent variables. We here introduce the inflation technique for tackling this problem. An inflation of a causal structure is a new causal structure that can contain multiple copies of each of the original variables, but where the ancestry of each copy mirrors that of the original. For every distribution compatible with the original causal structure we identify a corresponding family of distributions, over certain subsets of inflation variables, which is compatible with the inflation structure. It follows that compatibility constraints at the inflation level can be translated to compatibility constraints at the level of the original causal structure; even if the former are weak, such as observable statistical independences implied by disjoint causal ancestry, the translated constraints can be strong. In particular, we can derive inequalities whose violation by a distribution witnesses that distribution's incompatibility with the causal structure (of which Bell inequalities and Pearl's instrumental inequality are prominent examples). We describe an algorithm for deriving all of the inequalities for the original causal structure that follow from ancestral independences in the inflation. Applied to an inflation of the Triangle scenario with binary variables, it yields inequalities that are stronger in at least some aspects than those obtainable by existing methods. We also describe an algorithm that derives a weaker set of inequalities but is much more efficient. Finally, we discuss which inflations are such that the inequalities one obtains from them remain valid even for quantum (and post-quantum) generalizations of the notion of a causal model. "
772594922163695616,2016-09-05 00:39:18,https://t.co/AbEoB9pXG2,Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model. (arXiv:1609.00680v1 [q-bio.BM]) https://t.co/AbEoB9pXG2,0,6," Abstract: Recently exciting progress has been made on protein contact prediction, but the predicted contacts for proteins without many sequence homologs is still of low quality and not very useful for de novo structure prediction. This paper presents a new deep learning method that predicts contacts by integrating both evolutionary coupling (EC) and sequence conservation information through an ultra-deep neural network formed by two deep residual networks. This deep neural network allows us to model very complex sequence-contact relationship as well as long-range inter-contact correlation. Our method greatly outperforms existing contact prediction methods and leads to much more accurate contact-assisted protein folding. Tested on three datasets of 579 proteins, the average top L long-range prediction accuracy obtained our method, the representative EC method CCMpred and the CASP11 winner MetaPSICOV is 0.47, 0.21 and 0.30, respectively; the average top L/10 long-range accuracy of our method, CCMpred and MetaPSICOV is 0.77, 0.47 and 0.59, respectively. Ab initio folding using our predicted contacts as restraints can yield correct folds (i.e., TMscore>0.6) for 203 test proteins, while that using MetaPSICOV- and CCMpred-predicted contacts can do so for only 79 and 62 proteins, respectively. Further, our contact-assisted models have much better quality than template-based models. Using our predicted contacts as restraints, we can (ab initio) fold 208 of the 398 membrane proteins with TMscore>0.5. By contrast, when the training proteins of our method are used as templates, homology modeling can only do so for 10 of them. One interesting finding is that even if we do not train our prediction models with any membrane proteins, our method works very well on membrane protein prediction. Finally, in recent blind CAMEO benchmark our method successfully folded 5 test proteins with a novel fold. "
772594920796332032,2016-09-05 00:39:17,https://t.co/lIACnlJikp,Convolutional Neural Networks for Text Categorization: Shallow Word-level vs. Deep Character-level. (arXiv:1609.00… https://t.co/lIACnlJikp,4,10," Abstract: This paper reports the performances of shallow word-level convolutional neural networks (CNN), our earlier work (2015), on the eight datasets with relatively large training data that were used for testing the very deep character-level CNN in Conneau et al. (2016). Our findings are as follows. The shallow word-level CNNs achieve better error rates than the error rates reported in Conneau et al., though the results should be interpreted with some consideration due to the unique pre-processing of Conneau et al. The shallow word-level CNN uses more parameters and therefore requires more storage than the deep character-level CNN; however, the shallow word-level CNN computes much faster. "
772594918137159680,2016-09-05 00:39:17,https://t.co/gBBKrymm6c,MCMC assisted by Belief Propagaion. (arXiv:1605.09042v2 [stat.ML] UPDATED) https://t.co/gBBKrymm6c,0,10," Abstract: Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most popular algorithms for computational inference in Graphical Models (GM). In principle, MCMC is an exact probabilistic method which, however, often suffers from exponentially slow mixing. In contrast, BP is a deterministic method, which is typically fast, empirically very successful, however in general lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC algorithms correcting the approximation error of BP, i.e., we provide a way to compensate for BP errors via a consecutive BP-aware MCMC. Our framework is based on the Loop Calculus (LC) approach which allows to express the BP error as a sum of weighted generalized loops. Although the full series is computationally intractable, it is known that a truncated series, summing up all 2-regular loops, is computable in polynomial-time for planar pair-wise binary GMs and it also provides a highly accurate approximation empirically. Motivated by this, we first propose a polynomial-time approximation MCMC scheme for the truncated series of general (non-planar) pair-wise binary models. Our main idea here is to use the Worm algorithm, known to provide fast mixing in other (related) problems, and then design an appropriate rejection scheme to sample 2-regular loops. Furthermore, we also design an efficient rejection-free MCMC scheme for approximating the full series. The main novelty underlying our design is in utilizing the concept of cycle basis, which provides an efficient decomposition of the generalized loops. In essence, the proposed MCMC schemes run on transformed GM built upon the non-trivial BP solution, and our experiments show that this synthesis of BP and MCMC outperforms both direct MCMC and bare BP schemes. "
771507241371467776,2016-09-02 00:37:14,https://t.co/XpT8weP5KD,Human Pose Estimation in Space and Time using 3D CNN. (arXiv:1609.00036v1 [cs.CV]) https://t.co/XpT8weP5KD,1,8," Abstract: This paper explores the capabilities of convolutional neural networks to deal with a task that is easily manageable for humans: perceiving 3D pose of a human body from varying angles. However, in our approach, we are restricted to using a monocular vision system. For this purpose, we apply a convolutional neural network approach on RGB videos and extend it to three dimensional convolutions. This is done via encoding the time dimension in videos as the 3\ts{rd} dimension in convolutional space, and directly regressing to human body joint positions in 3D coordinate space. This research shows the ability of such a network to achieve state-of-the-art performance on the selected Human3.6M dataset, thus demonstrating the possibility of successfully representing temporal data with an additional dimension in the convolutional operation. "
771507239903498240,2016-09-02 00:37:14,https://t.co/5dRfdLh0xW,Randomized single-view algorithms for low-rank matrix approximation. (arXiv:1609.00048v1 [cs.NA]) https://t.co/5dRfdLh0xW,0,3," Abstract: This paper develops a suite of algorithms for constructing low-rank approximations of an input matrix from a random linear image of the matrix, called a sketch. These methods can preserve structural properties of the input matrix, such as positive-semidefiniteness, and they can produce approximations with a user-specified rank. The algorithms are simple, accurate, numerically stable, and provably correct. Moreover, each method is accompanied by an informative error bound that allows users to select parameters a priori to achieve a given approximation quality. These claims are supported by computer experiments. "
771507238578028544,2016-09-02 00:37:14,https://t.co/IFAykjHhth,A Review of Multivariate Distributions for Count Data Derived from the Poisson Distribution. (arXiv:1609.00066v1 [… https://t.co/IFAykjHhth,1,7," Abstract: The Poisson distribution has been widely studied and used for modeling univariate count-valued data. Multivariate generalizations of the Poisson distribution that permit dependencies, however, have been far less popular. Yet, real-world high-dimensional count-valued data found in word counts, genomics, and crime statistics, for example, exhibit rich dependencies, and motivate the need for multivariate distributions that can appropriately model this data. We review multivariate distributions derived from the univariate Poisson, categorizing these models into three main classes: 1) where the marginal distributions are Poisson, 2) where the joint distribution is a mixture of Poissons, and 3) where the node-conditional distributions are derived from the Poisson. We discuss the development of multiple instances of these classes. Then, we extensively compare multiple models from each class on five real-world datasets from traffic accident data, crime statistics, biological next generation sequencing data and text corpora. These empirical experiments develop intuition about the comparative advantages and disadvantages of each class of multivariate distribution that was derived from the Poisson. Finally, we suggest new research directions as explored in the subsequent discussion section. (See arXiv paper comments for access to supplementary material.) "
771507237189742592,2016-09-02 00:37:13,https://t.co/QNhKGUEmaE,Neural Network Architecture Optimization through Submodularity and Supermodularity. (arXiv:1609.00074v1 [stat.ML]) https://t.co/QNhKGUEmaE,4,12," Abstract: Deep learning models' architectures, including depth and width, are key factors influencing models' performance, such as test accuracy and computation time. This paper solves two problems: given computation time budget, choose an architecture to maximize accuracy, and given accuracy requirement, choose an architecture to minimize computation time. We convert this architecture optimization into a subset selection problem. With accuracy's submodularity and computation time's supermodularity, we propose efficient greedy optimization algorithms. The experiments demonstrate our algorithm's ability to find more accurate models or faster models. By analyzing architecture evolution with growing time budget, we discuss relationships among accuracy, time and architecture, and give suggestions on neural network architecture design. "
771507234471809025,2016-09-02 00:37:13,https://t.co/2E3D67Itmu,Adaptive Acceleration of Sparse Coding via Matrix Factorization. (arXiv:1609.00285v1 [stat.ML]) https://t.co/2E3D67Itmu,1,5," Abstract: Sparse coding is a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, that are optimal in the class of first-order methods for non-smooth, convex functions, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). However, these methods don't exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks was proposed in \cite{Gregor10}, coined LISTA, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately. In this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram kernel of the dictionary, which attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of the $\ell_1$ ball. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. We further validate our analysis by showing that on dictionaries where this factorization does not exist, adaptive acceleration fails. "
771507232978731008,2016-09-02 00:37:12,https://t.co/e0cith5877,Ten Steps of EM Suffice for Mixtures of Two Gaussians. (arXiv:1609.00368v1 [stat.ML]) https://t.co/e0cith5877,2,3," Abstract: We provide global convergence guarantees for the expectation-maximization (EM) algorithm applied to mixtures of two Gaussians with known covariance matrices. We show that EM converges geometrically to the correct mean vectors, and provide simple, closed-form expressions for the convergence rate. As a simple illustration, we show that in one dimension ten steps of the EM algorithm initialized at $+ \infty$ result in less than 1% error estimation of the means. "
771507230524993536,2016-09-02 00:37:12,https://t.co/Poae5SAbyI,Importance weighting without importance weights: An efficient algorithm for combinatorial semi-bandits. (arXiv:150… https://t.co/Poae5SAbyI,3,5," Abstract: We propose a sample-efficient alternative for importance weighting for situations where one only has sample access to the probability distribution that generates the observations. Our new method, called Geometric Resampling (GR), is described and analyzed in the context of online combinatorial optimization under semi-bandit feedback, where a learner sequentially selects its actions from a combinatorial decision set so as to minimize its cumulative loss. In particular, we show that the well-known Follow-the-Perturbed-Leader (FPL) prediction method coupled with Geometric Resampling yields the first computationally efficient reduction from offline to online optimization in this setting. We provide a thorough theoretical analysis for the resulting algorithm, showing that its performance is on par with previous, inefficient solutions. Our main contribution is showing that, despite the relatively large variance induced by the GR procedure, our performance guarantees hold with high probability rather than only in expectation. As a side result, we also improve the best known regret bounds for FPL in online combinatorial optimization with full feedback, closing the perceived performance gap between FPL and exponential weights in this setting. "
771507229174497283,2016-09-02 00:37:12,https://t.co/jrcvdkM4zY,Complete Dictionary Recovery over the Sphere I: Overview and the Geometric Picture. (arXiv:1511.03607v3 [cs.IT] UP… https://t.co/jrcvdkM4zY,0,3," Abstract: We consider the problem of recovering a complete (i.e., square and invertible) matrix $\mathbf A_0$, from $\mathbf Y \in \mathbb{R}^{n \times p}$ with $\mathbf Y = \mathbf A_0 \mathbf X_0$, provided $\mathbf X_0$ is sufficiently sparse. This recovery problem is central to theoretical understanding of dictionary learning, which seeks a sparse representation for a collection of input signals and finds numerous applications in modern signal processing and machine learning. We give the first efficient algorithm that provably recovers $\mathbf A_0$ when $\mathbf X_0$ has $O(n)$ nonzeros per column, under suitable probability model for $\mathbf X_0$. In contrast, prior results based on efficient algorithms either only guarantee recovery when $\mathbf X_0$ has $O(\sqrt{n})$ zeros per column, or require multiple rounds of SDP relaxation to work when $\mathbf X_0$ has $O(n^{1-\delta})$ nonzeros per column (for any constant $\delta \in (0, 1)$). } Our algorithmic pipeline centers around solving a certain nonconvex optimization problem with a spherical constraint. In this paper, we provide a geometric characterization of the objective landscape. In particular, we show that the problem is highly structured: with high probability, (1) there are no ""spurious"" local minimizers; and (2) around all saddle points the objective has a negative directional curvature. This distinctive structure makes the problem amenable to efficient optimization algorithms. In a companion paper (arXiv:1511.04777), we design a second-order trust-region algorithm over the sphere that provably converges to a local minimizer from arbitrary initializations, despite the presence of saddle points. "
771507227534495745,2016-09-02 00:37:11,https://t.co/yeDKkkIGMv,Complete Dictionary Recovery over the Sphere II: Recovery by Riemannian Trust-region Method. (arXiv:1511.04777v3 [… https://t.co/yeDKkkIGMv,0,3," Abstract: We consider the problem of recovering a complete (i.e., square and invertible) matrix $\mathbf A_0$, from $\mathbf Y \in \mathbb{R}^{n \times p}$ with $\mathbf Y = \mathbf A_0 \mathbf X_0$, provided $\mathbf X_0$ is sufficiently sparse. This recovery problem is central to theoretical understanding of dictionary learning, which seeks a sparse representation for a collection of input signals and finds numerous applications in modern signal processing and machine learning. We give the first efficient algorithm that provably recovers $\mathbf A_0$ when $\mathbf X_0$ has $O(n)$ nonzeros per column, under suitable probability model for $\mathbf X_0$. Our algorithmic pipeline centers around solving a certain nonconvex optimization problem with a spherical constraint, and hence is naturally phrased in the language of manifold optimization. In a companion paper (arXiv:1511.03607), we have showed that with high probability our nonconvex formulation has no ""spurious"" local minimizers and around any saddle point the objective function has a negative directional curvature. In this paper, we take advantage of the particular geometric structure, and describe a Riemannian trust region algorithm that provably converges to a local minimizer with from arbitrary initializations. Such minimizers give excellent approximations to rows of $\mathbf X_0$. The rows are then recovered by linear programming rounding and deflation. "
771507225651191808,2016-09-02 00:37:11,https://t.co/fVt2ltHZyz,Statistical Properties of the Single Linkage Hierarchical Clustering Estimator. (arXiv:1511.07715v2 [stat.ML] UPDA… https://t.co/fVt2ltHZyz,1,5," Abstract: Distance-based hierarchical clustering (HC) methods are widely used in unsupervised data analysis but few authors take account of uncertainty in the distance data. We incorporate a statistical model of the uncertainty through corruption or noise in the pairwise distances and investigate the problem of estimating the HC as unknown parameters from measurements. Specifically, we focus on single linkage hierarchical clustering (SLHC) and study its geometry. We prove that under fairly reasonable conditions on the probability distribution governing measurements, SLHC is equivalent to maximum partial profile likelihood estimation (MPPLE) with some of the information contained in the data ignored. At the same time, we show that direct evaluation of SLHC on maximum likelihood estimation (MLE) of pairwise distances yields a consistent estimator. Consequently, a full MLE is expected to perform better than SLHC in getting the correct HC results for the ground truth metric. "
771507223788920832,2016-09-02 00:37:10,https://t.co/hTGqVefNWF,A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction. (arXiv:1512.06293v2 [cs.IT] UP… https://t.co/hTGqVefNWF,8,20," Abstract: Deep convolutional neural networks have led to breakthrough results in numerous practical machine learning tasks such as classification of images in the ImageNet data set, control-policy-learning to play Atari games or the board game Go, and image captioning. Many of these applications first perform feature extraction and then feed the results thereof into a trainable classifier. The mathematical analysis of deep convolutional neural networks for feature extraction was initiated by Mallat, 2012. Specifically, Mallat considered so-called scattering networks based on a wavelet transform followed by the modulus non-linearity in each network layer, and proved translation invariance (asymptotically in the wavelet scale parameter) and deformation stability of the corresponding feature extractor. This paper complements Mallat's results by developing a theory of deep convolutional neural networks for feature extraction encompassing general convolutional transforms, or in more technical parlance, general semi-discrete frames (including Weyl-Heisenberg, curvelet, shearlet, ridgelet, and wavelet frames), general Lipschitz-continuous non-linearities (e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents, and modulus functions), and general Lipschitz-continuous pooling operators emulating sub-sampling and averaging. In addition, all of these elements can be different in different network layers. For the resulting feature extractor we prove a translation invariance result which is of vertical nature in the sense of the network depth determining the amount of invariance, and we establish deformation sensitivity bounds that apply to signal classes with inherent deformation insensitivity such as, e.g., band-limited functions. "
771507222631346176,2016-09-02 00:37:10,https://t.co/9ZANzjkWrH,Distributed Bayesian Learning with Stochastic Natural-gradient Expectation Propagation and the Posterior Server. (… https://t.co/9ZANzjkWrH,4,9," Abstract: This paper makes two contributions to Bayesian machine learning algorithms. Firstly, we propose stochastic natural gradient expectation propagation (SNEP), a novel alternative to expectation propagation (EP), a popular variational inference algorithm. SNEP is a black box variational algorithm, in that it does not require any simplifying assumptions on the distribution of interest, beyond the existence of some Monte Carlo sampler for estimating the moments of the EP tilted distributions. Further, as opposed to EP which has no guarantee of convergence, SNEP can be shown to be convergent, even when using Monte Carlo moment estimates. Secondly, we propose a novel architecture for distributed Bayesian learning which we call the posterior server. The posterior server allows scalable and robust Bayesian learning in cases where a dataset is stored in a distributed manner across a cluster, with each compute node containing a disjoint subset of data. An independent Monte Carlo sampler is run on each compute node, with direct access only to the local data subset, but which targets an approximation to the global posterior distribution given all data across the whole cluster. This is achieved by using a distributed asynchronous implementation of SNEP to pass messages across the cluster. We demonstrate SNEP and the posterior server on distributed Bayesian learning of logistic regression and neural networks. Keywords: Distributed Learning, Large Scale Learning, Deep Learning, Bayesian Learning, Variational Inference, Expectation Propagation, Stochastic Approximation, Natural Gradient, Markov chain Monte Carlo, Parameter Server, Posterior Server. "
771507221414998016,2016-09-02 00:37:10,https://t.co/czSESL5Sh8,Minimizing Quadratic Functions in Constant Time. (arXiv:1608.07179v1 [cs.LG] CROSS LISTED) https://t.co/czSESL5Sh8,1,3," Abstract: A sampling-based optimization method for quadratic functions is proposed. Our method approximately solves the following $n$-dimensional quadratic minimization problem in constant time, which is independent of $n$: $z^*=\min_{\mathbf{v} \in \mathbb{R}^n}\langle\mathbf{v}, A \mathbf{v}\rangle + n\langle\mathbf{v}, \mathrm{diag}(\mathbf{d})\mathbf{v}\rangle + n\langle\mathbf{b}, \mathbf{v}\rangle$, where $A \in \mathbb{R}^{n \times n}$ is a matrix and $\mathbf{d},\mathbf{b} \in \mathbb{R}^n$ are vectors. Our theoretical analysis specifies the number of samples $k(\delta, \epsilon)$ such that the approximated solution $z$ satisfies $|z - z^*| = O(\epsilon n^2)$ with probability $1-\delta$. The empirical performance (accuracy and runtime) is positively confirmed by numerical experiments. "
771507219657584640,2016-09-02 00:37:09,https://t.co/gc0HfpWs7e,Robustness of classifiers: from adversarial to random noise. (arXiv:1608.08967v1 [cs.LG] CROSS LISTED) https://t.co/gc0HfpWs7e,1,8," Abstract: Several recent works have shown that state-of-the-art classifiers are vulnerable to worst-case (i.e., adversarial) perturbations of the datapoints. On the other hand, it has been empirically observed that these same classifiers are relatively robust to random noise. In this paper, we propose to study a \textit{semi-random} noise regime that generalizes both the random and worst-case noise regimes. We propose the first quantitative analysis of the robustness of nonlinear classifiers in this general noise regime. We establish precise theoretical bounds on the robustness of classifiers in this general regime, which depend on the curvature of the classifier's decision boundary. Our bounds confirm and quantify the empirical observations that classifiers satisfying curvature constraints are robust to random noise. Moreover, we quantify the robustness of classifiers in terms of the subspace dimension in the semi-random noise regime, and show that our bounds remarkably interpolate between the worst-case and random noise regimes. We perform experiments and show that the derived bounds provide very accurate estimates when applied to various state-of-the-art deep neural networks and datasets. This result suggests bounds on the curvature of the classifiers' decision boundaries that we support experimentally, and more generally offers important insights onto the geometry of high dimensional classification problems. "
771144117925580800,2016-09-01 00:34:19,https://t.co/GcKWq0ZkrC,Joint Estimation of Multiple Dependent Gaussian Graphical Models with Applications to Mouse Genomics. (arXiv:1608.… https://t.co/GcKWq0ZkrC,1,4," Abstract: Gaussian graphical models are widely used to represent conditional dependence among random variables. In this paper, we propose a novel estimator for data arising from a group of Gaussian graphical models that are themselves dependent. A motivating example is that of modeling gene expression collected on multiple tissues from the same individual: here the multivariate outcome is affected by dependencies acting not only at the level of the specific tissues, but also at the level of the whole body; existing methods that assume independence among graphs are not applicable in this case. To estimate multiple dependent graphs, we decompose the problem into two graphical layers: the systemic layer, which affects all outcomes and thereby induces cross- graph dependence, and the category-specific layer, which represents graph-specific variation. We propose a graphical EM technique that estimates both layers jointly, establish estimation consistency and selection sparsistency of the proposed estimator, and confirm by simulation that the EM method is superior to a simple one-step method. We apply our technique to mouse genomics data and obtain biologically plausible results. "
771144115618799617,2016-09-01 00:34:18,https://t.co/P1CmkXaaaz,Reconstructing parameters of spreading models from partial observations. (arXiv:1608.08698v1 [cs.SI]) https://t.co/P1CmkXaaaz,0,2," Abstract: Spreading processes are often modelled as a stochastic dynamics occurring on top of a given network with edge weights corresponding to the transmission probabilities. Knowledge of veracious transmission probabilities is essential for prediction, optimization, and control of diffusion dynamics. Unfortunately, in most cases the transmission rates are unknown and need to be reconstructed from the spreading data. Moreover, in realistic settings it is impossible to monitor the state of each node at every time, and thus the data is highly incomplete. We introduce an efficient dynamic message-passing algorithm, which is able to reconstruct parameters of the spreading model given only partial information on the activation times of nodes in the network. The method is generalizable to a large class of dynamic models, as well to the case of temporal graphs. "
771144113144070144,2016-09-01 00:34:18,https://t.co/6jXSdfETC2,A Mathematical Framework for Feature Selection from Real-World Data with Non-Linear Observations. (arXiv:1608.0885… https://t.co/6jXSdfETC2,3,10," Abstract: In this paper, we study the challenge of feature selection based on a relatively small collection of sample pairs $\{(x_i, y_i)\}_{1 \leq i \leq m}$. The observations $y_i \in \mathbb{R}$ are thereby supposed to follow a noisy single-index model, depending on a certain set of signal variables. A major difficulty is that these variables usually cannot be observed directly, but rather arise as hidden factors in the actual data vectors $x_i \in \mathbb{R}^d$ (feature variables). We will prove that a successful variable selection is still possible in this setup, even when the applied estimator does not have any knowledge of the underlying model parameters and only takes the 'raw' samples $\{(x_i, y_i)\}_{1 \leq i \leq m}$ as input. The model assumptions of our results will be fairly general, allowing for non-linear observations, arbitrary convex signal structures as well as strictly convex loss functions. This is particularly appealing for practical purposes, since in many applications, already standard methods, e.g., the Lasso or logistic regression, yield surprisingly good outcomes. Apart from a general discussion of the practical scope of our theoretical findings, we will also derive a rigorous guarantee for a specific real-world problem, namely sparse feature extraction from (proteomics-based) mass spectrometry data. "
771144110552088576,2016-09-01 00:34:17,https://t.co/v1eVibHqWK,Learning to Personalize from Observational Data. (arXiv:1608.08925v1 [stat.ML]) https://t.co/v1eVibHqWK,1,3," Abstract: We study the problem of learning to choose from m discrete treatment options (e.g., medical drugs) the one with best causal effect for a particular instance (e.g., patient) characterized by an observation of covariates. The training data consists of observations of covariates, treatment, and the outcome of the treatment. We recast the problem of learning to personalize from these observational data as a single learning task, which we use to develop four specific machine learning methods to directly address the personalization problem, two with a unique interpretability property. We also show how to validate personalization models on observational data, proposing the new coefficient of personalization as a unitless measure of effectiveness. We demonstrate the power of the new methods in two specific personalized medicine and policymaking applications and show they provide a significant advantage over standard approaches. "
771144108152856576,2016-09-01 00:34:17,https://t.co/L9cGiOwYLE,The Bayesian SLOPE. (arXiv:1608.08968v1 [stat.ME]) https://t.co/L9cGiOwYLE,1,2," Abstract: The SLOPE estimates regression coefficients by minimizing a regularized residual sum of squares using a sorted-$\ell_1$-norm penalty. The SLOPE combines testing and estimation in regression problems. It exhibits suitable variable selection and prediction properties, as well as minimax optimality. This paper introduces the Bayesian SLOPE procedure for linear regression. The classical SLOPE estimate is the posterior mode in the normal regression problem with an appropriate prior on the coefficients. The Bayesian SLOPE considers the full Bayesian model and has the advantage of offering credible sets and standard error estimates for the parameters. Moreover, the hierarchical Bayesian framework allows for full Bayesian and empirical Bayes treatment of the penalty coefficients; whereas it is not clear how to choose these coefficients when using the SLOPE on a general design matrix. A direct characterization of the posterior is provided which suggests a Gibbs sampler that does not involve latent variables. An efficient hybrid Gibbs sampler for the Bayesian SLOPE is introduced. Point estimation using the posterior mean is highlighted, which automatically facilitates the Bayesian prediction of future observations. These are demonstrated on real and synthetic data. "
771144105707597824,2016-09-01 00:34:16,https://t.co/HvLnuZoKLY,Towards Competitive Classifiers for Unbalanced Classification Problems: A Study on the Performance Scores. (arXiv:… https://t.co/HvLnuZoKLY,4,6," Abstract: Although a great methodological effort has been invested in proposing competitive solutions to the class-imbalance problem, little effort has been made in pursuing a theoretical understanding of this matter. In order to shed some light on this topic, we perform, through a novel framework, an exhaustive analysis of the adequateness of the most commonly used performance scores to assess this complex scenario. We conclude that using unweighted H\""older means with exponent $p \leq 1$ to average the recalls of all the classes produces adequate scores which are capable of determining whether a classifier is competitive. Then, we review the major solutions presented in the class-imbalance literature. Since any learning task can be defined as an optimisation problem where a loss function, usually connected to a particular score, is minimised, our goal, here, is to find whether the learning tasks found in the literature are also oriented to maximise the previously detected adequate scores. We conclude that they usually maximise the unweighted H\""older mean with $p = 1$ (a-mean). Finally, we provide bounds on the values of the studied performance scores which guarantee a classifier with a higher recall than the random classifier in each and every class. "
771144103308460033,2016-09-01 00:34:16,https://t.co/FkMTg6z02M,A Tutorial on Online Supervised Learning with Applications to Node Classification in Social Networks. (arXiv:1608.… https://t.co/FkMTg6z02M,2,9," Abstract: We revisit the elegant observation of T. Cover '65 which, perhaps, is not as well-known to the broader community as it should be. The first goal of the tutorial is to explain---through the prism of this elementary result---how to solve certain sequence prediction problems by modeling sets of solutions rather than the unknown data-generating mechanism. We extend Cover's observation in several directions and focus on computational aspects of the proposed algorithms. The applicability of the methods is illustrated on several examples, including node classification in a network. The second aim of this tutorial is to demonstrate the following phenomenon: it is possible to predict as well as a combinatorial ""benchmark"" for which we have a certain multiplicative approximation algorithm, even if the exact computation of the benchmark given all the data is NP-hard. The proposed prediction methods, therefore, circumvent some of the computational difficulties associated with finding the best model given the data. These difficulties arise rather quickly when one attempts to develop a probabilistic model for graph-based or other problems with a combinatorial structure. "
771144100515028992,2016-09-01 00:34:15,https://t.co/kMt8FhsgmJ,Unreasonable Effectiveness of Learning Neural Networks: From Accessible States and Robust Ensembles to Basic Algor… https://t.co/kMt8FhsgmJ,4,15," Abstract: In artificial neural networks, learning from data is a computationally demanding task in which a large number of connection weights are iteratively tuned through stochastic-gradient-based heuristic processes over a cost-function. It is not well understood how learning occurs in these systems, in particular how they avoid getting trapped in configurations with poor computational performance. Here we study the difficult case of networks with discrete weights, where the optimization landscape is very rough even for simple architectures, and provide theoretical and numerical evidence of the existence of rare - but extremely dense and accessible - regions of configurations in the network weight space. We define a novel measure, which we call the ""robust ensemble"" (RE), which suppresses trapping by isolated configurations and amplifies the role of these dense regions. We analytically compute the RE in some exactly solvable models, and also provide a general algorithmic scheme which is straightforward to implement: define a cost-function given by a sum of a finite number of replicas of the original cost-function, with a constraint centering the replicas around a driving assignment. To illustrate this, we derive several powerful new algorithms, ranging from Markov Chains to message passing to gradient descent processes, where the algorithms target the robust dense states, resulting in substantial improvements in performance. The weak dependence on the number of precision bits of the weights leads us to conjecture that very similar reasoning applies to more conventional neural networks. Analogous algorithmic schemes can also be applied to other optimization problems. "
771144098388537344,2016-09-01 00:34:14,https://t.co/EN3ASABncX,"European Union regulations on algorithmic decision-making and a ""right to explanation"". (arXiv:1606.08813v3 [stat.… https://t.co/EN3ASABncX",2,4," Abstract: We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which ""significantly affect"" users. The law will also effectively create a ""right to explanation,"" whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation. "
771144091694432257,2016-09-01 00:34:13,https://t.co/JXRhqnaRQm,Visualizing and Understanding Sum-Product Networks. (arXiv:1608.08266v1 [cs.LG] CROSS LISTED) https://t.co/JXRhqnaRQm,0,7," Abstract: Sum-Product Networks (SPNs) are recently introduced deep tractable probabilistic models by which several kinds of inference queries can be answered exactly and in a tractable time. Up to now, they have been largely used as black box density estimators, assessed only by comparing their likelihood scores only. In this paper we explore and exploit the inner representations learned by SPNs. We do this with a threefold aim: first we want to get a better understanding of the inner workings of SPNs; secondly, we seek additional ways to evaluate one SPN model and compare it against other probabilistic models, providing diagnostic tools to practitioners; lastly, we want to empirically evaluate how good and meaningful the extracted representations are, as in a classic Representation Learning framework. In order to do so we revise their interpretation as deep neural networks and we propose to exploit several visualization techniques on their node activations and network outputs under different types of inference queries. To investigate these models as feature extractors, we plug some SPNs, learned in a greedy unsupervised fashion on image datasets, in supervised classification learning tasks. We extract several embedding types from node activations by filtering nodes by their type, by their associated feature abstraction level and by their scope. In a thorough empirical comparison we prove them to be competitive against those generated from popular feature extractors as Restricted Boltzmann Machines. Finally, we investigate embeddings generated from random probabilistic marginal queries as means to compare other tractable probabilistic models on a common ground, extending our experiments to Mixtures of Trees. "
770781770782482432,2016-08-31 00:34:29,https://t.co/sJvjdeYTfo,Why does deep and cheap learning work so well?. (arXiv:1608.08225v1 [cond-mat.dis-nn]) https://t.co/sJvjdeYTfo,7,23," Abstract: We show how the success of deep learning depends not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can be approximated through ""cheap learning"" with exponentially fewer parameters than generic ones, because they have simplifying properties tracing back to the laws of physics. The exceptional simplicity of physics-based functions hinges on properties such as symmetry, locality, compositionality and polynomial log-probability, and we explore how these properties translate into exceptionally simple neural networks approximating both natural phenomena such as images and abstract representations thereof such as drawings. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to renormalization group procedures. We prove various ""no-flattening theorems"" showing when such efficient deep networks cannot be accurately approximated by shallow ones without efficiency loss: flattening even linear functions can be costly, and flattening polynomials is exponentially expensive; we use group theoretic techniques to show that n variables cannot be multiplied using fewer than 2^n neurons in a single hidden layer. "
770781769486434305,2016-08-31 00:34:28,https://t.co/u3u1Cia6Ce,Method for Improved DL CoMP Implementation in Heterogeneous Networks. (arXiv:1608.08306v1 [stat.ML]) https://t.co/u3u1Cia6Ce,0,4," Abstract: I propose a novel method for practical Joint Processing DL CoMP implementation in LTE/LTE-A systems using a supervised machine learning technique. DL CoMP has not been thoroughly studied in previous work although cluster formation and interference mitigation have been studied extensively. In this paper, I attempt to improve the cell-edge user data rate served by a heterogeneous network cluster by means of dynamically changing the DL SINR threshold at which DL CoMP is triggered. I do so by allowing the base stations to derive a threshold on the basis of machine learning inference. The simulation results show an improved user throughput at the cell edge of 40% and a 6.4% improvement to the average cell throughput compared to the baseline of static triggering. "
770781768316248065,2016-08-31 00:34:28,https://t.co/DDdQvQt8X7,Incremental Nonlinear System Identification and Adaptive Particle Filtering Using Gaussian Process. (arXiv:1608.08… https://t.co/DDdQvQt8X7,3,8," Abstract: An incremental/online state dynamic learning method is proposed for identification of the nonlinear Gaussian state space models. The method embeds the stochastic variational sparse Gaussian process as the probabilistic state dynamic model inside a particle filter framework. Model updating is done at measurement sample rate using stochastic gradient descent based optimization implemented in the state estimation filtering loop. The performance of the proposed method is compared with state-of-the-art Gaussian process based batch learning methods. Finally, it is shown that the state estimation performance significantly improves due to the online learning of state dynamics. "
770781767053799424,2016-08-31 00:34:28,https://t.co/0ZV06JUEod,Optimal learning with Bernstein Online Aggregation. (arXiv:1404.1356v4 [stat.ML] UPDATED) https://t.co/0ZV06JUEod,0,5," Abstract: We introduce a new recursive aggregation procedure called Bernstein Online Aggregation (BOA). The exponential weights include an accuracy term and a second order term that is a proxy of the quadratic variation as in Hazan and Kale (2010). This second term stabilizes the procedure that is optimal in different senses. We first obtain optimal regret bounds in the deterministic context. Then, an adaptive version is the first exponential weights algorithm that exhibits a second order bound with excess losses that appears first in Gaillard et al. (2014). The second order bounds in the deterministic context are extended to a general stochastic context using the cumulative predictive risk. Such conversion provides the main result of the paper, an inequality of a novel type comparing the procedure with any deterministic aggregation procedure for an integrated criteria. Then we obtain an observable estimate of the excess of risk of the BOA procedure. To assert the optimality, we consider finally the iid case for strongly convex and Lipschitz continuous losses and we prove that the optimal rate of aggregation of Tsybakov (2003) is achieved. The batch version of the BOA procedure is then the first adaptive explicit algorithm that satisfies an optimal oracle inequality with high probability. "
770781765715787776,2016-08-31 00:34:28,https://t.co/gVL2dpXtQ2,Limits on Support Recovery with Probabilistic Models: An Information-Theoretic Framework. (arXiv:1501.07440v3 [cs.… https://t.co/gVL2dpXtQ2,0,5," Abstract: The support recovery problem consists of determining a sparse subset of a set of variables that is relevant in generating a set of observations, and arises in a diverse range of settings such as compressive sensing, and subset selection in regression, and group testing. In this paper, we take a unified approach to support recovery problems, considering general probabilistic models relating a sparse data vector to an observation vector. We study the information-theoretic limits of both exact and partial support recovery, taking a novel approach motivated by thresholding techniques in channel coding. We provide general achievability and converse bounds characterizing the trade-off between the error probability and number of measurements, and we specialize these to the linear, 1-bit, and group testing models. In several cases, our bounds not only provide matching scaling laws in the necessary and sufficient number of measurements, but also sharp thresholds with matching constant factors. Our approach has several advantages over previous approaches: For the achievability part, we obtain sharp thresholds under broader scalings of the sparsity level and other parameters (e.g., signal-to-noise ratio) compared to several previous works, and for the converse part, we not only provide conditions under which the error probability fails to vanish, but also conditions under which it tends to one. "
770781764138729472,2016-08-31 00:34:27,https://t.co/coKpeZZ7QX,Multiple penalized principal curves: analysis and computation. (arXiv:1512.05010v2 [math.AP] UPDATED) https://t.co/coKpeZZ7QX,0,1," Abstract: We study the problem of finding the one-dimensional structure in a given data set. In other words we consider ways to approximate a given measure (data) by curves. We consider an objective functional whose minimizers are a regularization of principal curves and introduce a new functional which allows for multiple curves. We prove the existence of minimizers and establish their basic properties. We develop an efficient algorithm for obtaining (near) minimizers of the functional. While both of the functionals used are nonconvex, we argue that enlarging the configuration space to allow for multiple curves leads to a simpler energy landscape with fewer undesirable (high-energy) local minima. Furthermore we note that the approach proposed is able to find the one-dimensional structure even for data with considerable amount of noise. "
770781762066718720,2016-08-31 00:34:27,https://t.co/AFMVKqOwoE,A Framework for Fast Image Deconvolution with Incomplete Observations. (arXiv:1602.01410v2 [cs.CV] UPDATED) https://t.co/AFMVKqOwoE,1,9," Abstract: In image deconvolution problems, the diagonalization of the underlying operators by means of the FFT usually yields very large speedups. When there are incomplete observations (e.g., in the case of unknown boundaries), standard deconvolution techniques normally involve non-diagonalizable operators, resulting in rather slow methods, or, otherwise, use inexact convolution models, resulting in the occurrence of artifacts in the enhanced images. In this paper, we propose a new deconvolution framework for images with incomplete observations that allows us to work with diagonalized convolution operators, and therefore is very fast. We iteratively alternate the estimation of the unknown pixels and of the deconvolved image, using, e.g., an FFT-based deconvolution method. This framework is an efficient, high-quality alternative to existing methods of dealing with the image boundaries, such as edge tapering. It can be used with any fast deconvolution method. We give an example in which a state-of-the-art method that assumes periodic boundary conditions is extended, through the use of this framework, to unknown boundary conditions. Furthermore, we propose a specific implementation of this framework, based on the alternating direction method of multipliers (ADMM). We provide a proof of convergence for the resulting algorithm, which can be seen as a ""partial"" ADMM, in which not all variables are dualized. We report experimental comparisons with other primal-dual methods, where the proposed one performed at the level of the state of the art. Four different kinds of applications were tested in the experiments: deconvolution, deconvolution with inpainting, superresolution, and demosaicing, all with unknown boundaries. "
770419481680175104,2016-08-30 00:34:52,https://t.co/uYnIaPz1Uj,A Randomized Approach to Efficient Kernel Clustering. (arXiv:1608.07597v1 [stat.ML]) https://t.co/uYnIaPz1Uj,0,4," Abstract: Kernel-based K-means clustering has gained popularity due to its simplicity and the power of its implicit non-linear representation of the data. A dominant concern is the memory requirement since memory scales as the square of the number of data points. We provide a new analysis of a class of approximate kernel methods that have more modest memory requirements, and propose a specific one-pass randomized kernel approximation followed by standard K-means on the transformed data. The analysis and experiments suggest the method is accurate, while requiring drastically less memory than standard kernel K-means and significantly less memory than Nystrom based approximations. "
770419480451289093,2016-08-30 00:34:52,https://t.co/X1n1rHn7qd,Clustering and Community Detection with Imbalanced Clusters. (arXiv:1608.07605v1 [stat.ML]) https://t.co/X1n1rHn7qd,1,2," Abstract: Spectral clustering methods which are frequently used in clustering and community detection applications are sensitive to the specific graph constructions particularly when imbalanced clusters are present. We show that ratio cut (RCut) or normalized cut (NCut) objectives are not tailored to imbalanced cluster sizes since they tend to emphasize cut sizes over cut values. We propose a graph partitioning problem that seeks minimum cut partitions under minimum size constraints on partitions to deal with imbalanced cluster sizes. Our approach parameterizes a family of graphs by adaptively modulating node degrees on a fixed node set, yielding a set of parameter dependent cuts reflecting varying levels of imbalance. The solution to our problem is then obtained by optimizing over these parameters. We present rigorous limit cut analysis results to justify our approach and demonstrate the superiority of our method through experiments on synthetic and real datasets for data clustering, semi-supervised learning and community detection. "
770419479134236672,2016-08-30 00:34:52,https://t.co/YklGzwGhnj,Global analysis of Expectation Maximization for mixtures of two Gaussians. (arXiv:1608.07630v1 [math.ST]) https://t.co/YklGzwGhnj,0,2," Abstract: Expectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models. However, EM, which is an iterative algorithm based on the maximum likelihood principle, is generally only guaranteed to find stationary points of the likelihood objective, and these points may be far from any maximizer. This article addresses this disconnect between the statistical principles behind EM and its algorithmic properties. Specifically, it provides a global analysis of EM for specific models in which the observations comprise an i.i.d. sample from a mixture of two Gaussians. This is achieved by (i) studying the sequence of parameters from idealized execution of EM in the infinite sample limit, and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis, establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by EM. "
770419477641060352,2016-08-30 00:34:51,https://t.co/oTvgmATddb,Learning Temporal Dependence from Time-Series Data with Latent Variables. (arXiv:1608.07636v1 [cs.LG]) https://t.co/oTvgmATddb,1,17," Abstract: We consider the setting where a collection of time series, modeled as random processes, evolve in a causal manner, and one is interested in learning the graph governing the relationships of these processes. A special case of wide interest and applicability is the setting where the noise is Gaussian and relationships are Markov and linear. We study this setting with two additional features: firstly, each random process has a hidden (latent) state, which we use to model the internal memory possessed by the variables (similar to hidden Markov models). Secondly, each variable can depend on its latent memory state through a random lag (rather than a fixed lag), thus modeling memory recall with differing lags at distinct times. Under this setting, we develop an estimator and prove that under a genericity assumption, the parameters of the model can be learned consistently. We also propose a practical adaption of this estimator, which demonstrates significant performance gains in both synthetic and real-world datasets. "
770419476403814400,2016-08-30 00:34:51,https://t.co/7MdH3Mpunw,A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples. (arXiv:1608.07690v1 [cs.LG]) https://t.co/7MdH3Mpunw,0,1," Abstract: Deep neural networks have been shown to suffer from a surprising weakness: their classification outputs can be changed by small, non-random perturbations of their inputs. This adversarial example phenomenon has been explained as originating from deep networks being ""too linear"" (Goodfellow et al., 2014). We show here that the linear explanation of adversarial examples presents a number of limitations: the formal argument is not convincing, linear classifiers do not always suffer from the phenomenon, and when they do their adversarial examples are different from the ones affecting deep networks. We propose a new perspective on the phenomenon. We argue that adversarial examples exist when the classification boundary lies close to the submanifold of sampled data, and present a mathematical analysis of this new perspective in the linear case. We define the notion of adversarial strength and show that it can be reduced to the deviation angle between the classifier considered and the nearest centroid classifier. Then, we show that the adversarial strength can be made arbitrarily high independently of the classification performance due to a mechanism that we call boundary tilting. This result leads us to defining a new taxonomy of adversarial examples. Finally, we show that the adversarial strength observed in practice is directly dependent on the level of regularisation used and the strongest adversarial examples, symptomatic of overfitting, can be avoided by using a proper level of regularisation. "
770419475166429184,2016-08-30 00:34:51,https://t.co/RCrgQpELXk,Random Forest for Label Ranking. (arXiv:1608.07710v1 [cs.LG]) https://t.co/RCrgQpELXk,0,2," Abstract: Label ranking aims to learn a mapping from instances to rankings over a finite number of predefined labels. Random forest is a powerful and one of the most successfully general-purpose machine learning algorithms of modern times. In the literature, there seems no research has yet been done in applying random forest to label ranking. In this paper, We present a powerful random forest label ranking method which uses random decision trees to retrieve nearest neighbors that are not only similar in the feature space but also in the ranking space. We have developed a novel two-step rank aggregation strategy to effectively aggregate neighboring rankings discovered by the random forest into a final predicted ranking. Compared with existing methods, the new random forest method has many advantages including its intrinsically scalable tree data structure, highly parallel-able computational architecture and much superior performances. We present extensive experimental results to demonstrate that our new method achieves the best predictive accuracy performances compared with state-of-the-art methods for datasets with complete ranking and datasets with only partial ranking information. "
770419473576845312,2016-08-30 00:34:50,https://t.co/v50mqCeW9j,Learning Bayesian Networks without Assuming Missing at Random. (arXiv:1608.07734v1 [cs.AI]) https://t.co/v50mqCeW9j,0,6," Abstract: We present new algorithms for learning Bayesian networks from data with missing values using a data augmentation approach. An exact Bayesian network learning algorithm is obtained by recasting the problem into a standard Bayesian network learning problem without missing data. To the best of our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large domains. We build on the exact method to create an approximate algorithm using a hill-climbing technique. This algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available. We perform a wide range of experiments to demonstrate the benefits of learning Bayesian networks with such new approach. "
770419472108748800,2016-08-30 00:34:50,https://t.co/objb7aQOjY,Bayesian selection for the regularization parameter in TVl0 denoising problems. (arXiv:1608.07739v1 [cs.LG]) https://t.co/objb7aQOjY,0,2," Abstract: Piecewise constant denoising can be solved either by deterministic optimization approaches, based on total variation (TV), or by stochastic Bayesian procedures. The former lead to low computational time but requires the selection of a regularization parameter, whose value significantly impacts the achieved solution, and whose automated selection remains an involved and challenging problem. Conversely, fully Bayesian formalisms encapsulate the regularization parameter selection into hierarchical models, at the price of large computational costs. This contribution proposes an operational strategy that combines hierarchical Bayesian and TVl0 formulations, with the double aim of automatically tuning the regularization parameter and of maintaining computational efficiency. The proposed procedure relies on formally connecting a Bayesian framework to a TVl0 minimization formulation. Behaviors and performance for the proposed piecewise constant denoising and regularization parameter tuning techniques are studied qualitatively and assessed quantitatively, and shown to compare favorably against those of a fully Bayesian hierarchical procedure, both in accuracy and in computational load. "
770419470263353349,2016-08-30 00:34:50,https://t.co/0wOdgnGzH2,Optimizing Recurrent Neural Networks Architectures under Time Constraints. (arXiv:1608.07892v1 [stat.ML]) https://t.co/0wOdgnGzH2,2,11," Abstract: Recurrent neural network (RNN)'s architecture is a key factor influencing its performance. We propose algorithms to optimize hidden sizes under running time constraint. We convert the discrete optimization into a subset selection problem. By novel transformations, the objective function becomes submodular and constraint becomes supermodular. A greedy algorithm with bounds is suggested to solve the transformed problem. And we show how transformations influence the bounds. To speed up optimization, surrogate functions are proposed which balance exploration and exploitation. Experiments show that our algorithms can find more accurate models or faster models than manually tuned state-of-the-art and random search. We also compare popular RNN architectures using our algorithms. "
770419468803735553,2016-08-30 00:34:49,https://t.co/kJBtFyyjI1,Discovering Patterns in Time-Varying Graphs: A Triclustering Approach. (arXiv:1608.07929v1 [stat.ML]) https://t.co/kJBtFyyjI1,0,4," Abstract: This paper introduces a novel technique to track structures in time varying graphs. The method uses a maximum a posteriori approach for adjusting a three-dimensional co-clustering of the source vertices, the destination vertices and the time, to the data under study, in a way that does not require any hyper-parameter tuning. The three dimensions are simultaneously segmented in order to build clusters of source vertices, destination vertices and time segments where the edge distributions across clusters of vertices follow the same evolution over the time segments. The main novelty of this approach lies in that the time segments are directly inferred from the evolution of the edge distribution between the vertices, thus not requiring the user to make any a priori quantization. Experiments conducted on artificial data illustrate the good behavior of the technique, and a study of a real-life data set shows the potential of the proposed approach for exploratory data analysis. "
770419467272720384,2016-08-30 00:34:49,https://t.co/v1zXhcjBql,Relevant based structure learning for feature selection. (arXiv:1608.07934v1 [cs.LG]) https://t.co/v1zXhcjBql,0,2," Abstract: Feature selection is an important task in many problems occurring in pattern recognition, bioinformatics, machine learning and data mining applications. The feature selection approach enables us to reduce the computation burden and the falling accuracy effect of dealing with huge number of features in typical learning problems. There is a variety of techniques for feature selection in supervised learning problems based on different selection metrics. In this paper, we propose a novel unified framework for feature selection built on the graphical models and information theoretic tools. The proposed approach exploits the structure learning among features to select more relevant and less redundant features to the predictive modeling problem according to a primary novel likelihood based criterion. In line with the selection of the optimal subset of features through the proposed method, it provides us the Bayesian network classifier without the additional cost of model training on the selected subset of features. The optimal properties of our method are established through empirical studies and computational complexity analysis. Furthermore the proposed approach is evaluated on a bunch of benchmark datasets based on the well-known classification algorithms. Extensive experiments confirm the significant improvement of the proposed approach compared to the earlier works. "
770419465792196608,2016-08-30 00:34:49,https://t.co/llq0430AdO,Manifold Langevin Monte Carlo with Partial Metric Updates. (arXiv:1608.07986v1 [stat.ML]) https://t.co/llq0430AdO,0,4," Abstract: Manifold Markov chain Monte Carlo algorithms have been introduced to sample more effectively from challenging target densities exhibiting multiple modes or strong correlations. Such algorithms exploit the local geometry of the parameter space, thus enabling chains to achieve a faster convergence rate when measured in number of steps. However, often acquiring local geometric information increases computational complexity per step to the extent that sampling from high-dimensional targets becomes inefficient in terms of total computational time. This paper proposes a manifold Langevin Monte Carlo framework aimed at balancing the benefits of exploiting local geometry with computational requirements to achieve a high effective sample size for a given computational cost. The suggested strategy regulates the frequency of manifold-based updates via a schedule. An exponentially decaying schedule is put forward that enables more frequent updates of geometric information in early transient phases of the chain, while saving computational time in late stationary phases. Alternatively, a modulo schedule is introduced to allow for infrequent yet recurring geometric updates throughout the sampling course, acting to adaptively update proposals to learn the covariance structure of the parameter space. The average complexity can be manually set for either of these two schedules depending on the need for geometric exploitation posed by the underlying model. "
770419464269598720,2016-08-30 00:34:48,https://t.co/xtfwRXC7op,Robust Discriminative Clustering with Sparse Regularizers. (arXiv:1608.08052v1 [stat.ML]) https://t.co/xtfwRXC7op,0,2," Abstract: Clustering high-dimensional data often requires some form of dimensionality reduction, where clustered variables are separated from ""noise-looking"" variables. We cast this problem as finding a low-dimensional projection of the data which is well-clustered. This yields a one-dimensional projection in the simplest situation with two clusters, and extends naturally to a multi-label scenario for more than two clusters. In this paper, (a) we first show that this joint clustering and dimension reduction formulation is equivalent to previously proposed discriminative clustering frameworks, thus leading to convex relaxations of the problem, (b) we propose a novel sparse extension, which is still cast as a convex relaxation and allows estimation in higher dimensions, (c) we propose a natural extension for the multi-label scenario, (d) we provide a new theoretical analysis of the performance of these formulations with a simple probabilistic model, leading to scalings over the form $d=O(\sqrt{n})$ for the affine invariant case and $d=O(n)$ for the sparse case, where $n$ is the number of examples and $d$ the ambient dimension, and finally, (e) we propose an efficient iterative algorithm with running-time complexity proportional to $O(nd^2)$, improving on earlier algorithms which had quadratic complexity in the number of examples. "
770419462847823872,2016-08-30 00:34:48,https://t.co/eZREQ8l5dJ,Wasserstein Discriminant Analysis. (arXiv:1608.08063v1 [stat.ML]) https://t.co/eZREQ8l5dJ,0,2," Abstract: Wasserstein Discriminant Analysis (WDA) is a new supervised method that can improve classification of high-dimensional data by computing a suitable linear map onto a lower dimensional subspace. Following the blueprint of classical Linear Discriminant Analysis (LDA), WDA selects the projection matrix that maximizes the ratio of two quantities: the dispersion of projected points coming from different classes, divided by the dispersion of projected points coming from the same class. To quantify dispersion, WDA uses regularized Wasserstein distances, rather than cross-variance measures which have been usually considered, notably in LDA. Thanks to the the underlying principles of optimal transport, WDA is able to capture both global (at distribution scale) and local (at samples scale) interactions between classes. Regularized Wasserstein distances can be computed using the Sinkhorn matrix scaling algorithm; We show that the optimization of WDA can be tackled using automatic differentiation of Sinkhorn iterations. Numerical experiments show promising results both in terms of prediction and visualization on toy examples and real life datasets such as MNIST and on deep features obtained from a subset of the Caltech dataset. "
770419461346164736,2016-08-30 00:34:47,https://t.co/6YS2hNviAW,Marginalization and Conditioning for LWF Chain Graphs. (arXiv:1405.7129v4 [stat.OT] UPDATED) https://t.co/6YS2hNviAW,0,1," Abstract: In this paper, we deal with the problem of marginalization over and conditioning on two disjoint subsets of the node set of chain graphs (CGs) with the LWF Markov property. For this purpose, we define the class of chain mixed graphs (CMGs) with three types of edges and, for this class, provide a separation criterion under which the class of CMGs is stable under marginalization and conditioning and contains the class of LWF CGs as its subclass. We provide a method for generating such graphs after marginalization and conditioning for a given CMG or a given LWF CG. We then define and study the class of anterial graphs, which is also stable under marginalization and conditioning and contains LWF CGs, but has a simpler structure than CMGs. "
770419459752361985,2016-08-30 00:34:47,https://t.co/3bCyoVQx89,Formal Hypothesis Tests for Additive Structure in Random Forests. (arXiv:1406.1845v3 [stat.ML] UPDATED) https://t.co/3bCyoVQx89,1,2," Abstract: While statistical learning methods have proved powerful tools for predictive modeling, the black-box nature of the models they produce can severely limit their interpretability and the ability to conduct formal inference. However, the natural structure of ensemble learners like bagged trees and random forests has been shown to admit desirable asymptotic properties when base learners are built with proper subsamples. In this work, we demonstrate that by defining an appropriate grid structure on the covariate space, we may carry out formal hypothesis tests for both variable importance and underlying additive model structure. To our knowledge, these tests represent the first statistical tools for investigating the underlying regression structure in a context such as random forests. We develop notions of total and partial additivity and further demonstrate that testing can be carried out at no additional computational cost by estimating the variance within the process of constructing the ensemble. Furthermore, we propose a novel extension of these testing procedures utilizing random projections in order to allow for computationally efficient testing procedures that retain high power even when the grid size is much larger than that of the training set. "
770419458280218624,2016-08-30 00:34:47,https://t.co/JXdWSJuvV6,Submodular Learning and Covering with Response-Dependent Costs. (arXiv:1602.07120v2 [cs.LG] UPDATED) https://t.co/JXdWSJuvV6,0,1," Abstract: We consider interactive learning and covering problems, in a setting where actions may incur different costs, depending on the response to the action. We propose a natural greedy algorithm for response-dependent costs. We bound the approximation factor of this greedy algorithm in active learning settings as well as in the general setting. We show that a different property of the cost function controls the approximation factor in each of these scenarios. We further show that in both settings, the approximation factor of this greedy algorithm is near-optimal among all greedy algorithms. Experiments demonstrate the advantages of the proposed algorithm in the response-dependent cost setting. "
770419456908599296,2016-08-30 00:34:46,https://t.co/8a8OetoZr9,Robust and Sparse Regression via $\gamma$-divergence. (arXiv:1604.06637v3 [stat.ME] UPDATED) https://t.co/8a8OetoZr9,0,1," Abstract: In high-dimensional data, many sparse regression methods have been proposed. However, they may not be robust against outliers. Recently, the use of density power weight has been studied for robust parameter estimation and the corresponding divergences have been discussed. One of such divergences is the $\gamma$-divergence and the robust estimator using the $\gamma$-divergence is known for having a strong robustness. In this paper, we consider the robust and sparse regression based on $\gamma$-divergence. We extend the $\gamma$-divergence to the regression problem and show that it has a strong robustness under heavy contamination even when outliers are heterogeneous. The loss function is constructed by an empirical estimate of the $\gamma$-divergence with sparse regularization and the parameter estimate is defined as the minimizer of the loss function. To obtain the robust and sparse estimate, we propose an efficient update algorithm which has a monotone decreasing property of the loss function. Particularly, we discuss a linear regression problem with $L_1$ regularization in detail. In numerical experiments and real data analyses, we see that the proposed method outperforms past robust and sparse methods. "
770419454870192128,2016-08-30 00:34:46,https://t.co/8LviylK8kH,Further properties of the forward-backward envelope with applications to difference-of-convex programming. (arXiv:… https://t.co/8LviylK8kH,0,1," Abstract: In this paper, we further study the forward-backward envelope first introduced in [28] and [30] for problems whose objective is the sum of a proper closed convex function and a twice continuously differentiable possibly nonconvex function with Lipschitz continuous gradient. We derive sufficient conditions on the original problem for the corresponding forward-backward envelope to be a level-bounded and Kurdyka-{\L}ojasiewicz function with an exponent of $\frac12$; these results are important for the efficient minimization of the forward-backward envelope by classical optimization algorithms. In addition, we demonstrate how to minimize some difference-of-convex regularized least squares problems by minimizing a suitably constructed forward-backward envelope. Our preliminary numerical results on randomly generated instances of large-scale $\ell_{1-2}$ regularized least squares problems [37] illustrate that an implementation of this approach with a limited-memory BFGS scheme usually outperforms standard first-order methods such as the nonmonotone proximal gradient method in [35]. "
770419453490294784,2016-08-30 00:34:46,https://t.co/yYQm2y9J4O,Understanding the Energy and Precision Requirements for Online Learning. (arXiv:1607.00669v3 [stat.ML] UPDATED) https://t.co/yYQm2y9J4O,0,4," Abstract: It is well-known that the precision of data, hyperparameters, and internal representations employed in learning systems directly impacts its energy, throughput, and latency. The precision requirements for the training algorithm are also important for systems that learn on-the-fly. Prior work has shown that the data and hyperparameters can be quantized heavily without incurring much penalty in classification accuracy when compared to floating point implementations. These works suffer from two key limitations. First, they assume uniform precision for the classifier and for the training algorithm and thus miss out on the opportunity to further reduce precision. Second, prior works are empirical studies. In this article, we overcome both these limitations by deriving analytical lower bounds on the precision requirements of the commonly employed stochastic gradient descent (SGD) on-line learning algorithm in the specific context of a support vector machine (SVM). Lower bounds on the data precision are derived in terms of the the desired classification accuracy and precision of the hyperparameters used in the classifier. Additionally, lower bounds on the hyperparameter precision in the SGD training algorithm are obtained. These bounds are validated using both synthetic and the UCI breast cancer dataset. Additionally, the impact of these precisions on the energy consumption of a fixed-point SVM with on-line training is studied. "
770419452127117312,2016-08-30 00:34:45,https://t.co/bw4yzduEhZ,On the Consistency of the Likelihood Maximization Vertex Nomination Scheme: Bridging the Gap Between Maximum Likel… https://t.co/bw4yzduEhZ,0,1," Abstract: Given a graph in which a few vertices are deemed interesting a priori, the vertex nomination task is to order the remaining vertices into a nomination list such that there is a concentration of interesting vertices at the top of the list. Previous work has yielded several approaches to this problem, with theoretical results in the setting where the graph is drawn from a stochastic block model (SBM), including a vertex nomination analogue of the Bayes optimal classifier. In this paper, we prove that maximum likelihood (ML)-based vertex nomination is consistent, in the sense that the performance of the ML-based scheme asymptotically matches that of the Bayes optimal scheme. We prove theorems of this form both when model parameters are known and unknown. Additionally, we introduce and prove consistency of a related, more scalable restricted-focus ML vertex nomination scheme. Finally, we incorporate vertex and edge features into ML-based vertex nomination and briefly explore the empirical effectiveness of this approach. "
770419450797522944,2016-08-30 00:34:45,https://t.co/UfhBZgoE1d,Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in Social Good Applications. (arXiv:1607.024… https://t.co/UfhBZgoE1d,2,7," Abstract: This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York. "
770057205060075521,2016-08-29 00:35:19,https://t.co/zR4Pt7Qell,Hard Negative Mining for Metric Learning Based Zero-Shot Classification. (arXiv:1608.07441v1 [cs.LG]) https://t.co/zR4Pt7Qell,4,9," Abstract: Zero-Shot learning has been shown to be an efficient strategy for domain adaptation. In this context, this paper builds on the recent work of Bucher et al. [1], which proposed an approach to solve Zero-Shot classification problems (ZSC) by introducing a novel metric learning based objective function. This objective function allows to learn an optimal embedding of the attributes jointly with a measure of similarity between images and attributes. This paper extends their approach by proposing several schemes to control the generation of the negative pairs, resulting in a significant improvement of the performance and giving above state-of-the-art results on three challenging ZSC datasets. "
770057200454791168,2016-08-29 00:35:18,https://t.co/sz9ZOFQVI5,Estimating the Number of Clusters via Normalized Cluster Instability. (arXiv:1608.07494v1 [stat.ML]) https://t.co/sz9ZOFQVI5,3,6," Abstract: We improve existing instability-based methods for the selection of the number of clusters $k$ in cluster analysis by normalizing instability. In contrast to existing instability methods which show good performance only for bounded sequence of small $k$s, our improved method achieves high estimation performance for $k$ across the whole sequence of possible $k$s. In addition, we compare for the first time model-based and model-free variants of $k$ selection via cluster instability and find that their performance is similar. We make our method available in the R-package cstab. "
770057195497095169,2016-08-29 00:35:17,https://t.co/A1Id6LMTPY,Entity Embedding-based Anomaly Detection for Heterogeneous Categorical Events. (arXiv:1608.07502v1 [cs.LG]) https://t.co/A1Id6LMTPY,1,3," Abstract: Anomaly detection plays an important role in modern data-driven security applications, such as detecting suspicious access to a socket from a process. In many cases, such events can be described as a collection of categorical values that are considered as entities of different types, which we call heterogeneous categorical events. Due to the lack of intrinsic distance measures among entities, and the exponentially large event space, most existing work relies heavily on heuristics to calculate abnormal scores for events. Different from previous work, we propose a principled and unified probabilistic model APE (Anomaly detection via Probabilistic pairwise interaction and Entity embedding) that directly models the likelihood of events. In this model, we embed entities into a common latent space using their observed co-occurrence in different events. More specifically, we first model the compatibility of each pair of entities according to their embeddings. Then we utilize the weighted pairwise interactions of different entity types to define the event probability. Using Noise-Contrastive Estimation with ""context-dependent"" noise distribution, our model can be learned efficiently regardless of the large event space. Experimental results on real enterprise surveillance data show that our methods can accurately detect abnormal events compared to other state-of-the-art abnormal detection techniques. "
770057191487311873,2016-08-29 00:35:16,https://t.co/dIkbJWjJhS,Maximum Correntropy Unscented Filter. (arXiv:1608.07526v1 [stat.ML]) https://t.co/dIkbJWjJhS,0,2," Abstract: The unscented transformation (UT) is an efficient method to solve the state estimation problem for a non-linear dynamic system, utilizing a derivative-free higher-order approximation by approximating a Gaussian distribution rather than approximating a non-linear function. Applying the UT to a Kalman filter type estimator leads to the well-known unscented Kalman filter (UKF). Although the UKF works very well in Gaussian noises, its performance may deteriorate significantly when the noises are non-Gaussian, especially when the system is disturbed by some heavy-tailed impulsive noises. To improve the robustness of the UKF against impulsive noises, a new filter for nonlinear systems is proposed in this work, namely the maximum correntropy unscented filter (MCUF). In MCUF, the UT is applied to obtain the prior estimates of the state and covariance matrix, and a robust statistical linearization regression based on the maximum correntropy criterion (MCC) is then used to obtain the posterior estimates of the state and covariance. The satisfying performance of the new algorithm is confirmed by two illustrative examples. "
770057186823340037,2016-08-29 00:35:14,https://t.co/NuZn7Cr1Yt,Leveraging over intact priors for boosting control and dexterity of prosthetic hands by amputees. (arXiv:1608.0753… https://t.co/NuZn7Cr1Yt,0,1," Abstract: Non-invasive myoelectric prostheses require a long training time to obtain satisfactory control dexterity. These training times could possibly be reduced by leveraging over training efforts by previous subjects. So-called domain adaptation algorithms formalize this strategy and have indeed been shown to significantly reduce the amount of required training data for intact subjects for myoelectric movements classification. It is not clear, however, whether these results extend also to amputees and, if so, whether prior information from amputees and intact subjects is equally useful. To overcome this problem, we evaluated several domain adaptation algorithms on data coming from both amputees and intact subjects. Our findings indicate that: (1) the use of previous experience from other subjects allows us to reduce the training time by about an order of magnitude; (2) this improvement holds regardless of whether an amputee exploits previous information from other amputees or from intact subjects. "
770057183346163714,2016-08-29 00:35:14,https://t.co/GecD2svrpr,Sparse Signal Processing with Linear and Nonlinear Observations: A Unified Shannon-Theoretic Approach. (arXiv:1304… https://t.co/GecD2svrpr,1,6," Abstract: We derive fundamental sample complexity bounds for recovering sparse and structured signals for linear and nonlinear observation models including sparse regression, group testing, multivariate regression and problems with missing features. In general, sparse signal processing problems can be characterized in terms of the following Markovian property. We are given a set of $N$ variables $X_1,X_2,\ldots,X_N$, and there is an unknown subset of variables $S \subset \{1,\ldots,N\}$ that are relevant for predicting outcomes $Y$. More specifically, when $Y$ is conditioned on $\{X_n\}_{n\in S}$ it is conditionally independent of the other variables, $\{X_n\}_{n \not \in S}$. Our goal is to identify the set $S$ from samples of the variables $X$ and the associated outcomes $Y$. We characterize this problem as a version of the noisy channel coding problem. Using asymptotic information theoretic analyses, we establish mutual information formulas that provide sufficient and necessary conditions on the number of samples required to successfully recover the salient variables. These mutual information expressions unify conditions for both linear and nonlinear observations. We then compute sample complexity bounds for the aforementioned models, based on the mutual information expressions in order to demonstrate the applicability and flexibility of our results in general sparse signal processing models. "
770057179147755521,2016-08-29 00:35:13,https://t.co/DuHIRyYVrD,A Meta-Analysis of the Anomaly Detection Problem. (arXiv:1503.01158v2 [cs.AI] UPDATED) https://t.co/DuHIRyYVrD,0,6," Abstract: This article provides a thorough meta-analysis of the anomaly detection problem. To accomplish this we first identify approaches to benchmarking anomaly detection algorithms across the literature and produce a large corpus of anomaly detection benchmarks that vary in their construction across several dimensions we deem important to real-world applications: (a) point difficulty, (b) relative frequency of anomalies, (c) clusteredness of anomalies, and (d) relevance of features. We apply a representative set of anomaly detection algorithms to this corpus, yielding a very large collection of experimental results. We analyze these results to understand many phenomena observed in previous work. First we observe the effects of experimental design on experimental results. Second, results are evaluated with two metrics, ROC Area Under the Curve and Average Precision. We employ statistical hypothesis testing to demonstrate the value (or lack thereof) of our benchmarks. We then offer several approaches to summarizing our experimental results, drawing several conclusions about the impact of our methodology as well as the strengths and weaknesses of some algorithms. Last, we compare results against a trivial solution as an alternate means of normalizing the reported performance of algorithms. The intended contributions of this article are many; in addition to providing a large publicly-available corpus of anomaly detection benchmarks, we provide an ontology for describing anomaly detection contexts, a methodology for controlling various aspects of benchmark creation, guidelines for future experimental design and a discussion of the many potential pitfalls of trying to measure success in this field. "
770057175028928512,2016-08-29 00:35:12,https://t.co/5nYec1OBjR,Variational methods for Conditional Multimodal Deep Learning. (arXiv:1603.01801v2 [cs.CV] UPDATED) https://t.co/5nYec1OBjR,6,15," Abstract: In this paper, we address the problem of conditional modality learning, whereby one is interested in generating one modality given the other. While it is straightforward to learn a joint distribution over multiple modalities using a deep multimodal architecture, we observe that such models aren't very effective at conditional generation. Hence, we address the problem by learning conditional distributions between the modalities. We use variational methods for maximizing the corresponding conditional log-likelihood. The resultant deep model, which we refer to as conditional multimodal autoencoder (CMMA), forces the latent representation obtained from a single modality alone to be `close' to the joint representation obtained from multiple modalities. We use the proposed model to generate faces from attributes. We show that the faces generated from attributes using the proposed model, are qualitatively and quantitatively more representative of the attributes from which they were generated, than those obtained by other deep generative models. We also propose a secondary task, whereby the existing faces are modified by modifying the corresponding attributes. We observe that the modifications in face introduced by the proposed model are representative of the corresponding modifications in attributes. "
770057170268393478,2016-08-29 00:35:11,https://t.co/jAsbp5FkTq,Online Rules for Control of False Discovery Rate and False Discovery Exceedance. (arXiv:1603.09000v2 [math.ST] UPD… https://t.co/jAsbp5FkTq,0,5," Abstract: Multiple hypothesis testing is a core problem in statistical inference and arises in almost every scientific field. Given a set of null hypotheses $\mathcal{H}(n) = (H_1,\dotsc, H_n)$, Benjamini and Hochberg introduced the false discovery rate (FDR), which is the expected proportion of false positives among rejected null hypotheses, and proposed a testing procedure that controls FDR below a pre-assigned significance level. Nowadays FDR is the criterion of choice for large scale multiple hypothesis testing. In this paper we consider the problem of controlling FDR in an ""online manner"". Concretely, we consider an ordered --possibly infinite-- sequence of null hypotheses $\mathcal{H} = (H_1,H_2,H_3,\dots )$ where, at each step $i$, the statistician must decide whether to reject hypothesis $H_i$ having access only to the previous decisions. This model was introduced by Foster and Stine. We study a class of ""generalized alpha-investing"" procedures and prove that any rule in this class controls online FDR, provided $p$-values corresponding to true nulls are independent from the other $p$-values. (Earlier work only established mFDR control.) Next, we obtain conditions under which generalized alpha-investing controls FDR in the presence of general $p$-values dependencies. Finally, we develop a modified set of procedures that also allow to control the false discovery exceedance (the tail of the proportion of false discoveries). Numerical simulations and analytical results indicate that online procedures do not incur a large loss in statistical power with respect to offline approaches, such as Benjamini-Hochberg. "
770057164916400128,2016-08-29 00:35:09,https://t.co/yirElogAmn,Analyzing features learned for Offline Signature Verification using Deep CNNs. (arXiv:1607.04573v2 [cs.CV] UPDATED) https://t.co/yirElogAmn,4,6," Abstract: Research on Offline Handwritten Signature Verification explored a large variety of handcrafted feature extractors, ranging from graphology, texture descriptors to interest points. In spite of advancements in the last decades, performance of such systems is still far from optimal when we test the systems against skilled forgeries - signature forgeries that target a particular individual. In previous research, we proposed a formulation of the problem to learn features from data (signature images) in a Writer-Independent format, using Deep Convolutional Neural Networks (CNNs), seeking to improve performance on the task. In this research, we push further the performance of such method, exploring a range of architectures, and obtaining a large improvement in state-of-the-art performance on the GPDS dataset, the largest publicly available dataset on the task. In the GPDS-160 dataset, we obtained an Equal Error Rate of 2.74%, compared to 6.97% in the best result published in literature (that used a combination of multiple classifiers). We also present a visual analysis of the feature space learned by the model, and an analysis of the errors made by the classifier. Our analysis shows that the model is very effective in separating signatures that have a different global appearance, while being particularly vulnerable to forgeries that very closely resemble genuine signatures, even if their line quality is bad, which is the case of slowly-traced forgeries. "
768970604728971264,2016-08-26 00:37:33,https://t.co/n2yHDOcxen,Incremental Minimax Optimization based Fuzzy Clustering for Large Multi-view Data. (arXiv:1608.07001v1 [cs.AI]) https://t.co/n2yHDOcxen,2,5," Abstract: Incremental clustering approaches have been proposed for handling large data when given data set is too large to be stored. The key idea of these approaches is to find representatives to represent each cluster in each data chunk and final data analysis is carried out based on those identified representatives from all the chunks. However, most of the incremental approaches are used for single view data. As large multi-view data generated from multiple sources becomes prevalent nowadays, there is a need for incremental clustering approaches to handle both large and multi-view data. In this paper we propose a new incremental clustering approach called incremental minimax optimization based fuzzy clustering (IminimaxFCM) to handle large multi-view data. In IminimaxFCM, representatives with multiple views are identified to represent each cluster by integrating multiple complementary views using minimax optimization. The detailed problem formulation, updating rules derivation, and the in-depth analysis of the proposed IminimaxFCM are provided. Experimental studies on several real world multi-view data sets have been conducted. We observed that IminimaxFCM outperforms related incremental fuzzy clustering in terms of clustering accuracy, demonstrating the great potential of IminimaxFCM for large multi-view data analysis. "
768970593890893824,2016-08-26 00:37:31,https://t.co/4p6OMqkNWB,Multi-View Fuzzy Clustering with Minimax Optimization for Effective Clustering of Data from Multiple Sources. (arX… https://t.co/4p6OMqkNWB,2,5," Abstract: Multi-view data clustering refers to categorizing a data set by making good use of related information from multiple representations of the data. It becomes important nowadays because more and more data can be collected in a variety of ways, in different settings and from different sources, so each data set can be represented by different sets of features to form different views of it. Many approaches have been proposed to improve clustering performance by exploring and integrating heterogeneous information underlying different views. In this paper, we propose a new multi-view fuzzy clustering approach called MinimaxFCM by using minimax optimization based on well-known Fuzzy c means. In MinimaxFCM the consensus clustering results are generated based on minimax optimization in which the maximum disagreements of different weighted views are minimized. Moreover, the weight of each view can be learned automatically in the clustering process. In addition, there is only one parameter to be set besides the fuzzifier. The detailed problem formulation, updating rules derivation, and the in-depth analysis of the proposed MinimaxFCM are provided here. Experimental studies on nine multi-view data sets including real world image and document data sets have been conducted. We observed that MinimaxFCM outperforms related multi-view clustering approaches in terms of clustering accuracy, demonstrating the great potential of MinimaxFCM for multi-view data analysis. "
768970585527517184,2016-08-26 00:37:29,https://t.co/D06Uj5bKSb,Comparison among dimensionality reduction techniques based on Random Projection for cancer classification. (arXiv:… https://t.co/D06Uj5bKSb,1,6," Abstract: Random Projection (RP) technique has been widely applied in many scenarios because it can reduce high-dimensional features into low-dimensional space within short time and meet the need of real-time analysis of massive data. There is an urgent need of dimensionality reduction with fast increase of big genomics data. However, the performance of RP is usually lower. We attempt to improve classification accuracy of RP through combining other reduction dimension methods such as Principle Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Feature Selection (FS). We compared classification accuracy and running time of different combination methods on three microarray datasets and a simulation dataset. Experimental results show a remarkable improvement of 14.77% in classification accuracy of FS followed by RP compared to RP on BC-TCGA dataset. LDA followed by RP also helps RP to yield a more discriminative subspace with an increase of 13.65% on classification accuracy on the same dataset. FS followed by RP outperforms other combination methods in classification accuracy on most of the datasets. "
768970577780568065,2016-08-26 00:37:27,https://t.co/pxHw18W7bS,Formal Concept Analysis of Rodent Carriers of Zoonotic Disease. (arXiv:1608.07241v1 [stat.ML]) https://t.co/pxHw18W7bS,0,2," Abstract: The technique of Formal Concept Analysis is applied to a dataset describing the traits of rodents, with the goal of identifying zoonotic disease carriers,or those species carrying infections that can spillover to cause human disease. The concepts identified among these species together provide rules-of-thumb about the intrinsic biological features of rodents that carry zoonotic diseases, and offer utility for better targeting field surveillance efforts in the search for novel disease carriers in the wild. "
768970569287077888,2016-08-26 00:37:25,https://t.co/tsvGyYAAMS,Large-scale Collaborative Imaging Genetics Studies of Risk Genetic Factors for Alzheimer's Disease Across Multiple… https://t.co/tsvGyYAAMS,0,3," Abstract: Genome-wide association studies (GWAS) offer new opportunities to identify genetic risk factors for Alzheimer's disease (AD). Recently, collaborative efforts across different institutions emerged that enhance the power of many existing techniques on individual institution data. However, a major barrier to collaborative studies of GWAS is that many institutions need to preserve individual data privacy. To address this challenge, we propose a novel distributed framework, termed Local Query Model (LQM) to detect risk SNPs for AD across multiple research institutions. To accelerate the learning process, we propose a Distributed Enhanced Dual Polytope Projection (D-EDPP) screening rule to identify irrelevant features and remove them from the optimization. To the best of our knowledge, this is the first successful run of the computationally intensive model selection procedure to learn a consistent model across different institutions without compromising their privacy while ranking the SNPs that may collectively affect AD. Empirical studies are conducted on 809 subjects with 5.9 million SNP features which are distributed across three individual institutions. D-EDPP achieved a 66-fold speed-up by effectively identifying irrelevant features. "
768970559497576448,2016-08-26 00:37:22,https://t.co/ScTFfoKgIe,Community Detection and Classification in Hierarchical Stochastic Blockmodels. (arXiv:1503.02115v4 [stat.ML] UPDAT… https://t.co/ScTFfoKgIe,1,6," Abstract: We propose a robust, scalable, integrated methodology for community detection and community comparison in graphs. In our procedure, we first embed a graph into an appropriate Euclidean space to obtain a low-dimensional representation, and then cluster the vertices into communities. We next employ nonparametric graph inference techniques to identify structural similarity among these communities. These two steps are then applied recursively on the communities, allowing us to detect more fine-grained structure. We describe a hierarchical stochastic blockmodel---namely, a stochastic blockmodel with a natural hierarchical structure---and establish conditions under which our algorithm yields consistent estimates of model parameters and motifs, which we define to be stochastically similar groups of subgraphs. Finally, we demonstrate the effectiveness of our algorithm in both simulated and real data. Specifically, we address the problem of locating similar subcommunities in a partially reconstructed Drosophila connectome and in the social network Friendster. "
768970547313205248,2016-08-26 00:37:19,https://t.co/KlUo1nnw7v,Variance Reduction for Faster Non-Convex Optimization. (arXiv:1603.05643v2 [math.OC] UPDATED) https://t.co/KlUo1nnw7v,0,8," Abstract: We consider the fundamental problem in non-convex optimization of efficiently reaching a stationary point. In contrast to the convex case, in the long history of this basic problem, the only known theoretical results on first-order non-convex optimization remain to be full gradient descent that converges in $O(1/\varepsilon)$ iterations for smooth objectives, and stochastic gradient descent that converges in $O(1/\varepsilon^2)$ iterations for objectives that are sum of smooth functions. We provide the first improvement in this line of research. Our result is based on the variance reduction trick recently introduced to convex optimization, as well as a brand new analysis of variance reduction that is suitable for non-convex optimization. For objectives that are sum of smooth functions, our first-order minibatch stochastic method converges with an $O(1/\varepsilon)$ rate, and is faster than full gradient descent by $\Omega(n^{1/3})$. We demonstrate the effectiveness of our methods on empirical risk minimizations with non-convex loss functions and training neural nets. "
768970540426072065,2016-08-26 00:37:18,https://t.co/NGZIGOHlsz,The Quality of the Covariance Selection Through Detection Problem and AUC Bounds. (arXiv:1605.05776v3 [cs.IT] UPDA… https://t.co/NGZIGOHlsz,0,4," Abstract: We consider the problem of quantifying the quality of a model selection problem for a graphical model. We discuss this by formulating the problem as a detection problem. Model selection problems usually minimize a distance between the original distribution and the model distribution. For the special case of Gaussian distributions, the model selection problem simplifies to the covariance selection problem which is widely discussed in literature by Dempster [2] where the likelihood criterion is maximized or equivalently the Kullback-Leibler (KL) divergence is minimized to compute the model covariance matrix. While this solution is optimal for Gaussian distributions in the sense of the KL divergence, it is not optimal when compared with other information divergences and criteria such as Area Under the Curve (AUC). In this paper, we analytically compute upper and lower bounds for the AUC and discuss the quality of model selection problem using the AUC and its bounds as an accuracy measure in detection problem. We define the correlation approximation matrix (CAM) and show that analytical computation of the KL divergence, the AUC and its bounds only depend on the eigenvalues of CAM. We also show the relationship between the AUC, the KL divergence and the ROC curve by optimizing with respect to the ROC curve. In the examples provided, we pick tree structures as the simplest graphical models. We perform simulations on fully-connected graphs and compute the tree structured models by applying the widely used Chow-Liu algorithm [3]. Examples show that the quality of tree approximation models are not good in general based on information divergences, the AUC and its bounds when the number of nodes in the graphical model is large. We show both analytically and by simulations that the 1-AUC for the tree approximation model decays exponentially as the dimension of graphical model increases. "
768970531605454848,2016-08-26 00:37:16,https://t.co/lgN3XS8XeG,PSF : Introduction to R Package for Pattern Sequence Based Forecasting Algorithm. (arXiv:1606.05492v2 [stat.ML] UP… https://t.co/lgN3XS8XeG,0,4," Abstract: This paper discusses about an R package that implements the Pattern Sequence based Forecasting (PSF) algorithm, which was developed for univariate time series forecasting. This algorithm has been successfully applied to many different fields. The PSF algorithm consists of two major parts: clustering and prediction. The clustering part includes selection of the optimum number of clusters. It labels time series data with reference to such clusters. The prediction part includes functions like optimum window size selection for specific patterns and prediction of future values with reference to past pattern sequences. The PSF package consists of various functions to implement the PSF algorithm. It also contains a function which automates all other functions to obtain optimized prediction results. The aim of this package is to promote the PSF algorithm and to ease its implementation with minimum efforts. This paper describes all the functions in the PSF package with their syntax. It also provides a simple example of usage. Finally, the usefulness of this package is discussed by comparing it to auto.arima and ets, well-known time series forecasting functions available on CRAN repository. "
768970512521371648,2016-08-26 00:37:11,https://t.co/U69s3UYGkC,Revisiting Causality Inference In Markov Chain. (arXiv:1608.02658v2 [stat.ML] UPDATED) https://t.co/U69s3UYGkC,1,8," Abstract: Identifying causal relationships is a key premise of scientific research. Given the mass of observational data in many disciplines, new machine learning methods offer the possibility of using an empirical approach to identifying unappreciated causal relationships and to understanding causal behavior. Conventional methods of causality inference from observational data require a considerable length of time series data to capture cause and effect relationships. We believe that important causal relationships can be inferred from the composition of one-step transition rates (Markov Chains) to and from an event. Here we introduce 'Causality Inference using Composition of Transitions' (CICT), a computationally efficient method that reveals causal structure with high accuracy. We characterize the differences in causes, effects, and random events in the composition of their inputs and outputs. To demonstrate our method, we have used an administrative inpatient healthcare dataset to set up a graph network of patients transition between different diagnoses. Then we apply our method to patients transition graph, revealing deep and complex causal structure between clinical conditions. Our method is highly accurate in predicting whether a transition in a Markov chain is causal or random and performs well in identifying the direction of causality in bidirectional associations. Moreover, CICT brings in new information that enables unsupervised clustering methods to discriminate causality from randomness. Comprehensive performance analysis using C-statistics, goodness-of-fit statistics and decision analysis of predictive models, as well as comparison with the medical ground truth, validates our findings. "
768606909872111616,2016-08-25 00:32:21,https://t.co/tkztEQljmP,Kullback-Leibler Penalized Sparse Discriminant Analysis for Event-Related Potential Classification. (arXiv:1608.06… https://t.co/tkztEQljmP,0,5," Abstract: A brain computer interface (BCI) is a system which provides direct communication between the mind of a person and the outside world by using only brain activity (EEG). The event-related potential (ERP)-based BCI problem consists of a binary pattern recognition. Linear discriminant analysis (LDA) is widely used to solve this type of classification problems, but it fails when the number of features is large relative to the number of observations. In this work we propose a penalized version of the sparse discriminant analysis (SDA), called Kullback-Leibler penalized sparse discriminant analysis (KLSDA). This method inherits both the discriminative feature selection and classification properties of SDA and it also improves SDA performance through the addition of Kullback-Leibler class discrepancy information. The KLSDA method is design to automatically select the optimal regularization parameters. Numerical experiments with two real ERP-EEG datasets show that this new method outperforms standard SDA. "
768606908492185600,2016-08-25 00:32:21,https://t.co/sZEkEP9qBY,AIDE: Fast and Communication Efficient Distributed Optimization. (arXiv:1608.06879v1 [math.OC]) https://t.co/sZEkEP9qBY,0,4," Abstract: In this paper, we present two new communication-efficient methods for distributed minimization of an average of functions. The first algorithm is an inexact variant of the DANE algorithm that allows any local algorithm to return an approximate solution to a local subproblem. We show that such a strategy does not affect the theoretical guarantees of DANE significantly. In fact, our approach can be viewed as a robustification strategy since the method is substantially better behaved than DANE on data partition arising in practice. It is well known that DANE algorithm does not match the communication complexity lower bounds. To bridge this gap, we propose an accelerated variant of the first method, called AIDE, that not only matches the communication lower bounds but can also be implemented using a purely first-order oracle. Our empirical results show that AIDE is superior to other communication efficient algorithms in settings that naturally arise in machine learning applications. "
768606907158372352,2016-08-25 00:32:21,https://t.co/2cCYCSTJgD,Towards Bayesian Deep Learning: A Framework and Some Existing Methods. (arXiv:1608.06884v1 [stat.ML]) https://t.co/2cCYCSTJgD,2,16," Abstract: While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This paper proposes a general framework for Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this paper, we also discuss the relationship and differences between Bayesian deep learning and other related topics like Bayesian treatment of neural networks. "
768606905036075008,2016-08-25 00:32:20,https://t.co/cn7Y0dsnS4,Matrix Completion with Noisy Entries and Outliers. (arXiv:1503.00214v2 [stat.ML] UPDATED) https://t.co/cn7Y0dsnS4,0,3," Abstract: This paper considers the problem of matrix completion when the observed entries are noisy and contain outliers. It begins with introducing a new optimization criterion for which the recovered matrix is defined as its solution. This criterion uses the celebrated Huber function from the robust statistics literature to downweigh the effects of outliers. A practical algorithm is developed to solve the optimization involved. This algorithm is fast, straightforward to implement, and monotonic convergent. Furthermore, the proposed methodology is theoretically shown to be stable in a well defined sense. Its promising empirical performance is demonstrated via a sequence of simulation experiments, including image inpainting. "
768606903488356352,2016-08-25 00:32:20,https://t.co/EQ3Xnr1YAR,Bridging AIC and BIC: a new criterion for autoregression. (arXiv:1508.02473v4 [math.ST] UPDATED) https://t.co/EQ3Xnr1YAR,0,4," Abstract: We introduce a new criterion to determine the order of an autoregressive model fitted to time series data. It has the benefits of the two well-known model selection techniques, the Akaike information criterion and the Bayesian information criterion. When the data is generated from a finite order autoregression, the Bayesian information criterion is known to be consistent, and so is the new criterion. When the true order is infinity or suitably high with respect to the sample size, the Akaike information criterion is known to be efficient in the sense that its prediction performance is asymptotically equivalent to the best offered by the candidate models; in this case, the new criterion behaves in a similar manner. Different from the two classical criteria, the proposed criterion adaptively achieves either consistency or efficiency depending on the underlying true model. In practice where the observed time series is given without any prior information about the model specification, the proposed order selection criterion is more flexible and robust compared with classical approaches. Numerical results are presented demonstrating the adaptivity of the proposed technique when applied to various datasets. "
768606901932322816,2016-08-25 00:32:20,https://t.co/b2JdeK3B1h,An Oracle Inequality for Quasi-Bayesian Non-Negative Matrix Factorization. (arXiv:1601.01345v3 [stat.ML] UPDATED) https://t.co/b2JdeK3B1h,2,4, Abstract: The aim of this paper is to provide some theoretical understanding of Bayesian non-negative matrix factorization methods. We derive an oracle inequality for a quasi-Bayesian estimator. This result holds for a very general class of prior distributions and shows how the prior affects the rate of convergence. We illustrate our theoretical results with a short numerical study along with a discussion on existing implementations. 
768606900393021442,2016-08-25 00:32:19,https://t.co/ukBYQpnxfy,Fixed Points of Belief Propagation -- An Analysis via Polynomial Homotopy Continuation. (arXiv:1605.06451v2 [stat.… https://t.co/ukBYQpnxfy,1,5," Abstract: Belief propagation (BP) is an iterative method to perform approximate inference on arbitrary graphical models. Whether BP converges and if the solution is a unique fixed point depends on both, the structure and the parametrization of the model. To understand this dependence we are interested in finding \emph{all} fixed points. In this work, we formulate BP as a set of polynomial equations, the solutions of which correspond to the BP fixed points. We apply the numerical polynomial-homotopy-continuation (NPHC) method to solve such systems. It is commonly believed that uniqueness of BP fixed points implies convergence to this fixed point. Contrary to this conjecture, we find graphs for which BP fails to converge, even though a unique fixed point exists. Moreover, we show that this fixed point gives a good approximation of the exact marginal distribution. "
768606899277275136,2016-08-25 00:32:19,https://t.co/hNpaTpzcns,Adversarial examples in the physical world. (arXiv:1607.02533v2 [cs.CV] UPDATED) https://t.co/hNpaTpzcns,1,6," Abstract: Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera. "
768606897742213121,2016-08-25 00:32:19,https://t.co/DtvPS7KOGG,Identifiable Phenotyping using Constrained Non-Negative Matrix Factorization. (arXiv:1608.00704v2 [stat.ML] UPDATE… https://t.co/DtvPS7KOGG,1,3," Abstract: This work proposes a new algorithm for automated and simultaneous phenotyping of multiple co-occurring medical conditions, also referred as comorbidities, using clinical notes from the electronic health records (EHRs). A basic latent factor estimation technique of non-negative matrix factorization (NMF) is augmented with domain specific constraints to obtain sparse latent factors that are anchored to a fixed set of chronic conditions. The proposed anchoring mechanism ensures a one-to-one identifiable and interpretable mapping between the latent factors and the target comorbidities. Qualitative assessment of the empirical results by clinical experts suggests that the proposed model learns clinically interpretable phenotypes while being predictive of 30 day mortality. The proposed method can be readily adapted to any non-negative EHR data across various healthcare institutions. "
768606896345448450,2016-08-25 00:32:18,https://t.co/AOxUnkK4Zm,Sequence Graph Transform (SGT): A Feature Extraction Function for Sequence Data Mining. (arXiv:1608.03533v3 [stat.… https://t.co/AOxUnkK4Zm,4,8," Abstract: The ubiquitous presence of sequence data across fields such as the web, healthcare, bioinformatics, and text mining has made sequence mining a vital research area. However, sequence mining is particularly challenging because of difficulty in finding (dis)similarity/distance between sequences. This is because a distance measure between sequences is not obvious due to their unstructuredness---arbitrary strings of arbitrary length. Feature representations, such as n-grams, are often used but they either compromise on extracting both short- and long-term sequence patterns or have a high computation. We propose a new function, Sequence Graph Transform (SGT), that extracts the short- and long-term sequence features and embeds them in a finite-dimensional feature space. Importantly, SGT has low computation and can extract any amount of short- to long-term patterns without any increase in the computation, also proved theoretically in this paper. Due to this, SGT yields superior result with significantly higher accuracy and lower computation compared to the existing methods. We show it via several experimentation and SGT's real world application for clustering, classification, search and visualization as examples. "
768245897276686336,2016-08-24 00:37:49,https://t.co/PoOalbgin1,Neural networks for the prediction organic chemistry reactions. (arXiv:1608.06296v1 [physics.chem-ph]) https://t.co/PoOalbgin1,6,12," Abstract: Reaction prediction remains one of the major challenges for organic chemistry, and is a pre-requisite for efficient synthetic planning. It is desirable to develop algorithms that, like humans, ""learn"" from being exposed to examples of the application of the rules of organic chemistry. We explore the use of neural networks for predicting reaction types, using a new reaction fingerprinting method. We combine this predictor with SMARTS transformations to build a system which, given a set of reagents and re- actants, predicts the likely products. We test this method on problems from a popular organic chemistry textbook. "
768245892755447810,2016-08-24 00:37:48,https://t.co/P3lADVzxYG,LFADS - Latent Factor Analysis via Dynamical Systems. (arXiv:1608.06315v1 [cs.LG]) https://t.co/P3lADVzxYG,0,4," Abstract: Neuroscience is experiencing a data revolution in which many hundreds or thousands of neurons are recorded simultaneously. Currently, there is little consensus on how such data should be analyzed. Here we introduce LFADS (Latent Factor Analysis via Dynamical Systems), a method to infer latent dynamics from simultaneously recorded, single-trial, high-dimensional neural spiking data. LFADS is a sequential model based on a variational auto-encoder. By making a dynamical systems hypothesis regarding the generation of the observed data, LFADS reduces observed spiking to a set of low-dimensional temporal factors, per-trial initial conditions, and inferred inputs. We compare LFADS to existing methods on synthetic data and show that it significantly out-performs them in inferring neural firing rates and latent dynamics. "
768245887973875712,2016-08-24 00:37:47,https://t.co/Yl9GTkKdyK,Softplus Regressions and Convex Polytopes. (arXiv:1608.06383v1 [stat.ML]) https://t.co/Yl9GTkKdyK,0,2," Abstract: To construct flexible nonlinear predictive distributions, the paper introduces a family of softplus function based regression models that convolve, stack, or combine both operations by convolving countably infinite stacked gamma distributions, whose scales depend on the covariates. Generalizing logistic regression that uses a single hyperplane to partition the covariate space into two halves, softplus regressions employ multiple hyperplanes to construct a confined space, related to a single convex polytope defined by the intersection of multiple half-spaces or a union of multiple convex polytopes, to separate one class from the other. The gamma process is introduced to support the convolution of countably infinite (stacked) covariate-dependent gamma distributions. For Bayesian inference, Gibbs sampling derived via novel data augmentation and marginalization techniques is used to deconvolve and/or demix the highly complex nonlinear predictive distribution. Example results demonstrate that softplus regressions provide flexible nonlinear decision boundaries, achieving classification accuracies comparable to that of kernel support vector machine while requiring significant less computation for out-of-sample prediction. "
768245883490148352,2016-08-24 00:37:46,https://t.co/5cW8nV1PBE,Stability revisited: new generalisation bounds for the Leave-one-Out. (arXiv:1608.06412v1 [stat.ML]) https://t.co/5cW8nV1PBE,1,4," Abstract: The present paper provides a new generic strategy leading to non-asymptotic theoretical guarantees on the Leave-one-Out procedure applied to a broad class of learning algorithms. This strategy relies on two main ingredients: the new notion of $L^q$ stability, and the strong use of moment inequalities. $L^q$ stability extends the ongoing notion of hypothesis stability while remaining weaker than the uniform stability. It leads to new PAC exponential generalisation bounds for Leave-one-Out under mild assumptions. In the literature, such bounds are available only for uniform stable algorithms under boundedness for instance. Our generic strategy is applied to the Ridge regression algorithm as a first step. "
768245872354353152,2016-08-24 00:37:43,https://t.co/w2seOt9CQp,Approximation and inference methods for stochastic biochemical kinetics - a tutorial review. (arXiv:1608.06582v1 [… https://t.co/w2seOt9CQp,0,3," Abstract: Stochastic fluctuations of molecule numbers are ubiquitous in biological systems. Important examples include gene expression and enzymatic processes in living cells. Such systems are typically modelled as chemical reaction networks whose dynamics are governed by the Chemical Master Equation. Despite its simple structure, no analytic solutions to the Chemical Master Equation are known for most systems. Moreover, stochastic simulations are computationally expensive, making systematic analysis and statistical inference a challenging task. Consequently, significant effort has been spent in recent decades on the development of efficient approximation and inference methods. This article gives an introduction to basic modelling concepts as well as an overview of state of the art methods. First, we motivate and introduce deterministic and stochastic models for chemical networks, and give an overview of simulation and exact solution methods. Next, we discuss several approximation methods, including the chemical Langevin equation, the system size expansion, moment closure approximations, time-scale separation approximations and hybrid methods. We discuss their various properties and review recent advances and remaining challenges for these methods. We present a comparison of several of these methods by means of a numerical case study and highlight various of their respective advantages and disadvantages. Finally, we discuss the problem of inference from experimental data in the Bayesian framework and review recent methods developed the literature. In summary, this review gives a self-contained introduction to modelling, approximations and inference methods for stochastic chemical kinetics. "
768245865203064832,2016-08-24 00:37:42,https://t.co/u2SgCeHlqe,The discriminative Kalman filter for nonlinear and non-Gaussian sequential Bayesian filtering. (arXiv:1608.06622v1… https://t.co/u2SgCeHlqe,2,11," Abstract: The Kalman filter (KF) is used in a variety of applications for computing the posterior distribution of latent states in a state space model. The model requires a linear relationship between states and observations. Extensions to the Kalman filter have been proposed that incorporate linear approximations to nonlinear models, such as the extended Kalman filter (EKF) and the unscented Kalman filter (UKF). However, we argue that in cases where the dimensionality of observed variables greatly exceeds the dimensionality of state variables, a model for $p(\text{state}|\text{observation})$ proves both easier to learn and more accurate for latent space estimation. We derive and validate what we call the discriminative Kalman filter (DKF): a closed-form discriminative version of Bayesian filtering that readily incorporates off-the-shelf discriminative learning techniques. Further, we demonstrate that given mild assumptions, highly non-linear models for $p(\text{state}|\text{observation})$ can be specified. We motivate and validate on synthetic datasets and in neural decoding from non-human primates, showing substantial increases in decoding performance versus the standard Kalman filter. "
768245860576718848,2016-08-24 00:37:41,https://t.co/dxgN77AegE,Perceptron like Algorithms for Online Learning to Rank. (arXiv:1508.00842v4 [cs.LG] UPDATED) https://t.co/dxgN77AegE,0,4," Abstract: Perceptron is a classic online algorithm for learning a classification function. In this paper, we provide a novel extension of the perceptron algorithm to the learning to rank problem in information retrieval. We consider popular listwise performance measures such as Normalized Discounted Cumulative Gain (NDCG) and Average Precision (AP). A modern perspective on perceptron for classification is that it is simply an instance of online gradient descent (OGD), during mistake rounds, using the hinge loss function. Motivated by this interpretation, we propose a novel family of listwise, large margin ranking surrogates. Members of this family can be thought of as analogs of the hinge loss. Exploiting a certain self-bounding property of the proposed family, we provide a guarantee on the cumulative NDCG (or AP) induced loss incurred by our perceptron-like algorithm. We show that, if there exists a perfect oracle ranker which can correctly rank each instance in an online sequence of ranking data, with some margin, the cumulative loss of perceptron algorithm on that sequence is bounded by a constant, irrespective of the length of the sequence. This result is reminiscent of Novikoff's convergence theorem for the classification perceptron. Moreover, we prove a lower bound on the cumulative loss achievable by any deterministic algorithm, under the assumption of existence of perfect oracle ranker. The lower bound shows that our perceptron bound is not tight, and we propose another, \emph{purely online}, algorithm which achieves the lower bound. We provide empirical results on simulated and large commercial datasets to corroborate our theoretical results. "
768245854587252736,2016-08-24 00:37:39,https://t.co/tMrP0WcjdZ,Unifying Decision Trees Split Criteria Using Tsallis Entropy. (arXiv:1511.08136v5 [stat.ML] UPDATED) https://t.co/tMrP0WcjdZ,0,3," Abstract: The construction of efficient and effective decision trees remains a key topic in machine learning because of their simplicity and flexibility. A lot of heuristic algorithms have been proposed to construct near-optimal decision trees. ID3, C4.5 and CART are classical decision tree algorithms and the split criteria they used are Shannon entropy, Gain Ratio and Gini index respectively. All the split criteria seem to be independent, actually, they can be unified in a Tsallis entropy framework. Tsallis entropy is a generalization of Shannon entropy and provides a new approach to enhance decision trees' performance with an adjustable parameter $q$. In this paper, a Tsallis Entropy Criterion (TEC) algorithm is proposed to unify Shannon entropy, Gain Ratio and Gini index, which generalizes the split criteria of decision trees. More importantly, we reveal the relations between Tsallis entropy with different $q$ and other split criteria. Experimental results on UCI data sets indicate that the TEC algorithm achieves statistically significant improvement over the classical algorithms. "
768245850300710912,2016-08-24 00:37:38,https://t.co/F5Kh7g8WjH,Privacy-preserving Analysis of Correlated Data. (arXiv:1603.03977v2 [cs.LG] UPDATED) https://t.co/F5Kh7g8WjH,0,2," Abstract: Many modern machine learning applications involve sensitive correlated data, such as private information on users connected together in a social network, and measurements of physical activity of a single user across time. However, the current standard of privacy in machine learning, differential privacy, does not adequately address privacy issues in this kind of data. This work looks at a recent generalization of differential privacy, called Pufferfish, that can be used to address privacy in correlated data. The main challenge in applying Pufferfish is a lack of suitable mechanisms. In this paper, we provide a general mechanism, called the Wasserstein Mechanism, which applies to any Pufferfish framework. Since the Wasserstein Mechanism may be computationally inefficient, we provide an additional mechanism that applies to some practical cases such as physical activity measurements across time, is computationally efficient, and provides both privacy and utility. "
768245839756201986,2016-08-24 00:37:36,https://t.co/zDlFOUwzeU,Extended Gauss-Newton and Gauss-Newton-ADMM Algorithms for Low-Rank Matrix Optimization. (arXiv:1606.03358v2 [math… https://t.co/zDlFOUwzeU,0,5," Abstract: We develop a generic Gauss-Newton (GN) framework for solving a class of nonconvex optimization problems involving low-rank matrix variables. As opposed to standard Gauss-Newton method, our framework allows one to handle general smooth convex cost function via its surrogate. The main complexity-per-iteration consists of the inverse of two rank-size matrices and at most six small matrix multiplications to compute a closed form Gauss-Newton direction, and a backtracking linesearch. We show, under mild conditions, that the proposed algorithm globally and locally converges to a stationary point of the original nonconvex problem. We also show empirically that the Gauss-Newton algorithm achieves much higher accurate solutions compared to the well studied alternating direction method (ADM). Then, we specify our Gauss-Newton framework to handle the symmetric case and prove its convergence, where ADM is not applicable without lifting variables. Next, we incorporate our Gauss-Newton scheme into the alternating direction method of multipliers (ADMM) to design a GN-ADMM algorithm for solving the low-rank optimization problem. We prove that, under mild conditions and a proper choice of the penalty parameter, our GN-ADMM globally converges to a stationary point of the original problem. Finally, we apply our algorithms to solve several problems in practice such as low-rank approximation, matrix completion, robust low-rank matrix recovery, and matrix recovery in quantum tomography. The numerical experiments provide encouraging results to motivate the use of nonconvex optimization. "
768245830218285061,2016-08-24 00:37:33,https://t.co/4KKUwdZ6iS,Medical image denoising using convolutional denoising autoencoders. (arXiv:1608.04667v1 [cs.CV] CROSS LISTED) https://t.co/4KKUwdZ6iS,1,6," Abstract: Image denoising is an important pre-processing step in medical image analysis. Different algorithms have been proposed in past three decades with varying denoising performances. More recently, having outperformed all conventional methods, deep learning based models have shown a great promise. These methods are however limited for requirement of large training sample size and high computational costs. In this paper we show that using small sample size, denoising autoencoders constructed using convolutional layers can be used for efficient denoising of medical images. Heterogeneous images can be combined to boost sample size for increased denoising performance. Simplest of networks can reconstruct images with corruption levels so high that noise and signal are not differentiable to human eye. "
767888193656090624,2016-08-23 00:56:26,https://t.co/VNunLpugUN,Space-Filling Curves as a Novel Crystal Structure Representation for Machine Learning Models. (arXiv:1608.05747v1 … https://t.co/VNunLpugUN,1,5," Abstract: A fundamental problem in applying machine learning techniques for chemical problems is to find suitable representations for molecular and crystal structures. While the structure representations based on atom connectivities are prevalent for molecules, two-dimensional descriptors are not suitable for describing molecular crystals. In this work, we introduce the SFC-M family of feature representations, which are based on Morton space-filling curves, as an alternative means of representing crystal structures. Latent Semantic Indexing (LSI) was employed in a novel setting to reduce sparsity of feature representations. The quality of the SFC-M representations were assessed by using them in combination with artificial neural networks to predict Density Functional Theory (DFT) single point, Ewald summed, lattice, and many-body dispersion energies of 839 organic molecular crystal unit cells from the Cambridge Structural Database that consist of the elements C, H, N, and O. Promising initial results suggest that the SFC-M representations merit further exploration to improve its ability to predict solid-state properties of organic crystal structures "
767888192481726464,2016-08-23 00:56:26,https://t.co/wcXg8rCwxM,Solving a Mixture of Many Random Linear Equations by Tensor Decomposition and Alternating Minimization. (arXiv:160… https://t.co/wcXg8rCwxM,1,6," Abstract: We consider the problem of solving mixed random linear equations with $k$ components. This is the noiseless setting of mixed linear regression. The goal is to estimate multiple linear models from mixed samples in the case where the labels (which sample corresponds to which model) are not observed. We give a tractable algorithm for the mixed linear equation problem, and show that under some technical conditions, our algorithm is guaranteed to solve the problem exactly with sample complexity linear in the dimension, and polynomial in $k$, the number of components. Previous approaches have required either exponential dependence on $k$, or super-linear dependence on the dimension. The proposed algorithm is a combination of tensor decomposition and alternating minimization. Our analysis involves proving that the initialization provided by the tensor method allows alternating minimization, which is equivalent to EM in our setting, to converge to the global optimum at a linear rate. "
767888191026302976,2016-08-23 00:56:26,https://t.co/33dQPfy5nl,Reweighting with Boosted Decision Trees. (arXiv:1608.05806v1 [physics.data-an]) https://t.co/33dQPfy5nl,0,6," Abstract: Machine learning tools are commonly used in modern high energy physics (HEP) experiments. Different models, such as boosted decision trees (BDT) and artificial neural networks (ANN), are widely used in analyses and even in the software triggers. In most cases, these are classification models used to select the ""signal"" events from data. Monte Carlo simulated events typically take part in training of these models. While the results of the simulation are expected to be close to real data, in practical cases there is notable disagreement between simulated and observed data. In order to use available simulation in training, corrections must be introduced to generated data. One common approach is reweighting - assigning weights to the simulated events. We present a novel method of event reweighting based on boosted decision trees. The problem of checking the quality of reweighting step in analyses is also discussed. "
767888189637947394,2016-08-23 00:56:25,https://t.co/wdBcJDu9nu,The ground truth about metadata and community detection in networks. (arXiv:1608.05878v1 [cs.SI]) https://t.co/wdBcJDu9nu,0,5," Abstract: Across many scientific domains, there is common need to automatically extract a simplified view or a coarse-graining of how a complex system's components interact. This general task is called community detection in networks and is analogous to searching for clusters in independent vector data. It is common to evaluate the performance of community detection algorithms by their ability to find so-called \textit{ground truth} communities. This works well in synthetic networks with planted communities because such networks' links are formed explicitly based on the planted communities. However, there are no planted communities in real world networks. Instead, it is standard practice to treat some observed discrete-valued node attributes, or metadata, as ground truth. Here, we show that metadata are not the same as ground truth, and that treating them as such induces severe theoretical and practical problems. We prove that no algorithm can uniquely solve community detection, and we prove a general No Free Lunch theorem for community detection, which implies that no algorithm can perform better than any other across all inputs. However, node metadata still have value and a careful exploration of their relationship with network structure can yield insights of genuine worth. We illustrate this point by introducing two statistical techniques that can quantify the relationship between metadata and community structure for a broad class models. We demonstrate these techniques using both synthetic and real-world networks, and for multiple types of metadata and community structure. "
767888188308328450,2016-08-23 00:56:25,https://t.co/XGxA3T7d8h,Online Feature Selection with Group Structure Analysis. (arXiv:1608.05889v1 [cs.CV]) https://t.co/XGxA3T7d8h,0,3," Abstract: Online selection of dynamic features has attracted intensive interest in recent years. However, existing online feature selection methods evaluate features individually and ignore the underlying structure of feature stream. For instance, in image analysis, features are generated in groups which represent color, texture and other visual information. Simply breaking the group structure in feature selection may degrade performance. Motivated by this fact, we formulate the problem as an online group feature selection. The problem assumes that features are generated individually but there are group structure in the feature stream. To the best of our knowledge, this is the first time that the correlation among feature stream has been considered in the online feature selection process. To solve this problem, we develop a novel online group feature selection method named OGFS. Our proposed approach consists of two stages: online intra-group selection and online inter-group selection. In the intra-group selection, we design a criterion based on spectral analysis to select discriminative features in each group. In the inter-group selection, we utilize a linear regression model to select an optimal subset. This two-stage procedure continues until there are no more features arriving or some predefined stopping conditions are met. %Our method has been applied Finally, we apply our method to multiple tasks including image classification %, face verification and face verification. Extensive empirical studies performed on real-world and benchmark data sets demonstrate that our method outperforms other state-of-the-art online feature selection %method methods. "
767888187112947713,2016-08-23 00:56:25,https://t.co/Qtguw68m84,Validating search protocols for mining of health and disease events on Twitter. (arXiv:1608.05910v1 [stat.ML]) https://t.co/Qtguw68m84,1,5," Abstract: In the year of 2016, there were more than 24 million Indonesian twitter users sharing news, events, as well as personal feelings and experiences on Twitter. This study seeks to validate a search protocol of health-related terms using real-time Twitter data which can later be used to understand if, and how, twitter can reveal information on the current health situation in Indonesia. In this validation study of mining protocols, we extracted geo-located conversations related to health and disease postings on Twitter using a set of pre-defined keywords, assessed the prevalence, frequency and timing of such content in these conversations, and validated how this search protocol was able to detect relevant disease tweets. Groups of words and phrases relevant to disease symptoms and health outcomes were used in a protocol developed in the Indonesian language in order to extract relevant content from geo-tagged Twitter feeds. A supervised learning algorithm using Classification and Regression Trees was used to validate search protocols of disease and health hits comparing to those identified by a team of human experts. The experts categorized tweets as positive or negative in respect to health events. The model fit was evaluated based on prediction performance. We observed 390 tweets from historical Twitter feeds and 1,145,649 tweets from Twitter stream feeds during the period July 26th to August 1st, 2016. Only twitter hits with health related keywords in the Indonesian language were obtained. The accuracy of predictions of mined hits versus expert validated hits using the CART algorithm showed good validity with AUC beyond 0.8. Our study shows that monitoring of public sentiment on Twitter, combined with contextual knowledge about the disease, can detect health and disease tweets and potentially be used as a valuable real-time proxy for health events over space and time. "
767888185871523840,2016-08-23 00:56:24,https://t.co/4abFrWL13F,Probabilistic Knowledge Graph Construction: Compositional and Incremental Approaches. (arXiv:1608.05921v1 [stat.ML… https://t.co/4abFrWL13F,0,3," Abstract: Knowledge graph construction consists of two tasks: extracting information from external resources (knowledge population) and inferring missing information through a statistical analysis on the extracted information (knowledge completion). In many cases, insufficient external resources in the knowledge population hinder the subsequent statistical inference. The gap between these two processes can be reduced by an incremental population approach. We propose a new probabilistic knowledge graph factorisation method that benefits from the path structure of existing knowledge (e.g. syllogism) and enables a common modelling approach to be used for both incremental population and knowledge completion tasks. More specifically, the probabilistic formulation allows us to develop an incremental population algorithm that trades off exploitation-exploration. Experiments on three benchmark datasets show that the balanced exploitation-exploration helps the incremental population, and the additional path structure helps to predict missing information in knowledge completion. "
767888184743194625,2016-08-23 00:56:24,https://t.co/0CsyreQod0,Spatial Modeling of Oil Exploration Areas Using Neural Networks and ANFIS in GIS. (arXiv:1608.05934v1 [stat.ML]) https://t.co/0CsyreQod0,1,6," Abstract: Exploration of hydrocarbon resources is a highly complicated and expensive process where various geological, geochemical and geophysical factors are developed then combined together. It is highly significant how to design the seismic data acquisition survey and locate the exploratory wells since incorrect or imprecise locations lead to waste of time and money during the operation. The objective of this study is to locate high-potential oil and gas field in 1: 250,000 sheet of Ahwaz including 20 oil fields to reduce both time and costs in exploration and production processes. In this regard, 17 maps were developed using GIS functions for factors including: minimum and maximum of total organic carbon (TOC), yield potential for hydrocarbons production (PP), Tmax peak, production index (PI), oxygen index (OI), hydrogen index (HI) as well as presence or proximity to high residual Bouguer gravity anomalies, proximity to anticline axis and faults, topography and curvature maps obtained from Asmari Formation subsurface contours. To model and to integrate maps, this study employed artificial neural network and adaptive neuro-fuzzy inference system (ANFIS) methods. The results obtained from model validation demonstrated that the 17x10x5 neural network with R=0.8948, RMS=0.0267, and kappa=0.9079 can be trained better than other models such as ANFIS and predicts the potential areas more accurately. However, this method failed to predict some oil fields and wrongly predict some areas as potential zones. "
767888183673688065,2016-08-23 00:56:24,https://t.co/LNsz0VfTon,Deep Generative Models for Spectroscopic Analysis on Mars. (arXiv:1608.05983v1 [cs.LG]) https://t.co/LNsz0VfTon,0,6," Abstract: Hyperspectral instruments (HSIs) measure the electromagnetic energy emitted by materials at high resolution (hundreds to thousands of channels) enabling material identification through spectroscopic analysis. Laser-induced breakdown spectroscopy (LIBS) is used by the ChemCam instrument on the Curiosity rover to measure the emission spectra of surface materials on Mars. From orbit, hyperspectral instruments (HSIs) on the CRISM instrument of the Mars Reconnaissance Orbiter (MRO) measure the electromagnetic energy emitted by materials at high resolution (hundreds to thousands of channels) enabling material identification through spectroscopic analysis. The data received are noisy, high-dimensional, and largely unlabeled. The ability to accurately predict elemental and material compositions of surface samples as well as to simulate spectra from hypothetical compositions, collectively known as hyperspectral unmixing, is invaluable to the exploration process. The nature of the problem allows us to construct deep (semi-supervised) generative models to accomplish both these tasks while making use of a large unlabeled dataset. Our main technical contribution is an invertibility trick where we train our model in reverse. "
767888182314762248,2016-08-23 00:56:23,https://t.co/GqMGMGpj2c,A Non-convex One-Pass Framework for Generalized Factorization Machines and Rank-One Matrix Sensing. (arXiv:1608.05… https://t.co/GqMGMGpj2c,1,6," Abstract: We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from $d$ dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank $k$, our algorithm converges linearly, achieves $O(\epsilon)$ recovery error after retrieving $O(k^{3}d\log(1/\epsilon))$ training instances, consumes $O(kd)$ memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval. "
767888181165424646,2016-08-23 00:56:23,https://t.co/gWBkkcF298,Feedback-Controlled Sequential Lasso Screening. (arXiv:1608.06010v1 [cs.LG]) https://t.co/gWBkkcF298,0,4," Abstract: One way to solve lasso problems when the dictionary does not fit into available memory is to first screen the dictionary to remove unneeded features. Prior research has shown that sequential screening methods offer the greatest promise in this endeavor. Most existing work on sequential screening targets the context of tuning parameter selection, where one screens and solves a sequence of $N$ lasso problems with a fixed grid of geometrically spaced regularization parameters. In contrast, we focus on the scenario where a target regularization parameter has already been chosen via cross-validated model selection, and we then need to solve many lasso instances using this fixed value. In this context, we propose and explore a feedback controlled sequential screening scheme. Feedback is used at each iteration to select the next problem to be solved. This allows the sequence of problems to be adapted to the instance presented and the number of intermediate problems to be automatically selected. We demonstrate our feedback scheme using several datasets including a dictionary of approximate size 100,000 by 300,000. "
767888180024598528,2016-08-23 00:56:23,https://t.co/VPZlBBjOWg,The Symmetry of a Simple Optimization Problem in Lasso Screening. (arXiv:1608.06014v1 [cs.LG]) https://t.co/VPZlBBjOWg,1,4," Abstract: Recently dictionary screening has been proposed as an effective way to improve the computational efficiency of solving the lasso problem, which is one of the most commonly used method for learning sparse representations. To address today's ever increasing large dataset, effective screening relies on a tight region bound on the solution to the dual lasso. Typical region bounds are in the form of an intersection of a sphere and multiple half spaces. One way to tighten the region bound is using more half spaces, which however, adds to the overhead of solving the high dimensional optimization problem in lasso screening. This paper reveals the interesting property that the optimization problem only depends on the projection of features onto the subspace spanned by the normals of the half spaces. This property converts an optimization problem in high dimension to much lower dimension, and thus sheds light on reducing the computation overhead of lasso screening based on tighter region bounds. "
767888178980200449,2016-08-23 00:56:23,https://t.co/Ya5YYafg25,Towards Instance Optimal Bounds for Best Arm Identification. (arXiv:1608.06031v1 [cs.LG]) https://t.co/Ya5YYafg25,0,4," Abstract: In the best arm identification (Best-1-Arm) problem, we are given $n$ stochastic bandit arms, each associated with a reward distribution with an unknown mean. We would like to identify the arm with the largest mean with probability $1-\delta$, using as few samples as possible. Understanding the sample complexity of Best-1-Arm has attracted significant attention since the last decade. However, the optimal sample complexity is still unknown. Recently, Chen and Li made an interesting conjecture, called gap-entropy conjecture, concerning the instance optimal sample complexity of Best-1-Arm. Given a Best-1-Arm instance, let $\mu_{[i]}$ denote the $i$th largest mean and $\Delta_{[i]}=\mu_{[1]}-\mu_{[i]}$ denote the corresponding gap. $H(I)=\sum_{i=2}^{n}\Delta_{[i]}^{-2}$ denotes the complexity of the instance. The gap-entropy conjecture states that for any instance $I$, $\Omega(H(I)\cdot(\log\delta^{-1}+\mathsf{Ent}(I)))$ is an instance lower bound, where $\mathsf{Ent}(I)$ is an entropy-like term completely determined by $\Delta_{[i]}$s, and there is a $\delta$-correct algorithm for Best-1-Arm with sample complexity $O(H(I)\cdot(\log\delta^{-1}+\mathsf{Ent}(I))+\Delta_{[2]}^{-2}\log\log\Delta_{[2]}^{-1})$. If the conjecture is true, we would have a complete understanding of the instance-wise sample complexity of Best-1-Arm. We make significant progress towards the resolution of the gap-entropy conjecture. For the upper bound, we provide a highly nontrivial $\delta$-correct algorithm which requires $$O(H(I)\cdot(\log\delta^{-1}+\mathsf{Ent}(I))+\Delta_{[2]}^{-2}\log\log \Delta_{[2]}^{-1}\operatorname*{polylog}(n,\delta^{-1}))$$ samples. For the lower bound, we show that for any Best-1-Arm instance with all gaps of the form $2^{-k}$, any $\delta$-correct monotone algorithm requires at least $\Omega(H(I)\cdot(\log\delta^{-1}+\mathsf{Ent}(I)))$ samples in expectation. "
767888177759711236,2016-08-23 00:56:22,https://t.co/UG7j3dsajr,Survey of resampling techniques for improving classification performance in unbalanced datasets. (arXiv:1608.06048… https://t.co/UG7j3dsajr,2,9," Abstract: A number of classification problems need to deal with data imbalance between classes. Often it is desired to have a high recall on the minority class while maintaining a high precision on the majority class. In this paper, we review a number of resampling techniques proposed in literature to handle unbalanced datasets and study their effect on classification performance. "
767888176459444225,2016-08-23 00:56:22,https://t.co/q0wPQYtKx9,"Uniform Generalization, Concentration, and Adaptive Learning. (arXiv:1608.06072v1 [cs.LG]) https://t.co/q0wPQYtKx9",1,6," Abstract: One fundamental goal in any learning algorithm is to mitigate its risk for overfitting. Mathematically, this requires that the learning algorithm enjoys a small generalization risk, which is defined either in expectation or in probability. Both types of generalization are commonly used in the literature. For instance, generalization in expectation has been used to analyze algorithms, such as ridge regression and SGD, whereas generalization in probability is used in the VC theory, among others. Recently, a third notion of generalization has been studied, called uniform generalization, which requires that the generalization risk vanishes uniformly in expectation across all bounded parametric losses. It has been shown that uniform generalization is, in fact, equivalent to an information-theoretic stability constraint, and that it recovers classical results in learning theory. It is achievable under various settings, such as sample compression schemes, finite hypothesis spaces, finite domains, and differential privacy. However, the relationship between uniform generalization and concentration remained unknown. In this paper, we answer this question by proving that, while a generalization in expectation does not imply a generalization in probability, a uniform generalization in expectation does imply concentration. We establish a chain rule for the uniform generalization risk of the composition of hypotheses and use it to derive a large deviation bound. Finally, we prove that the bound is tight. "
767888175146659842,2016-08-23 00:56:22,https://t.co/zIW41EaM45,Computational and Statistical Tradeoffs in Learning to Rank. (arXiv:1608.06203v1 [cs.LG]) https://t.co/zIW41EaM45,2,6," Abstract: For massive and heterogeneous modern datasets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of learning to rank, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data. "
767888173838000131,2016-08-23 00:56:21,https://t.co/VeDIKattXx,Single-shot Adaptive Measurement for Quantum-enhanced Metrology. (arXiv:1608.06238v1 [quant-ph]) https://t.co/VeDIKattXx,1,3," Abstract: Quantum-enhanced metrology aims to estimate an unknown parameter such that the precision scales better than the shot-noise bound. Single-shot adaptive quantum-enhanced metrology (AQEM) is a promising approach that uses feedback to tweak the quantum process according to previous measurement outcomes. Techniques and formalism for the adaptive case are quite different from the usual non-adaptive quantum metrology approach due to the causal relationship between measurements and outcomes. We construct a formal framework for AQEM by modeling the procedure as a decision-making process, and we derive the imprecision and the Cram\'{e}r-Rao lower bound with explicit dependence on the feedback policy. We also explain the reinforcement learning approach for generating quantum control policies, which is adopted due to the optimal policy being non-trivial to devise. Applying a learning algorithm based on differential evolution enables us to attain imprecision for adaptive interferometric phase estimation, which turns out to be SQL when non-entangled particles are used in the scheme. "
767888172743262208,2016-08-23 00:56:21,https://t.co/iHeuKpULBn,Multi-Dueling Bandits and Their Application to Online Ranker Evaluation. (arXiv:1608.06253v1 [cs.IR]) https://t.co/iHeuKpULBn,0,4," Abstract: New ranking algorithms are continually being developed and refined, necessitating the development of efficient methods for evaluating these rankers. Online ranker evaluation focuses on the challenge of efficiently determining, from implicit user feedback, which ranker out of a finite set of rankers is the best. Online ranker evaluation can be modeled by dueling ban- dits, a mathematical model for online learning under limited feedback from pairwise comparisons. Comparisons of pairs of rankers is performed by interleaving their result sets and examining which documents users click on. The dueling bandits model addresses the key issue of which pair of rankers to compare at each iteration, thereby providing a solution to the exploration-exploitation trade-off. Recently, methods for simultaneously comparing more than two rankers have been developed. However, the question of which rankers to compare at each iteration was left open. We address this question by proposing a generalization of the dueling bandits model that uses simultaneous comparisons of an unrestricted number of rankers. We evaluate our algorithm on synthetic data and several standard large-scale online ranker evaluation datasets. Our experimental results show that the algorithm yields orders of magnitude improvement in performance compared to stateof- the-art dueling bandit algorithms. "
767888171258507264,2016-08-23 00:56:21,https://t.co/L7wwXX606q,Screening Tests for Lasso Problems. (arXiv:1405.4897v2 [cs.LG] UPDATED) https://t.co/L7wwXX606q,1,7," Abstract: This paper is a survey of dictionary screening for the lasso problem. The lasso problem seeks a sparse linear combination of the columns of a dictionary to best match a given target vector. This sparse representation has proven useful in a variety of subsequent processing and decision tasks. For a given target vector, dictionary screening quickly identifies a subset of dictionary columns that will receive zero weight in a solution of the corresponding lasso problem. These columns can be removed from the dictionary prior to solving the lasso problem without impacting the optimality of the solution obtained. This has two potential advantages: it reduces the size of the dictionary, allowing the lasso problem to be solved with less resources, and it may speed up obtaining a solution. Using a geometrically intuitive framework, we provide basic insights for understanding useful lasso screening tests and their limitations. We also provide illustrative numerical studies on several datasets. "
767883192900739072,2016-08-23 00:36:34,https://t.co/zQMtpXbUH5,Linear Readout of Object Manifolds. (arXiv:1512.01834v2 [cond-mat.dis-nn] UPDATED) https://t.co/zQMtpXbUH5,0,4," Abstract: Objects are represented in sensory systems by continuous manifolds due to sensitivity of neuronal responses to changes in physical features such as location, orientation, and intensity. What makes certain sensory representations better suited for invariant decoding of objects by downstream networks? We present a theory that characterizes the ability of a linear readout network, the perceptron, to classify objects from variable neural responses. We show how the readout perceptron capacity depends on the dimensionality, size, and shape of the object manifolds in its input neural representation. "
767883191852077060,2016-08-23 00:36:34,https://t.co/aO1BsFq4Yl,Private Causal Inference. (arXiv:1512.05469v2 [stat.ML] UPDATED) https://t.co/aO1BsFq4Yl,1,6," Abstract: Causal inference deals with identifying which random variables ""cause"" or control other random variables. Recent advances on the topic of causal inference based on tools from statistical estimation and machine learning have resulted in practical algorithms for causal inference. Causal inference has the potential to have significant impact on medical research, prevention and control of diseases, and identifying factors that impact economic changes to name just a few. However, these promising applications for causal inference are often ones that involve sensitive or personal data of users that need to be kept private (e.g., medical records, personal finances, etc). Therefore, there is a need for the development of causal inference methods that preserve data privacy. We study the problem of inferring causality using the current, popular causal inference framework, the additive noise model (ANM) while simultaneously ensuring privacy of the users. Our framework provides differential privacy guarantees for a variety of ANM variants. We run extensive experiments, and demonstrate that our techniques are practical and easy to implement. "
767883190828724224,2016-08-23 00:36:33,https://t.co/Jrfxxm4jCI,Cox process representation and inference for stochastic reaction-diffusion processes. (arXiv:1601.01972v2 [cond-ma… https://t.co/Jrfxxm4jCI,0,3," Abstract: Complex behaviour in many systems arises from the stochastic interactions of spatially distributed particles or agents. Stochastic reaction-diffusion processes are widely used to model such behaviour in disciplines ranging from biology to the social sciences, yet they are notoriously difficult to simulate and calibrate to observational data. Here we use ideas from statistical physics and machine learning to provide a solution to the inverse problem of learning a stochastic reaction-diffusion process from data. Our solution relies on a non-trivial connection between stochastic reaction-diffusion processes and spatio-temporal Cox processes, a well-studied class of models from computational statistics. This connection leads to an efficient and flexible algorithm for parameter inference and model selection. Our approach shows excellent accuracy on numeric and real data examples from systems biology and epidemiology. Our work provides both insights into spatio-temporal stochastic systems, and a practical solution to a long-standing problem in computational modelling. "
767883189406883840,2016-08-23 00:36:33,https://t.co/URHhK9guUt,Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. (arXiv:1603.05953v4 [math.OC] UPDATED) https://t.co/URHhK9guUt,1,9," Abstract: We introduce $\mathtt{Katyusha}$, the first direct, primal-only stochastic gradient method that has a provably accelerated convergence rate in convex optimization. In contrast, previous methods are based on dual coordinate descent which are more restrictive, or based on outer-inner loops which make them ""blind"" to the underlying stochastic nature of the optimization process. $\mathtt{Katyusha}$ is the first algorithm that incorporates acceleration directly into stochastic gradient updates. Unlike previous results, $\mathtt{Katyusha}$ obtains an optimal convergence rate. It also supports proximal updates, non-Euclidean norm smoothness, non-uniform sampling, and mini-batch sampling. When applied to interesting classes of convex objectives, including smooth objectives (e.g., Lasso, Logistic Regression), strongly-convex objectives (e.g., SVM), and non-smooth objectives (e.g., L1SVM), $\mathtt{Katyusha}$ improves the best known convergence rates. The main ingredient behind our result is $\textit{Katyusha momentum}$, a novel ""negative momentum on top of momentum"" that can be incorporated into a variance-reduction based algorithm and speed it up. As a result, since variance reduction has been successfully applied to a fast growing list of practical problems, our paper suggests that in each of such cases, one had better hurry up and give Katyusha a hug. "
767883188333150210,2016-08-23 00:36:33,https://t.co/MBNDvdeXAx,Composing graphical models with neural networks for structured representations and fast inference. (arXiv:1603.062… https://t.co/MBNDvdeXAx,1,12," Abstract: We propose a general modeling and inference framework that composes probabilistic graphical models with deep learning methods and combines their respective strengths. Our model family augments graphical structure in latent variables with neural network observation models. For inference, we extend variational autoencoders to use graphical model approximating distributions with recognition networks that output conjugate potentials. All components of these models are learned simultaneously with a single objective, giving a scalable algorithm that leverages stochastic variational inference, natural gradients, graphical model message passing, and the reparameterization trick. We illustrate this framework with several example models and an application to mouse behavioral phenotyping. "
767883187179708416,2016-08-23 00:36:33,https://t.co/UhwcvvQtH0,The Matrix Generalized Inverse Gaussian Distribution: Properties and Applications. (arXiv:1604.03463v2 [stat.ML] U… https://t.co/UhwcvvQtH0,1,5," Abstract: While the Matrix Generalized Inverse Gaussian ($\mathcal{MGIG}$) distribution arises naturally in some settings as a distribution over symmetric positive semi-definite matrices, certain key properties of the distribution and effective ways of sampling from the distribution have not been carefully studied. In this paper, we show that the $\mathcal{MGIG}$ is unimodal, and the mode can be obtained by solving an Algebraic Riccati Equation (ARE) equation [7]. Based on the property, we propose an importance sampling method for the $\mathcal{MGIG}$ where the mode of the proposal distribution matches that of the target. The proposed sampling method is more efficient than existing approaches [32, 33], which use proposal distributions that may have the mode far from the $\mathcal{MGIG}$'s mode. Further, we illustrate that the the posterior distribution in latent factor models, such as probabilistic matrix factorization (PMF) [25], when marginalized over one latent factor has the $\mathcal{MGIG}$ distribution. The characterization leads to a novel Collapsed Monte Carlo (CMC) inference algorithm for such latent factor models. We illustrate that CMC has a lower log loss or perplexity than MCMC, and needs fewer samples. "
767883185904549888,2016-08-23 00:36:32,https://t.co/ScoVIVE5KI,Algorithms for stochastic optimization with expectation constraints. (arXiv:1604.03887v2 [math.OC] UPDATED) https://t.co/ScoVIVE5KI,0,7," Abstract: This paper considers the problem of minimizing an expectation function over a closed convex set, coupled with an expectation constraint on either decision variables or problem parameters. We first present a new stochastic approximation (SA) type algorithm, namely the corporative SA (CSA), to handle problems with the expectation constraint on devision variables. We show that this algorithm exhibits the optimal ${\cal O}(1/\sqrt{N})$ rate of convergence, in terms of both optimality gap and constraint violation, when the objective and constraint functions are generally convex, where $N$ denotes the number of iterations. Moreover, we show that this rate of convergence can be improved to ${\cal O}(1/N)$ if the objective and constraint functions are strongly convex. We then present a variant of CSA, namely the corporative stochastic parameter approximation (CSPA) algorithm, to deal with the situation when the expectation constraint is defined over problem parameters and show that it exhibits similar optimal rate of convergence to CSA. It is worth noting that CSA and CSPA are primal methods which do not require the iterations on the dual space and/or the estimation on the size of the dual variables. To the best of our knowledge, this is the first time that such optimal SA methods for solving expectation constrained stochastic optimization are presented in the literature. "
767883184314912768,2016-08-23 00:36:32,https://t.co/fg2dnsh0j0,Piece-wise quadratic approximations of arbitrary error functions for fast and robust machine learning. (arXiv:1605… https://t.co/fg2dnsh0j0,0,3," Abstract: Most of machine learning approaches have stemmed from the application of minimizing the mean squared distance principle, based on the computationally efficient quadratic optimization methods. However, when faced with high-dimensional and noisy data, the quadratic error functionals demonstrated many weaknesses including high sensitivity to contaminating factors and dimensionality curse. Therefore, a lot of recent applications in machine learning exploited properties of non-quadratic error functionals based on $L_1$ norm or even sub-linear potentials corresponding to quasinorms $L_p$ ($0<p<1$). The back side of these approaches is increase in computational cost for optimization. Till so far, no approaches have been suggested to deal with {\it arbitrary} error functionals, in a flexible and computationally efficient framework. In this paper, we develop a theory and basic universal data approximation algorithms ($k$-means, principal components, principal manifolds and graphs, regularized and sparse regression), based on piece-wise quadratic error potentials of subquadratic growth (PQSQ potentials). We develop a new and universal framework to minimize {\it arbitrary sub-quadratic error potentials} using an algorithm with guaranteed fast convergence to the local or global error minimum. The theory of PQSQ potentials is based on the notion of the cone of minorant functions, and represents a natural approximation formalism based on the application of min-plus algebra. The approach can be applied in most of existing machine learning methods, including methods of data approximation and regularized and sparse regression, leading to the improvement in the computational cost/accuracy trade-off. We demonstrate that on synthetic and real-life datasets PQSQ-based machine learning methods achieve orders of magnitude faster computational performance than the corresponding state-of-the-art methods. "
767883182951784448,2016-08-23 00:36:32,https://t.co/heHgOyZlxS,An Information Criterion for Inferring Coupling in Complex Networks. (arXiv:1605.06931v2 [cs.LG] UPDATED) https://t.co/heHgOyZlxS,0,3," Abstract: The behaviour of many real-world phenomena can be modelled by nonlinear dynamical systems whereby a latent system state is observed through a filter. We are interested in interacting subsystems of this form, which we model by a set of coupled maps as a synchronous update graph dynamical systems. Specifically, we study the structure learning problem for spatially distributed dynamical systems coupled via a directed acyclic graph. Unlike established structure learning procedures that find locally maximum posterior probabilities of a network structure containing latent variables, our work exploits the properties of dynamical systems to compute globally optimal approximations of these distributions. We arrive at this result by the use of time delay embedding theorems. Taking an information-theoretic perspective, we show that the log-likelihood has an intuitive interpretation in terms of information transfer. "
767519673588019200,2016-08-22 00:32:04,https://t.co/5f3ESBWnne,Network Volume Anomaly Detection and Identification in Large-scale Networks based on Online Time-structured Traffi… https://t.co/5f3ESBWnne,4,7," Abstract: This paper addresses network anomography, that is, the problem of inferring network-level anomalies from indirect link measurements. This problem is cast as a low-rank subspace tracking problem for normal flows under incomplete observations, and an outlier detection problem for abnormal flows. Since traffic data is large-scale time-structured data accompanied with noise and outliers under partial observations, an efficient modeling method is essential. To this end, this paper proposes an online subspace tracking of a Hankelized time-structured traffic tensor for normal flows based on the Candecomp/PARAFAC decomposition exploiting the recursive least squares (RLS) algorithm. We estimate abnormal flows as outlier sparse flows via sparsity maximization in the underlying under-constrained linear-inverse problem. A major advantage is that our algorithm estimates normal flows by low-dimensional matrices with time-directional features as well as the spatial correlation of multiple links without using the past observed measurements and the past model parameters. Extensive numerical evaluations show that the proposed algorithm achieves faster convergence per iteration of model approximation, and better volume anomaly detection performance compared to state-of-the-art algorithms. "
767519671188946944,2016-08-22 00:32:04,https://t.co/TxaVKWPhMu,Iterative Views Agreement: An Iterative Low-Rank based Structured Optimization Method to Multi-View Spectral Clust… https://t.co/TxaVKWPhMu,0,4," Abstract: Multi-view spectral clustering, which aims at yielding an agreement or consensus data objects grouping across multi-views with their graph laplacian matrices, is a fundamental clustering problem. Among the existing methods, Low-Rank Representation (LRR) based method is quite superior in terms of its effectiveness, intuitiveness and robustness to noise corruptions. However, it aggressively tries to learn a common low-dimensional subspace for multi-view data, while inattentively ignoring the local manifold structure in each view, which is critically important to the spectral clustering; worse still, the low-rank minimization is enforced to achieve the data correlation consensus among all views, failing to flexibly preserve the local manifold structure for each view. In this paper, 1) we propose a multi-graph laplacian regularized LRR with each graph laplacian corresponding to one view to characterize its local manifold structure. 2) Instead of directly enforcing the low-rank minimization among all views for correlation consensus, we separately impose low-rank constraint on each view, coupled with a mutual structural consensus constraint, where it is able to not only well preserve the local manifold structure but also serve as a constraint for that from other views, which iteratively makes the views more agreeable. Extensive experiments on real-world multi-view data sets demonstrate its superiority. "
767519668047339520,2016-08-22 00:32:03,https://t.co/MnNalgVd0s,Unsupervised Feature Selection Based on the Morisita Estimator of Intrinsic Dimension. (arXiv:1608.05581v1 [stat.M… https://t.co/MnNalgVd0s,4,9," Abstract: This paper deals with a new filter algorithm for selecting the smallest subset of features carrying all the information content of a data set (i.e. for removing redundant features). It is a new version of the fractal dimension reduction algorithm following a sequential forward search strategy and it relies on the recently introduced Morisita estimator of Intrinsic Dimension (ID). Here, the ID is used to quantify dependencies between subsets of features, which allows the effective processing of highly non-linear data. The proposed algorithm is successfully tested on simulated and real world case studies. Different levels of sample size and noise are examined along with the variability of the results. In addition, a comprehensive procedure based on random forests shows that the data dimensionality is significantly reduced by the algorithm without information loss. "
767519664591208448,2016-08-22 00:32:02,https://t.co/XznqnZ0sR2,PAC-Bayesian Aggregation without Cross-Validation. (arXiv:1608.05610v1 [cs.LG]) https://t.co/XznqnZ0sR2,1,10," Abstract: We propose a new PAC-Bayesian procedure for aggregating prediction models and a new way of constructing a hypothesis space with which the procedure works particularly well. The procedure is based on alternating minimization of a new PAC-Bayesian bound, which is convex in the posterior distribution used for aggregation and also convex in a trade-off parameter between empirical performance of the distribution and its complexity, measured by the Kullback-Leibler divergence to a prior. The hypothesis space is constructed by training a finite number of weak classifiers, where each classifier is trained on a small subsample of the data and validated on the corresponding complementary subset of the data. The weak classifiers are then weighted with respect to their validation performance through minimization of the PAC-Bayesian bound. We provide experimental results demonstrating that the proposed aggregation strategy is on par with the prediction accuracy of kernel SVMs tuned by cross-validation. The comparable accuracy is achieved at a much lower computation cost, since training many SVMs on small subsamples is significantly cheaper than training one SVM on the whole data due to super-quadratic training time of kernel SVMs. Remarkably, our prediction approach is based on minimization of a theoretical bound and does not require parameter cross-validation, as opposed to the majority of theoretical results that cannot be rigorously applied in practice. "
767519661562929152,2016-08-22 00:32:01,https://t.co/LB9fVxpfTv,High-dimensional Mixed Graphical Models. (arXiv:1304.2810v3 [stat.ML] UPDATED) https://t.co/LB9fVxpfTv,2,11," Abstract: While graphical models for continuous data (Gaussian graphical models) and discrete data (Ising models) have been extensively studied, there is little work on graphical models linking both continuous and discrete variables (mixed data), which are common in many scientific applications. We propose a novel graphical model for mixed data, which is simple enough to be suitable for high-dimensional data, yet flexible enough to represent all possible graph structures. We develop a computationally efficient regression-based algorithm for fitting the model by focusing on the conditional log-likelihood of each variable given the rest. The parameters have a natural group structure, and sparsity in the fitted graph is attained by incorporating a group lasso penalty, approximated by a weighted $\ell_1$ penalty for computational efficiency. We demonstrate the effectiveness of our method through an extensive simulation study and apply it to a music annotation data set (CAL500), obtaining a sparse and interpretable graphical model relating the continuous features of the audio signal to categorical variables such as genre, emotions, and usage associated with particular songs. While we focus on binary discrete variables, we also show that the proposed methodology can be easily extended to general discrete variables. "
767519658337562624,2016-08-22 00:32:01,https://t.co/24rjTKvUEJ,String and Membrane Gaussian Processes. (arXiv:1507.06977v4 [stat.ML] UPDATED) https://t.co/24rjTKvUEJ,1,6," Abstract: In this paper we introduce a novel framework for making exact nonparametric Bayesian inference on latent functions, that is particularly suitable for Big Data tasks. Firstly, we introduce a class of stochastic processes we refer to as string Gaussian processes (string GPs), which are not to be mistaken for Gaussian processes operating on text. We construct string GPs so that their finite-dimensional marginals exhibit suitable local conditional independence structures, which allow for scalable, distributed, and flexible nonparametric Bayesian inference, without resorting to approximations, and while ensuring some mild global regularity constraints. Furthermore, string GP priors naturally cope with heterogeneous input data, and the gradient of the learned latent function is readily available for explanatory analysis. Secondly, we provide some theoretical results relating our approach to the standard GP paradigm. In particular, we prove that some string GPs are Gaussian processes, which provides a complementary global perspective on our framework. Finally, we derive a scalable and distributed MCMC scheme for supervised learning tasks under string GP priors. The proposed MCMC scheme has computational time complexity $\mathcal{O}(N)$ and memory requirement $\mathcal{O}(dN)$, where $N$ is the data size and $d$ the dimension of the input space. We illustrate the efficacy of the proposed approach on several synthetic and real-world datasets, including a dataset with $6$ millions input points and $8$ attributes. "
767519655862865920,2016-08-22 00:32:00,https://t.co/IVya4dsDYn,Bayesian Networks for Variable Groups. (arXiv:1508.07753v2 [stat.ML] UPDATED) https://t.co/IVya4dsDYn,2,10," Abstract: Bayesian networks, and especially their structures, are powerful tools for representing conditional independencies and dependencies between random variables. In applications where related variables form a priori known groups, chosen to represent different ""views"" to or aspects of the same entities, one may be more interested in modeling dependencies between groups of variables rather than between individual variables. Motivated by this, we study prospects of representing relationships between variable groups using Bayesian network structures. We show that for dependency structures between groups to be expressible exactly, the data have to satisfy the so-called groupwise faithfulness assumption. We also show that one cannot learn causal relations between groups using only groupwise conditional independencies, but also variable-wise relations are needed. Additionally, we present algorithms for finding the groupwise dependency structures. "
767519653937709056,2016-08-22 00:31:59,https://t.co/8KVVvOmdW5,Fast k-NN search. (arXiv:1509.06957v2 [stat.ML] UPDATED) https://t.co/8KVVvOmdW5,2,6," Abstract: Efficient index structures for fast approximate nearest neighbor queries are required in many applications such as recommendation systems. In high-dimensional spaces, many conventional methods suffer from excessive usage of memory and slow response times. We propose a method where multiple random projection trees are combined by a novel voting scheme. The key idea is to exploit the redundancy in a large number of candidate sets obtained by independently generated random projections in order to reduce the number of expensive exact distance evaluations. The method is straightforward to implement using sparse projections which leads to a reduced memory footprint and fast index construction. Furthermore, it enables grouping of the required computations into big matrix multiplications, which leads to additional savings due to cache effects and low-level parallelization. We demonstrate by extensive experiments on a wide variety of data sets that the method is faster than existing partitioning tree or hashing based approaches, making it the fastest available technique on high accuracy levels. "
767519651899334656,2016-08-22 00:31:59,https://t.co/FQ4aHc0gJf,Estimation of the sample covariance matrix from compressive measurements. (arXiv:1512.08887v2 [stat.ML] UPDATED) https://t.co/FQ4aHc0gJf,2,5," Abstract: This paper focuses on the estimation of the sample covariance matrix from low-dimensional random projections of data known as compressive measurements. In particular, we present an unbiased estimator to extract the covariance structure from compressive measurements obtained by a general class of random projection matrices consisting of i.i.d. zero-mean entries and finite first four moments. In contrast to previous works, we make no structural assumptions about the underlying covariance matrix such as being low-rank. In fact, our analysis is based on a non-Bayesian data setting which requires no distributional assumptions on the set of data samples. Furthermore, inspired by the generality of the projection matrices, we propose an approach to covariance estimation that utilizes sparse Rademacher matrices. Therefore, our algorithm can be used to estimate the covariance matrix in applications with limited memory and computation power at the acquisition devices. Experimental results demonstrate that our approach allows for accurate estimation of the sample covariance matrix on several real-world data sets, including video data. "
766433602779377664,2016-08-19 00:36:25,https://t.co/dLmtPT90Fw,Efficient Exploration for Dialog Policy Learning with Deep BBQ Networks \&amp; Replay Buffer Spiking. (arXiv:1608.0508… https://t.co/dLmtPT90Fw,0,5," Abstract: When rewards are sparse and action spaces large, Q-learning with $\epsilon$-greedy exploration can be inefficient. This poses problems for otherwise promising applications such as task-oriented dialogue systems, where the primary reward signal, indicating successful completion of a task, requires a complex sequence of appropriate actions. Under these circumstances, a randomly exploring agent might never stumble upon a successful outcome in reasonable time. We present two techniques that significantly improve the efficiency of exploration for deep Q-learning agents in dialogue systems. First, we introduce an exploration technique based on Thompson sampling, drawing Monte Carlo samples from a Bayes-by-backprop neural network, demonstrating marked improvement over common approaches such as $\epsilon$-greedy and Boltzmann exploration. Second, we show that spiking the replay buffer with experiences from a small number of successful episodes, as are easy to harvest for dialogue tasks, can make Q-learning feasible when it might otherwise fail. "
766433601181327361,2016-08-19 00:36:24,https://t.co/GK6tYxfbbX,A Bayesian Network approach to County-Level Corn Yield Prediction using historical data and expert knowledge. (arX… https://t.co/GK6tYxfbbX,0,6," Abstract: Crop yield forecasting is the methodology of predicting crop yields prior to harvest. The availability of accurate yield prediction frameworks have enormous implications from multiple standpoints, including impact on the crop commodity futures markets, formulation of agricultural policy, as well as crop insurance rating. The focus of this work is to construct a corn yield predictor at the county scale. Corn yield (forecasting) depends on a complex, interconnected set of variables that include economic, agricultural, management and meteorological factors. Conventional forecasting is either knowledge-based computer programs (that simulate plant-weather-soil-management interactions) coupled with targeted surveys or statistical model based. The former is limited by the need for painstaking calibration, while the latter is limited to univariate analysis or similar simplifying assumptions that fail to capture the complex interdependencies affecting yield. In this paper, we propose a data-driven approach that is ""gray box"" i.e. that seamlessly utilizes expert knowledge in constructing a statistical network model for corn yield forecasting. Our multivariate gray box model is developed on Bayesian network analysis to build a Directed Acyclic Graph (DAG) between predictors and yield. Starting from a complete graph connecting various carefully chosen variables and yield, expert knowledge is used to prune or strengthen edges connecting variables. Subsequently the structure (connectivity and edge weights) of the DAG that maximizes the likelihood of observing the training data is identified via optimization. We curated an extensive set of historical data (1948-2012) for each of the 99 counties in Iowa as data to train the model. "
766433599088459776,2016-08-19 00:36:24,https://t.co/KQYhgxyuwp,Hybrid CPU-GPU Framework for Network Motifs. (arXiv:1608.05138v1 [cs.DC]) https://t.co/KQYhgxyuwp,0,3," Abstract: Massively parallel architectures such as the GPU are becoming increasingly important due to the recent proliferation of data. In this paper, we propose a key class of hybrid parallel graphlet algorithms that leverages multiple CPUs and GPUs simultaneously for computing k-vertex induced subgraph statistics (called graphlets). In addition to the hybrid multi-core CPU-GPU framework, we also investigate single GPU methods (using multiple cores) and multi-GPU methods that leverage all available GPUs simultaneously for computing induced subgraph statistics. Both methods leverage GPU devices only, whereas the hybrid multi-core CPU-GPU framework leverages all available multi-core CPUs and multiple GPUs for computing graphlets in large networks. Compared to recent approaches, our methods are orders of magnitude faster, while also more cost effective enjoying superior performance per capita and per watt. In particular, the methods are up to 300 times faster than the recent state-of-the-art method. To the best of our knowledge, this is the first work to leverage multiple CPUs and GPUs simultaneously for computing induced subgraph statistics. "
766433596601237504,2016-08-19 00:36:23,https://t.co/ab3hDnKkwY,Conditional Sparse Linear Regression. (arXiv:1608.05152v1 [cs.LG]) https://t.co/ab3hDnKkwY,0,4," Abstract: Machine learning and statistics typically focus on building models that capture the vast majority of the data, possibly ignoring a small subset of data as ""noise"" or ""outliers."" By contrast, here we consider the problem of jointly identifying a significant (but perhaps small) segment of a population in which there is a highly sparse linear regression fit, together with the coefficients for the linear fit. We contend that such tasks are of interest both because the models themselves may be able to achieve better predictions in such special cases, but also because they may aid our understanding of the data. We give algorithms for such problems under the sup norm, when this unknown segment of the population is described by a k-DNF condition and the regression fit is s-sparse for constant k and s. For the variants of this problem when the regression fit is not so sparse or using expected error, we also give a preliminary algorithm and highlight the question as a challenge for future work. "
766433594701189120,2016-08-19 00:36:23,https://t.co/Tw4tpilpT4,A Bayesian Nonparametic Approach for Estimating Individualized Treatment-Response Curves. (arXiv:1608.05182v1 [cs.… https://t.co/Tw4tpilpT4,6,18," Abstract: We study the problem of estimating the continuous response over time of interventions from observational time series---a retrospective dataset where the policy by which the data are generated are unknown to the learner. We are motivated by applications where response varies by individuals and therefore, estimating responses at the individual-level are valuable for personalizing decision-making. We refer to this as the problem of estimating individualized treatment response (ITR) curves. In statistics, G-computation formula has been commonly used for estimating treatment responses from observational data containing sequential treatment assignments. However, past studies have focused predominantly on obtaining point-in-time estimates at the population level. We leverage G-computation formula and develop a novel method based on Bayesian nonparametrics (BNP) that can flexibly model functional data and provide posterior inference over the treatment response curves both at the individual and population level. On a challenging dataset containing time series from patients admitted to a hospital, we estimate treatment responses for treatments used in managing kidney function and show that the resulting fits are more accurate than alternative approaches. Accurate methods for obtaining ITRs from observational data can dramatically accelerate the pace at which personalized treatment plans become possible. "
766433591979040768,2016-08-19 00:36:22,https://t.co/0xvRRdxAF8,Active Learning for Approximation of Expensive Functions with Normal Distributed Output Uncertainty. (arXiv:1608.0… https://t.co/0xvRRdxAF8,0,6," Abstract: When approximating a black-box function, sampling with active learning focussing on regions with non-linear responses tends to improve accuracy. We present the FLOLA-Voronoi method introduced previously for deterministic responses, and theoretically derive the impact of output uncertainty. The algorithm automatically puts more emphasis on exploration to provide more information to the models. "
766433590318104576,2016-08-19 00:36:22,https://t.co/PwIeAY8W7j,Parameter Learning for Log-supermodular Distributions. (arXiv:1608.05258v1 [stat.ML]) https://t.co/PwIeAY8W7j,0,3," Abstract: We consider log-supermodular models on binary variables, which are probabilistic models with negative log-densities which are submodular. These models provide probabilistic interpretations of common combinatorial optimization tasks such as image segmentation. In this paper, we focus primarily on parameter estimation in the models from known upper-bounds on the intractable log-partition function. We show that the bound based on separable optimization on the base polytope of the submodular function is always inferior to a bound based on ""perturb-and-MAP"" ideas. Then, to learn parameters, given that our approximation of the log-partition function is an expectation (over our own randomization), we use a stochastic subgradient technique to maximize a lower-bound on the log-likelihood. This can also be extended to conditional maximum likelihood. We illustrate our new results in a set of experiments in binary image denoising, where we highlight the flexibility of a probabilistic model to learn with missing data. "
766433588048961537,2016-08-19 00:36:21,https://t.co/E9YsgqPwUK,A Tight Convex Upper Bound on the Likelihood of a Finite Mixture. (arXiv:1608.05275v1 [cs.LG]) https://t.co/E9YsgqPwUK,0,3," Abstract: The likelihood function of a finite mixture model is a non-convex function with multiple local maxima and commonly used iterative algorithms such as EM will converge to different solutions depending on initial conditions. In this paper we ask: is it possible to assess how far we are from the global maximum of the likelihood? Since the likelihood of a finite mixture model can grow unboundedly by centering a Gaussian on a single datapoint and shrinking the covariance, we constrain the problem by assuming that the parameters of the individual models are members of a large discrete set (e.g. estimating a mixture of two Gaussians where the means and variances of both Gaussians are members of a set of a million possible means and variances). For this setting we show that a simple upper bound on the likelihood can be computed using convex optimization and we analyze conditions under which the bound is guaranteed to be tight. This bound can then be used to assess the quality of solutions found by EM (where the final result is projected on the discrete set) or any other mixture estimation algorithm. For any dataset our method allows us to find a finite mixture model together with a dataset-specific bound on how far the likelihood of this mixture is from the global optimum of the likelihood "
766433585456877568,2016-08-19 00:36:21,https://t.co/Q4ncdqRLwg,Probabilistic Data Analysis with Probabilistic Programming. (arXiv:1608.05347v1 [cs.AI]) https://t.co/Q4ncdqRLwg,0,13," Abstract: Probabilistic techniques are central to data analysis, but different approaches can be difficult to apply, combine, and compare. This paper introduces composable generative population models (CGPMs), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques. Examples include hierarchical Bayesian models, multivariate kernel methods, discriminative machine learning, clustering algorithms, dimensionality reduction, and arbitrary probabilistic programs. We also demonstrate the integration of CGPMs into BayesDB, a probabilistic programming platform that can express data analysis tasks using a modeling language and a structured query language. The practical value is illustrated in two ways. First, CGPMs are used in an analysis that identifies satellite data records which probably violate Kepler's Third Law, by composing causal probabilistic programs with non-parametric Bayes in under 50 lines of probabilistic code. Second, for several representative data analysis tasks, we report on lines of code and accuracy measurements of various CGPMs, plus comparisons with standard baseline solutions from Python and MATLAB libraries. "
766433583473065984,2016-08-19 00:36:20,https://t.co/N53w8MgevC,Locally Adaptive Dynamic Networks. (arXiv:1505.05668v3 [stat.AP] UPDATED) https://t.co/N53w8MgevC,0,4," Abstract: Our focus is on realistically modeling and forecasting dynamic networks of face-to-face contacts among individuals. Important aspects of such data that lead to problems with current methods include the tendency of the contacts to move between periods of slow and rapid changes, and the dynamic heterogeneity in the actors' connectivity behaviors. Motivated by this application, we develop a novel method for Locally Adaptive DYnamic (LADY) network inference. The proposed model relies on a dynamic latent space representation in which each actor's position evolves in time via stochastic differential equations. Using a state space representation for these stochastic processes and P\'olya-gamma data augmentation, we develop an efficient MCMC algorithm for posterior inference along with tractable procedures for online updating and forecasting of future networks. We evaluate performance in simulation studies, and consider an application to face-to-face contacts among individuals in a primary school. "
766433581484875777,2016-08-19 00:36:20,https://t.co/adJOrhZQ6i,Multi-task and Lifelong Learning of Kernels. (arXiv:1602.06531v2 [stat.ML] UPDATED) https://t.co/adJOrhZQ6i,0,4," Abstract: We consider a problem of learning kernels for use in SVM classification in the multi-task and lifelong scenarios and provide generalization bounds on the error of a large margin classifier. Our results show that, under mild conditions on the family of kernels used for learning, solving several related tasks simultaneously is beneficial over single task learning. In particular, as the number of observed tasks grows, assuming that in the considered family of kernels there exists one that yields low approximation error on all tasks, the overhead associated with learning such a kernel vanishes and the complexity converges to that of learning when this good kernel is given to the learner. "
766433578121060352,2016-08-19 00:36:19,https://t.co/Nv5b2fWSvL,Molecular Graph Convolutions: Moving Beyond Fingerprints. (arXiv:1603.00856v3 [stat.ML] UPDATED) https://t.co/Nv5b2fWSvL,1,6," Abstract: Molecular ""fingerprints"" encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular ""graph convolutions"", a machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph---atoms, bonds, distances, etc.---which allows the model to take greater advantage of information in the graph structure. Although graph convolutions do not outperform all fingerprint-based methods, they (along with other graph-based methods) represent a new paradigm in ligand-based virtual screening with exciting opportunities for future improvement. "
766433576334295040,2016-08-19 00:36:18,https://t.co/zIabsQLnm3,Modeling Missing Data in Clinical Time Series with RNNs. (arXiv:1606.04130v3 [cs.LG] UPDATED) https://t.co/zIabsQLnm3,0,5," Abstract: We demonstrate a simple strategy to cope with missing data in sequential inputs, addressing the task of multilabel classification of diagnoses given clinical time series. Collected from the pediatric intensive care unit (PICU) at Children's Hospital Los Angeles, our data consists of multivariate time series of observations. The measurements are irregularly spaced, leading to missingness patterns in temporally discretized sequences. While these artifacts are typically handled by imputation, we achieve superior predictive performance by treating the artifacts as features. Unlike linear models, recurrent neural networks can realize this improvement using only simple binary indicators of missingness. For linear models, we show an alternative strategy to capture this signal. Training models on missingness patterns only, we show that for some diseases, what tests are run can be as predictive as the results themselves. "
766433574815952897,2016-08-19 00:36:18,https://t.co/9BEQ20nUkT,On the expressive power of deep neural networks. (arXiv:1606.05336v3 [stat.ML] UPDATED) https://t.co/9BEQ20nUkT,0,14," Abstract: We study the expressive power of deep neural networks before and after training. Considering neural nets after random initialization, we show that three natural measures of expressivity all display an exponential dependence on the depth of the network. We prove, theoretically and experimentally, that all of these measures are in fact related to a fourth quantity, trajectory length. This quantity grows exponentially in the depth of the network, and is responsible for the depth sensitivity observed. These results translate to consequences for networks during and after training. The connection of all expressivity measures to trajectory length suggests that parameters earlier in the network have greater influence on the expressive power -- in particular, given a layer, its influence on expressivity is determined by the remaining depth of the network after that layer. This is verified with experiments on MNIST and CIFAR-10. We also find that the training process decreases depth sensitivity for real and synthetic data, but at different rates. "
766433571145912320,2016-08-19 00:36:17,https://t.co/8msdlTGotx,Double Machine Learning for Treatment and Causal Parameters. (arXiv:1608.00060v3 [stat.ML] UPDATED) https://t.co/8msdlTGotx,0,7," Abstract: Most modern supervised statistical/machine learning (ML) methods are explicitly designed to solve prediction problems very well. Achieving this goal does not imply that these methods automatically deliver good estimators of causal parameters. Examples of such parameters include individual regression coefficients, average treatment effects, average lifts, and demand or supply elasticities. In fact, estimates of such causal parameters obtained via naively plugging ML estimators into estimating equations for such parameters can behave very poorly due to the regularization bias. Fortunately, this regularization bias can be removed by solving auxiliary prediction problems via ML tools. Specifically, we can form an orthogonal score for the target low-dimensional parameter by combining auxiliary and main ML predictions. The score is then used to build a de-biased estimator of the target parameter which typically will converge at the fastest possible 1/root(n) rate and be approximately unbiased and normal, and from which valid confidence intervals for these parameters of interest may be constructed. The resulting method thus could be called a ""double ML"" method because it relies on estimating primary and auxiliary predictive models. In order to avoid overfitting, our construction also makes use of the K-fold sample splitting, which we call cross-fitting. This allows us to use a very broad set of ML predictive methods in solving the auxiliary and main prediction problems, such as random forest, lasso, ridge, deep neural nets, boosted trees, as well as various hybrids and aggregators of these methods. "
766071596800958464,2016-08-18 00:37:56,https://t.co/2mih4pALJn,An Efficient Character-Level Neural Machine Translation. (arXiv:1608.04738v1 [cs.CL]) https://t.co/2mih4pALJn,0,5," Abstract: Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems on the task of English-to-French translation. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose an efficient architecture to train a deep character-level neural machine translation by introducing a decimator and an interpolator. The decimator is used to sample the source sequence before encoding while the interpolator is used to resample after decoding. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is much faster and more memory-efficient in training than conventional character-based models. More interestingly, our model is able to translate the misspelled word like human beings. "
766071594540273666,2016-08-18 00:37:55,https://t.co/brQIEv4dSl,Faster Principal Component Regression via Optimal Polynomial Approximation to sgn(x). (arXiv:1608.04773v1 [stat.ML… https://t.co/brQIEv4dSl,0,3," Abstract: We solve principle component regression (PCR) by providing an efficient algorithm to project any vector onto the subspace formed by the top principle components of a matrix. Our algorithm does not require any explicit construction of the top principle components, and therefore is suitable for large-scale PCR instances. Specifically, to project onto the subspace formed by principle components with eigenvalues above a threshold $\lambda$ and with a multiplicative accuracy $(1\pm \gamma) \lambda$, our algorithm requires $\tilde{O}(\gamma^{-1})$ black-box calls of ridge regression. In contrast, previous result requires $\tilde{O}(\gamma^{-2})$ such calls. We obtain this result by designing a degree-optimal polynomial approximation of the sign function. "
766071591876829184,2016-08-18 00:37:55,https://t.co/ia3uzdMclU,Application of multiview techniques to NHANES dataset. (arXiv:1608.04783v1 [cs.LG]) https://t.co/ia3uzdMclU,0,2," Abstract: Disease prediction or classification using health datasets involve using well-known predictors associated with the disease as features for the models. This study considers multiple data components of an individual's health, using the relationship between variables to generate features that may improve the performance of disease classification models. In order to capture information from different aspects of the data, this project uses a multiview learning approach, using Canonical Correlation Analysis (CCA), a technique that finds projections with maximum correlations between two data views. Data categories collected from the NHANES survey (1999-2014) are used as views to learn the multiview representations. The usefulness of the representations is demonstrated by applying them as features in a Diabetes classification task. "
766071588462686208,2016-08-18 00:37:54,https://t.co/LO0nGsgWF4,Large-scale Learning With Global Non-Decomposable Objectives. (arXiv:1608.04802v1 [stat.ML]) https://t.co/LO0nGsgWF4,0,2," Abstract: Modern retrieval systems are often driven by an underlying machine learning model. The goal of such systems is to identify and possibly rank the few most relevant items for a given query or context. Thus, the objective we would like to optimize in such scenarios is typically a globaln on-decomposable one such as the area under the precision-recall curve, the $F_\beta$ score, precision at fixed recall, etc. In practice, due to the scalability limitations of existing approaches for optimizing such objectives, large-scale systems are trained to maximize classification accuracy, in the hope that performance as measured via the true objective will also be favorable. In this work we present a unified framework that, using straightforward building block bounds, allows for highly scalable optimization of a wide range of ranking-based objectives. We demonstrate the advantage of our approach on several real-life retrieval problems that are significantly larger than those considered in the literature, while achieving substantial improvement in performance over the accuracy-objective baseline. "
766071584838868992,2016-08-18 00:37:53,https://t.co/fvzHVPQ2EO,Outlier Detection on Mixed-Type Data: An Energy-based Approach. (arXiv:1608.04830v1 [stat.ML]) https://t.co/fvzHVPQ2EO,1,4," Abstract: Outlier detection amounts to finding data points that differ significantly from the norm. Classic outlier detection methods are largely designed for single data type such as continuous or discrete. However, real world data is increasingly heterogeneous, where a data point can have both discrete and continuous attributes. Handling mixed-type data in a disciplined way remains a great challenge. In this paper, we propose a new unsupervised outlier detection method for mixed-type data based on Mixed-variate Restricted Boltzmann Machine (Mv.RBM). The Mv.RBM is a principled probabilistic method that models data density. We propose to use \emph{free-energy} derived from Mv.RBM as outlier score to detect outliers as those data points lying in low density regions. The method is fast to learn and compute, is scalable to massive datasets. At the same time, the outlier score is identical to data negative log-density up-to an additive constant. We evaluate the proposed method on synthetic and real-world datasets and demonstrate that (a) a proper handling mixed-types is necessary in outlier detection, and (b) free-energy of Mv.RBM is a powerful and efficient outlier scoring method, which is highly competitive against state-of-the-arts. "
766071582154428422,2016-08-18 00:37:52,https://t.co/clnGu4KH1U,Dynamic Collaborative Filtering with Compound Poisson Factorization. (arXiv:1608.04839v1 [cs.LG]) https://t.co/clnGu4KH1U,0,2," Abstract: Model-based collaborative filtering analyzes user-item interactions to infer latent factors that represent user preferences and item characteristics in order to predict future interactions. Most collaborative filtering algorithms assume that these latent factors are static, although it has been shown that user preferences and item perceptions drift over time. In this paper, we propose a conjugate and numerically stable dynamic matrix factorization (DCPF) based on compound Poisson matrix factorization that models the smoothly drifting latent factors using Gamma-Markov chains. We propose a numerically stable Gamma chain construction, and then present a stochastic variational inference approach to estimate the parameters of our model. We apply our model to time-stamped ratings data sets: Netflix, Yelp, and Last.fm, where DCPF achieves a higher predictive accuracy than state-of-the-art static and dynamic factorization models. "
766071578866114564,2016-08-18 00:37:51,https://t.co/8BAucNVYJ3,Lecture Notes on Spectral Graph Methods. (arXiv:1608.04845v1 [cs.DS]) https://t.co/8BAucNVYJ3,2,10, Abstract: These are lecture notes that are based on the lectures from a class I taught on the topic of Spectral Graph Methods at UC Berkeley during the Spring 2015 semester. 
766071576357900288,2016-08-18 00:37:51,https://t.co/iBklurRejr,A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation. (arXiv:1608.04846v1 [stat.ML]) https://t.co/iBklurRejr,1,6, Abstract: Finding the most effective way to aggregate multi-subject fMRI data is a long-standing and challenging problem. It is of increasing interest in contemporary fMRI studies of human cognition due to the scarcity of data per subject and the variability of brain anatomy and functional response across subjects. Recent work on latent factor models shows promising results in this task but this approach does not preserve spatial locality in the brain. We examine two ways to combine the ideas of a factor model and a searchlight based analysis to aggregate multi-subject fMRI data while preserving spatial locality. We first do this directly by combining a recent factor method known as a shared response model with searchlight analysis. Then we design a multi-view convolutional autoencoder for the same task. Both approaches preserve spatial locality and have competitive or better performance compared with standard searchlight analysis and the shared response model applied across the whole brain. We also report a system design to handle the computational challenge of training the convolutional autoencoder. 
766071573765849089,2016-08-18 00:37:50,https://t.co/fxiqMk81Ey,Clustering Mixed Datasets Using Homogeneity Analysis with Applications to Big Data. (arXiv:1608.04961v1 [stat.ML]) https://t.co/fxiqMk81Ey,1,3," Abstract: Clustering datasets with a mix of continuous and categorical attributes is encountered routinely by data analysts. This work presents a method for clustering such datasets using Homogeneity Analysis. An optimal Euclidean representation of mixed datasets is obtained using Homogeneity Analysis. This representation is then clustered. The clustering solutions from this method are compared to the clustering solutions obtained using the method based on the Gower distance that is popularly used with such datasets. This comparison is made on datasets that have been the subject of other research investigations. The Homogeneity Analysis solution is an eigenvalue based solution. The eigenvalues are used to produce the optimal Euclidean representation. Even with a single eigenvalue, the Homogeneity Analysis based solution performed better than the method based on the Gower distance. Extending the solution to use multiple eigenvalues from the Homogeneity Analysis solution is illustrated on real world datasets. This method can be used in conjunction with the mini-batch K-Means algorithm to cluster large datasets. This is illustrated on a real world dataset. The relevant theory from Homogeneity Analysis is presented. "
766071571215712258,2016-08-18 00:37:50,https://t.co/JYay2o630I,A Three Spatial Dimension Wave Latent Force Model for Describing Excitation Sources and Electric Potentials Produc… https://t.co/JYay2o630I,0,3," Abstract: Deep brain stimulation (DBS) is a surgical treatment for Parkinson's Disease. Static models based on quasi-static approximation are common approaches for DBS modeling. While this simplification has been validated for bioelectric sources, its application to rapid stimulation pulses, which contain more high-frequency power, may not be appropriate, as DBS therapeutic results depend on stimulus parameters such as frequency and pulse width, which are related to time variations of the electric field. We propose an alternative hybrid approach based on probabilistic models and differential equations, by using Gaussian processes and wave equation. Our model avoids quasi-static approximation, moreover, it is able to describe dynamic behavior of DBS. Therefore, the proposed model may be used to obtain a more realistic phenomenon description. The proposed model can also solve inverse problems, i.e. to recover the corresponding source of excitation, given electric potential distribution. The electric potential produced by a time-varying source was predicted using proposed model. For static sources, the electric potential produced by different electrode configurations were modeled. Four different sources of excitation were recovered by solving the inverse problem. We compare our outcomes with the electric potential obtained by solving Poisson's equation using the Finite Element Method (FEM). Our approach is able to take into account time variations of the source and the produced field. Also, inverse problem can be addressed using the proposed model. The electric potential calculated with the proposed model is close to the potential obtained by solving Poisson's equation using FEM. "
766071566706864128,2016-08-18 00:37:49,https://t.co/0zX9PU8Awu,Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version. (arXiv:1207.2940v5 [stat.ML] UPDA… https://t.co/0zX9PU8Awu,0,5," Abstract: Rich and complex time-series data, such as those generated from engineering systems, financial markets, videos or neural recordings, are now a common feature of modern data analysis. Explaining the phenomena underlying these diverse data sets requires flexible and accurate models. In this paper, we promote Gaussian process dynamical systems (GPDS) as a rich model class that is appropriate for such analysis. In particular, we present a message passing algorithm for approximate inference in GPDSs based on expectation propagation. By posing inference as a general message passing problem, we iterate forward-backward smoothing. Thus, we obtain more accurate posterior distributions over latent structures, resulting in improved predictive performance compared to state-of-the-art GPDS smoothers, which are special cases of our general message passing algorithm. Hence, we provide a unifying approach within which to contextualize message passing in GPDSs. "
766071564072812545,2016-08-18 00:37:48,https://t.co/YOW8jGiMMa,Guaranteed inference in topic models. (arXiv:1512.03308v2 [stat.ML] UPDATED) https://t.co/YOW8jGiMMa,0,3," Abstract: One of the core problems in statistical models is the estimation of a posterior distribution. For topic models, the problem of posterior inference for individual texts is particularly important, especially when dealing with data streams, but is often intractable in the worst case. As a consequence, existing methods for posterior inference are approximate and do not have any guarantee on neither quality nor convergence rate. In this paper, we introduce a provably fast algorithm, namely Online Maximum a Posteriori Estimation (OPE), for posterior inference in topic models. OPE has more attractive properties than existing inference approaches, including theoretical guarantees on quality and fast rate of convergence to a local maximal/stationary point of the inference problem. The discussions about OPE are very general and hence can be easily employed in a wide range of contexts. Finally, we employ OPE to design three methods for learning Latent Dirichlet Allocation from text streams or large corpora. Extensive experiments demonstrate some superior behaviors of OPE and of our new learning methods. "
766071560818003968,2016-08-18 00:37:47,https://t.co/xI0ELkqos2,Local Network Community Detection with Continuous Optimization of Conductance and Weighted Kernel K-Means. (arXiv:… https://t.co/xI0ELkqos2,1,4," Abstract: Local network community detection is the task of finding a single community of nodes concentrated around few given seed nodes in a localized way. Conductance is a popular objective function used in many algorithms for local community detection. This paper studies a continuous relaxation of conductance. We show that continuous optimization of this objective still leads to discrete communities. We investigate the relation of conductance with weighted kernel k-means for a single community, which leads to the introduction of a new objective function, $\sigma$-conductance. Conductance is obtained by setting $\sigma$ to $0$. Two algorithms, EMc and PGDc, are proposed to locally optimize $\sigma$-conductance and automatically tune the parameter $\sigma$. They are based on expectation maximization and projected gradient descent, respectively. We prove locality and give performance guarantees for EMc and PGDc for a class of dense and well separated communities centered around the seeds. Experiments are conducted on networks with ground-truth communities, comparing to state-of-the-art graph diffusion algorithms for conductance optimization. On large graphs, results indicate that EMc and PGDc stay localized and produce communities most similar to the ground, while graph diffusion algorithms generate large communities of lower quality. "
766071557345120256,2016-08-18 00:37:46,https://t.co/q4DQhm1gmz,Model Interpolation with Trans-dimensional Random Field Language Models for Speech Recognition. (arXiv:1603.09170v… https://t.co/q4DQhm1gmz,0,2," Abstract: The dominant language models (LMs) such as n-gram and neural network (NN) models represent sentence probabilities in terms of conditionals. In contrast, a new trans-dimensional random field (TRF) LM has been recently introduced to show superior performances, where the whole sentence is modeled as a random field. In this paper, we examine how the TRF models can be interpolated with the NN models, and obtain 12.1\% and 17.9\% relative error rate reductions over 6-gram LMs for English and Chinese speech recognition respectively through log-linear combination. "
766071553234722816,2016-08-18 00:37:45,https://t.co/DpFe0TUwBa,Variational Information Maximizing Exploration. (arXiv:1605.09674v2 [cs.LG] UPDATED) https://t.co/DpFe0TUwBa,0,11," Abstract: Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards. "
766071546767179777,2016-08-18 00:37:44,https://t.co/f66Pxw8hEX,Guaranteed bounds on the Kullback-Leibler divergence of univariate mixtures using piecewise log-sum-exp inequaliti… https://t.co/f66Pxw8hEX,0,4," Abstract: Information-theoretic measures such as the entropy, cross-entropy and the Kullback-Leibler divergence between two mixture models is a core primitive in many signal processing tasks. Since the Kullback-Leibler divergence of mixtures provably does not admit a closed-form formula, it is in practice either estimated using costly Monte-Carlo stochastic integration, approximated, or bounded using various techniques. We present a fast and generic method that builds algorithmically closed-form lower and upper bounds on the entropy, the cross-entropy and the Kullback-Leibler divergence of mixtures. We illustrate the versatile method by reporting on our experiments for approximating the Kullback-Leibler divergence between univariate exponential mixtures, Gaussian mixtures, Rayleigh mixtures, and Gamma mixtures. "
765711446663696384,2016-08-17 00:46:49,https://t.co/QduPOYnMw5,Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back. (arXiv:1608.04414v1 [cs.LG]) https://t.co/QduPOYnMw5,0,4," Abstract: In stochastic convex optimization the goal is to minimize a convex function $F(x) \doteq {\mathbf E}_{{\mathbf f}\sim D}[{\mathbf f}(x)]$ over a convex set $\cal K \subset {\mathbb R}^d$ where $D$ is some unknown distribution and each $f(\cdot)$ in the support of $D$ is convex over $\cal K$. The optimization is commonly based on i.i.d.~samples $f^1,f^2,\ldots,f^n$ from $D$. A standard approach to such problems is empirical risk minimization (ERM) that optimizes $F_S(x) \doteq \frac{1}{n}\sum_{i\leq n} f^i(x)$. Here we consider the question of how many samples are necessary for ERM to succeed and the closely related question of uniform convergence of $F_S$ to $F$ over $\cal K$. We demonstrate that in the standard $\ell_p/\ell_q$ setting of Lipschitz-bounded functions over a $\cal K$ of bounded radius, ERM requires sample size that scales linearly with the dimension $d$. This nearly matches standard upper bounds and improves on $\Omega(\log d)$ dependence proved for $\ell_2/\ell_2$ setting by Shalev-Shwartz et al. (2009). In stark contrast, these problems can be solved using dimension-independent number of samples for $\ell_2/\ell_2$ setting and $\log d$ dependence for $\ell_1/\ell_\infty$ setting using other approaches. We further show that our lower bound applies even if the functions in the support of $D$ are smooth and efficiently computable and even if an $\ell_1$ regularization term is added. Finally, we demonstrate that for a more general class of bounded-range (but not Lipschitz-bounded) stochastic convex programs an infinite gap appears already in dimension 2. "
765711445334130688,2016-08-17 00:46:49,https://t.co/kQJiXCAVh9,Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm. (arXiv:1608.04471v1 [stat.ML]) https://t.co/kQJiXCAVh9,11,38," Abstract: We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest. "
765711444092551168,2016-08-17 00:46:49,https://t.co/7Cbe8hEsM0,A Geometrical Approach to Topic Model Estimation. (arXiv:1608.04478v1 [stat.ME]) https://t.co/7Cbe8hEsM0,0,8," Abstract: In the probabilistic topic models, the quantity of interest---a low-rank matrix consisting of topic vectors---is hidden in the text corpus matrix, masked by noise, and the Singular Value Decomposition (SVD) is a potentially useful tool for learning such a low-rank matrix. However, the connection between this low-rank matrix and the singular vectors of the text corpus matrix are usually complicated and hard to spell out, so how to use SVD for learning topic models faces challenges. In this paper, we overcome the challenge by revealing a surprising insight: there is a low-dimensional simplex structure which can be viewed as a bridge between the low-rank matrix of interest and the SVD of the text corpus matrix, and allows us to conveniently reconstruct the former using the latter. Such an insight motivates a new SVD approach to learning topic models, which we analyze with delicate random matrix theory and derive the rate of convergence. We support our methods and theory numerically, using both simulated data and real data. "
765711442788118528,2016-08-17 00:46:48,https://t.co/gYUe2840xr,Lecture Notes on Randomized Linear Algebra. (arXiv:1608.04481v1 [cs.DS]) https://t.co/gYUe2840xr,4,14, Abstract: These are lecture notes that are based on the lectures from a class I taught on the topic of Randomized Linear Algebra (RLA) at UC Berkeley during the Fall 2013 semester. 
765711440988831744,2016-08-17 00:46:48,https://t.co/bAGtDCcci3,Fast Calculation of the Knowledge Gradient for Optimization of Deterministic Engineering Simulations. (arXiv:1608.… https://t.co/bAGtDCcci3,0,5," Abstract: A novel efficient method for computing the Knowledge-Gradient policy for Continuous Parameters (KGCP) for deterministic optimization is derived. The differences with Expected Improvement (EI), a popular choice for Bayesian optimization of deterministic engineering simulations, are explored. Both policies and the Upper Confidence Bound (UCB) policy are compared on a number of benchmark functions including a problem from structural dynamics. It is empirically shown that KGCP has similar performance as the EI policy for many problems, but has better convergence properties for complex (multi-modal) optimization problems as it emphasizes more on exploration when the model is confident about the shape of optimal regions. In addition, the relationship between Maximum Likelihood Estimation (MLE) and slice sampling for estimation of the hyperparameters of the underlying models, and the complexity of the problem at hand, is studied. "
765711439701090304,2016-08-17 00:46:48,https://t.co/xbl4PLKs5j,A novel transfer learning method based on common space mapping and weighted domain matching. (arXiv:1608.04581v1 [… https://t.co/xbl4PLKs5j,1,9," Abstract: In this paper, we propose a novel learning framework for the problem of domain transfer learning. We map the data of two domains to one single common space, and learn a classifier in this common space. Then we adapt the common classifier to the two domains by adding two adaptive functions to it respectively. In the common space, the target domain data points are weighted and matched to the target domain in term of distributions. The weighting terms of source domain data points and the target domain classification responses are also regularized by the local reconstruction coefficients. The novel transfer learning framework is evaluated over some benchmark cross-domain data sets, and it outperforms the existing state-of-the-art transfer learning methods. "
765711438585405440,2016-08-17 00:46:47,https://t.co/BDQUuVCppw,Conformalized density- and distance-based anomaly detection in time-series data. (arXiv:1608.04585v1 [stat.AP]) https://t.co/BDQUuVCppw,0,5," Abstract: Anomalies (unusual patterns) in time-series data give essential, and often actionable information in critical situations. Examples can be found in such fields as healthcare, intrusion detection, finance, security and flight safety. In this paper we propose new conformalized density- and distance-based anomaly detection algorithms for a one-dimensional time-series data. The algorithms use a combination of a feature extraction method, an approach to assess a score whether a new observation differs significantly from a previously observed data, and a probabilistic interpretation of this score based on the conformal paradigm. "
765711437159432192,2016-08-17 00:46:47,https://t.co/lPZvHjLq6T,Scalable Modeling of Multivariate Longitudinal Data for Prediction of Chronic Kidney Disease Progression. (arXiv:1… https://t.co/lPZvHjLq6T,0,5," Abstract: Prediction of the future trajectory of a disease is an important challenge for personalized medicine and population health management. However, many complex chronic diseases exhibit large degrees of heterogeneity, and furthermore there is not always a single readily available biomarker to quantify disease severity. Even when such a clinical variable exists, there are often additional related biomarkers routinely measured for patients that may better inform the predictions of their future disease state. To this end, we propose a novel probabilistic generative model for multivariate longitudinal data that captures dependencies between multivariate trajectories. We use a Gaussian process based regression model for each individual trajectory, and build off ideas from latent class models to induce dependence between their mean functions. We fit our method using a scalable variational inference algorithm to a large dataset of longitudinal electronic patient health records, and find that it improves dynamic predictions compared to a recent state of the art method. Our local accountable care organization then uses the model predictions during chart reviews of high risk patients with chronic kidney disease. "
765711435632705537,2016-08-17 00:46:47,https://t.co/FC9pLr0BQI,Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-\L{}ojasiewicz Condition. (arXiv:160… https://t.co/FC9pLr0BQI,0,7," Abstract: In 1963, Polyak proposed a simple condition that is sufficient to show a global linear convergence rate for gradient descent. This condition is a special case of the \L{}ojasiewicz inequality proposed in the same year, and it does not require strong convexity (or even convexity). In this work, we show that this much-older Polyak-\L{}ojasiewicz (PL) inequality is actually weaker than the main conditions that have been explored to show linear convergence rates without strong convexity over the last 25 years. We also use the PL inequality to give new analyses of randomized and greedy coordinate descent methods, sign-based gradient descent methods, and stochastic gradient methods in the classic setting (with decreasing or constant step-sizes) as well as the variance-reduced setting. We further propose a generalization that applies to proximal-gradient methods for non-smooth optimization, leading to simple proofs of linear convergence of these methods. Along the way, we give simple convergence results for a wide variety of problems in machine learning: least squares, logistic regression, boosting, resilient backpropagation, L1-regularization, support vector machines, stochastic dual coordinate ascent, and stochastic variance-reduced gradient methods. "
765711434428841984,2016-08-17 00:46:46,https://t.co/okRMtYTeAC,Enabling Factor Analysis on Thousand-Subject Neuroimaging Datasets. (arXiv:1608.04647v1 [stat.ML]) https://t.co/okRMtYTeAC,0,3," Abstract: The scale of functional magnetic resonance image data is rapidly increasing as large multi-subject datasets are becoming widely available and high-resolution scanners are adopted. The inherent low-dimensionality of the information in this data has led neuroscientists to consider factor analysis methods to extract and analyze the underlying brain activity. In this work, we consider two recent multi-subject factor analysis methods: the Shared Response Model and Hierarchical Topographic Factor Analysis. We perform analytical, algorithmic, and code optimization to enable multi-node parallel implementations to scale. Single-node improvements result in 99x and 1812x speedups on these two methods, and enables the processing of larger datasets. Our distributed implementations show strong scaling of 3.3x and 5.5x respectively with 20 nodes on real datasets. We also demonstrate weak scaling on a synthetic dataset with 1024 subjects, on up to 1024 nodes and 32,768 cores. "
765711433267019777,2016-08-17 00:46:46,https://t.co/SZWsEvTUeX,Variational Gaussian Process Auto-Encoder for Ordinal Prediction of Facial Action Units. (arXiv:1608.04664v1 [stat… https://t.co/SZWsEvTUeX,1,8," Abstract: We address the task of simultaneous feature fusion and modeling of discrete ordinal outputs. We propose a novel Gaussian process(GP) auto-encoder modeling approach. In particular, we introduce GP encoders to project multiple observed features onto a latent space, while GP decoders are responsible for reconstructing the original features. Inference is performed in a novel variational framework, where the recovered latent representations are further constrained by the ordinal output labels. In this way, we seamlessly integrate the ordinal structure in the learned manifold, while attaining robust fusion of the input features. We demonstrate the representation abilities of our model on benchmark datasets from machine learning and affect analysis. We further evaluate the model on the tasks of feature fusion and joint ordinal prediction of facial action units. Our experiments demonstrate the benefits of the proposed approach compared to the state of the art. "
765711431752945664,2016-08-17 00:46:46,https://t.co/L7IrM7Jhgl,Shape Constrained Tensor Decompositions using Sparse Representations in Over-Complete Libraries. (arXiv:1608.04674… https://t.co/L7IrM7Jhgl,0,4," Abstract: We consider $N$-way data arrays and low-rank tensor factorizations where the time mode is coded as a sparse linear combination of temporal elements from an over-complete library. Our method, Shape Constrained Tensor Decomposition (SCTD) is based upon the CANDECOMP/PARAFAC (CP) decomposition which produces $r$-rank approximations of data tensors via outer products of vectors in each dimension of the data. By constraining the vector in the temporal dimension to known analytic forms which are selected from a large set of candidate functions, more readily interpretable decompositions are achieved and analytic time dependencies discovered. The SCTD method circumvents traditional {\em flattening} techniques where an $N$-way array is reshaped into a matrix in order to perform a singular value decomposition. A clear advantage of the SCTD algorithm is its ability to extract transient and intermittent phenomena which is often difficult for SVD-based methods. We motivate the SCTD method using several intuitively appealing results before applying it on a number of high-dimensional, real-world data sets in order to illustrate the efficiency of the algorithm in extracting interpretable spatio-temporal modes. With the rise of data-driven discovery methods, the decomposition proposed provides a viable technique for analyzing multitudes of data in a more comprehensible fashion. "
765711430448508928,2016-08-17 00:46:45,https://t.co/cul39Y9N15,A Shallow High-Order Parametric Approach to Data Visualization and Compression. (arXiv:1608.04689v1 [cs.AI]) https://t.co/cul39Y9N15,0,3," Abstract: Explicit high-order feature interactions efficiently capture essential structural knowledge about the data of interest and have been used for constructing generative models. We present a supervised discriminative High-Order Parametric Embedding (HOPE) approach to data visualization and compression. Compared to deep embedding models with complicated deep architectures, HOPE generates more effective high-order feature mapping through an embarrassingly simple shallow model. Furthermore, two approaches to generating a small number of exemplars conveying high-order interactions to represent large-scale data sets are proposed. These exemplars in combination with the feature mapping learned by HOPE effectively capture essential data variations. Moreover, through HOPE, these exemplars are employed to increase the computational efficiency of kNN classification for fast information retrieval by thousands of times. For classification in two-dimensional embedding space on MNIST and USPS datasets, our shallow method HOPE with simple Sigmoid transformations significantly outperforms state-of-the-art supervised deep embedding models based on deep neural networks, and even achieved historically low test error rate of 0.65% in two-dimensional space on MNIST, which demonstrates the representational efficiency and power of supervised shallow models with high-order feature interactions. "
765711429093748736,2016-08-17 00:46:45,https://t.co/C8IX4hbkgW,Online and stochastic Douglas-Rachford splitting method for large scale machine learning. (arXiv:1308.4757v6 [cs.N… https://t.co/C8IX4hbkgW,0,4," Abstract: Online and stochastic learning has emerged as powerful tool in large scale optimization. In this work, we generalize the Douglas-Rachford splitting (DRs) method for minimizing composite functions to online and stochastic settings (to our best knowledge this is the first time DRs been generalized to sequential version). We first establish an $O(1/\sqrt{T})$ regret bound for batch DRs method. Then we proved that the online DRs splitting method enjoy an $O(1)$ regret bound and stochastic DRs splitting has a convergence rate of $O(1/\sqrt{T})$. The proof is simple and intuitive, and the results and technique can be served as a initiate for the research on the large scale machine learning employ the DRs method. Numerical experiments of the proposed method demonstrate the effectiveness of the online and stochastic update rule, and further confirm our regret and convergence analysis. "
765711427889946625,2016-08-17 00:46:45,https://t.co/5EE3008IWZ,Learning values across many orders of magnitude. (arXiv:1602.07714v2 [cs.LG] UPDATED) https://t.co/5EE3008IWZ,0,3," Abstract: Most learning algorithms are not invariant to the scale of the function that is being approximated. We propose to adaptively normalize the targets used in learning. This is useful in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were all clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using the adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance. "
765711426078011392,2016-08-17 00:46:44,https://t.co/6DutUE1aU0,"Laplacian Eigenmaps from Sparse, Noisy Similarity Measurements. (arXiv:1603.03972v2 [stat.ML] UPDATED) https://t.co/6DutUE1aU0",1,6," Abstract: Manifold learning and dimensionality reduction techniques are ubiquitous in science and engineering, but can be computationally expensive procedures when applied to large data sets or when similarities are expensive to compute. To date, little work has been done to investigate the tradeoff between computational resources and the quality of learned representations. We present both theoretical and experimental explorations of this question. In particular, we consider Laplacian eigenmaps embeddings based on a kernel matrix, and explore how the embeddings behave when this kernel matrix is corrupted by occlusion and noise. Our main theoretical result shows that under modest noise and occlusion assumptions, we can (with high probability) recover a good approximation to the Laplacian eigenmaps embedding based on the uncorrupted kernel matrix. Our results also show how regularization can aid this approximation. Experimentally, we explore the effects of noise and occlusion on Laplacian eigenmaps embeddings of two real-world data sets, one from speech processing and one from neuroscience, as well as a synthetic data set. "
765711423771176960,2016-08-17 00:46:44,https://t.co/yYQm2y9J4O,Understanding the Energy and Precision Requirements for Online Learning. (arXiv:1607.00669v2 [stat.ML] UPDATED) https://t.co/yYQm2y9J4O,1,5," Abstract: It is well-known that the precision of data, hyperparameters, and internal representations employed in learning systems directly impacts its energy, throughput, and latency. The precision requirements for the training algorithm are also important for systems that learn on-the-fly. Prior work has shown that the data and hyperparameters can be quantized heavily without incurring much penalty in classification accuracy when compared to floating point implementations. These works suffer from two key limitations. First, they assume uniform precision for the classifier and for the training algorithm and thus miss out on the opportunity to further reduce precision. Second, prior works are empirical studies. In this article, we overcome both these limitations by deriving analytical lower bounds on the precision requirements of the commonly employed stochastic gradient descent (SGD) on-line learning algorithm in the specific context of a support vector machine (SVM). Lower bounds on the data precision are derived in terms of the the desired classification accuracy and precision of the hyperparameters used in the classifier. Additionally, lower bounds on the hyperparameter precision in the SGD training algorithm are obtained. These bounds are validated using both synthetic and the UCI breast cancer dataset. Additionally, the impact of these precisions on the energy consumption of a fixed-point SVM with on-line training is studied. "
765346029784993796,2016-08-16 00:34:47,https://t.co/eDIpXqoXrT,Hybrid Jacobian and Gauss-Seidel proximal block coordinate update methods for linearly constrained convex programm… https://t.co/eDIpXqoXrT,0,2," Abstract: Recent years have witnessed the rapid development of block coordinate update (BCU) methods, which are particularly suitable for problems involving large-sized data and/or variables. In optimization, BCU first appears as the coordinate descent method that works well for smooth problems or those with separable nonsmooth terms and/or separable constraints. As nonseparable constraints exist, BCU can be applied under primal-dual settings. In the literature, it has been shown that for weakly convex problems with nonseparable linear constraint, BCU with fully Gauss-Seidel updating rule may fail to converge and that with fully Jacobian rule can converge sublinearly. However, empirically the method with Jacobian update is usually slower than that with Gauss-Seidel rule. To maintain their advantages, we propose a hybrid Jacobian and Gauss-Seidel BCU method for solving linearly constrained multi-block structured convex programming, where the objective may have a nonseparable quadratic term and separable nonsmooth terms. At each primal block variable update, the method approximates the augmented Lagrangian function at an affine combination of the previous two iterates, and the affinely mixing matrix with desired nice properties can be chosen through solving a semidefinite programming. We show that the hybrid method enjoys the theoretical convergence guarantee as Jacobian BCU. In addition, we numerically demonstrate that the method can perform as well as Gauss-Seidel method and better than a recently proposed randomized primal-dual BCU method. "
765346027448721408,2016-08-16 00:34:47,https://t.co/BNCZmbUJLS,Recurrent Fully Convolutional Neural Networks for Multi-slice MRI Cardiac Segmentation. (arXiv:1608.03974v1 [stat.… https://t.co/BNCZmbUJLS,0,6," Abstract: In cardiac magnetic resonance imaging, fully-automatic segmentation of the heart enables precise structural and functional measurements to be taken, e.g. from short-axis MR images of the left-ventricle. In this work we propose a recurrent fully-convolutional network (RFCN) that learns image representations from the full stack of 2D slices and has the ability to leverage inter-slice spatial dependences through internal memory units. RFCN combines anatomical detection and segmentation into a single architecture that is trained end-to-end thus significantly reducing computational time, simplifying the segmentation pipeline, and potentially enabling real-time applications. We report on an investigation of RFCN using two datasets, including the publicly available MICCAI 2009 Challenge dataset. Comparisons have been carried out between fully convolutional networks and deep restricted Boltzmann machines, including a recurrent version that leverages inter-slice spatial correlation. Our studies suggest that RFCN produces state-of-the-art results and can substantially improve the delineation of contours near the apex of the heart. "
765346024705683456,2016-08-16 00:34:46,https://t.co/uydO3yB9Wt,An approach to dealing with missing values in heterogeneous data using k-nearest neighbors. (arXiv:1608.04037v1 [c… https://t.co/uydO3yB9Wt,0,5," Abstract: Techniques such as clusterization, neural networks and decision making usually rely on algorithms that are not well suited to deal with missing values. However, real world data frequently contains such cases. The simplest solution is to either substitute them by a best guess value or completely disregard the missing values. Unfortunately, both approaches can lead to biased results. In this paper, we propose a technique for dealing with missing values in heterogeneous data using imputation based on the k-nearest neighbors algorithm. It can handle real (which we refer to as crisp henceforward), interval and fuzzy data. The effectiveness of the algorithm is tested on several datasets and the numerical results are promising. "
765346021585158148,2016-08-16 00:34:45,https://t.co/mvINuG0LgD,Ultra High-Dimensional Nonlinear Feature Selection for Big Biological Data. (arXiv:1608.04048v1 [stat.ML]) https://t.co/mvINuG0LgD,8,17," Abstract: Machine learning methods are used to discover complex nonlinear relationships in biological and medical data. However, sophisticated learning models are computationally unfeasible for data with millions of features. Here we introduce the first feature selection method for nonlinear learning problems that can scale up to large, ultra-high dimensional biological data. More specifically, we scale up the novel Hilbert-Schmidt Independence Criterion Lasso (HSIC Lasso) to handle millions of features with tens of thousand samples. The proposed method is guaranteed to find an optimal subset of maximally predictive features with minimal redundancy, yielding higher predictive power and improved interpretability. Its effectiveness is demonstrated through applications to classify phenotypes based on module expression in human prostate cancer patients and to detect enzymes among protein structures. We achieve high accuracy with as few as 20 out of one million features --- a dimensionality reduction of 99.998%. Our algorithm can be implemented on commodity cloud computing platforms. The dramatic reduction of features may lead to the ubiquitous deployment of sophisticated prediction models in mobile health care applications. "
765346018787491841,2016-08-16 00:34:44,https://t.co/h4kdVF2zzS,Bayesian Model Selection Methods for Mutual and Symmetric $k$-Nearest Neighbor Classification. (arXiv:1608.04063v1… https://t.co/h4kdVF2zzS,0,5," Abstract: The $k$-nearest neighbor classification method ($k$-NNC) is one of the simplest nonparametric classification methods. The mutual $k$-NN classification method (M$k$NNC) is a variant of $k$-NNC based on mutual neighborship. We propose another variant of $k$-NNC, the symmetric $k$-NN classification method (S$k$NNC) based on both mutual neighborship and one-sided neighborship. The performance of M$k$NNC and S$k$NNC depends on the parameter $k$ as the one of $k$-NNC does. We propose the ways how M$k$NN and S$k$NN classification can be performed based on Bayesian mutual and symmetric $k$-NN regression methods with the selection schemes for the parameter $k$. Bayesian mutual and symmetric $k$-NN regression methods are based on Gaussian process models, and it turns out that they can do M$k$NN and S$k$NN classification with new encodings of target values (class labels). The simulation results show that the proposed methods are better than or comparable to $k$-NNC, M$k$NNC and S$k$NNC with the parameter $k$ selected by the leave-one-out cross validation method not only for an artificial data set but also for real world data sets. "
765346016845529088,2016-08-16 00:34:44,https://t.co/OItmTrmGq9,Viewpoint and Topic Modeling of Current Events. (arXiv:1608.04089v1 [cs.CL]) https://t.co/OItmTrmGq9,0,3," Abstract: There are multiple sides to every story, and while statistical topic models have been highly successful at topically summarizing the stories in corpora of text documents, they do not explicitly address the issue of learning the different sides, the viewpoints, expressed in the documents. In this paper, we show how these viewpoints can be learned completely unsupervised and represented in a human interpretable form. We use a novel approach of applying CorrLDA2 for this purpose, which learns topic-viewpoint relations that can be used to form groups of topics, where each group represents a viewpoint. A corpus of documents about the Israeli-Palestinian conflict is then used to demonstrate how a Palestinian and an Israeli viewpoint can be learned. By leveraging the magnitudes and signs of the feature weights of a linear SVM, we introduce a principled method to evaluate associations between topics and viewpoints. With this, we demonstrate, both quantitatively and qualitatively, that the learned topic groups are contextually coherent, and form consistently correct topic-viewpoint associations. "
765346014278590465,2016-08-16 00:34:43,https://t.co/uFsa4flLGi]),Depth and depth-based classification with R-package ddalpha. (arXiv:1608.04109v1 [https://t.co/uFsa4flLGi]) https://t.co/S0Bn9Cch00,0,3,INDEXERROR
765346011673927680,2016-08-16 00:34:43,https://t.co/Sek9X3qOF2,The Spectral Condition Number Plot for Regularization Parameter Determination. (arXiv:1608.04123v1 [stat.C… https://t.co/Sek9X3qOF2,1,5," Abstract: Many modern statistical applications ask for the estimation of a covariance (or precision) matrix in settings where the number of variables is larger than the number of observations. There exists a broad class of ridge-type estimators that employs regularization to cope with the subsequent singularity of the sample covariance matrix. These estimators depend on a penalty parameter and choosing its value can be hard, in terms of being computationally unfeasible or tenable only for a restricted set of ridge-type estimators. Here we introduce a simple graphical tool, the spectral condition number plot, for informed heuristic penalty parameter selection. The proposed tool is computationally friendly and can be employed for the full class of ridge-type covariance (precision) estimators. "
765346006594707460,2016-08-16 00:34:42,https://t.co/s42spNDTdQ,Generative and Discriminative Voxel Modeling with Convolutional Neural Networks. (arXiv:1608.04236v1 [cs.CV]) https://t.co/s42spNDTdQ,1,7," Abstract: When working with three-dimensional data, choice of representation is key. We explore voxel-based models, and present evidence for the viability of voxellated representations in applications including shape modeling and object classification. Our key contributions are methods for training voxel-based variational autoencoders, a user interface for exploring the latent space learned by the autoencoder, and a deep convolutional neural network architecture for object classification. We address challenges unique to voxel-based representations, and empirically evaluate our models on the ModelNet benchmark, where we demonstrate a 51.5% relative improvement in the state of the art for object classification. "
765346002857517057,2016-08-16 00:34:41,https://t.co/JjKH5rfZFn,The Bayesian Low-Rank Determinantal Point Process Mixture Model. (arXiv:1608.04245v1 [stat.ML]) https://t.co/JjKH5rfZFn,0,4," Abstract: Determinantal point processes (DPPs) are an elegant model for encoding probabilities over subsets, such as shopping baskets, of a ground set, such as an item catalog. They are useful for a number of machine learning tasks, including product recommendation. DPPs are parametrized by a positive semi-definite kernel matrix. Recent work has shown that using a low-rank factorization of this kernel provides remarkable scalability improvements that open the door to training on large-scale datasets and computing online recommendations, both of which are infeasible with standard DPP models that use a full-rank kernel. In this paper we present a low-rank DPP mixture model that allows us to represent the latent structure present in observed subsets as a mixture of a number of component low-rank DPPs, where each component DPP is responsible for representing a portion of the observed data. The mixture model allows us to effectively address the capacity constraints of the low-rank DPP model. We present an efficient and scalable Markov Chain Monte Carlo (MCMC) learning algorithm for our model that uses Gibbs sampling and stochastic gradient Hamiltonian Monte Carlo (SGHMC). Using an evaluation on several real-world product recommendation datasets, we show that our low-rank DPP mixture model provides substantially better predictive performance than is possible with a single low-rank or full-rank DPP, and significantly better performance than several other competing recommendation methods in many cases. "
765346000571625474,2016-08-16 00:34:40,https://t.co/dqyce3JoBp,Robust Volume Minimization-Based Matrix Factorization for Remote Sensing and Document Clustering. (arXiv:1608.0429… https://t.co/dqyce3JoBp,0,2," Abstract: This paper considers \emph{volume minimization} (VolMin)-based structured matrix factorization (SMF). VolMin is a factorization criterion that decomposes a given data matrix into a basis matrix times a structured coefficient matrix via finding the minimum-volume simplex that encloses all the columns of the data matrix. Recent work showed that VolMin guarantees the identifiability of the factor matrices under mild conditions that are realistic in a wide variety of applications. This paper focuses on both theoretical and practical aspects of VolMin. On the theory side, exact equivalence of two independently developed sufficient conditions for VolMin identifiability is proven here, thereby providing a more comprehensive understanding of this aspect of VolMin. On the algorithm side, computational complexity and sensitivity to outliers are two key challenges associated with real-world applications of VolMin. These are addressed here via a new VolMin algorithm that handles volume regularization in a computationally simple way, and automatically detects and {iteratively downweights} outliers, simultaneously. Simulations and real-data experiments using a remotely sensed hyperspectral image and the Reuters document corpus are employed to showcase the effectiveness of the proposed algorithm. "
765345997635682305,2016-08-16 00:34:39,https://t.co/8wMsalMOyC,Consistency constraints for overlapping data clustering. (arXiv:1608.04331v1 [cs.LG]) https://t.co/8wMsalMOyC,0,3," Abstract: We examine overlapping clustering schemes with functorial constraints, in the spirit of Carlsson--Memoli. This avoids issues arising from the chaining required by partition-based methods. Our principal result shows that any clustering functor is naturally constrained to refine single-linkage clusters and be refined by maximal-linkage clusters. We work in the context of metric spaces with non-expansive maps, which is appropriate for modeling data processing which does not increase information content. "
765345995047759872,2016-08-16 00:34:39,https://t.co/9SD5ljxa9O,A Geometric Framework for Convolutional Neural Networks. (arXiv:1608.04374v1 [stat.ML]) https://t.co/9SD5ljxa9O,1,11," Abstract: In this paper, a geometric framework for neural networks is proposed. This framework uses the inner product space structure underlying the parameter set to perform gradient descent not in a component-based form, but in a coordinate-free manner. Convolutional neural networks are described in this framework in a compact form, with the gradients of standard --- and higher-order --- loss functions calculated for each layer of the network. This approach can be applied to other network structures and provides a basis on which to create new networks. "
765345991973302272,2016-08-16 00:34:38,https://t.co/EQ3Xnr1YAR,Order Selection of Autoregressive Processes using Bridge Criterion. (arXiv:1508.02473v3 [math.ST] UPDATED) https://t.co/EQ3Xnr1YAR,1,3," Abstract: We introduce a new criterion to determine the order of an autoregressive model fitted to time series data. It has the benefits of the two well-known model selection techniques, the Akaike information criterion and the Bayesian information criterion. When the data is generated from a finite order autoregression, the Bayesian information criterion is known to be consistent, and so is the new criterion. When the true order is infinity or suitably high with respect to the sample size, the Akaike information criterion is known to be efficient in the sense that its prediction performance is asymptotically equivalent to the best offered by the candidate models; in this case, the new criterion behaves in a similar manner. Different from the two classical criteria, the proposed criterion adaptively achieves either consistency or efficiency depending on the underlying true model. In practice where the observed time series is given without any prior information about the model specification, the proposed order selection criterion is more flexible and robust compared with classical approaches. Numerical results are presented demonstrating the adaptivity of the proposed technique when applied to various datasets. "
765345989590917121,2016-08-16 00:34:38,https://t.co/omVZdsj6jb,On the Online Frank-Wolfe Algorithms for Convex and Non-convex Optimizations. (arXiv:1510.01171v2 [stat.ML] UPDATE… https://t.co/omVZdsj6jb,0,8," Abstract: In this paper, the online variants of the classical Frank-Wolfe algorithm are considered. We consider minimizing the regret with a stochastic cost. The online algorithms only require simple iterative updates and a non-adaptive step size rule, in contrast to the hybrid schemes commonly considered in the literature. Several new results are derived for convex and non-convex losses. With a strongly convex stochastic cost and when the optimal solution lies in the interior of the constraint set or the constraint set is a polytope, the regret bound and anytime optimality are shown to be ${\cal O}( \log^3 T / T )$ and ${\cal O}( \log^2 T / T)$, respectively, where $T$ is the number of rounds played. These results are based on an improved analysis on the stochastic Frank-Wolfe algorithms. Moreover, the online algorithms are shown to converge even when the loss is non-convex, i.e., the algorithms find a stationary point to the time-varying/stochastic loss at a rate of ${\cal O}(\sqrt{1/T})$. Numerical experiments on realistic data sets are presented to support our theoretical claims. "
765345986512318465,2016-08-16 00:34:37,https://t.co/H60pJ4Ylhi,Incremental Method for Spectral Clustering of Increasing Orders. (arXiv:1512.07349v4 [cs.SI] UPDATED) https://t.co/H60pJ4Ylhi,1,7," Abstract: The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs) of a graph Laplacian matrix have been widely used for spectral clustering and community detection. However, in real-life applications the number of clusters or communities (say, $K$) is generally unknown a-priori. Consequently, the majority of the existing methods either choose $K$ heuristically or they repeat the clustering method with different choices of $K$ and accept the best clustering result. The first option, more often, yields suboptimal result, while the second option is computationally expensive. In this work, we propose an incremental method for constructing the eigenspectrum of the graph Laplacian matrix. This method leverages the eigenstructure of graph Laplacian matrix to obtain the $K$-th eigenpairs of the Laplacian matrix given a collection of all the $K-1$ smallest eigenpairs. Our proposed method adapts the Laplacian matrix such that the batch eigenvalue decomposition problem transforms into an efficient sequential leading eigenpair computation problem. As a practical application, we consider user-guided spectral clustering. Specifically, we demonstrate that users can utilize the proposed incremental method for effective eigenpair computation and determining the desired number of clusters based on multiple clustering metrics. "
765345983869911040,2016-08-16 00:34:36,https://t.co/oGhAmN2LdO,"Coordinate Friendly Structures, Algorithms and Applications. (arXiv:1601.00863v3 [math.OC] UPDATED) https://t.co/oGhAmN2LdO",0,2," Abstract: This paper focuses on coordinate update methods, which are useful for solving problems involving large or high-dimensional datasets. They decompose a problem into simple subproblems, where each updates one, or a small block of, variables while fixing others. These methods can deal with linear and nonlinear mappings, smooth and nonsmooth functions, as well as convex and nonconvex problems. In addition, they are easy to parallelize. The great performance of coordinate update methods depends on solving simple sub-problems. To derive simple subproblems for several new classes of applications, this paper systematically studies coordinate-friendly operators that perform low-cost coordinate updates. Based on the discovered coordinate friendly operators, as well as operator splitting techniques, we obtain new coordinate update algorithms for a variety of problems in machine learning, image processing, as well as sub-areas of optimization. Several problems are treated with coordinate update for the first time in history. The obtained algorithms are scalable to large instances through parallel and even asynchronous computing. We present numerical examples to illustrate how effective these algorithms are. "
765345981240049664,2016-08-16 00:34:36,https://t.co/tpb6FyqDiL,An Adaptive Resample-Move Algorithm for Estimating Normalizing Constants. (arXiv:1604.01972v2 [stat.ML] UPDATED) https://t.co/tpb6FyqDiL,0,2," Abstract: The estimation of normalizing constants is a fundamental step in probabilistic model comparison. Sequential Monte Carlo methods may be used for this task and have the advantage of being inherently parallelizable. However, the standard choice of using a fixed number of particles at each iteration is suboptimal because some steps will contribute disproportionately to the variance of the estimate. We introduce an adaptive version of the Resample-Move algorithm, in which the particle set is adaptively expanded whenever a better approximation of an intermediate distribution is needed. The algorithm builds on the expression for the optimal number of particles and the corresponding minimum variance found under ideal conditions. Benchmark results on challenging Gaussian Process Classification and Restricted Boltzmann Machine applications show that Adaptive Resample-Move (ARM) estimates the normalizing constant with a smaller variance, using less computational resources, than either Resample-Move with a fixed number of particles or Annealed Importance Sampling. A further advantage over Annealed Importance Sampling is that ARM is easier to tune. "
765345978983514113,2016-08-16 00:34:35,https://t.co/x7TywAZQbW,Agnostic Estimation of Mean and Covariance. (arXiv:1604.06968v2 [cs.DS] UPDATED) https://t.co/x7TywAZQbW,0,2," Abstract: We consider the problem of estimating the mean and covariance of a distribution from iid samples in $\mathbb{R}^n$, in the presence of an $\eta$ fraction of malicious noise; this is in contrast to much recent work where the noise itself is assumed to be from a distribution of known type. The agnostic problem includes many interesting special cases, e.g., learning the parameters of a single Gaussian (or finding the best-fit Gaussian) when $\eta$ fraction of data is adversarially corrupted, agnostically learning a mixture of Gaussians, agnostic ICA, etc. We present polynomial-time algorithms to estimate the mean and covariance with error guarantees in terms of information-theoretic lower bounds. As a corollary, we also obtain an agnostic algorithm for Singular Value Decomposition. "
765345975737184256,2016-08-16 00:34:34,https://t.co/Qfh9Rt0wWu,Tutorial on Variational Autoencoders. (arXiv:1606.05908v2 [stat.ML] UPDATED) https://t.co/Qfh9Rt0wWu,4,17," Abstract: In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed. "
765345970712371201,2016-08-16 00:34:33,https://t.co/wJPSNrTN5p,Scaling Up Sparse Support Vector Machine by Simultaneous Feature and Sample Reduction. (arXiv:1607.06996v2 [stat.M… https://t.co/wJPSNrTN5p,0,3," Abstract: Sparse support vector machine (SVM) is a popular classification technique that can simultaneously learn a small set of the most interpretable features. It has achieved great success in many real-world applications. However, for large-scale problems involving a huge number of samples and extremely high-dimensional features, solving sparse SVM remains challenging. By noting that sparse SVM induces sparsities in both feature and sample spaces, we propose a novel approach---that is based on accurate estimations of the primal and dual optimums of sparse SVM---to simultaneously identify the features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identified samples and features from the training phase, which may lead to substantial savings in both memory usage and computational cost without sacrificing accuracy. To the best of our knowledge, the proposed method is the \emph{first} \emph{static} feature and sample reduction method for sparse SVM. Experiments on both synthetic and real data sets (e.g., the kddb data set with about 20 million of samples and 30 million of features) demonstrate that our approach significantly outperforms existing state-of-the-art methods and the speedup gained by our approach can be orders of magnitude. "
764982763128774656,2016-08-15 00:31:18,https://t.co/BChOGiJENQ,Faster Training of Very Deep Networks Via p-Norm Gates. (arXiv:1608.03639v1 [stat.ML]) https://t.co/BChOGiJENQ,2,12," Abstract: A major contributing factor to the recent advances in deep neural networks is structural units that let sensory information and gradients to propagate easily. Gating is one such structure that acts as a flow control. Gates are employed in many recent state-of-the-art recurrent models such as LSTM and GRU, and feedforward models such as Residual Nets and Highway Networks. This enables learning in very deep networks with hundred layers and helps achieve record-breaking results in vision (e.g., ImageNet with Residual Nets) and NLP (e.g., machine translation with GRU). However, there is limited work in analysing the role of gating in the learning process. In this paper, we propose a flexible $p$-norm gating scheme, which allows user-controllable flow and as a consequence, improve the learning speed. This scheme subsumes other existing gating schemes, including those in GRU, Highway Networks and Residual Nets as special cases. Experiments on large sequence and vector datasets demonstrate that the proposed gating scheme helps improve the learning speed significantly without extra overhead. "
764982761518198784,2016-08-15 00:31:17,https://t.co/yGvNYKFUhv,Beyond Spectral: Tight Bounds for Planted Gaussians. (arXiv:1608.03643v1 [cs.LG]) https://t.co/yGvNYKFUhv,0,3," Abstract: We consider the following general hidden hubs model: an $n \times n$ random matrix $A$ with a subset $S$ of $k$ special rows (hubs): entries in rows outside $S$ are generated from the probability distribution $p_0 \sim N(0,\sigma_0^2)$; for each row in $S$, some $k$ of its entries are generated from $p_1 \sim N(0,\sigma_1^2)$, $\sigma_1>\sigma_0$, and the rest of the entries from $p_0$. The problem is to identify the high-degree hubs efficiently. This model includes and significantly generalizes the planted Gaussian Submatrix Model, where the special entries are all in a $k \times k$ submatrix. There are two well-known barriers: if $k\geq c\sqrt{n\ln n}$, just the row sums are sufficient to find $S$ in the general model. For the submatrix problem, this can be improved by a $\sqrt{\ln n}$ factor to $k \ge c\sqrt{n}$ by spectral methods or combinatorial methods. In the variant with $p_0=\pm 1$ (with probability $1/2$ each) and $p_1\equiv 1$, neither barrier has been broken. We give a polynomial-time algorithm to identify all the hidden hubs with high probability for $k \ge n^{0.5-\delta}$ for some $\delta >0$, when $\sigma_1^2>2\sigma_0^2$. The algorithm extends to the setting where planted entries might have different variances each at least as large as $\sigma_1^2$. We also show a nearly matching lower bound: for $\sigma_1^2 \le 2\sigma_0^2$, there is no polynomial-time Statistical Query algorithm for distinguishing between a matrix whose entries are all from $N(0,\sigma_0^2)$ and a matrix with $k=n^{0.5-\delta}$ hidden hubs for any $\delta >0$. The lower bound as well as the algorithm are related to whether the chi-squared distance of the two distributions diverges. At the critical value $\sigma_1^2=2\sigma_0^2$, we show that the general hidden hubs problem can be solved for $k\geq c\sqrt n(\ln n)^{1/4}$, improving on the naive row sum-based method. "
764982760268333056,2016-08-15 00:31:17,https://t.co/ae6HEebjhv,Learning Structured Sparsity in Deep Neural Networks. (arXiv:1608.03665v1 [cs.NE]) https://t.co/ae6HEebjhv,2,7," Abstract: High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation. Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by around ~1%. Open source code is in this https URL "
764982758791901188,2016-08-15 00:31:17,https://t.co/RWO3HHyBQU,Content-based image retrieval tutorial. (arXiv:1608.03811v1 [stat.ML]) https://t.co/RWO3HHyBQU,2,8," Abstract: This paper functions as a tutorial for individuals interested to enter the field of information retrieval but wouldn't know where to begin from. It describes two fundamental yet efficient image retrieval techniques, the first being k - nearest neighbors (knn) and the second support vector machines(svm). The goal is to provide the reader with both the theoretical and practical aspects in order to acquire a better understanding. Along with this tutorial we have also developed the equivalent software1 using the MATLAB environment in order to illustrate the techniques, so that the reader can have a hands-on experience. "
764982756950548480,2016-08-15 00:31:16,https://t.co/172DkPmEg4,Scaling Factorial Hidden Markov Models: Stochastic Variational Inference without Messages. (arXiv:1608.03817v1 [st… https://t.co/172DkPmEg4,4,14," Abstract: Factorial Hidden Markov Models (FHMMs) are powerful models for sequential data but they do not scale well with long sequences. We propose a scalable inference and learning algorithm for FHMMs that draws on ideas from the stochastic variational inference, neural network and copula literatures. Unlike existing approaches, the proposed algorithm requires no message passing procedure among latent variables and can be distributed to a network of computers to speed up learning. Our experiments corroborate that the proposed algorithm does not introduce further approximation bias compared to the proven structured mean-field algorithm, and achieves better performance with long sequences and large FHMMs. "
764982755155439616,2016-08-15 00:31:16,https://t.co/BqTPCgalwb,Faster Stochastic Variational Inference using Proximal-Gradient Methods with General Divergence Functions. (arXiv:… https://t.co/BqTPCgalwb,1,8," Abstract: Several recent works have explored stochastic gradient methods for variational inference that exploit the geometry of the variational-parameter space. However, the theoretical properties of these methods are not well-understood and these methods typically only apply to conditionally-conjugate models. We present a new stochastic method for variational inference which exploits the geometry of the variational-parameter space and also yields simple closed-form updates even for non-conjugate models. We also give a convergence-rate analysis of our method and many other previous methods which exploit the geometry of the space. Our analysis generalizes existing convergence results for stochastic mirror-descent on non-convex objectives by using a more general class of divergence functions. Beyond giving a theoretical justification for a variety of recent methods, our experiments show that new algorithms derived in this framework lead to state of the art results on a variety of problems. Further, due to its generality, we expect that our theoretical analysis could also apply to other applications. "
764982752064204800,2016-08-15 00:31:15,https://t.co/o59WJAsqpi,RSG: Beating Subgradient Method without Smoothness and Strong Convexity. (arXiv:1512.03107v11 [math.OC] UPDATED) https://t.co/o59WJAsqpi,0,3," Abstract: In this paper, we study the efficiency of a {\bf R}estarted {\bf S}ub{\bf G}radient (RSG) method that periodically restarts the standard subgradient method (SG). We show that, when applied to a broad class of convex optimization problems, RSG method can find an $\epsilon$-optimal solution with a low complexity than SG method. In particular, we first show that RSG can reduce the dependence of SG's iteration complexity on the distance between the initial solution and the optimal set to that between the $\epsilon$-level set and the optimal set. In addition, we show the advantages of RSG over SG in solving three different families of convex optimization problems. (a) For the problems whose epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with local quadratic growth property, RSG has an $O(\frac{1}{\epsilon}\log(\frac{1}{\epsilon}))$ iteration complexity. (c) For the problems that admit a local Kurdyka-\L ojasiewicz property with a power constant of $\beta\in[0,1)$, RSG has an $O(\frac{1}{\epsilon^{2\beta}}\log(\frac{1}{\epsilon}))$ iteration complexity. On the contrary, with only the standard analysis, the iteration complexity of SG is known to be $O(\frac{1}{\epsilon^2})$ for these three classes of problems. The novelty of our analysis lies at exploiting the lower bound of the first-order optimality residual at the $\epsilon$-level set. It is this novelty that allows us to explore the local properties of functions (e.g., local quadratic growth property, local Kurdyka-\L ojasiewicz property, more generally local error bounds) to develop the improved convergence of RSG. We demonstrate the effectiveness of the proposed algorithms on several machine learning tasks including regression and classification. "
764982750130630660,2016-08-15 00:31:14,https://t.co/jnQgFN5aIV,Revealing Fundamental Physics from the Daya Bay Neutrino Experiment using Deep Neural Networks. (arXiv:1601.07621v… https://t.co/jnQgFN5aIV,1,5," Abstract: Experiments in particle physics produce enormous quantities of data that must be analyzed and interpreted by teams of physicists. This analysis is often exploratory, where scientists are unable to enumerate the possible types of signal prior to performing the experiment. Thus, tools for summarizing, clustering, visualizing and classifying high-dimensional data are essential. In this work, we show that meaningful physical content can be revealed by transforming the raw data into a learned high-level representation using deep neural networks, with measurements taken at the Daya Bay Neutrino Experiment as a case study. We further show how convolutional deep neural networks can provide an effective classification filter with greater than 97% accuracy across different classes of physics events, significantly better than other machine learning approaches. "
764982748117340165,2016-08-15 00:31:14,https://t.co/JkVbdAPQo2,Does quantification without adjustments work?. (arXiv:1602.08780v2 [stat.ML] UPDATED) https://t.co/JkVbdAPQo2,0,3," Abstract: Classification is the task of predicting the class labels of objects based on the observation of their features. In contrast, quantification has been defined as the task of determining the prevalences of the different sorts of class labels in a target dataset. The simplest approach to quantification is Classify & Count where a classifier is optimised for classification on a training set and applied to the target dataset for the prediction of class labels. In the case of binary quantification, the number of predicted positive labels is then used as an estimate of the prevalence of the positive class in the target dataset. Since the performance of Classify & Count for quantification is known to be inferior its results typically are subject to adjustments. However, some researchers recently have suggested that Classify & Count might actually work without adjustments if it is based on a classifer that was specifically trained for quantification. We discuss the theoretical foundation for this claim and explore its potential and limitations with a numerical example based on the binormal model with equal variances. In order to identify an optimal quantifier in the binormal setting, we introduce the concept of local Bayes optimality. As a side remark, we present a complete proof of a theorem by Ye et al. (2012). "
764982746028576768,2016-08-15 00:31:14,https://t.co/sk6KFAkoJE,"Jacques Lacan's Registers of the Psychoanalytic Field, Applied using Geometric Data Analysis to Edgar Allan Poe's … https://t.co/sk6KFAkoJE",0,2," Abstract: In a first investigation, a Lacan-motivated template of the Poe story is fitted to the data. A segmentation of the storyline is used in order to map out the diachrony. Based on this, it will be shown how synchronous aspects, potentially related to Lacanian registers, can be sought. This demonstrates the effectiveness of an approach based on a model template of the storyline narrative. In a second and more comprehensive investigation, we develop an approach for revealing, that is, uncovering, Lacanian register relationships. Objectives of this work include the wide and general application of our methodology. This methodology is strongly based on the ""letting the data speak"" Correspondence Analysis analytics platform of Jean-Paul Benz\'ecri, that is also the geometric data analysis, both qualitative and quantitative analytics, developed by Pierre Bourdieu. "
764982743965065217,2016-08-15 00:31:13,https://t.co/zHBDOJqNyt,Solving Systems of Random Quadratic Equations via Truncated Amplitude Flow. (arXiv:1605.08285v3 [stat.ML] UPDATED) https://t.co/zHBDOJqNyt,0,3," Abstract: This paper puts forth a new algorithm, termed \emph{truncated amplitude flow} (TAF), to recover an unknown $n$-dimensional real-/complex-valued vector $\bm{x}$ from $m$ quadratic equations of the form $y_i=|\langle\bm{a}_i,\bm{x}\rangle|^2$. This problem is known to be \emph{NP-hard} in general. We prove that as soon as the number of equations $m$ is on the order of the number of unknowns $n$, TAF recovers the solution exactly (up to a global unimodular constant) with high probability and complexity growing linearly with the time required to read the data. Our method adopts the \emph{amplitude-based} cost function and proceeds in two stages: In stage one, we introduce an \emph{orthogonality-promoting} initialization that is obtained with a few simple power iterations. Stage two refines the initial estimate by successive updates of scalable \emph{truncated generalized gradient iterations}. The former is in sharp contrast to existing spectral initializations, while the latter handles the rather challenging nonconvex and nonsmooth amplitude-based cost function. In particular for real-valued vectors, our gradient truncation rule provably eliminates the erroneously estimated signs with high probability to markedly improve upon its untruncated version. Numerical tests demonstrate that our initialization method returns more accurate and robust estimates relative to its spectral counterparts. Furthermore, even under the same initialization, our amplitude-based refinement outperforms Wirtinger-based alternatives, corroborating the superior performance of TAF over state-of-the-art algorithms. "
764982741083492352,2016-08-15 00:31:12,https://t.co/BqsyQaIjIO,Mini-Batch Spectral Clustering. (arXiv:1607.02024v2 [stat.ML] UPDATED) https://t.co/BqsyQaIjIO,0,5," Abstract: The cost of computing the spectrum of Laplacian matrices hinders the application of spectral clustering to large data sets. While approximations recover computational tractability, they can potentially affect clustering performance. This paper proposes a practical approach to learn spectral clustering based on adaptive stochastic gradient optimization. Crucially, the proposed approach recovers the exact spectrum of Laplacian matrices in the limit of the iterations, and the cost of each iteration is linear in the number of samples. Extensive experimental validation on data sets with up to half a million samples demonstrate its scalability and its ability to outperform state-of-the-art approximate methods to learn spectral clustering for a given computational budget. "
764982736058802177,2016-08-15 00:31:11,https://t.co/MbiuPcldpu,A Low Complexity Algorithm with $O(\sqrt{T})$ Regret and Constraint Violations for Online Convex Optimization with… https://t.co/MbiuPcldpu,0,4," Abstract: This paper considers online convex optimization over a complicated constraint set, which typically consists of multiple functional constraints and a set constraint. The conventional projection based online projection algorithm (Zinkevich, 2003) can be difficult to implement due to the potentially high computation complexity of the projection operation. In this paper, we relax the functional constraints by allowing them to be violated at each round but still requiring them to be satisfied in the long term. This type of relaxed online convex optimization (with long term constraints) was first considered in Mahdavi et al. (2012). That prior work proposes an algorithm to achieve $O(\sqrt{T})$ regret and $O(T^{3/4})$ constraint violations for general problems and another algorithm to achieve an $O(T^{2/3})$ bound for both regret and constraint violations when the constraint set can be described by a finite number of linear constraints. A recent extension in Jenatton et al. (2016) can achieve $O(T^{\max\{\beta,1-\beta\}})$ regret and $O(T^{1-\beta/2})$ constraint violations where $\beta\in (0,1)$. The current paper proposes a new simple algorithm that yields improved performance in comparison to prior works. The new algorithm achieves an $O(\sqrt{T})$ regret bound with finite constraint violations. "
763896640608567297,2016-08-12 00:35:26,https://t.co/7yzR4bfp9a,Temporal Learning and Sequence Modeling for a Job Recommender System. (arXiv:1608.03333v1 [cs.LG]) https://t.co/7yzR4bfp9a,1,11," Abstract: We present our solution to the job recommendation task for RecSys Challenge 2016. The main contribution of our work is to combine temporal learning with sequence modeling to capture complex user-item activity patterns to improve job recommendations. First, we propose a time-based ranking model applied to historical observations and a hybrid matrix factorization over time re-weighted interactions. Second, we exploit sequence properties in user-items activities and develop a RNN-based recommendation model. Our solution achieved 5$^{th}$ place in the challenge among more than 100 participants. Notably, the strong performance of our RNN approach shows a promising new direction in employing sequence modeling for recommendation systems. "
763896636884082688,2016-08-12 00:35:25,https://t.co/Rb1SMcgEQ7,Distributed Learning with Regularized Least Squares. (arXiv:1608.03339v1 [cs.LG]) https://t.co/Rb1SMcgEQ7,3,4," Abstract: We study distributed learning with the least squares regularization scheme in a reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach, the algorithm partitions a data set into disjoint data subsets, applies the least squares regularization scheme to each data subset to produce an output function, and then takes an average of the individual output functions as a final global estimator or predictor. We show with error bounds in expectation in both the $L^2$-metric and RKHS-metric that the global output function of this distributed learning is a good approximation to the algorithm processing the whole data in one single machine. Our error bounds are sharp and stated in a general setting without any eigenfunction assumption. The analysis is achieved by a novel second order decomposition of operator differences in our integral operator approach. Even for the classical least squares regularization scheme in the RKHS associated with a general kernel, we give the best learning rate in the literature. "
763896632198959105,2016-08-12 00:35:24,https://t.co/jhFvQBMJLE,The Future of Data Analysis in the Neurosciences. (arXiv:1608.03465v1 [q-bio.NC]) https://t.co/jhFvQBMJLE,1,13," Abstract: Neuroscience is undergoing faster changes than ever before. Over 100 years our field qualitatively described and invasively manipulated single or few organisms to gain anatomical, physiological, and pharmacological insights. In the last 10 years neuroscience spawned quantitative big-sample datasets on microanatomy, synaptic connections, optogenetic brain-behavior assays, and high-level cognition. While growing data availability and information granularity have been amply discussed, we direct attention to a routinely neglected question: How will the unprecedented data richness shape data analysis practices? Statistical reasoning is becoming more central to distill neurobiological knowledge from healthy and pathological brain recordings. We believe that large-scale data analysis will use more models that are non-parametric, generative, mixing frequentist and Bayesian aspects, and grounded in different statistical inferences. "
763896628214427648,2016-08-12 00:35:23,https://t.co/SoM6rMWeSH,A Richer Theory of Convex Constrained Optimization with Reduced Projections and Improved Rates. (arXiv:1608.03487v… https://t.co/SoM6rMWeSH,1,5," Abstract: This paper focuses on convex constrained optimization problems, where the solution is subject to a convex inequality constraint. In particular, we aim at challenging problems for which both projection into the constrained domain and a linear optimization under the inequality constraint are time-consuming, which render both projected gradient methods and conditional gradient methods (a.k.a. the Frank-Wolfe algorithm) expensive. In this paper, we develop projection reduced optimization algorithms for both smooth and non-smooth optimization with improved convergence rates. We first present a general theory of optimization with only one projection. Its application to smooth optimization with only one projection yields $O(1/\epsilon)$ iteration complexity, which can be further reduced under strong convexity and improves over the $O(1/\epsilon^2)$ iteration complexity established before for non-smooth optimization. Then we introduce the local error bound condition and develop faster convergent algorithms for non-strongly convex optimization at the price of a logarithmic number of projections. In particular, we achieve a convergence rate of $\widetilde O(1/\epsilon^{2(1-\theta)})$ for non-smooth optimization and $\widetilde O(1/\epsilon^{1-\theta})$ for smooth optimization, where $\theta\in(0,1]$ is a constant in the local error bound condition. An experiment on solving the constrained $\ell_1$ minimization problem in compressive sensing demonstrates that the proposed algorithm achieve significant speed-up. "
763896624217219077,2016-08-12 00:35:22,https://t.co/syDZRzN98g,Semi-Supervised Prediction of Gene Regulatory Networks Using Machine Learning Algorithms. (arXiv:1608.03530v1 [cs.… https://t.co/syDZRzN98g,2,6," Abstract: Use of computational methods to predict gene regulatory networks (GRNs) from gene expression data is a challenging task. Many studies have been conducted using unsupervised methods to fulfill the task; however, such methods usually yield low prediction accuracies due to the lack of training data. In this article, we propose semi-supervised methods for GRN prediction by utilizing two machine learning algorithms, namely support vector machines (SVM) and random forests (RF). The semi-supervised methods make use of unlabeled data for training. We investigate inductive and transductive learning approaches, both of which adopt an iterative procedure to obtain reliable negative training data from the unlabeled data. We then apply our semi-supervised methods to gene expression data of Escherichia coli and Saccharomyces cerevisiae, and evaluate the performance of our methods using the expression data. Our analysis indicated that the transductive learning approach outperformed the inductive learning approach for both organisms. However, there was no conclusive difference identified in the performance of SVM and RF. Experimental results also showed that the proposed semi-supervised methods performed better than existing supervised methods for both organisms. "
763896621214171136,2016-08-12 00:35:21,https://t.co/e7bwVqltzh,QPass: a Merit-based Evaluation of Soccer Passes. (arXiv:1608.03532v1 [cs.AI]) https://t.co/e7bwVqltzh,0,2," Abstract: Quantitative analysis of soccer players' passing ability focuses on descriptive statistics without considering the players' real contribution to the passing and ball possession strategy of their team. Which player is able to help the build-up of an attack, or to maintain the possession of the ball? We introduce a novel methodology called QPass to answer questions like these quantitatively. Based on the analysis of an entire season, we rank the players based on the intrinsic value of their passes using QPass. We derive an album of pass trajectories for different gaming styles. Our methodology reveals a quite counterintuitive paradigm: losing the ball possession could lead to better chances to win a game. "
763896617925828608,2016-08-12 00:35:20,https://t.co/AOxUnkK4Zm,Sequence Graph Transform (SGT): A Feature Extraction Function for Sequence Data Mining. (arXiv:1608.03533v1 [stat.… https://t.co/AOxUnkK4Zm,4,8," Abstract: The ubiquitous presence of sequence data across fields such as the web, healthcare, bioinformatics, and text mining has made sequence mining a vital research area. However, sequence mining is particularly challenging because of difficulty in finding (dis)similarity/distance between sequences. This is because a distance measure between sequences is not obvious due to their unstructuredness---arbitrary strings of arbitrary length. Feature representations, such as n-grams, are often used but they either compromise on extracting both short- and long-term sequence patterns or have a high computation. We propose a new function, Sequence Graph Transform (SGT), that extracts the short- and long-term sequence features and embeds them in a finite-dimensional feature space. Importantly, SGT has low computation and can extract any amount of short- to long-term patterns without any increase in the computation, also proved theoretically in this paper. Due to this, SGT yields superior result with significantly higher accuracy and lower computation compared to the existing methods. We show it via several experimentation and SGT's real world application for clustering, classification, search and visualization as examples. "
763896615019118593,2016-08-12 00:35:20,https://t.co/BouBKkwk1H,Online Context-Dependent Clustering in Recommendations based on Exploration-Exploitation Algorithms. (arXiv:1608.0… https://t.co/BouBKkwk1H,0,4," Abstract: We investigate two context-dependent clustering techniques for content recommendation based on exploration-exploitation strategies in contextual multi-armed bandit settings. Our algorithms dynamically group users based on the items under consideration and, possibly, group items based on the similarity of the clusterings induced over the users. The resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods. We provide an empirical analysis on extensive real-world datasets, showing scalability and increased prediction performance over state-of-the-art methods for clustering bandits. For one of the two algorithms we also give a regret analysis within a standard linear stochastic noise setting. "
763896611701460994,2016-08-12 00:35:19,https://t.co/rjukZ2SGCo,Warm Starting Bayesian Optimization. (arXiv:1608.03585v1 [stat.ML]) https://t.co/rjukZ2SGCo,0,8," Abstract: We develop a framework for warm-starting Bayesian optimization, that reduces the solution time required to solve an optimization problem that is one in a sequence of related problems. This is useful when optimizing the output of a stochastic simulator that fails to provide derivative information, for which Bayesian optimization methods are well-suited. Solving sequences of related optimization problems arises when making several business decisions using one optimization model and input data collected over different time periods or markets. While many gradient-based methods can be warm started by initiating optimization at the solution to the previous problem, this warm start approach does not apply to Bayesian optimization methods, which carry a full metamodel of the objective function from iteration to iteration. Our approach builds a joint statistical model of the entire collection of related objective functions, and uses a value of information calculation to recommend points to evaluate. "
763896608966709248,2016-08-12 00:35:18,https://t.co/3TuwxEgGNl,Dual Control for Approximate Bayesian Reinforcement Learning. (arXiv:1510.03591v2 [stat.ML] UPDATED) https://t.co/3TuwxEgGNl,1,8," Abstract: Control of non-episodic, finite-horizon dynamical systems with uncertain dynamics poses a tough and elementary case of the exploration-exploitation trade-off. Bayesian reinforcement learning, reasoning about the effect of actions and future observations, offers a principled solution, but is intractable. We review, then extend an old approximate approach from control theory---where the problem is known as dual control---in the context of modern regression methods, specifically generalized linear regression. Experiments on simulated systems show that this framework offers a useful approximation to the intractable aspects of Bayesian RL, producing structured exploration strategies that differ from standard RL approaches. We provide simple examples for the use of this framework in (approximate) Gaussian process regression and feedforward neural networks for the control of exploration. "
763896605728710656,2016-08-12 00:35:17,https://t.co/Jugwtm7vKN,Q($\lambda$) with Off-Policy Corrections. (arXiv:1602.04951v2 [cs.AI] UPDATED) https://t.co/Jugwtm7vKN,0,4," Abstract: We propose and analyze an alternate approach to off-policy multi-step temporal difference learning, in which off-policy returns are corrected with the current Q-function in terms of rewards, rather than with the target policy in terms of transition probabilities. We prove that such approximate corrections are sufficient for off-policy convergence both in policy evaluation and control, provided certain conditions. These conditions relate the distance between the target and behavior policies, the eligibility trace parameter and the discount factor, and formalize an underlying tradeoff in off-policy TD($\lambda$). We illustrate this theoretical relationship empirically on a continuous-state control task. "
763896599034654726,2016-08-12 00:35:16,https://t.co/rdvkaGfucc,Towards Representation Learning with Tractable Probabilistic Models. (arXiv:1608.02341v1 [cs.LG] CROSS LISTED) https://t.co/rdvkaGfucc,1,10," Abstract: Probabilistic models learned as density estimators can be exploited in representation learning beside being toolboxes used to answer inference queries only. However, how to extract useful representations highly depends on the particular model involved. We argue that tractable inference, i.e. inference that can be computed in polynomial time, can enable general schemes to extract features from black box models. We plan to investigate how Tractable Probabilistic Models (TPMs) can be exploited to generate embeddings by random query evaluations. We devise two experimental designs to assess and compare different TPMs as feature extractors in an unsupervised representation learning framework. We show some experimental results on standard image datasets by applying such a method to Sum-Product Networks and Mixture of Trees as tractable models generating embeddings. "
763536504673210368,2016-08-11 00:44:23,https://t.co/wJiMg74eGk,Dynamic Principal Component Analysis: Identifying the Relationship between Multiple Air Pollutants. (arXiv:1608.03… https://t.co/wJiMg74eGk,1,5," Abstract: The dynamic nature of air quality chemistry and transport makes it difficult to identify the mixture of air pollutants for a region. In this study of air quality in the Houston metropolitan area we apply dynamic principal component analysis (DPCA) to a normalized multivariate time series of daily concentration measurements of five pollutants (O3, CO, NO2, SO2, PM2.5) from January 1, 2009 through December 31, 2011 for each of the 24 hours in a day. The resulting dynamic components are examined by hour across days for the 3 year period. Diurnal and seasonal patterns are revealed underlining times when DPCA performs best and two principal components (PCs) explain most variability in the multivariate series. DPCA is shown to be superior to static principal component analysis (PCA) in discovery of linear relations among transformed pollutant measurements. DPCA captures the time-dependent correlation structure of the underlying pollutants recorded at up to 34 monitoring sites in the region. In winter mornings the first principal component (PC1) (mainly CO and NO2) explains up to 70% of variability. Augmenting with the second principal component (PC2) (mainly driven by SO2) the explained variability rises to 90%. In the afternoon, O3 gains prominence in the second principal component. The seasonal profile of PCs' contribution to variance loses its distinction in the afternoon, yet cumulatively PC1 and PC2 still explain up to 65% of variability in ambient air data. DPCA provides a strategy for identifying the changing air quality profile for the region studied. "
763536503519707136,2016-08-11 00:44:22,https://t.co/fsQ9SXGqW0,Stochastic Rank-1 Bandits. (arXiv:1608.03023v1 [cs.LG]) https://t.co/fsQ9SXGqW0,1,3," Abstract: We propose stochastic rank-$1$ bandits, a class of online learning problems where at each step a learning agent chooses a pair of row and column arms, and receives the product of their values as a reward. The challenge is that the values of the row and column are unobserved. These values are stochastic and drawn independently of each other. We propose an efficient algorithm for solving our problem, Rank1Elim, and derive a $O((K + L) (1 / \Delta) \log n)$ upper bound on its $n$-step regret, where $K$ is the number of rows, $L$ is the number of columns, and $\Delta$ is the minimum of the row and column gaps. This is the first bandit algorithm for finding the maximum entry of a rank-$1$ matrix whose regret is linear in $K + L$, $1 / \Delta$, and $\log n$. We evaluate our proposed algorithm on both synthetic and real-world problems, and observe that it leverages the structure of our problems and can learn near-optimal solutions even when our modeling assumptions are mildly violated. "
763536502181732353,2016-08-11 00:44:22,https://t.co/GBdv9iQdNz,Combinatorial Inference for Graphical Models. (arXiv:1608.03045v1 [math.ST]) https://t.co/GBdv9iQdNz,1,6," Abstract: We propose a new family of combinatorial inference problems for graphical models. Unlike classical statistical inference where the main interest is point estimation or parameter testing, combinatorial inference aims at testing the global structure of the underlying graph. Examples include testing the graph connectivity, the presence of a cycle of certain size, or the maximum degree of the graph. To begin with, we develop a unified theory for the fundamental limits of a large family of combinatorial inference problems. We propose new concepts including structural packing and buffer entropies to characterize how the complexity of combinatorial graph structures impacts the corresponding minimax lower bounds. On the other hand, we propose a family of novel and practical structural testing algorithms to match the lower bounds. We provide thorough numerical results on both synthetic graphical models and brain networks to illustrate the usefulness of these proposed methods. "
763536500453675008,2016-08-11 00:44:22,https://t.co/pOxquMdFEZ,Estimation from Indirect Supervision with Linear Moments. (arXiv:1608.03100v1 [stat.ML]) https://t.co/pOxquMdFEZ,0,3," Abstract: In structured prediction problems where we have indirect supervision of the output, maximum marginal likelihood faces two computational obstacles: non-convexity of the objective and intractability of even a single gradient computation. In this paper, we bypass both obstacles for a class of what we call linear indirectly-supervised problems. Our approach is simple: we solve a linear system to estimate sufficient statistics of the model, which we then use to estimate parameters via convex optimization. We analyze the statistical properties of our approach and show empirically that it is effective in two settings: learning with local privacy constraints and learning from low-cost count-based annotations. "
763536499254190080,2016-08-11 00:44:21,https://t.co/uFsa4flLGi],Interacting Particle Markov Chain Monte Carlo. (arXiv:1602.05128v2 [https://t.co/uFsa4flLGi] UPDATED) https://t.co/GHK3iNYYO4,0,5,INDEXERROR
763536496896933889,2016-08-11 00:44:21,https://t.co/HKvXlNqlac,Demystifying Fixed k-Nearest Neighbor Information Estimators. (arXiv:1604.03006v2 [cs.LG] UPDATED) https://t.co/HKvXlNqlac,1,4," Abstract: Estimating mutual information from i.i.d. samples drawn from an unknown joint density function is a basic statistical problem of broad interest with multitudinous applications. The most popular estimator is one proposed by Kraskov and St\""ogbauer and Grassberger (KSG) in 2004, and is nonparametric and based on the distances of each sample to its $k^{\rm th}$ nearest neighboring sample, where $k$ is a fixed small integer. Despite its widespread use (part of scientific software packages), theoretical properties of this estimator have been largely unexplored. In this paper we demonstrate that the estimator is consistent and also identify an upper bound on the rate of convergence of the bias as a function of number of samples. We argue that the superior performance benefits of the KSG estimator stems from a curious ""correlation boosting"" effect and build on this intuition to modify the KSG estimator in novel ways to construct a superior estimator. As a byproduct of our investigations, we obtain nearly tight rates of convergence of the $\ell_2$ error of the well known fixed $k$ nearest neighbor estimator of differential entropy by Kozachenko and Leonenko. "
763171810301079553,2016-08-10 00:35:13,https://t.co/U69s3UYGkC,Revisiting Causality in Markov Chains. (arXiv:1608.02658v1 [stat.ML]) https://t.co/U69s3UYGkC,3,11," Abstract: Identifying causal relationships is a key premise of scientific research. Given the mass of observational data in many disciplines, new machine learning methods offer the possibility of using an empirical approach to identifying unappreciated causal relationships and to understanding causal behavior. Conventional methods of causality inference from observational data require a considerable length of time series data to capture cause and effect relationships. We believe that important causal relationships can be inferred from the composition of one-step transition rates (Markov Chains) to and from an event. Here we introduce 'Causality Inference using Composition of Transitions' (CICT), a computationally efficient method that reveals causal structure with high accuracy. We characterize the differences in causes, effects, and random events in the composition of their inputs and outputs. To demonstrate our method, we have used an administrative inpatient healthcare dataset to set up a graph network of patients transition between different diagnoses. Then we apply our method to patients transition graph, revealing deep and complex causal structure between clinical conditions. Our method is highly accurate in predicting whether a transition in a Markov chain is causal or random and performs well in identifying the direction of causality in bidirectional associations. Moreover, CICT brings in new information that enables unsupervised clustering methods to discriminate causality from randomness. Comprehensive performance analysis using C-statistics, goodness-of-fit statistics and decision analysis of predictive models, as well as comparison with the medical ground truth, validates our findings. "
763171808455565312,2016-08-10 00:35:12,https://t.co/A2sGT2yUQs,A deep language model for software code. (arXiv:1608.02715v1 [cs.SE]) https://t.co/A2sGT2yUQs,1,7," Abstract: Existing language models such as n-grams for software code often fail to capture a long context where dependent code elements scatter far apart. In this paper, we propose a novel approach to build a language model for software code to address this particular issue. Our language model, partly inspired by human memory, is built upon the powerful deep learning-based Long Short Term Memory architecture that is capable of learning long-term dependencies which occur frequently in software code. Results from our intrinsic evaluation on a corpus of Java projects have demonstrated the effectiveness of our language model. This work contributes to realizing our vision for DeepSoft, an end-to-end, generic deep learning-based framework for modeling software and its development process. "
763171807184687104,2016-08-10 00:35:12,https://t.co/2XRW9ajOEV,Posterior Sampling for Reinforcement Learning Without Episodes. (arXiv:1608.02731v1 [stat.ML]) https://t.co/2XRW9ajOEV,0,11," Abstract: This is a brief technical note to clarify some of the issues with applying the application of the algorithm posterior sampling for reinforcement learning (PSRL) in environments without fixed episodes. In particular, this paper aims to: - Review some of results which have been proven for finite horizon MDPs (Osband et al 2013, 2014a, 2014b, 2016) and also for MDPs with finite ergodic structure (Gopalan et al 2014). - Review similar results for optimistic algorithms in infinite horizon problems (Jaksch et al 2010, Bartlett and Tewari 2009, Abbasi-Yadkori and Szepesvari 2011), with particular attention to the dynamic episode growth. - Highlight the delicate technical issue which has led to a fault in the proof of the lazy-PSRL algorithm (Abbasi-Yadkori and Szepesvari 2015). We present an explicit counterexample to this style of argument. Therefore, we suggest that the Theorem 2 in (Abbasi-Yadkori and Szepesvari 2015) be instead considered a conjecture, as it has no rigorous proof. - Present pragmatic approaches to apply PSRL in infinite horizon problems. We conjecture that, under some additional assumptions, it will be possible to obtain bounds $O( \sqrt{T} )$ even without episodic reset. We hope that this note serves to clarify existing results in the field of reinforcement learning and provides interesting motivation for future work. "
763171805955784705,2016-08-10 00:35:12,https://t.co/EAxMIST1VH,On Lower Bounds for Regret in Reinforcement Learning. (arXiv:1608.02732v1 [stat.ML]) https://t.co/EAxMIST1VH,1,5," Abstract: This is a brief technical note to clarify the state of lower bounds on regret for reinforcement learning. In particular, this paper: - Reproduces a lower bound on regret for reinforcement learning, similar to the result of Theorem 5 in the journal UCRL2 paper (Jaksch et al 2010). - Clarifies that the proposed proof of Theorem 6 in the REGAL paper (Bartlett and Tewari 2009) does not hold using the standard techniques without further work. We suggest that this result should instead be considered a conjecture as it has no rigorous proof. - Suggests that the conjectured lower bound given by (Bartlett and Tewari 2009) is incorrect and, in fact, it is possible to improve the scaling of the upper bound to match the weaker lower bounds presented in this paper. We hope that this note serves to clarify existing results in the field of reinforcement learning and provides interesting motivation for future work. "
763171804689096705,2016-08-10 00:35:11,https://t.co/OoFMRgAYNk,Classification with the pot-pot plot. (arXiv:1608.02861v1 [stat.ML]) https://t.co/OoFMRgAYNk,0,3," Abstract: We propose a procedure for supervised classification that is based on potential functions. The potential of a class is defined as a kernel density estimate multiplied by the class's prior probability. The method transforms the data to a potential-potential (pot-pot) plot, where each data point is mapped to a vector of potentials. Separation of the classes, as well as classification of new data points, is performed on this plot. For this, either the $\alpha$-procedure ($\alpha$-P) or $k$-nearest neighbors ($k$-NN) are employed. For data that are generated from continuous distributions, these classifiers prove to be strongly Bayes-consistent. The potentials depend on the kernel and its bandwidth used in the density estimate. We investigate several variants of bandwidth selection, including joint and separate pre-scaling and a bandwidth regression approach. The new method is applied to benchmark data from the literature, including simulated data sets as well as 50 sets of real data. It compares favorably to known classification methods such as LDA, QDA, max kernel density estimates, $k$-NN, and $DD$-plot classification using depth functions. "
763171803338567681,2016-08-10 00:35:11,https://t.co/zuNNnhseJl,Linear Regression with an Unknown Permutation: Statistical and Computational Limits. (arXiv:1608.02902v1 [math.ST]) https://t.co/zuNNnhseJl,1,2," Abstract: Consider a noisy linear observation model with an unknown permutation, based on observing $y = \Pi^* A x^* + w$, where $x^* \in \mathbb{R}^d$ is an unknown vector, $\Pi^*$ is an unknown $n \times n$ permutation matrix, and $w \in \mathbb{R}^n$ is additive Gaussian noise. We analyze the problem of permutation recovery in a random design setting in which the entries of the matrix $A$ are drawn i.i.d. from a standard Gaussian distribution, and establish sharp conditions on the SNR, sample size $n$, and dimension $d$ under which $\Pi^*$ is exactly and approximately recoverable. On the computational front, we show that the maximum likelihood estimate of $\Pi^*$ is NP-hard to compute, while also providing a polynomial time algorithm when $d =1$. "
763171801597939713,2016-08-10 00:35:11,https://t.co/grUlZHbgAu,Bayesian multi-tensor factorization. (arXiv:1412.4679v4 [stat.ML] UPDATED) https://t.co/grUlZHbgAu,1,4," Abstract: We introduce Bayesian multi-tensor factorization, a model that is the first Bayesian formulation for joint factorization of multiple matrices and tensors. The research problem generalizes the joint matrix-tensor factorization problem to arbitrary sets of tensors of any depth, including matrices, can be interpreted as unsupervised multi-view learning from multiple data tensors, and can be generalized to relax the usual trilinear tensor factorization assumptions. The result is a factorization of the set of tensors into factors shared by any subsets of the tensors, and factors private to individual tensors. We demonstrate the performance against existing baselines in multiple tensor factorization tasks in structural toxicogenomics and functional neuroimaging. "
763171799496523776,2016-08-10 00:35:10,https://t.co/8JtRK3mnYr,Learning a Tree-Structured Ising Model in Order to Make Predictions. (arXiv:1604.06749v2 [math.ST] UPDATED) https://t.co/8JtRK3mnYr,1,4," Abstract: We study the problem of learning a tree graphical model from samples such that low-order marginals are accurate. We define a distance (""small set TV"" or ssTV) between distributions $P$ and $Q$ by taking the maximum, over all subsets $\mathcal{S}$ of a given size, of the total variation between the marginals of P and Q on $\mathcal{S}$. Approximating a distribution to within small ssTV allows making predictions based on partial observations. Focusing on pairwise marginals and tree-structured Ising models on $p$ nodes with maximum edge strength $\beta$, we prove that $\max\{e^{2\beta}\log p, \eta^{-2}\log(p/\eta)\} $ i.i.d. samples suffices to get a distribution (from the same class) with ssTV at most $\eta$ from the one generating the samples. "
762811861662392320,2016-08-09 00:44:54,https://t.co/1QUqXL5ZXl,Weighted diffusion LMP algorithm for distributed estimation in non-uniform noise conditions. (arXiv:1608.02060v1 [… https://t.co/1QUqXL5ZXl,0,1," Abstract: This letter presents an improved version of diffusion least mean ppower (LMP) algorithm for distributed estimation. Instead of sum of mean square errors, a weighted sum of mean square error is defined as the cost function for global and local cost functions of a network of sensors. The weight coefficients are updated by a simple steepest-descent recursion to minimize the error signal of the global and local adaptive algorithm. Simulation results show the advantages of the proposed weighted diffusion LMP over the diffusion LMP algorithm specially in the non-uniform noise conditions in a sensor network. "
762811858210488320,2016-08-09 00:44:54,https://t.co/StxIRRutmY,Deep Survival Analysis. (arXiv:1608.02158v1 [stat.ML]) https://t.co/StxIRRutmY,0,8," Abstract: The electronic health record (EHR) provides an unprecedented opportunity to build actionable tools to support physicians at the point of care. In this paper, we investigate survival analysis in the context of EHR data. We introduce deep survival analysis, a hierarchical generative approach to survival analysis. It departs from previous approaches in two primary ways: (1) all observations, including covariates, are modeled jointly conditioned on a rich latent structure; and (2) the observations are aligned by their failure time, rather than by an arbitrary time zero as in traditional survival analysis. Further, it (3) scalably handles heterogeneous (continuous and discrete) data types that occur in the EHR. We validate deep survival analysis model by stratifying patients according to risk of developing coronary heart disease (CHD). Specifically, we study a dataset of 313,000 patients corresponding to 5.5 million months of observations. When compared to the clinically validated Framingham CHD risk score, deep survival analysis is significantly superior in stratifying patients according to their risk. "
762811856201351168,2016-08-09 00:44:53,https://t.co/ivZ3WAlvhB,A General Characterization of the Statistical Query Complexity. (arXiv:1608.02198v1 [cs.LG]) https://t.co/ivZ3WAlvhB,0,1," Abstract: Statistical query (SQ) algorithms are algorithms that have access to an {\em SQ oracle} for the input distribution $D$ instead of i.i.d.~ samples from $D$. Given a query function $\phi:X \rightarrow [-1,1]$, the oracle returns an estimate of ${\bf E}_{ x\sim D}[\phi(x)]$ within some tolerance $\tau_\phi$ that roughly corresponds to the number of samples. In this work we demonstrate that the complexity of solving general problems over distributions using SQ algorithms can be captured by a relatively simple notion of statistical dimension that we introduce. SQ algorithms capture a broad spectrum of algorithmic approaches used in theory and practice, most notably, convex optimization techniques. Hence our statistical dimension allows to investigate the power of a variety of algorithmic approaches by analyzing a single linear-algebraic parameter. Such characterizations were investigated over the past 20 years in learning theory but prior characterizations are restricted to the much simpler setting of classification problems relative to a fixed distribution on the domain (Blum et al., 1994; Bshouty and Feldman, 2002; Yang, 2001; Simon, 2007; Feldman, 2012; Szorenyi, 2009). Our characterization is also the first to precisely characterize the necessary tolerance of queries. We give applications of our techniques to two open problems in learning theory and to algorithms that are subject to memory and communication constraints. "
762811853881872385,2016-08-09 00:44:52,https://t.co/lcJswj56kg,Bayesian Learning of Dynamic Multilayer Networks. (arXiv:1608.02209v1 [stat.ML]) https://t.co/lcJswj56kg,0,11," Abstract: A plethora of networks is being collected in a growing number of fields, including disease transmission, international relations, social interactions, and others. As data streams continue to grow, the complexity associated with these highly multidimensional connectivity data presents new challenges. In this paper, we focus on time-varying interconnections among a set of actors in multiple contexts, called layers. Current literature lacks flexible statistical models for dynamic multilayer networks, which can enhance quality in inference and prediction by efficiently borrowing information within each network, across time, and between layers. Motivated by this gap, we develop a Bayesian nonparametric model leveraging latent space representations. Our formulation characterizes the edge probabilities as a function of shared and layer-specific actors positions in a latent space, with these positions changing in time via Gaussian processes. This representation facilitates dimensionality reduction and incorporates different sources of information in the observed data. In addition, we obtain tractable procedures for posterior computation, inference, and prediction. We provide theoretical results on the flexibility of our model. Our methods are tested on simulations and infection studies monitoring dynamic face-to-face contacts among individuals in multiple days, where we perform better than current methods in inference and prediction. "
762811851314962432,2016-08-09 00:44:52,https://t.co/LJrbYY68Hy,Robust High-Dimensional Linear Regression. (arXiv:1608.02257v1 [cs.LG]) https://t.co/LJrbYY68Hy,0,4," Abstract: The effectiveness of supervised learning techniques has made them ubiquitous in research and practice. In high-dimensional settings, supervised learning commonly relies on dimensionality reduction to improve performance and identify the most important factors in predicting outcomes. However, the economic importance of learning has made it a natural target for adversarial manipulation of training data, which we term poisoning attacks. Prior approaches to dealing with robust supervised learning rely on strong assumptions about the nature of the feature matrix, such as feature independence and sub-Gaussian noise with low variance. We propose an integrated method for robust regression that relaxes these assumptions, assuming only that the feature matrix can be well approximated by a low-rank matrix. Our techniques integrate improved robust low-rank matrix approximation and robust principle component regression, and yield strong performance guarantees. Moreover, we experimentally show that our methods significantly outperform state of the art both in running time and prediction error. "
762811848739721216,2016-08-09 00:44:51,https://t.co/Cnpr0vOyLX,Statistical Guarantees for Estimating the Centers of a Two-component Gaussian Mixture by EM. (arXiv:1608.02280v1 [… https://t.co/Cnpr0vOyLX,0,3," Abstract: Recently, a general method for analyzing the statistical accuracy of the EM algorithm has been developed and applied to some simple latent variable models [Balakrishnan et al. 2016]. In that method, the basin of attraction for valid initialization is required to be a ball around the truth. Using Stein's Lemma, we extend these results in the case of estimating the centers of a two-component Gaussian mixture in $d$ dimensions. In particular, we significantly expand the basin of attraction to be the intersection of a half space and a ball around the origin. If the signal-to-noise ratio is at least a constant multiple of $ \sqrt{d\log d} $, we show that a random initialization strategy is feasible. "
762811846567030785,2016-08-09 00:44:51,https://t.co/ckpV7pczES,Blankets Joint Posterior score for learning irregular Markov network structures. (arXiv:1608.02315v1 [cs.AI]) https://t.co/ckpV7pczES,0,2," Abstract: Markov networks are extensively used to model complex sequential, spatial, and relational interactions in a wide range of fields. By learning the structure of independences of a domain, more accurate joint probability distributions can be obtained for inference tasks or, more directly, for interpreting the most significant relations among the variables. However, the performance of current available methods for learning the structure is heavily dependent on the choice of two factors: the structure representation, and the approach for learning such representation. This work follows the probabilistic maximum-a-posteriori approach for learning undirected graph structures, which has gained interest recently. Thus, the Blankets Joint Posterior score is designed for computing the posterior probability of structures given data. In particular, the score proposed can improve the learning process when the solution structure is irregular (that is, when there exists an imbalance in the number of edges over the nodes), which is a property present in many real-world networks. The approximation proposed computes the joint posterior distribution from the collection of Markov blankets of the structure. Essentially, a series of conditional distributions are calculated by using, information about other Markov blankets in the network as evidence. Our experimental results demonstrate that the proposed score has better sample complexity for learning irregular structures, when compared to state-of-the-art scores. By considering optimization with greedy hill-climbing search, we prove for several study cases that our score identifies structures with fewer errors than competitors. "
762811843467444225,2016-08-09 00:44:50,https://t.co/Ug9gZGTH0d,Boosting as a kernel-based method. (arXiv:1608.02485v1 [stat.ML]) https://t.co/Ug9gZGTH0d,0,4," Abstract: Boosting is an important learning technique for classification and regression. In this paper, we show a connection between boosting and kernel based methods, highlighting both theoretical and practical applications. In the context of $\ell_2$ boosting, we start with a weak linear learner defined by a kernel $K$. By applying boosting, we show that the resulting method is equivalent to an estimation procedure hinging on a special {\it boosting kernel} derived from $K$. The boosting kernel depends on $K$ as well as on the regression matrix, noise variance, and hyperparameters. Through this connection, the number of boosting iterations can be modeled as a continuous hyperparameter, and fit along with other parameters using standard techniques. The boosting kernel is then generalized to an entire new class of boosting approaches exploiting general weak learners, including those based on the $\ell_1$, hinge and Vapnik losses. The approach allows fast hyperparameter tuning for a general class of boosting kernels, and has a wide range of applications. We illustrate some of these applications with numerical examples on synthetic and real data. "
762811840816611329,2016-08-09 00:44:49,https://t.co/nE07b2ngdT,Sampling Requirements and Accelerated Schemes for Sparse Linear Regression with Orthogonal Least-Squares. (arXiv:1… https://t.co/nE07b2ngdT,0,1," Abstract: The Orthogonal Least Squares (OLS) algorithm sequentially selects columns of the coefficient matrix to greedily find an approximate sparse solution to an underdetermined system of linear equations. Previous work on the analysis of OLS has been limited; in particular, there exist no guarantees on the performance of OLS for sparse linear regression from random measurements. In this paper, the problem of inferring a sparse vector from random linear combinations of its components using OLS is studied. For the noiseless scenario, it is shown that when the entries of a coefficient matrix are samples from a Gaussian or a Bernoulli distribution, OLS with high probability recovers a $k$-sparse $m$-dimensional sparse vector using ${\cal O}\left(k\log m\right)$ measurements. Similar result is established for the bounded-noise scenario where an additional condition on the smallest nonzero element of the unknown vector is required. Moreover, generalizations that reduce computational complexity of OLS and thus extend its practical feasibility are proposed. The generalized OLS algorithm is empirically shown to outperform broadly used existing algorithms in terms of accuracy, running time, or both. "
762811837872279552,2016-08-09 00:44:49,https://t.co/XD8nwYquez,Sparse recovery via Orthogonal Least-Squares under presence of Noise. (arXiv:1608.02554v1 [stat.ML]) https://t.co/XD8nwYquez,0,1," Abstract: We consider the Orthogonal Least-Squares (OLS) algorithm for the recovery of a $m$-dimensional $k$-sparse signal from a low number of noisy linear measurements. The Exact Recovery Condition (ERC) in bounded noisy scenario is established for OLS under certain condition on nonzero elements of the signal. The new result also improves the existing guarantees for Orthogonal Matching Pursuit (OMP) algorithm. In addition, This framework is employed to provide probabilistic guarantees for the case that the coefficient matrix is drawn at random according to Gaussian or Bernoulli distribution where we exploit some concentration properties. It is shown that under certain conditions, OLS recovers the true support in $k$ iterations with high probability. This in turn demonstrates that ${\cal O}\left(k\log m\right)$ measurements is sufficient for exact recovery of sparse signals via OLS. "
762811834974007296,2016-08-09 00:44:48,https://t.co/iENc9LGsH5,Convex Factorization Machine for Regression. (arXiv:1507.01073v4 [stat.ML] UPDATED) https://t.co/iENc9LGsH5,0,3," Abstract: We propose the convex factorization machine (CFM), which is a convex variant of the widely used Factorization Machines (FMs). Specifically, we employ a linear+quadratic model and regularize the linear term with the $\ell_2$-regularizer and the quadratic term with the trace norm regularizer. Then, we formulate the CFM optimization as a semidefinite programming problem and propose an efficient optimization procedure with Hazan's algorithm. A key advantage of CFM over existing FMs is that it can find a globally optimal solution, while FMs may get a poor locally optimal solution since the objective function of FMs is non-convex. In addition, the proposed algorithm is simple yet effective and can be implemented easily. Finally, CFM is a general factorization method and can also be used for other factorization problems including including multi-view matrix factorization and tensor completion problems. Through synthetic and movielens datasets, we first show that the proposed CFM achieves results competitive to FMs. Furthermore, in a toxicogenomics prediction task, we show that CFM outperforms a state-of-the-art tensor factorization method. "
762811831446597633,2016-08-09 00:44:47,https://t.co/uFsa4flLGi],Variational Inference: A Review for Statisticians. (arXiv:1601.00670v3 [https://t.co/uFsa4flLGi] UPDATED) https://t.co/EWMJ6a17re,0,15,INDEXERROR
762811828837683200,2016-08-09 00:44:47,https://t.co/PGq3c46SOh,Parallelizing Word2Vec in Shared and Distributed Memory. (arXiv:1604.04661v2 [cs.DC] UPDATED) https://t.co/PGq3c46SOh,0,5," Abstract: Word2Vec is a widely used algorithm for extracting low-dimensional vector representations of words. It generated considerable excitement in the machine learning and natural language processing (NLP) communities recently due to its exceptional performance in many NLP applications such as named entity recognition, sentiment analysis, machine translation and question answering. State-of-the-art algorithms including those by Mikolov et al. have been parallelized for multi-core CPU architectures but are based on vector-vector operations that are memory-bandwidth intensive and do not efficiently use computational resources. In this paper, we improve reuse of various data structures in the algorithm through the use of minibatching, hence allowing us to express the problem using matrix multiply operations. We also explore different techniques to distribute word2vec computation across nodes in a compute cluster, and demonstrate good strong scalability up to 32 nodes. In combination, these techniques allow us to scale up the computation near linearly across cores and nodes, and process hundreds of millions of words per second, which is the fastest word2vec implementation to the best of our knowledge. "
762811825851363329,2016-08-09 00:44:46,https://t.co/W8Fj3GbEAW,Generative Topic Embedding: a Continuous Representation of Documents (Extended Version with Proofs). (arXiv:1606.0… https://t.co/W8Fj3GbEAW,0,5," Abstract: Word embedding maps words into a low-dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is influenced by both its local context and its topic. A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classification tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document. "
762811822323994624,2016-08-09 00:44:45,https://t.co/zIabsQLnm3,Directly Modeling Missing Data in Sequences with RNNs: Improved Classification of Clinical Time Series. (arXiv:160… https://t.co/zIabsQLnm3,0,2," Abstract: We demonstrate a simple strategy to cope with missing data in sequential inputs, addressing the task of multilabel classification of diagnoses given clinical time series. Collected from the pediatric intensive care unit (PICU) at Children's Hospital Los Angeles, our data consists of multivariate time series of observations. The measurements are irregularly spaced, leading to missingness patterns in temporally discretized sequences. While these artifacts are typically handled by imputation, we achieve superior predictive performance by treating the artifacts as features. Unlike linear models, recurrent neural networks can realize this improvement using only simple binary indicators of missingness. For linear models, we show an alternative strategy to capture this signal. Training models on missingness patterns only, we show that for some diseases, what tests are run can be as predictive as the results themselves. "
762811819631251462,2016-08-09 00:44:44,https://t.co/7iTBYiXWct,Learning without Forgetting. (arXiv:1606.09282v2 [cs.CV] UPDATED) https://t.co/7iTBYiXWct,1,6," Abstract: When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning as standard practice for improved new task performance. "
762811814895902720,2016-08-09 00:44:43,https://t.co/J3bcju7BON,Uniform Approximation by Neural Networks Activated by First and Second Order Ridge Splines. (arXiv:1607.07819v2 [s… https://t.co/J3bcju7BON,0,4, Abstract: We establish sup-norm error bounds for functions that are approximated by linear combinations of first and second order ridge splines and show that these bounds are near-optimal. 
762449077816614917,2016-08-08 00:43:20,https://t.co/6OUtLenzw0,A Distance for HMMs based on Aggregated Wasserstein Metric and State Registration. (arXiv:1608.01747v1 [cs.LG]) https://t.co/6OUtLenzw0,0,4," Abstract: We propose a framework, named Aggregated Wasserstein, for computing a dissimilarity measure or distance between two Hidden Markov Models with state conditional distributions being Gaussian. For such HMMs, the marginal distribution at any time spot follows a Gaussian mixture distribution, a fact exploited to softly match, aka register, the states in two HMMs. We refer to such HMMs as Gaussian mixture model-HMM (GMM-HMM). The registration of states is inspired by the intrinsic relationship of optimal transport and the Wasserstein metric between distributions. Specifically, the components of the marginal GMMs are matched by solving an optimal transport problem where the cost between components is the Wasserstein metric for Gaussian distributions. The solution of the optimization problem is a fast approximation to the Wasserstein metric between two GMMs. The new Aggregated Wasserstein distance is a semi-metric and can be computed without generating Monte Carlo samples. It is invariant to relabeling or permutation of the states. This distance quantifies the dissimilarity of GMM-HMMs by measuring both the difference between the two marginal GMMs and the difference between the two transition matrices. Our new distance is tested on the tasks of retrieval and classification of time series. Experiments on both synthetic data and real data have demonstrated its advantages in terms of accuracy as well as efficiency in comparison with existing distances based on the Kullback-Leibler divergence. "
762449075618795520,2016-08-08 00:43:19,https://t.co/NoCWlsgMVa,Community Detection in Political Twitter Networks using Nonnegative Matrix Factorization Methods. (arXiv:1608.0177… https://t.co/NoCWlsgMVa,2,6," Abstract: Community detection is a fundamental task in social network analysis. In this paper, first we develop an endorsement filtered user connectivity network by utilizing Heider's structural balance theory and certain Twitter triad patterns. Next, we develop three Nonnegative Matrix Factorization frameworks to investigate the contributions of different types of user connectivity and content information in community detection. We show that user content and endorsement filtered connectivity information are complementary to each other in clustering politically motivated users into pure political communities. Word usage is the strongest indicator of users' political orientation among all content categories. Incorporating user-word matrix and word similarity regularizer provides the missing link in connectivity only methods which suffer from detection of artificially large number of clusters for Twitter networks. "
762449073945255936,2016-08-08 00:43:19,https://t.co/i852ezFRE3,Kernel Ridge Regression via Partitioning. (arXiv:1608.01976v1 [stat.ML]) https://t.co/i852ezFRE3,0,4," Abstract: In this paper, we investigate a divide and conquer approach to Kernel Ridge Regression (KRR). Given n samples, the division step involves separating the points based on some underlying disjoint partition of the input space (possibly via clustering), and then computing a KRR estimate for each partition. The conquering step is simple: for each partition, we only consider its own local estimate for prediction. We establish conditions under which we can give generalization bounds for this estimator, as well as achieve optimal minimax rates. We also show that the approximation error component of the generalization error is lesser than when a single KRR estimate is fit on the data: thus providing both statistical and computational advantages over a single KRR estimate over the entire data (or an averaging over random partitions as in other recent work, [30]). Lastly, we provide experimental validation for our proposed estimator and our assumptions. "
762449071910948867,2016-08-08 00:43:19,https://t.co/xPjSl5EtwS,Classification with Asymmetric Label Noise: Consistency and Maximal Denoising. (arXiv:1303.1208v3 [stat.ML] UPDATE… https://t.co/xPjSl5EtwS,0,5," Abstract: In many real-world classification problems, the labels of training examples are randomly corrupted. Most previous theoretical work on classification with label noise assumes that the two classes are separable, that the label noise is independent of the true class label, or that the noise proportions for each class are known. In this work, we give conditions that are necessary and sufficient for the true class-conditional distributions to be identifiable. These conditions are weaker than those analyzed previously, and allow for the classes to be nonseparable and the noise levels to be asymmetric and unknown. The conditions essentially state that a majority of the observed labels are correct and that the true class-conditional distributions are ""mutually irreducible,"" a concept we introduce that limits the similarity of the two distributions. For any label noise problem, there is a unique pair of true class-conditional distributions satisfying the proposed conditions, and we argue that this pair corresponds in a certain sense to maximal denoising of the observed distributions. Our results are facilitated by a connection to ""mixture proportion estimation,"" which is the problem of estimating the maximal proportion of one distribution that is present in another. We establish a novel rate of convergence result for mixture proportion estimation, and apply this to obtain consistency of a discrimination rule based on surrogate loss minimization. Experimental results on benchmark data and a nuclear particle classification problem demonstrate the efficacy of our approach. "
762449069897678848,2016-08-08 00:43:18,https://t.co/resgfLeJKa,Bridging the Gap between Stochastic Gradient MCMC and Stochastic Optimization. (arXiv:1512.07962v3 [stat.ML] UPDAT… https://t.co/resgfLeJKa,4,19," Abstract: Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian analogs to popular stochastic optimization methods; however, this connection is not well studied. We explore this relationship by applying simulated annealing to an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two key components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii) adaptive element-wise momentum weights. The zero-temperature limit gives a novel stochastic optimization method with adaptive element-wise momentum weights, while conventional optimization methods only have a shared, static momentum weight. Under certain assumptions, our theoretical analysis suggests the proposed simulated annealing approach converges close to the global optima. Experiments on several deep neural network models show state-of-the-art results compared to related stochastic optimization algorithms. "
762449068387729408,2016-08-08 00:43:18,https://t.co/06afpigB8a,Peak Criterion for Choosing Gaussian Kernel Bandwidth in Support Vector Data Description. (arXiv:1602.05257v2 [cs.… https://t.co/06afpigB8a,0,5," Abstract: Support Vector Data Description (SVDD) is a machine-learning technique used for single class classification and outlier detection. SVDD formulation with kernel function provides a flexible boundary around data. The value of kernel function parameters affects the nature of the data boundary. For example, it is observed that with a Gaussian kernel, as the value of kernel bandwidth is lowered, the data boundary changes from spherical to wiggly. The spherical data boundary leads to underfitting, and an extremely wiggly data boundary leads to overfitting. In this paper, we propose empirical criterion to obtain good values of the Gaussian kernel bandwidth parameter. This criterion provides a smooth boundary that captures the essential geometric features of the data. "
762449066772996096,2016-08-08 00:43:17,https://t.co/L6pjhYZ3ga,Sampling Method for Fast Training of Support Vector Data Description. (arXiv:1606.05382v2 [cs.LG] CROSS LISTED) https://t.co/L6pjhYZ3ga,0,6, Abstract: Support Vector Data Description (SVDD) is a popular outlier detection technique which constructs a flexible description of the input data. SVDD computation time is high for large training datasets which limits its use in big-data process-monitoring applications. We propose a new iterative sampling-based method for SVDD training. The method incrementally learns the training data description at each iteration by computing SVDD on an independent random sample selected with replacement from the training data set. The experimental results indicate that the proposed method is extremely fast and provides a good data description . 
762449065179176960,2016-08-08 00:43:17,https://t.co/6Bsl4qkvIx,A Non-Parametric Control Chart For High Frequency Multivariate Data. (arXiv:1607.07423v3 [cs.LG] CROSS LISTED) https://t.co/6Bsl4qkvIx,0,4," Abstract: Support Vector Data Description (SVDD) is a machine learning technique used for single class classification and outlier detection. SVDD based K-chart was first introduced by Sun and Tsung for monitoring multivariate processes when underlying distribution of process parameters or quality characteristics depart from Normality. The method first trains a SVDD model on data obtained from stable or in-control operations of the process to obtain a threshold $R^2$ and kernel center a. For each new observation, its Kernel distance from the Kernel center a is calculated. The kernel distance is compared against the threshold $R^2$ to determine if the observation is within the control limits. The non-parametric K-chart provides an attractive alternative to the traditional control charts such as the Hotelling's $T^2$ charts when distribution of the underlying multivariate data is either non-normal or is unknown. But there are challenges when K-chart is deployed in practice. The K-chart requires calculating kernel distance of each new observation but there are no guidelines on how to interpret the kernel distance plot and infer about shifts in process mean or changes in process variation. This limits the application of K-charts in big-data applications such as equipment health monitoring, where observations are generated at a very high frequency. In this scenario, the analyst using the K-chart is inundated with kernel distance results at a very high frequency, generally without any recourse for detecting presence of any assignable causes of variation. We propose a new SVDD based control chart, called as $K_T$ chart, which addresses challenges encountered when using K-chart for big-data applications. The $K_T$ charts can be used to simultaneously track process variation and central tendency. We illustrate the successful use of $K_T$ chart using the Tennessee Eastman process data. "
762449062083727360,2016-08-08 00:43:16,https://t.co/WcNdYZygUV,Leveraging Unstructured Data to Detect Emerging Reliability Issues. (arXiv:1607.07745v1 [cs.AI] CROSS LISTED) https://t.co/WcNdYZygUV,0,1," Abstract: Unstructured data refers to information that does not have a predefined data model or is not organized in a pre-defined manner. Loosely speaking, unstructured data refers to text data that is generated by humans. In after-sales service businesses, there are two main sources of unstructured data: customer complaints, which generally describe symptoms, and technician comments, which outline diagnostics and treatment information. A legitimate customer complaint can eventually be tracked to a failure or a claim. However, there is a delay between the time of a customer complaint and the time of a failure or a claim. A proactive strategy aimed at analyzing customer complaints for symptoms can help service providers detect reliability problems in advance and initiate corrective actions such as recalls. This paper introduces essential text mining concepts in the context of reliability analysis and a method to detect emerging reliability issues. The application of the method is illustrated using a case study. "
761359925251543046,2016-08-05 00:35:26,https://t.co/bBJ6KgQ2z5,Iterative Hard Thresholding for Model Selection in Genome-Wide Association Studies. (arXiv:1608.01398v1 [stat.ML]) https://t.co/bBJ6KgQ2z5,0,5," Abstract: A genome-wide association study (GWAS) correlates marker variation with trait variation in a sample of individuals. Each study subject is genotyped at a multitude of SNPs (single nucleotide polymorphisms) spanning the genome. Here we assume that subjects are unrelated and collected at random and that trait values are normally distributed or transformed to normality. Over the past decade, researchers have been remarkably successful in applying GWAS analysis to hundreds of traits. The massive amount of data produced in these studies present unique computational challenges. Penalized regression with LASSO or MCP penalties is capable of selecting a handful of associated SNPs from millions of potential SNPs. Unfortunately, model selection can be corrupted by false positives and false negatives, obscuring the genetic underpinning of a trait. This paper introduces the iterative hard thresholding (IHT) algorithm to the GWAS analysis of continuous traits. Our parallel implementation of IHT accommodates SNP genotype compression and exploits multiple CPU cores and graphics processing units (GPUs). This allows statistical geneticists to leverage commodity desktop computers in GWAS analysis and to avoid supercomputing. We evaluate IHT performance on both simulated and real GWAS data and conclude that it reduces false positive and false negative rates while remaining competitive in computational time with penalized regression. Source code is freely available at this https URL "
761359924005928960,2016-08-05 00:35:25,https://t.co/uCfQxarsAX,Bayesian Kernel and Mutual $k$-Nearest Neighbor Regression. (arXiv:1608.01410v1 [cs.LG]) https://t.co/uCfQxarsAX,1,7," Abstract: We propose Bayesian extensions of two nonparametric regression methods which are kernel and mutual $k$-nearest neighbor regression methods. Derived based on Gaussian process models for regression, the extensions provide distributions for target value estimates and the framework to select the hyperparameters. It is shown that both the proposed methods asymptotically converge to kernel and mutual $k$-nearest neighbor regression methods, respectively. The simulation results show that the proposed methods can select proper hyperparameters and are better than or comparable to the former methods for an artificial data set and a real world data set. "
761359922709889026,2016-08-05 00:35:25,https://t.co/OuZhDBjAVN,Black-Box Policy Search with Probabilistic Programs. (arXiv:1507.04635v4 [stat.ML] UPDATED) https://t.co/OuZhDBjAVN,0,24," Abstract: In this work, we explore how probabilistic programs can be used to represent policies in sequential decision problems. In this formulation, a probabilistic program is a black-box stochastic simulator for both the problem domain and the agent. We relate classic policy gradient techniques to recently introduced black-box variational methods which generalize to probabilistic program inference. We present case studies in the Canadian traveler problem, Rock Sample, and a benchmark for optimal diagnosis inspired by Guess Who. Each study illustrates how programs can efficiently represent policies using moderate numbers of parameters. "
761359921195675648,2016-08-05 00:35:25,https://t.co/qhD9duyX7L,Multiple Instance Dictionary Learning using Functions of Multiple Instances. (arXiv:1511.02825v3 [cs.CV] UPDATED) https://t.co/qhD9duyX7L,0,3," Abstract: A multiple instance dictionary learning method using functions of multiple instances (DL-FUMI) is proposed to address target detection and two-class classification problems with inaccurate training labels. Given inaccurate training labels, DL-FUMI learns a set of target dictionary atoms that describe the most distinctive and representative features of the true positive class as well as a set of nontarget dictionary atoms that account for the shared information found in both the positive and negative instances. Experimental results show that the estimated target dictionary atoms found by DL-FUMI are more representative prototypes and identify better discriminative features of the true positive class than existing methods in the literature. DL-FUMI is shown to have significantly better performance on several target detection and classification problems as compared to other multiple instance learning (MIL) dictionary learning algorithms on a variety of MIL problems. "
761359919996076032,2016-08-05 00:35:24,https://t.co/3T5pz7NxG2,Neural Programmer: Inducing Latent Programs with Gradient Descent. (arXiv:1511.04834v3 [cs.LG] UPDATED) https://t.co/3T5pz7NxG2,7,29," Abstract: Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy. "
761359917097840640,2016-08-05 00:35:24,https://t.co/JNS3bRhNE5,"""Why Should I Trust You?"": Explaining the Predictions of Any Classifier. (arXiv:1602.04938v2 [cs.LG] UPDATED) https://t.co/JNS3bRhNE5",29,46," Abstract: Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. "
761000823241531392,2016-08-04 00:48:29,https://t.co/AX9QfXnsLm,Markov Chain Sampling in Discrete Probabilistic Models with Constraints. (arXiv:1608.01008v1 [stat.ML]) https://t.co/AX9QfXnsLm,1,5," Abstract: We study probability measures induced by set functions with constraints. Such measures arise in a variety of real-world settings, where prior knowledge, resource limitations, or other pragmatic considerations impose constraints. We consider the task of rapidly sampling from such constrained measures, and develop fast Markov chain samplers for them. Our first main result is for MCMC sampling from Strongly Rayleigh (SR) measures, for which we present sharp polynomial bounds on the mixing time. As a corollary, this result yields a fast mixing sampler for Determinantal Point Processes (DPPs), yielding (to our knowledge) the first provably fast MCMC sampler for DPPs since their inception over four decades ago. Beyond SR measures, we develop MCMC samplers for probabilistic models with hard constraints and identify sufficient conditions under which their chains mix rapidly. We illustrate our claims by empirically verifying the dependence of mixing times on the key factors governing our theoretical bounds. "
761000821861543937,2016-08-04 00:48:29,https://t.co/VKhGXukc7H,A supermartingale approach to Gaussian process based sequential design of experiments. (arXiv:1608.01118v1 [stat.M… https://t.co/VKhGXukc7H,1,6," Abstract: Gaussian process (GP) models have become a well-established framework for the adap-tive design of costly experiments, and in particular, but not only, of computer experiments. GP-based sequential designs have been proposed for various objectives, such as global optimization (estimating the global maximum or maximizer(s) of a function), reliability analysis (estimating a probability of failure) or the estimation of level sets and excursion sets. In this paper, we tackle the convergence properties of an important class of such sequential design strategies, known as stepwise uncertainty reduction (SUR) strategies. Our approach relies on the key observation that the sequence of residual uncertainty measures, in a SUR strategy, is in general a supermartingale with respect to the filtration generated by the observations. We also provide some general results about GP-based sequential design, which are of independent interest. Our main application is a proof of almost sure convergence for one of the SUR strategies proposed by Bect, Ginsbourger, Li, Picheny and Vazquez (Stat. Comp., 2012). To the best of our knowledge, this is the first convergence proof for a GP-based sequential design algorithm dedicated to the estimation of probabilities of excursion and excursions sets. We also establish, using the same approach, a new proof of almost sure convergence for the expected improvement algorithm, which is the first proof for this algorithm that applies to any continuous GP. "
761000820506783744,2016-08-04 00:48:28,https://t.co/9NKXdBJoOq,Learning a Driving Simulator. (arXiv:1608.01230v1 [cs.LG]) https://t.co/9NKXdBJoOq,2,7," Abstract: Comma.ai's approach to Artificial Intelligence for self-driving cars is based on an agent that learns to clone driver behaviors and plans maneuvers by simulating future events in the road. This paper illustrates one of our research approaches for driving simulation. One where we learn to simulate. Here we investigate variational autoencoders with classical and learned cost functions using generative adversarial networks for embedding road frames. Afterwards, we learn a transition model in the embedded space using action conditioned Recurrent Neural Networks. We show that our approach can keep predicting realistic looking video for several frames despite the transition model being optimized without a cost function in the pixel space. "
761000819214999552,2016-08-04 00:48:28,https://t.co/4m4jig32sv,Fast Algorithms for Demixing Sparse Signals from Nonlinear Observations. (arXiv:1608.01234v1 [stat.ML]) https://t.co/4m4jig32sv,2,8," Abstract: We study the problem of demixing a pair of sparse signals from noisy, nonlinear observations of their superposition. Mathematically, we consider a nonlinear signal observation model, $y_i = g(a_i^Tx) + e_i, \ i=1,\ldots,m$, where $x = \Phi w+\Psi z$ denotes the superposition signal, $\Phi$ and $\Psi$ are orthonormal bases in $\mathbb{R}^n$, and $w, z\in\mathbb{R}^n$ are sparse coefficient vectors of the constituent signals, and $e_i$ represents the noise. Moreover, $g$ represents a nonlinear link function, and $a_i\in\mathbb{R}^n$ is the $i$-th row of the measurement matrix, $A\in\mathbb{R}^{m\times n}$. Problems of this nature arise in several applications ranging from astronomy, computer vision, and machine learning. In this paper, we make some concrete algorithmic progress for the above demixing problem. Specifically, we consider two scenarios: (i) the case when the demixing procedure has no knowledge of the link function, and (ii) the case when the demixing algorithm has perfect knowledge of the link function. In both cases, we provide fast algorithms for recovery of the constituents $w$ and $z$ from the observations. Moreover, we support these algorithms with a rigorous theoretical analysis, and derive (nearly) tight upper bounds on the sample complexity of the proposed algorithms for achieving stable recovery of the component signals. We also provide a range of numerical simulations to illustrate the performance of the proposed algorithms on both real and synthetic signals and images. "
761000817809907716,2016-08-04 00:48:28,https://t.co/pPUfQJJQno,Fast and Simple Optimization for Poisson Likelihood Models. (arXiv:1608.01264v1 [cs.LG]) https://t.co/pPUfQJJQno,1,4," Abstract: Poisson likelihood models have been prevalently used in imaging, social networks, and time series analysis. We propose fast, simple, theoretically-grounded, and versatile, optimization algorithms for Poisson likelihood modeling. The Poisson log-likelihood is concave but not Lipschitz-continuous. Since almost all gradient-based optimization algorithms rely on Lipschitz-continuity, optimizing Poisson likelihood models with a guarantee of convergence can be challenging, especially for large-scale problems. We present a new perspective allowing to efficiently optimize a wide range of penalized Poisson likelihood objectives. We show that an appropriate saddle point reformulation enjoys a favorable geometry and a smooth structure. Therefore, we can design a new gradient-based optimization algorithm with $O(1/t)$ convergence rate, in contrast to the usual $O(1/\sqrt{t})$ rate of non-smooth minimization alternatives. Furthermore, in order to tackle problems with large samples, we also develop a randomized block-decomposition variant that enjoys the same convergence rate yet more efficient iteration cost. Experimental results on several point process applications including social network estimation and temporal recommendation show that the proposed algorithm and its randomized block variant outperform existing methods both on synthetic and real-world datasets. "
761000816362782721,2016-08-04 00:48:27,https://t.co/zMJzsJSkQi,A Multivariate Hawkes Process with Gaps in Observations. (arXiv:1608.01282v1 [stat.ML]) https://t.co/zMJzsJSkQi,0,2," Abstract: Given a collection of entities (or nodes) in a network and our intermittent observations of activities from each entity, an important problem is to learn the hidden edges depicting directional relationships among these entities. Here, we study causal relationships (excitations) that are realized by a multivariate Hawkes process. The multivariate Hawkes process (MHP) and its variations (spatial-temporal point processes) have been used to study contagion in earthquakes, crimes, neural spiking activities, the stock and foreign exchange markets, etc. In this paper, we consider the case with intermittent observations (and hence gaps.) We propose a variational problem for detecting sparsely hidden relationships with a multivariate Hawkes process that takes into account the gaps from each entity (MHPG). We bypass the problem of dealing with a large amount of missing events by introducing a small number of unknown boundary conditions. In the case where our observations are sparse (e.g. from 10% to 30%), we show through numerical simulations that robust recovery with MHPG is still possible even if the lengths of the observed intervals are small but they are chosen accordingly. In these cases, the proposed MHPG outperforms the classical MHP in parameter estimations. The numerical results also show that the knowledge of gaps is very crucial in discovering the underlying patterns and hidden relationships. "
761000814953529344,2016-08-04 00:48:27,https://t.co/rzvSYUfvuk,A Physical Metaphor to Study Semantic Drift. (arXiv:1608.01298v1 [cs.CL]) https://t.co/rzvSYUfvuk,0,4," Abstract: In accessibility tests for digital preservation, over time we experience drifts of localized and labelled content in statistical models of evolving semantics represented as a vector field. This articulates the need to detect, measure, interpret and model outcomes of knowledge dynamics. To this end we employ a high-performance machine learning algorithm for the training of extremely large emergent self-organizing maps for exploratory data analysis. The working hypothesis we present here is that the dynamics of semantic drifts can be modeled on a relaxed version of Newtonian mechanics called social mechanics. By using term distances as a measure of semantic relatedness vs. their PageRank values indicating social importance and applied as variable `term mass', gravitation as a metaphor to express changes in the semantic content of a vector field lends a new perspective for experimentation. From `term gravitation' over time, one can compute its generating potential whose fluctuations manifest modifications in pairwise term similarity vs. social importance, thereby updating Osgood's semantic differential. The dataset examined is the public catalog metadata of Tate Galleries, London. "
761000813812715520,2016-08-04 00:48:27,https://t.co/6ehvmTIXQa,A balanced k-means algorithm for weighted point sets. (arXiv:1308.4004v2 [math.OC] UPDATED) https://t.co/6ehvmTIXQa,1,5," Abstract: The classical $k$-means algorithm for partitioning $n$ points in $\mathbb{R}^d$ into $k$ clusters is one of the most popular and widely spread clustering methods. The need to respect prescribed lower bounds on the cluster sizes has been observed in many scientific and business applications. In this paper, we present and analyze a generalization of $k$-means that is capable of handling weighted point sets and prescribed lower and upper bounds on the cluster sizes. We call it weight-balanced $k$-means. The key difference to existing models lies in the ability to handle the combination of weighted point sets with prescribed bounds on the cluster sizes. This imposes the need to perform partial membership clustering, and leads to significant differences. For example, while finite termination of all $k$-means variants for unweighted point sets is a simple consequence of the existence of only finitely many partitions of a given set of points, the situation is more involved for weighted point sets, as there are infinitely many partial membership clusterings. Using polyhedral theory, we show that the number of iterations of weight-balanced $k$-means is bounded above by $n^{O(dk)}$, so in particular it is polynomial for fixed $k$ and $d$. This is similar to the known worst-case upper bound for classical $k$-means for unweighted point sets and unrestricted cluster sizes, despite the much more general framework. We conclude with the discussion of some additional favorable properties of our method. "
761000812516638724,2016-08-04 00:48:27,https://t.co/N9jKibzb5i,Pathwise Coordinate Optimization for Sparse Learning: Algorithm and Theory. (arXiv:1412.7477v4 [stat.ML] UPDATED) https://t.co/N9jKibzb5i,3,13," Abstract: The pathwise coordinate optimization is one of the most important computational frameworks for high dimensional convex and nonconvex sparse learning problems. It differs from the classical coordinate optimization algorithms in three salient features: warm start initialization, active set updating, and strong rule for coordinate preselection. Such a complex algorithmic structure grants superior empirical performance, but also poses significant challenge to theoretical analysis. To tackle this long lasting problem, we develop a new theory showing that these three features play pivotal roles in guaranteeing the outstanding statistical and computational performance of the pathwise coordinate optimization framework. Particularly, we analyze the existing methods for pathwise coordinate optimization and provide new theoretical insights into them. The obtained insights further motivate the development of several modifications to improve the pathwise coordinate optimization framework, which guarantees linear convergence to a unique sparse local optimum with optimal statistical properties in parameter estimation and support recovery. This is the first result on the computational and statistical guarantees of the pathwise coordinate optimization framework in high dimensions. Thorough numerical experiments are provided to support our theory. "
761000811338076160,2016-08-04 00:48:26,https://t.co/8r2TJHybhI,PAC-Bayesian Theorems for Domain Adaptation with Specialization to Linear Classifiers PAC-Bayesian Theorems for Mu… https://t.co/8r2TJHybhI,0,6," Abstract: In this paper, we provide two main contributions in PAC-Bayesian theory for domain adaptation where the objective is to learn, from a source distribution, a well-performing majority vote on a different target distribution. On the one hand, we propose an improvement of the previous approach proposed by Germain et al. (2013), that relies on a novel distribution pseudodistance based on a disagreement averaging, allowing us to derive a new tighter PAC-Bayesian domain adaptation bound for the stochastic Gibbs classifier. We specialize it to linear classifiers, and design a learning algorithm which shows interesting results on a synthetic problem and on a popular sentiment annotation task. On the other hand, we generalize these results to multisource domain adaptation allowing us to take into account different source domains. This study opens the door to tackle domain adaptation tasks by making use of all the PAC-Bayesian tools. "
761000810092322816,2016-08-04 00:48:26,https://t.co/q7hjT6gutW,Causal Transfer in Machine Learning. (arXiv:1507.05333v3 [stat.ML] UPDATED) https://t.co/q7hjT6gutW,2,12," Abstract: Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We prove that in an adversarial setting using this subset for prediction is optimal if no examples from the test task are observed; we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set. "
761000808464977920,2016-08-04 00:48:26,https://t.co/a9Bj2gVGvq,Robust Non-linear Regression: A Greedy Approach Employing Kernels with Application to Image Denoising. (arXiv:1601… https://t.co/a9Bj2gVGvq,2,7," Abstract: We consider the task of robust non-linear regression in the presence of both inlier noise and outliers. Assuming that the unknown non-linear function belongs to a Reproducing Kernel Hilbert Space (RKHS), our goal is to estimate the set of the associated unknown parameters. Due to the presence of outliers, common techniques such as the Kernel Ridge Regression (KRR) or the Support Vector Regression (SVR) turn out to be inadequate. Instead, we employ sparse modeling arguments to explicitly model and estimate the outliers, adopting a greedy approach. The proposed robust scheme, i.e., Kernel Greedy Algorithm for Robust Denoising (KGARD), is inspired by the classical Orthogonal Matching Pursuit (OMP) algorithm. Specifically, the proposed method alternates between a KRR task and an OMP-like selection step. Theoretical results concerning the identification of the outliers are provided. Moreover, KGARD is compared against other cutting edge methods, where its performance is evaluated via a set of experiments with various types of noise. Finally, the proposed robust estimation framework is applied to the task of image denoising, and its enhanced performance in the presence of outliers is demonstrated. "
761000807164674048,2016-08-04 00:48:25,https://t.co/lyJE4fpWIv,DOLPHIn - Dictionary Learning for Phase Retrieval. (arXiv:1602.02263v2 [math.OC] UPDATED) https://t.co/lyJE4fpWIv,1,5," Abstract: We propose a new algorithm to learn a dictionary for reconstructing and sparsely encoding signals from measurements without phase. Specifically, we consider the task of estimating a two-dimensional image from squared-magnitude measurements of a complex-valued linear transformation of the original image. Several recent phase retrieval algorithms exploit underlying sparsity of the unknown signal in order to improve recovery performance. In this work, we consider such a sparse signal prior in the context of phase retrieval, when the sparsifying dictionary is not known in advance. Our algorithm jointly reconstructs the unknown signal - possibly corrupted by noise - and learns a dictionary such that each patch of the estimated image can be sparsely represented. Numerical experiments demonstrate that our approach can obtain significantly better reconstructions for phase retrieval problems with noise than methods that cannot exploit such ""hidden"" sparsity. Moreover, on the theoretical side, we provide a convergence result for our method. "
761000805013057536,2016-08-04 00:48:25,https://t.co/0pFt80uIVA,Combining Random Walks and Nonparametric Bayesian Topic Model for Community Detection. (arXiv:1607.05573v2 [stat.A… https://t.co/0pFt80uIVA,2,7," Abstract: Community detection has been an active research area for decades. Among all probabilistic models, Stochastic Block Model has been the most popular one. This paper introduces a novel probabilistic model: RW-HDP, based on random walks and Hierarchical Dirichlet Process, for community extraction. In RW-HDP, random walks conducted in a social network are treated as documents; nodes are treated as words. By using Hierarchical Dirichlet Process, a nonparametric Bayesian model, we are not only able to cluster nodes into different communities, but also determine the number of communities automatically. We use Stochastic Variational Inference for our model inference, which makes our method time efficient and can be easily extended to an online learning algorithm. "
760654476491493376,2016-08-03 01:52:14,https://t.co/r8r61SM08H,Efficient Multiple Incremental and Decremental Ridge Support Vector Machines for Big Streams. (arXiv:1608.00619v1 … https://t.co/r8r61SM08H,0,3," Abstract: This study presents a rapid multiple incremental and decremental mechanism based on Weight-Error Curves (WECs) for support-vector analysis. Recursion-free computation is proposed for predicting the Lagrangian multipliers of new samples. This study examines Ridge Support Vector Models, subsequently devising a recursion-free function derived from WECs. With the proposed function, all the new Lagrangian multipliers can be computed at once without using any gradual step sizes. Moreover, such a function relaxes a constraint, where the increment of new multiple Lagrangian multipliers should be the same in the previous work, thereby easily satisfying the requirement of KKT conditions. The proposed mechanism no longer requires typical bookkeeping strategies, which compute the step size by checking all the training samples in each incremental round. "
760654475224809472,2016-08-03 01:52:13,https://t.co/3qXKU4Lonk,Energy-Economic Multiple Incremental/Decremental Kernel Ridge Regression for Green Clouds. (arXiv:1608.00621v1 [cs… https://t.co/3qXKU4Lonk,0,2," Abstract: This study presents an energy-economic approach for incremental/decremental learning based on kernel ridge regression, a frequently used regressor on clouds. To avoid reanalyzing the entire dataset when data change every time, the proposed mechanism supports incremental/decremental processing for both single and multiple samples (i.e., batch processing). Moreover, incremental/decremental analyses in empirical and intrinsic space are also introduced to handle with data matrices with a large number of samples or feature dimensions. At the end of this study, we further the proposed mechanism to statistical Kernelized Bayesian Regression, so that incremental/decremental analyses become applicable. Experimental results showed that the performance in accuracy of the proposed method remained as well as the original nonincremental design. Furthermore, training time and power consumption were significantly reduced. These findings thereby demonstrate the effectiveness of the proposed method. "
760654473891045377,2016-08-03 01:52:13,https://t.co/85iWD9FvDi,Oracle Inequalities for High-dimensional Prediction. (arXiv:1608.00624v1 [math.ST]) https://t.co/85iWD9FvDi,0,5," Abstract: The abundance of high-dimensional data in the modern sciences has generated tremendous interest in penalized estimators such as the lasso, scaled lasso, square-root lasso, elastic net, and many others. However, the common theoretical bounds for the predictive performance of these estimators hinge on strong, in practice unverifiable assumptions on the design. In this paper, we introduce a new set of oracle inequalities for prediction in high-dimensional linear regression. These bounds hold irrespective of the design matrix. Moreover, since the proofs rely only on convexity and continuity arguments, the bounds apply to a wide range of penalized estimators. Overall, the bounds demonstrate that generic estimators can provide consistent prediction with any design matrix. From a practical point of view, the bounds can help to identify the potential of specific estimators, and they can help to get a sense of the prediction accuracy in a given application. "
760654472435658752,2016-08-03 01:52:13,https://t.co/fpvIFZXxwg,Clinical Tagging with Joint Probabilistic Models. (arXiv:1608.00686v1 [stat.ML]) https://t.co/fpvIFZXxwg,2,7," Abstract: We describe a method for parameter estimation in bipartite probabilistic graphical models for joint prediction of clinical conditions from the electronic medical record. The method does not rely on the availability of gold-standard labels, but rather uses noisy labels, called anchors, for learning. We provide a likelihood-based objective and a moments-based initialization that are effective at learning the model parameters. The learned model is evaluated in a task of assigning a heldout clinical condition to patients based on retrospective analysis of the records, and outperforms baselines which do not account for the noisiness in the labels or do not model the conditions jointly. "
760654470975983616,2016-08-03 01:52:12,https://t.co/zLuVbJaIOo,Can we trust the bootstrap in high-dimension?. (arXiv:1608.00696v1 [stat.ME]) https://t.co/zLuVbJaIOo,1,12," Abstract: We consider the performance of the bootstrap in high-dimensions for the setting of linear regression, where $p<n$ but $p/n$ is not close to zero. We consider ordinary least-squares as well as robust regression methods and adopt a minimalist performance requirement: can the bootstrap give us good confidence intervals for a single coordinate of $\beta$? (where $\beta$ is the true regression vector). We show through a mix of numerical and theoretical work that the bootstrap is fraught with problems. Both of the most commonly used methods of bootstrapping for regression -- residual bootstrap and pairs bootstrap -- give very poor inference on $\beta$ as the ratio $p/n$ grows. We find that the residuals bootstrap tend to give anti-conservative estimates (inflated Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of power) as the ratio $p/n$ grows. We also show that the jackknife resampling technique for estimating the variance of $\hat{\beta}$ severely overestimates the variance in high dimensions. We contribute alternative bootstrap procedures based on our theoretical results that mitigate these problems. However, the corrections depend on assumptions regarding the underlying data-generation model, suggesting that in high-dimensions it may be difficult to have universal, robust bootstrapping techniques. "
760654469818413057,2016-08-03 01:52:12,https://t.co/DtvPS7KOGG,Identifiable Phenotyping using Constrained Non-Negative Matrix Factorization. (arXiv:1608.00704v1 [stat.ML]) https://t.co/DtvPS7KOGG,1,4," Abstract: This work proposes a new algorithm for automated and simultaneous phenotyping of multiple co-occurring medical conditions, also referred as comorbidities, using clinical notes from the electronic health records (EHRs). A basic latent factor estimation technique of non-negative matrix factorization (NMF) is augmented with domain specific constraints to obtain sparse latent factors that are anchored to a fixed set of chronic conditions. The proposed anchoring mechanism ensures a one-to-one identifiable and interpretable mapping between the latent factors and the target comorbidities. Qualitative assessment of the empirical results by clinical experts suggests that the proposed model learns clinically interpretable phenotypes while being predictive of 30 day mortality. The proposed method can be readily adapted to any non-negative EHR data across various healthcare institutions. "
760654468664979457,2016-08-03 01:52:12,https://t.co/GbPOp1ucST,Exponential Family Embeddings. (arXiv:1608.00778v1 [stat.ML]) https://t.co/GbPOp1ucST,1,6," Abstract: Word embeddings are a powerful approach for capturing semantic similarity among terms in a vocabulary. In this paper, we develop exponential family embeddings, a class of methods that extends the idea of word embeddings to other types of high-dimensional data. As examples, we studied neural data with real-valued observations, count data from a market basket analysis, and ratings data from a movie recommendation system. The main idea is to model each observation conditioned on a set of other observations. This set is called the context, and the way the context is defined is a modeling choice that depends on the problem. In language the context is the surrounding words; in neuroscience the context is close-by neurons; in market basket data the context is other items in the shopping cart. Each type of embedding model defines the context, the exponential family of conditional distributions, and how the latent embedding vectors are shared across data. We infer the embeddings with a scalable algorithm based on stochastic gradient descent. On all three applications - neural activity of zebrafish, users' shopping behavior, and movie ratings - we found exponential family embedding models to be more effective than other types of dimension reduction. They better reconstruct held-out data and find interesting qualitative structure. "
760654467373072384,2016-08-03 01:52:11,https://t.co/TKScCsPuxY,Hierarchically Compositional Kernels for Scalable Nonparametric Learning. (arXiv:1608.00860v1 [cs.LG]) https://t.co/TKScCsPuxY,0,4," Abstract: We propose a novel class of kernels to alleviate the high computational cost of large-scale nonparametric learning with kernel methods. The proposed kernel is defined based on a hierarchical partitioning of the underlying data domain, where the Nystr\""{o}m method (a globally low-rank approximation) is married with a locally lossless approximation in a hierarchical fashion. The kernel maintains (strict) positive-definiteness. The corresponding kernel matrix admits a recursively off-diagonal low-rank structure, which allows for fast linear algebra computations. Suppressing the factor of data dimension, the memory and arithmetic complexities for training a regression or a classifier are reduced from $O(n^2)$ and $O(n^3)$ to $O(nr)$ and $O(nr^2)$, respectively, where $n$ is the number of training examples and $r$ is the rank on each level of the hierarchy. Although other randomized approximate kernels entail a similar complexity, empirical results show that the proposed kernel achieves a matching performance with a smaller $r$. We demonstrate comprehensive experiments to show the effective use of the proposed kernel on data sizes up to the order of millions. "
760654466047700997,2016-08-03 01:52:11,https://t.co/bShmxwRlQ0,Modelling and computation using NCoRM mixtures for density regression. (arXiv:1608.00874v1 [stat.ME]) https://t.co/bShmxwRlQ0,0,2, Abstract: Normalized compound random measures are flexible nonparametric priors for related distributions. We consider building general nonparametric regression models using normalized compound random measure mixture models. We develop a general approach to the unbiased estimation of Laplace functionals of compound random measure (which includes completely random measures as a special case). This allows us to propose a novel pseudo-marginal Metropolis-Hastings sampler for normalized compound random measure mixture models. The approach is illustrated on problems of density regression. 
760654464730685440,2016-08-03 01:52:11,https://t.co/zt8Ywh6ngy,Relational Similarity Machines. (arXiv:1608.00876v1 [stat.ML]) https://t.co/zt8Ywh6ngy,2,8," Abstract: This paper proposes Relational Similarity Machines (RSM): a fast, accurate, and flexible relational learning framework for supervised and semi-supervised learning tasks. Despite the importance of relational learning, most existing methods are hard to adapt to different settings, due to issues with efficiency, scalability, accuracy, and flexibility for handling a wide variety of classification problems, data, constraints, and tasks. For instance, many existing methods perform poorly for multi-class classification problems, graphs that are sparsely labeled or network data with low relational autocorrelation. In contrast, the proposed relational learning framework is designed to be (i) fast for learning and inference at real-time interactive rates, and (ii) flexible for a variety of learning settings (multi-class problems), constraints (few labeled instances), and application domains. The experiments demonstrate the effectiveness of RSM for a variety of tasks and data. "
760636729049157632,2016-08-03 00:41:42,https://t.co/5aztZk9HgZ,Capacity-achieving Sparse Superposition Codes via Approximate Message Passing Decoding. (arXiv:1501.05892v3 [cs.IT… https://t.co/5aztZk9HgZ,1,4," Abstract: Sparse superposition codes were recently introduced by Barron and Joseph for reliable communication over the AWGN channel at rates approaching the channel capacity. The codebook is defined in terms of a Gaussian design matrix, and codewords are sparse linear combinations of columns of the matrix. In this paper, we propose an approximate message passing decoder for sparse superposition codes, whose decoding complexity scales linearly with the size of the design matrix. The performance of the decoder is rigorously analyzed and it is shown to asymptotically achieve the AWGN capacity with an appropriate power allocation. Simulation results are provided to demonstrate the performance of the decoder at finite blocklengths. We introduce a power allocation scheme to improve the empirical performance, and demonstrate how the decoding complexity can be significantly reduced by using Hadamard design matrices. "
760636725773369344,2016-08-03 00:41:42,https://t.co/Nv5b2fWSvL,Molecular Graph Convolutions: Moving Beyond Fingerprints. (arXiv:1603.00856v2 [stat.ML] UPDATED) https://t.co/Nv5b2fWSvL,0,4," Abstract: Molecular ""fingerprints"" encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular ""graph convolutions"", a machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph---atoms, bonds, distances, etc.---which allows the model to take greater advantage of information in the graph structure. Although graph convolutions do not outperform all fingerprint-based methods, they (along with other graph-based methods) represent a new paradigm in ligand-based virtual screening with exciting opportunities for future improvement. "
760636722124296192,2016-08-03 00:41:41,https://t.co/DxGWT3teHR,On the Influence of Momentum Acceleration on Online Learning. (arXiv:1603.04136v3 [math.OC] UPDATED) https://t.co/DxGWT3teHR,0,4," Abstract: The article examines in some detail the convergence rate and mean-square-error performance of momentum stochastic gradient methods in the constant step-size and slow adaptation regime. The results establish that momentum methods are equivalent to the standard stochastic gradient method with a re-scaled (larger) step-size value. The size of the re-scaling is determined by the value of the momentum parameter. The equivalence result is established for all time instants and not only in steady-state. The analysis is carried out for general strongly convex and smooth risk functions, and is not limited to quadratic risks. One notable conclusion is that the well-known bene ts of momentum constructions for deterministic optimization problems do not necessarily carry over to the adaptive online setting when small constant step-sizes are used to enable continuous adaptation and learn- ing in the presence of persistent gradient noise. From simulations, the equivalence between momentum and standard stochastic gradient methods is also observed for non-differentiable and non-convex problems. "
760636718718615553,2016-08-03 00:41:40,https://t.co/g9ePzD49yS,Bounds on the Number of Measurements for Reliable Compressive Classification. (arXiv:1607.02801v2 [cs.IT] UPDATED) https://t.co/g9ePzD49yS,0,2," Abstract: This paper studies the classification of high-dimensional Gaussian signals from low-dimensional noisy, linear measurements. In particular, it provides upper bounds (sufficient conditions) on the number of measurements required to drive the probability of misclassification to zero in the low-noise regime, both for random measurements and designed ones. Such bounds reveal two important operational regimes that are a function of the characteristics of the source: i) when the number of classes is less than or equal to the dimension of the space spanned by signals in each class, reliable classification is possible in the low-noise regime by using a one-vs-all measurement design; ii) when the dimension of the spaces spanned by signals in each class is lower than the number of classes, reliable classification is guaranteed in the low-noise regime by using a simple random measurement design. Simulation results both with synthetic and real data show that our analysis is sharp, in the sense that it is able to gauge the number of measurements required to drive the misclassification probability to zero in the low-noise regime. "
760636715656708096,2016-08-03 00:41:39,https://t.co/f8CUjfqrmU,Fitting a Simplicial Complex using a Variation of k-means. (arXiv:1607.03849v2 [cs.LG] UPDATED) https://t.co/f8CUjfqrmU,1,2," Abstract: We give a simple and effective two stage algorithm for approximating a point cloud $\mathcal{S}\subset\mathbb{R}^m$ by a simplicial complex $K$. The first stage is an iterative fitting procedure that generalizes k-means clustering, while the second stage involves deleting redundant simplices. A form of dimension reduction of $\mathcal{S}$ is obtained as a consequence. "
760636709738545152,2016-08-03 00:41:38,https://t.co/CyB0n2ucbs,Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation. (arXiv:1607.03516v2 [cs.CV] CROSS … https://t.co/CyB0n2ucbs,1,2," Abstract: In this paper, we propose a novel unsupervised domain adaptation algorithm based on deep learning for visual object recognition. Specifically, we design a new model called Deep Reconstruction-Classification Network (DRCN), which jointly learns a shared encoding representation for two tasks: i) supervised classification of labeled source data, and ii) unsupervised reconstruction of unlabeled target data.In this way, the learnt representation not only preserves discriminability, but also encodes useful information from the target domain. Our new DRCN model can be optimized by using backpropagation similarly as the standard neural networks. We evaluate the performance of DRCN on a series of cross-domain object recognition tasks, where DRCN provides a considerable improvement (up to ~8% in accuracy) over the prior state-of-the-art algorithms. Interestingly, we also observe that the reconstruction pipeline of DRCN transforms images from the source domain into images whose appearance resembles the target dataset. This suggests that DRCN's performance is due to constructing a single composite representation that encodes information about both the structure of target images and the classification of source images. Finally, we provide a formal analysis to justify the algorithm's objective in domain adaptation context. "
760273994180362240,2016-08-02 00:40:20,https://t.co/taLRCZkzYp,gLOP: the global and Local Penalty for Capturing Predictive Heterogeneity. (arXiv:1608.00027v1 [stat.ML]) https://t.co/taLRCZkzYp,0,2," Abstract: When faced with a supervised learning problem, we hope to have rich enough data to build a model that predicts future instances well. However, in practice, problems can exhibit predictive heterogeneity: most instances might be relatively easy to predict, while others might be predictive outliers for which a model trained on the entire dataset does not perform well. Identifying these can help focus future data collection. We present gLOP, the global and Local Penalty, a framework for capturing predictive heterogeneity and identifying predictive outliers. gLOP is based on penalized regression for multitask learning, which improves learning by leveraging training signal information from related tasks. We give two optimization algorithms for gLOP, one space-efficient, and another giving the full regularization path. We also characterize uniqueness in terms of the data and tuning parameters, and present empirical results on synthetic data and on two health research problems. "
760273992842371072,2016-08-02 00:40:19,https://t.co/8msdlTGotx,Double Machine Learning for Treatment and Causal Parameters. (arXiv:1608.00060v1 [stat.ML]) https://t.co/8msdlTGotx,2,6," Abstract: Most modern supervised statistical/machine learning (ML) methods are explicitly designed to solve prediction problems very well. Achieving this goal does not imply that these methods automatically deliver good estimators of causal parameters. Examples of such parameters include individual regression coefficients, average treatment effects, average lifts, and demand or supply elasticities. In fact, estimates of such causal parameters obtained via naively plugging ML estimators into estimating equations for such parameters can behave very poorly due to the regularization bias. Fortunately, this regularization bias can be removed by solving auxiliary prediction problems via ML tools. Specifically, we can form an orthogonal score for the target low-dimensional parameter by combining auxiliary and main ML predictions. The score is then used to build a de-biased estimator of the target parameter which typically will converge at the fastest possible 1/root(n) rate and be approximately unbiased and normal, and from which valid confidence intervals for these parameters of interest may be constructed. The resulting method thus could be called a ""double ML"" method because it relies on estimating primary and auxiliary predictive models. In order to avoid overfitting, our construction also makes use of the K-fold sample splitting, which we call cross-fitting. This allows us to use a very broad set of ML predictive methods in solving the auxiliary and main prediction problems, such as random forest, lasso, ridge, deep neural nets, boosted trees, as well as various hybrids and aggregators of these methods. "
760273991445651456,2016-08-02 00:40:19,https://t.co/UGz4ndiegv,Online Nonnegative Matrix Factorization with General Divergences. (arXiv:1608.00075v1 [stat.ML]) https://t.co/UGz4ndiegv,0,5," Abstract: We develop a unified and systematic framework for performing online nonnegative matrix factorization under a wide variety of important divergences. The online nature of our algorithm makes it particularly amenable to large-scale data. We prove that the sequence of learned dictionaries converges almost surely to the set of critical points of the expected loss function. We do so by leveraging the theory of stochastic approximations and projected dynamical systems. This result substantially generalizes the previous results obtained only for the squared-$\ell_2$ loss. Moreover, the novel techniques involved in our analysis open new avenues for analyzing similar matrix factorization problems. The computational efficiency and the quality of the learned dictionary of our algorithm are verified empirically on both synthetic and real datasets. In particular, on the tasks of topic learning, shadow removal and image denoising, our algorithm achieves superior trade-offs between the quality of learned dictionary and running time over the batch and other online NMF algorithms. "
760273990237773824,2016-08-02 00:40:19,https://t.co/5ROVycnxmm,DeepSoft: A vision for a deep model of software. (arXiv:1608.00092v1 [cs.SE]) https://t.co/5ROVycnxmm,3,8," Abstract: Although software analytics has experienced rapid growth as a research area, it has not yet reached its full potential for wide industrial adoption. Most of the existing work in software analytics still relies heavily on costly manual feature engineering processes, and they mainly address the traditional classification problems, as opposed to predicting future events. We present a vision for \emph{DeepSoft}, an \emph{end-to-end} generic framework for modeling software and its development process to predict future risks and recommend interventions. DeepSoft, partly inspired by human memory, is built upon the powerful deep learning-based Long Short Term Memory architecture that is capable of learning long-term temporal dependencies that occur in software evolution. Such deep learned patterns of software can be used to address a range of challenging problems such as code and task recommendation and prediction. DeepSoft provides a new approach for research into modeling of source code, risk prediction and mitigation, developer modeling, and automatically generating code patches from bug reports. "
760273987242983424,2016-08-02 00:40:18,https://t.co/3mb7faDLHG,Learning Tree-Structured Detection Cascades for Heterogeneous Networks of Embedded Devices. (arXiv:1608.00159v1 [s… https://t.co/3mb7faDLHG,0,5," Abstract: In this paper, we present a new approach to learning cascaded classifiers for use in computing environments that involve networks of heterogeneous and resource-constrained, low-power embedded compute and sensing nodes. We present a generalization of the classical linear detection cascade to the case of tree-structured cascades where different branches of the tree execute on different physical compute nodes in the network. Different nodes have access to different features, as well as access to potentially different computation and energy resources. We concentrate on the problem of jointly learning the parameters for all of the classifiers in the cascade given a fixed cascade architecture and a known set of costs required to carry out the computation at each node.To accomplish the objective of joint learning of all detectors, we propose a novel approach to combining classifier outputs during training that better matches the hard cascade setting in which the learned system will be deployed. This work is motivated by research in the area of mobile health where energy efficient real time detectors integrating information from multiple wireless on-body sensors and a smart phone are needed for real-time monitoring and delivering just-in-time adaptive interventions. We apply our framework to the problem of cigarette smoking detection from a combination of wrist-worn actigraphy data and respiration chest band data. "
760273986085330944,2016-08-02 00:40:18,https://t.co/k7qB5DhcsT,Hyperparameter Transfer Learning through Surrogate Alignment for Efficient Deep Neural Network Training. (arXiv:16… https://t.co/k7qB5DhcsT,2,12," Abstract: Recently, several optimization methods have been successfully applied to the hyperparameter optimization of deep neural networks (DNNs). The methods work by modeling the joint distribution of hyperparameter values and corresponding error. Those methods become less practical when applied to modern DNNs whose training may take a few days and thus one cannot collect sufficient observations to accurately model the distribution. To address this challenging issue, we propose a method that learns to transfer optimal hyperparameter values for a small source dataset to hyperparameter values with comparable performance on a dataset of interest. As opposed to existing transfer learning methods, our proposed method does not use hand-designed features. Instead, it uses surrogates to model the hyperparameter-error distributions of the two datasets and trains a neural network to learn the transfer function. Extensive experiments on three CV benchmark datasets clearly demonstrate the efficiency of our method. "
760273985036836864,2016-08-02 00:40:17,https://t.co/5rMzNAdnId,On Regularization Parameter Estimation under Covariate Shift. (arXiv:1608.00250v1 [cs.LG]) https://t.co/5rMzNAdnId,2,5," Abstract: This paper identifies a problem with the usual procedure for L2-regularization parameter estimation in a domain adaptation setting. In such a setting, there are differences between the distributions generating the training data (source domain) and the test data (target domain). The usual cross-validation procedure requires validation data, which can not be obtained from the unlabeled target data. The problem is that if one decides to use source validation data, the regularization parameter is underestimated. One possible solution is to scale the source validation data through importance weighting, but we show that this correction is not sufficient. We conclude the paper with an empirical analysis of the effect of several importance weight estimators on the estimation of the regularization parameter. "
760273983761702912,2016-08-02 00:40:17,https://t.co/lq8t5qplws,hdm: High-Dimensional Metrics. (arXiv:1608.00354v1 [stat.ME]) https://t.co/lq8t5qplws,0,4," Abstract: In this article the package High-dimensional Metrics (\texttt{hdm}) is introduced. It is a collection of statistical methods for estimation and quantification of uncertainty in high-dimensional approximately sparse models. It focuses on providing confidence intervals and significance testing for (possibly many) low-dimensional subcomponents of the high-dimensional parameter vector. Efficient estimators and uniformly valid confidence intervals for regression coefficients on target variables (e.g., treatment or policy variable) in a high-dimensional approximately sparse regression model, for average treatment effect (ATE) and average treatment effect for the treated (ATET), as well for extensions of these parameters to the endogenous setting are provided. Theory grounded, data-driven methods for selecting the penalization parameter in Lasso regressions under heteroscedastic and non-Gaussian errors are implemented. Moreover, joint/ simultaneous confidence intervals for regression coefficients of a high-dimensional sparse regression are implemented. Data sets which have been used in the literature and might be useful for classroom demonstration and for testing new estimators are included. "
760273982528622592,2016-08-02 00:40:17,https://t.co/8dhOeph3V5,"Kernel Risk-Sensitive Loss: Definition, Properties and Application to Robust Adaptive Filtering. (arXiv:1608.00441… https://t.co/8dhOeph3V5",0,3," Abstract: Nonlinear similarity measures defined in kernel space, such as correntropy, can extract higher-order statistics of data and offer potentially significant performance improvement over their linear counterparts especially in non-Gaussian signal processing and machine learning. In this work, we propose a new similarity measure in kernel space, called the kernel risk-sensitive loss (KRSL), and provide some important properties. We apply the KRSL to adaptive filtering and investigate the robustness, and then develop the MKRSL algorithm and analyze the mean square convergence performance. Compared with correntropy, the KRSL can offer a more efficient performance surface, thereby enabling a gradient based method to achieve faster convergence speed and higher accuracy while still maintaining the robustness to outliers. Theoretical analysis results and superior performance of the new algorithm are confirmed by simulation. "
760273981094125568,2016-08-02 00:40:16,https://t.co/cEoiZRtnYC,Generalized Determinantal Point Processes: The Linear Case. (arXiv:1608.00554v1 [cs.DS]) https://t.co/cEoiZRtnYC,0,4," Abstract: Determinantal Point Processes (DPPs) are probabilistic models of repulsion that originate in quantum physics and random matrix theory and have been of recent interest in computer science. DPPs define distributions over subsets of a given ground set, and exhibit interesting properties such as negative correlation. When applied to kernel methods in machine learning, DPPs give rise to an efficient algorithm to select a small, diverse sample of the given data. Kulesza and Taskar [KT12] posed a natural open question: Can we sample from DPPs when there are additional constraints on the allowable subsets? In this paper, we study the complexity of sampling from DPPs over combinatorially-constrained families of subsets and present several connections and applications. We start by showing that it is at least as hard to sample from combinatorial DPPs as it is to compute the mixed discriminant of a tuple of positive semidefinite matrices. Subsequently, we give a polynomial time algorithm for sampling from combinatorial DPPs with a constant number of linear constraints; thus, we make significant progress towards answering the question of [KT12]. These results can lead to several non-trivial applications; e.g., we show how to derandomize a result of Nikolov and Singh [NS16] for maximizing subdeterminants under (a constant number of) partition constraints and -- motivated by making the [MSS13] proof for the Kadison-Singer problem algorithmic -- give an algorithm to compute the higher-order coefficients of mixed characteristic polynomials. "
760273979928088577,2016-08-02 00:40:16,https://t.co/kb7srxSGXr,Weighted SGD for $\ell_p$ Regression with Randomized Preconditioning. (arXiv:1502.03571v4 [math.OC] UPDATED) https://t.co/kb7srxSGXr,0,4," Abstract: In recent years, stochastic gradient descent (SGD) methods and randomized linear algebra (RLA) algorithms have been applied to many large-scale problems in machine learning and data analysis. We aim to bridge the gap between these two methods in solving constrained overdetermined linear regression problems---e.g., $\ell_2$ and $\ell_1$ regression problems. We propose a hybrid algorithm named pwSGD that uses RLA techniques for preconditioning and constructing an importance sampling distribution, and then performs an SGD-like iterative process with weighted sampling on the preconditioned system. We prove that pwSGD inherits faster convergence rates that only depend on the lower dimension of the linear system, while maintaining low computation complexity. Particularly, when solving $\ell_1$ regression with size $n$ by $d$, pwSGD returns an approximate solution with $\epsilon$ relative error in the objective value in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d)/\epsilon^2)$ time. This complexity is uniformly better than that of RLA methods in terms of both $\epsilon$ and $d$ when the problem is unconstrained. For $\ell_2$ regression, pwSGD returns an approximate solution with $\epsilon$ relative error in the objective value and the solution vector measured in prediction norm in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d) \log(1/\epsilon) /\epsilon)$ time. We also provide lower bounds on the coreset complexity for more general regression problems, indicating that still new ideas will be needed to extend similar RLA preconditioning ideas to weighted SGD algorithms for more general regression problems. Finally, the effectiveness of such algorithms is illustrated numerically on both synthetic and real datasets. "
760273978569199616,2016-08-02 00:40:16,https://t.co/0mPzV2ZV2c,Detection of Epigenomic Network Community Oncomarkers. (arXiv:1506.05244v3 [stat.AP] UPDATED) https://t.co/0mPzV2ZV2c,0,4," Abstract: In this paper we propose network methodology to infer prognostic cancer biomarkers based on the epigenetic pattern DNA methylation. Epigenetic processes such as DNA methylation reflect environmental risk factors, and are increasingly recognised for their fundamental role in diseases such as cancer. DNA methylation is a gene-regulatory pattern, and hence provides a means by which to assess genomic regulatory interactions. Network models are a natural way to represent and analyse groups of such interactions. The utility of network models also increases as the quantity of data and number of variables increase, making them increasingly relevant to large-scale genomic studies. We propose methodology to infer prognostic genomic networks from a DNA methylation-based measure of genomic interaction and association. We then show how to identify prognostic biomarkers from such networks, which we term `network community oncomarkers'. We illustrate the power of our proposed methodology in the context of a large publicly available breast cancer dataset. "
760273977403142144,2016-08-02 00:40:16,https://t.co/2yIzhwWxW9,Performance Limits of Online Stochastic Sub-Gradient Learning. (arXiv:1511.07902v3 [stat.ML] UPDATED) https://t.co/2yIzhwWxW9,0,5," Abstract: This work examines the performance of stochastic sub-gradient learning strategies under weaker conditions than usually considered in the literature. The conditions are shown to be automatically satisfied by several important cases of interest including the construction of Linear-SVM, LASSO, and Total-Variation denoising formulations. In comparison, these problems do not satisfy the traditional assumptions automatically and, therefore, conclusions derived based on these earlier assumptions are not directly applicable to these problems. The analysis establishes that stochastic sub-gradient strategies can attain exponential convergence rates, as opposed to sub-linear rates, to the steady-state. A realizable exponential-weighting procedure is proposed to smooth the intermediate iterates by the sub-gradient procedure and to guarantee the established performance bounds in terms of convergence rate and excessive risk performance. Both single-agent and multi-agent scenarios are studied, where the latter case assumes that a collection of agents are interconnected by a topology and can only interact locally with their neighbors. The theoretical conclusions are illustrated by several examples and simulations, including comparisons with the FISTA procedure. "
760273976346177536,2016-08-02 00:40:15,https://t.co/dcaaFprvht,Linear Convergence of Proximal Gradient Algorithm with Extrapolation for a Class of Nonconvex Nonsmooth Minimizati… https://t.co/dcaaFprvht,0,3," Abstract: In this paper, we study the proximal gradient algorithm with extrapolation for minimizing the sum of a Lipschitz differentiable function and a proper closed convex function. Under the error bound condition used in [19] for analyzing the convergence of the proximal gradient algorithm, we show that there exists a threshold such that if the extrapolation coefficients are chosen below this threshold, then the sequence generated converges $R$-linearly to a stationary point of the problem. Moreover, the corresponding sequence of objective values is also $R$-linearly convergent. In addition, the threshold reduces to $1$ for convex problems and, as a consequence, we obtain the $R$-linear convergence of the sequence generated by FISTA with fixed restart. Finally, we present some numerical experiments to illustrate our results. "
760273975285059584,2016-08-02 00:40:15,https://t.co/h3jFV7eP5U,High-Dimensional Metrics in R. (arXiv:1603.01700v2 [stat.ML] UPDATED) https://t.co/h3jFV7eP5U,1,4," Abstract: The package High-dimensional Metrics (\Rpackage{hdm}) is an evolving collection of statistical methods for estimation and quantification of uncertainty in high-dimensional approximately sparse models. It focuses on providing confidence intervals and significance testing for (possibly many) low-dimensional subcomponents of the high-dimensional parameter vector. Efficient estimators and uniformly valid confidence intervals for regression coefficients on target variables (e.g., treatment or policy variable) in a high-dimensional approximately sparse regression model, for average treatment effect (ATE) and average treatment effect for the treated (ATET), as well for extensions of these parameters to the endogenous setting are provided. Theory grounded, data-driven methods for selecting the penalization parameter in Lasso regressions under heteroscedastic and non-Gaussian errors are implemented. Moreover, joint/ simultaneous confidence intervals for regression coefficients of a high-dimensional sparse regression are implemented, including a joint significance test for Lasso regression. Data sets which have been used in the literature and might be useful for classroom demonstration and for testing new estimators are included. \R and the package \Rpackage{hdm} are open-source software projects and can be freely downloaded from CRAN: \texttt{this http URL}. "
760273974068645888,2016-08-02 00:40:15,https://t.co/vhPZCGBSWw,COCO: A Platform for Comparing Continuous Optimizers in a Black-Box Setting. (arXiv:1603.08785v3 [cs.AI] UPDATED) https://t.co/vhPZCGBSWw,0,7," Abstract: COCO is a platform for Comparing Continuous Optimizers in a black-box setting. It aims at automatizing the tedious and repetitive task of benchmarking numerical optimization algorithms to the greatest possible extent. We present the rationals behind the development of the platform as a general proposition for a guideline towards better benchmarking. We detail underlying fundamental concepts of COCO such as its definition of a problem, the idea of instances, the relevance of target values, and runtime as central performance measure. Finally, we give a quick overview of the basic code structure and the available test suites. "
760273973041037314,2016-08-02 00:40:15,https://t.co/toxFbP9wsb,Multi Level Monte Carlo methods for a class of ergodic stochastic differential equations. (arXiv:1605.01384v2 [mat… https://t.co/toxFbP9wsb,0,3," Abstract: We develop a framework that allows the use of the multi-level Monte Carlo (MLMC) methodology (Giles 2015) to calculate expectations with respect to the invariant measures of ergodic SDEs. In that context, we study the (over-damped) Langevin equations with strongly convex potential. We show that, when appropriate contracting couplings for the numerical integrators are available, one can obtain a time-uniform estimates of the MLMC variance in stark contrast to the majority of the results in the MLMC literature. As a consequence, one can approximate expectations with respect to the invariant measure in an unbiased way without the need of a Metropolis- Hastings step. In addition, a root mean square error of $\mathcal{O}(\epsilon)$ is achieved with $\mathcal{O}(\epsilon^{-2})$ complexity on par with Markov Chain Monte Carlo (MCMC) methods, which however can be computationally intensive when applied to large data sets. Finally, we present a multilevel version of the recently introduced Stochastic Gradient Langevin (SGLD) method (Welling and Teh, 2011) built for large datasets applications. We show that this is the first stochastic gradient MCMC method with complexity $\mathcal{O}(\epsilon^{-2}|\log {\epsilon}|^{3})$, which is asymptotically an order $\epsilon$ lower than the $ \mathcal{O}(\epsilon^{-3})$ complexity of all stochastic gradient MCMC methods that are currently available. Numerical experiments confirm our theoretical findings. "
760273971002634241,2016-08-02 00:40:14,https://t.co/ZSKAjpbikH,A Minimax Approach to Supervised Learning. (arXiv:1606.02206v2 [stat.ML] UPDATED) https://t.co/ZSKAjpbikH,0,6," Abstract: Given a task of predicting $Y$ from $X$, a loss function $L$, and a set of probability distributions $\Gamma$ on $(X,Y)$, what is the optimal decision rule minimizing the worst-case expected loss over $\Gamma$? In this paper, we address this question by introducing a generalization of the maximum entropy principle. Applying this principle to sets of distributions with marginal on $X$ constrained to be the empirical marginal, we provide a minimax interpretation of the maximum likelihood problem over generalized linear models, which connects the minimax problem for each loss function to a generalized linear model. While in some cases such as quadratic and logarithmic loss functions we revisit well-known linear and logistic regression models, our approach reveals novel models for other loss functions. In particular, for the 0-1 loss we derive a classification approach which we call the minimax SVM. The minimax SVM minimizes the worst-case expected 0-1 loss over the proposed $\Gamma$ by solving a tractable optimization problem. Moreover, applying the minimax approach to the Brier loss function we derive a new classification model called the minimax Brier. The maximum likelihood problem for this model uses the Huber penalty function. We perform several numerical experiments to show the power of the minimax SVM and the minimax Brier. "
759911463641243648,2016-08-01 00:39:46,https://t.co/dQOkjWeLaM,Asymptotic properties of Principal Component Analysis and shrinkage-bias adjustment under the Generalized Spiked P… https://t.co/dQOkjWeLaM,0,4," Abstract: With the development of high-throughput technologies, principal component analysis (PCA) in the high-dimensional regime is of great interest. Most of the existing theoretical and methodological results for high-dimensional PCA are based on the spiked population model in which all the population eigenvalues are equal except for a few large ones. Due to the presence of local correlation among features, however, this assumption may not be satisfied in many real-world datasets. To address this issue, we investigated the asymptotic behaviors of PCA under the generalized spiked population model. Based on the theoretical results, we proposed a series of methods for the consistent estimation of population eigenvalues, angles between the sample and population eigenvectors, correlation coefficients between the sample and population principal component (PC) scores, and the shrinkage bias adjustment for the predicted PC scores. Using numerical experiments and real data examples from the genetics literature, we showed that our methods can greatly reduce bias and improve prediction accuracy. "
759911461334294528,2016-08-01 00:39:45,https://t.co/zZajCrOKox,A Non-Parametric Learning Approach to Identify Online Human Trafficking. (arXiv:1607.08691v1 [cs.LG]) https://t.co/zZajCrOKox,1,5," Abstract: Human trafficking is among the most challenging law enforcement problems which demands persistent fight against from all over the globe. In this study, we leverage readily available data from the website ""Backpage""-- used for classified advertisement-- to discern potential patterns of human trafficking activities which manifest online and identify most likely trafficking related advertisements. Due to the lack of ground truth, we rely on two human analysts --one human trafficking victim survivor and one from law enforcement, for hand-labeling the small portion of the crawled data. We then present a semi-supervised learning approach that is trained on the available labeled and unlabeled data and evaluated on unseen data with further verification of experts. "
759911458046038016,2016-08-01 00:39:44,https://t.co/fNe9ksBUm6,TopicResponse: A Marriage of Topic Modelling and Rasch Modelling for Automatic Measurement in MOOCs. (arXiv:1607.0… https://t.co/fNe9ksBUm6,1,6," Abstract: This paper proposes adapting topic models to the psychometric testing of MOOC students based on their online forum postings. We explore the suitability of using automatically discovered topics from MOOC forums to measure students' academic abilities in a subject domain, under the Rasch model, which is the most common Item Response Theory (IRT) model. The challenge is to discover topics that can fit the Rasch model as evidence of measuring educationally meaningful ability. To solve this challenge, we combine the Rasch model with non-negative matrix factorisation (NMF)-based topic modelling. We demonstrate the suitability of our approach with both quantitative experiments on three Coursera MOOCs, and with qualitative results of topic interpretability on a Discrete Optimisation MOOC. "
759911455525249024,2016-08-01 00:39:44,https://t.co/NEVa92rIw4,Data Filtering for Cluster Analysis by $\ell_0$-Norm Regularization. (arXiv:1607.08756v1 [math.OC]) https://t.co/NEVa92rIw4,1,5," Abstract: A data filtering method for cluster analysis is proposed, based on minimizing a least squares function with a weighted $\ell_0$-norm penalty. To overcome the discontinuity of the objective function, smooth non-convex functions are employed to approximate the $\ell_0$-norm. The convergence of the global minimum points of the approximating problems towards global minimum points of the original problem is stated. The proposed method also exploits a suitable technique to choose the penalty parameter. Numerical results on synthetic and real data sets are finally provided, showing how some existing clustering methods can take advantages from the proposed filtering strategy. "
759911453004484608,2016-08-01 00:39:43,https://t.co/eqrc7mvav1,Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms. (arXiv:1607.08810v… https://t.co/eqrc7mvav1,3,12," Abstract: Polynomial networks and factorization machines are two recently-proposed models that can efficiently use feature interactions in classification and regression tasks. In this paper, we revisit both models from a unified perspective. Based on this new view, we study the properties of both models and propose new efficient training algorithms. Key to our approach is to cast parameter learning as a low-rank symmetric tensor estimation problem, which we solve by multi-convex optimization. We demonstrate our approach on regression and recommender system tasks. "
759911450244550656,2016-08-01 00:39:42,https://t.co/6AC7yplkin,The Phylogenetic LASSO and the Microbiome. (arXiv:1607.08877v1 [stat.ML]) https://t.co/6AC7yplkin,0,3," Abstract: Scientific investigations that incorporate next generation sequencing involve analyses of high-dimensional data where the need to organize, collate and interpret the outcomes are pressingly important. Currently, data can be collected at the microbiome level leading to the possibility of personalized medicine whereby treatments can be tailored at this scale. In this paper, we lay down a statistical framework for this type of analysis with a view toward synthesis of products tailored to individual patients. Although the paper applies the technique to data for a particular infectious disease, the methodology is sufficiently rich to be expanded to other problems in medicine, especially those in which coincident `-omics' covariates and clinical responses are simultaneously captured. "
759911447585431552,2016-08-01 00:39:42,https://t.co/68DQTOo2vM,Assessing Functional Neural Connectivity as an Indicator of Cognitive Performance. (arXiv:1607.08891v1 [stat.ML]) https://t.co/68DQTOo2vM,0,3," Abstract: Studies in recent years have demonstrated that neural organization and structure impact an individual's ability to perform a given task. Specifically, individuals with greater neural efficiency have been shown to outperform those with less organized functional structure. In this work, we compare the predictive ability of properties of neural connectivity on a working memory task. We provide two novel approaches for characterizing functional network connectivity from electroencephalography (EEG), and compare these features to the average power across frequency bands in EEG channels. Our first novel approach represents functional connectivity structure through the distribution of eigenvalues making up channel coherence matrices in multiple frequency bands. Our second approach creates a connectivity network at each frequency band, and assesses variability in average path lengths of connected components and degree across the network. Failures in digit and sentence recall on single trials are detected using a Gaussian classifier for each feature set, at each frequency band. The classifier results are then fused across frequency bands, with the resulting detection performance summarized using the area under the receiver operating characteristic curve (AUC) statistic. Fused AUC results of 0.63/0.58/0.61 for digit recall failure and 0.58/0.59/0.54 for sentence recall failure are obtained from the connectivity structure, graph variability, and channel power features respectively. "
759911444645150721,2016-08-01 00:39:41,https://t.co/9GsWDWvbkh,Sparse Linear Regression via Generalized Orthogonal Least-Squares. (arXiv:1602.06916v2 [stat.ML] UPDATED) https://t.co/9GsWDWvbkh,2,9," Abstract: Sparse linear regression, which entails finding a sparse solution to an underdetermined system of linear equations, can formally be expressed as an $l_0$-constrained least-squares problem. The Orthogonal Least-Squares (OLS) algorithm sequentially selects the features (i.e., columns of the coefficient matrix) to greedily find an approximate sparse solution. In this paper, a generalization of Orthogonal Least-Squares which relies on a recursive relation between the components of the optimal solution to select L features at each step and solve the resulting overdetermined system of equations is proposed. Simulation results demonstrate that the generalized OLS algorithm is computationally efficient and achieves performance superior to that of existing greedy algorithms broadly used in the literature. "
759911442518646784,2016-08-01 00:39:41,https://t.co/Rbyc1RQGzA,Predicting Student Dropout in Higher Education. (arXiv:1606.06364v3 [stat.ML] UPDATED) https://t.co/Rbyc1RQGzA,1,6," Abstract: Each year, roughly 30% of first-year students at US baccalaureate institutions do not return for their second year and over $9 billion is spent educating these students. Yet, little quantitative research has analyzed the causes and possible remedies for student attrition. Here, we describe initial efforts to model student dropout using the largest known dataset on higher education attrition, which tracks over 32,500 students' demographics and transcript records at one of the nation's largest public universities. Our results highlight several early indicators of student attrition and show that dropout can be accurately predicted even when predictions are based on a single term of academic transcript data. These results highlight the potential for machine learning to have an impact on student retention and success while pointing to several promising directions for future work. "
759911440031490050,2016-08-01 00:39:40,https://t.co/FQxtV75Egw,A review of undirected and acyclic directed Gaussian Markov model selection and estimation. (arXiv:1606.07282v2 [s… https://t.co/FQxtV75Egw,1,10," Abstract: Markov models lie at the interface between statistical independence in a probability distribution and graph separation properties. We review model selection and estimation in directed and undirected Markov models with Gaussian parametrization, emphasizing the main similarities and differences. These two model types are foundationally similar but not equivalent, as we highlight. We report existing results with a unified notation and terminology, taking into account literature from both the artificial intelligence and statistics research communities, which first developed these models. Finally, we point out the main active research areas and open problems now existing with regard to these traditional, albeit rich, Markov models. "
759911436583743488,2016-08-01 00:39:39,https://t.co/3fImHpYcde,Rademacher Complexity Bounds for a Penalized Multiclass Semi-Supervised Algorithm. (arXiv:1607.00567v2 [stat.ML] U… https://t.co/3fImHpYcde,0,8," Abstract: We propose Rademacher complexity bounds for multiclass classifiers trained with a two-step semi-supervised model. In the first step, the algorithm partitions the partially labeled data and then identifies dense clusters containing $\kappa$ predominant classes using the labeled training examples such that the proportion of their non-predominant classes is below a fixed threshold. In the second step, a classifier is trained by minimizing a margin empirical loss over the labeled training set and a penalization term measuring the disability of the learner to predict the $\kappa$ predominant classes of the identified clusters. The resulting data-dependent generalization error bound involves the margin distribution of the classifier, the stability of the clustering technique used in the first step and Rademacher complexity terms corresponding to partially labeled training data. Our theoretical result exhibit convergence rates extending those proposed in the literature for the binary case, and experimental results on different multiclass classification problems show empirical evidence that supports the theory. "
758823190936158209,2016-07-29 00:35:21,https://t.co/TGj3MrMGYh,Stochastic Frank-Wolfe Methods for Nonconvex Optimization. (arXiv:1607.08254v1 [math.OC]) https://t.co/TGj3MrMGYh,3,11," Abstract: We study Frank-Wolfe methods for nonconvex stochastic and finite-sum optimization problems. Frank-Wolfe methods (in the convex case) have gained tremendous recent interest in machine learning and optimization communities due to their projection-free property and their ability to exploit structured constraints. However, our understanding of these algorithms in the nonconvex setting is fairly limited. In this paper, we propose nonconvex stochastic Frank-Wolfe methods and analyze their convergence properties. For objective functions that decompose into a finite-sum, we leverage ideas from variance reduction techniques for convex optimization to obtain new variance reduced nonconvex Frank-Wolfe methods that have provably faster convergence than the classical Frank-Wolfe method. Finally, we show that the faster convergence rates of our variance reduced methods also translate into improved convergence rates for the stochastic setting. "
758823187337531394,2016-07-29 00:35:20,https://t.co/3vMorX3Q8s,Preterm Birth Prediction: Deriving Stable and Interpretable Rules from High Dimensional Data. (arXiv:1607.08310v1 … https://t.co/3vMorX3Q8s,0,5," Abstract: Preterm births occur at an alarming rate of 10-15%. Preemies have a higher risk of infant mortality, developmental retardation and long-term disabilities. Predicting preterm birth is difficult, even for the most experienced clinicians. The most well-designed clinical study thus far reaches a modest sensitivity of 18.2-24.2% at specificity of 28.6-33.3%. We take a different approach by exploiting databases of normal hospital operations. We aims are twofold: (i) to derive an easy-to-use, interpretable prediction rule with quantified uncertainties, and (ii) to construct accurate classifiers for preterm birth prediction. Our approach is to automatically generate and select from hundreds (if not thousands) of possible predictors using stability-aware techniques. Derived from a large database of 15,814 women, our simplified prediction rule with only 10 items has sensitivity of 62.3% at specificity of 81.5%. "
758823184967737344,2016-07-29 00:35:20,https://t.co/ZnbEjdFdJD,Hyperparameter Optimization of Deep Neural Networks Using Non-Probabilistic RBF Surrogate Model. (arXiv:1607.08316… https://t.co/ZnbEjdFdJD,2,13," Abstract: Recently, Bayesian optimization has been successfully applied for optimizing hyperparameters of deep neural networks, significantly outperforming the expert-set hyperparameter values. The methods approximate and minimize the validation error as a function of hyperparameter values through probabilistic models like Gaussian processes. However, probabilistic models that require a prior distribution of the errors may be not adequate for approximating very complex error functions of deep neural networks. In this work, we propose to employ radial basis function as the surrogate of the error functions for optimizing both continuous and integer hyperparameters. The proposed non-probabilistic algorithm, called Hyperparameter Optimization using RBF and DYCORS (HORD), searches the surrogate for the most promising hyperparameter values while providing a good balance between exploration and exploitation. Extensive evaluations demonstrate HORD significantly outperforms the well-established Bayesian optimization methods such as Spearmint and TPE, both in terms of finding a near optimal solution with fewer expensive function evaluations, and in terms of a final validation error. Further, HORD performs equally well in low- and high-dimensional hyperparameter spaces, and by avoiding expensive covariance computation can also scale to a high number of observations. "
758823182224596993,2016-07-29 00:35:19,https://t.co/Aokfv96wXN,Variational perturbation and extended Plefka approaches to dynamics on random networks: the case of the kinetic Is… https://t.co/Aokfv96wXN,0,3," Abstract: We describe and analyze some novel approaches for studying the dynamics of Ising spin glass models. We first briefly consider the variational approach based on minimizing the Kullback-Leibler divergence between independent trajectories and the real ones and note that this approach only coincides with the mean field equations from the saddle point approximation to the generating functional when the dynamics is defined through a logistic link function, which is the case for the kinetic Ising model with parallel update. We then spend the rest of the paper developing two ways of going beyond the saddle point approximation to the generating functional. In the first one, we develop a variational perturbative approximation to the generating functional by expanding the action around a quadratic function of the local fields and conjugate local fields whose parameters are optimized. We derive analytical expressions for the optimal parameters and show that when the optimization is suitably restricted, we recover the mean field equations that are exact for the fully asymmetric random couplings (M\'ezard and Sakellariou, 2011). However, without this restriction the results are different. We also describe an extended Plefka expansion in which in addition to the magnetization, we also fix the correlation and response functions. Finally, we numerically study the performance of these approximations for Sherrington-Kirkpatrick type couplings for various coupling strengths, degrees of coupling symmetry and external fields. We show that the dynamical equations derived from the extended Plefka expansion outperform the others in all regimes, although it is computationally more demanding. The unconstrained variational approach does not perform well in the small coupling regime, while it approaches dynamical TAP equations of (Roudi and Hertz, 2011) for strong couplings. "
758823179422871556,2016-07-29 00:35:18,https://t.co/tGGklNvlhO,Kernel functions based on triplet similarity comparisons. (arXiv:1607.08456v1 [stat.ML]) https://t.co/tGGklNvlhO,0,4," Abstract: We propose two ways of defining a kernel function on a data set when the only available information about the data set are similarity triplets of the form ""Object A is more similar to object B than to object C"". Studying machine learning and data mining problems based on such restricted information has become very popular in recent years since it can easily be provided by humans via crowd sourcing. While previous approaches try to construct a low-dimensional Euclidean embedding of the data set that reflects the given similarity triplets, we aim at defining meaningful kernel functions on the data set that correspond to high-dimensional embeddings. These kernel functions can subsequently be used to apply all the standard kernel methods to solve tasks such as clustering, classification or principal component analysis on the data set. "
758823176528760832,2016-07-29 00:35:18,https://t.co/gpvgpKyiRy,The iterative reweighted Mixed-Norm Estimate for spatio-temporal MEG/EEG source reconstruction. (arXiv:1607.08458v… https://t.co/gpvgpKyiRy,0,2," Abstract: Source imaging based on magnetoencephalography (MEG) and electroencephalography (EEG) allows for the non-invasive analysis of brain activity with high temporal and good spatial resolution. As the bioelectromagnetic inverse problem is ill-posed, constraints are required. For the analysis of evoked brain activity, spatial sparsity of the neuronal activation is a common assumption. It is often taken into account using convex constraints based on the l1-norm. The resulting source estimates are however biased in amplitude and often suboptimal in terms of source selection due to high correlations in the forward model. In this work, we demonstrate that an inverse solver based on a block-separable penalty with a Frobenius norm per block and a l0.5-quasinorm over blocks addresses both of these issues. For solving the resulting non-convex optimization problem, we propose the iterative reweighted Mixed Norm Estimate (irMxNE), an optimization scheme based on iterative reweighted convex surrogate optimization problems, which are solved efficiently using a block coordinate descent scheme and an active set strategy. We compare the proposed sparse imaging method to the dSPM and the RAP-MUSIC approach based on two MEG data sets. We provide empirical evidence based on simulations and analysis of MEG data that the proposed method improves on the standard Mixed Norm Estimate (MxNE) in terms of amplitude bias, support recovery, and stability. "
758823174117076992,2016-07-29 00:35:17,https://t.co/Hpp5oWyS9o,Limit theorems for eigenvectors of the normalized Laplacian for random graphs. (arXiv:1607.08601v1 [stat.ML]) https://t.co/Hpp5oWyS9o,0,4," Abstract: We prove a central limit theorem for the components of the eigenvectors corresponding to the $d$ largest eigenvalues of the normalized Laplacian matrix of a finite dimensional random dot product graph. As a corollary, we show that for stochastic blockmodel graphs, the rows of the spectral embedding of the normalized Laplacian converge to multivariate normals and furthermore the mean and the covariance matrix of each row are functions of the associated vertex's block membership. Together with prior results for the eigenvectors of the adjacency matrix, we then compare, via the Chernoff information between multivariate normal distributions, how the choice of embedding method impacts subsequent inference. We demonstrate that neither embedding method dominates with respect to the inference task of recovering the latent block assignments. "
758823171806023681,2016-07-29 00:35:17,https://t.co/UqUH2GwY58,Calibrated Multivariate Regression with Application to Neural Semantic Basis Discovery. (arXiv:1305.2238v2 [stat.M… https://t.co/UqUH2GwY58,0,3," Abstract: We propose a calibrated multivariate regression method named CMR for fitting high dimensional multivariate regression models. Compared with existing methods, CMR calibrates regularization for each regression task with respect to its noise level so that it simultaneously attains improved finite-sample performance and tuning insensitiveness. Theoretically, we provide sufficient conditions under which CMR achieves the optimal rate of convergence in parameter estimation. Computationally, we propose an efficient smoothed proximal gradient algorithm with a worst-case numerical rate of convergence $\cO(1/\epsilon)$, where $\epsilon$ is a pre-specified accuracy of the objective function value. We conduct thorough numerical simulations to illustrate that CMR consistently outperforms other high dimensional multivariate regression methods. We also apply CMR to solve a brain activity prediction problem and find that it is as competitive as a handcrafted model created by human experts. The R package \texttt{camel} implementing the proposed method is available on the Comprehensive R Archive Network \url{this http URL}. "
758823169025187840,2016-07-29 00:35:16,https://t.co/2b71ssbWya,Multiple Testing for Neuroimaging via Hidden Markov Random Field. (arXiv:1404.1371v2 [stat.AP] UPDATED) https://t.co/2b71ssbWya,0,8," Abstract: Traditional voxel-level multiple testing procedures in neuroimaging, mostly $p$-value based, often ignore the spatial correlations among neighboring voxels and thus suffer from substantial loss of power. We extend the local-significance-index based procedure originally developed for the hidden Markov chain models, which aims to minimize the false nondiscovery rate subject to a constraint on the false discovery rate, to three-dimensional neuroimaging data using a hidden Markov random field model. A generalized expectation-maximization algorithm for maximizing the penalized likelihood is proposed for estimating the model parameters. Extensive simulations show that the proposed approach is more powerful than conventional false discovery rate procedures. We apply the method to the comparison between mild cognitive impairment, a disease status with increased risk of developing Alzheimer's or another dementia, and normal controls in the FDG-PET imaging study of the Alzheimer's Disease Neuroimaging Initiative. "
758823165938192384,2016-07-29 00:35:15,https://t.co/LtKkb0bnvy,Generalized Majorization-Minimization. (arXiv:1506.07613v2 [cs.CV] UPDATED) https://t.co/LtKkb0bnvy,0,3," Abstract: Non-convex optimization is ubiquitous in machine learning. The Majorization-Minimization (MM) procedure systematically optimizes non-convex functions through an iterative construction and optimization of upper bounds on the objective function. The bound at each iteration is required to \emph{touch} the objective function at the optimizer of the previous bound. We show that this touching constraint is unnecessary and overly restrictive. We generalize MM by relaxing this constraint, and propose a new framework for designing optimization algorithms, named Generalized Majorization-Minimization (G-MM). Compared to MM, G-MM is much more flexible. For instance, it can incorporate application-specific biases into the optimization procedure without changing the objective function. We derive G-MM algorithms for several latent variable models and show that they consistently outperform their MM counterparts in optimizing non-convex objectives. In particular, G-MM algorithms appear to be less sensitive to initialization. "
758823163304095745,2016-07-29 00:35:15,https://t.co/idd67Quopc,Statistical physics of inference: Thresholds and algorithms. (arXiv:1511.02476v4 [cond-mat.stat-mech] UPDATED) https://t.co/idd67Quopc,1,11," Abstract: Many questions of fundamental interest in todays science can be formulated as inference problems: Some partial, or noisy, observations are performed over a set of variables and the goal is to recover, or infer, the values of the variables based on the indirect information contained in the measurements. For such problems, the central scientific questions are: Under what conditions is the information contained in the measurements sufficient for a satisfactory inference to be possible? What are the most efficient algorithms for this task? A growing body of work has shown that often we can understand and locate these fundamental barriers by thinking of them as phase transitions in the sense of statistical physics. Moreover, it turned out that we can use the gained physical insight to develop new promising algorithms. Connection between inference and statistical physics is currently witnessing an impressive renaissance and we review here the current state-of-the-art, with a pedagogical focus on the Ising model which formulated as an inference problem we call the planted spin glass. In terms of applications we review two classes of problems: (i) inference of clusters on graphs and networks, with community detection as a special case and (ii) estimating a signal from its noisy linear measurements, with compressed sensing as a case of sparse estimation. Our goal is to provide a pedagogical review for researchers in physics and other fields interested in this fascinating topic. "
758823160791785472,2016-07-29 00:35:14,https://t.co/QOP46pFEfi,Positive Definite Estimation of Large Covariance Matrix Using Generalized Nonconvex Penalties. (arXiv:1604.04348v3… https://t.co/QOP46pFEfi,0,3," Abstract: This work addresses the issue of large covariance matrix estimation in high-dimensional statistical analysis. Recently, improved iterative algorithms with positive-definite guarantee have been developed. However, these algorithms cannot be directly extended to use a nonconvex penalty for sparsity inducing. Generally, a nonconvex penalty has the capability of ameliorating the bias problem of the popular convex lasso penalty, and thus is more advantageous. In this work, we propose a class of positive-definite covariance estimators using generalized nonconvex penalties. We develop a first-order algorithm based on the alternating direction method framework to solve the nonconvex optimization problem efficiently. The convergence of this algorithm has been proved. Further, the statistical properties of the new estimators have been analyzed for generalized nonconvex penalties. Moreover, extension of this algorithm to covariance estimation from sketched measurements has been considered. The performances of the new estimators have been demonstrated by both a simulation study and a gene clustering example for tumor tissues. Code for the proposed estimators is available at this https URL "
758823156702257153,2016-07-29 00:35:13,https://t.co/8a8Oet7o2z,Robust and Sparse Regression via $\gamma$-divergence. (arXiv:1604.06637v2 [stat.ME] UPDATED) https://t.co/8a8Oet7o2z,0,3," Abstract: In high-dimensional data, many sparse regression methods have been proposed. However, they may not be robust against outliers. Recently, the use of density power weight has been studied for robust parameter estimation and the corresponding divergences have been discussed. One of such divergences is the $\gamma$-divergence and the robust estimator using the $\gamma$-divergence is known for having a strong robustness. In this paper, we consider the robust and sparse regression based on $\gamma$-divergence. We extend the $\gamma$-divergence to the regression problem and show that it has a strong robustness under heavy contamination even when outliers are heterogeneous. The loss function is constructed by an empirical estimate of the $\gamma$-divergence with sparse regularization and the parameter estimate is defined as the minimizer of the loss function. To obtain the robust and sparse estimate, we propose an efficient update algorithm which has a monotone decreasing property of the loss function. Particularly, we discuss a linear regression problem with $L_1$ regularization in detail. In numerical experiments and real data analyses, we see that the proposed method outperforms past robust and sparse methods. "
758463853877395456,2016-07-28 00:47:28,https://t.co/hiDIC299JZ,Using Kernel Methods and Model Selection for Prediction of Preterm Birth. (arXiv:1607.07959v1 [cs.LG]) https://t.co/hiDIC299JZ,1,5," Abstract: We describe an application of machine learning to the problem of predicting preterm birth. We conduct a secondary analysis on a clinical trial dataset collected by the National In- stitute of Child Health and Human Development (NICHD) while focusing our attention on predicting different classes of preterm birth. We compare three approaches for deriving predictive models: a support vector machine (SVM) approach with linear and non-linear kernels, logistic regression with different model selection along with a model based on decision rules prescribed by physician experts for prediction of preterm birth. Our approach highlights the pre-processing methods applied to handle the inherent dynamics, noise and gaps in the data and describe techniques used to handle skewed class distributions. Empirical experiments demonstrate significant improvement in predicting preterm birth compared to past work. "
758463852652662784,2016-07-28 00:47:28,https://t.co/FKxj1LetAs,Comparing the Performance of Graphical Structure Learning Algorithms with TETRAD. (arXiv:1607.08110v1 [stat.ML]) https://t.co/FKxj1LetAs,2,4," Abstract: In this report we describe a tool for comparing the performance of causal structure learning algorithms implemented in the TETRAD freeware suite of causal analysis methods. Currently the tool is available as a package in the TETRAD source code (written in Java), which can be loaded up in an Integrated Development Environment (IDE) such as IntelliJ IDEA. Simulations can be done varying the number of runs, sample sizes, and data modalities. Performance on this simulated data can then be compared for a number of algorithms, with parameters varied and with performance statistics as selected, producing a publishable report. The order of the algorithms in the output can be adjusted to the user's preference using a utility function over the statistics. Data sets from simulation can be saved along with their graphs to a file and loaded back in for further analysis, or used for analysis by other tools. "
758463851377614848,2016-07-28 00:47:28,https://t.co/cQt13RVqe9,Network-Guided Biomarker Discovery. (arXiv:1607.08161v1 [stat.ML]) https://t.co/cQt13RVqe9,1,3," Abstract: Identifying measurable genetic indicators (or biomarkers) of a specific condition of a biological system is a key element of precision medicine. Indeed it allows to tailor diagnostic, prognostic and treatment choice to individual characteristics of a patient. In machine learning terms, biomarker discovery can be framed as a feature selection problem on whole-genome data sets. However, classical feature selection methods are usually underpowered to process these data sets, which contain orders of magnitude more features than samples. This can be addressed by making the assumption that genetic features that are linked on a biological network are more likely to work jointly towards explaining the phenotype of interest. We review here three families of methods for feature selection that integrate prior knowledge in the form of networks. "
758463849301499904,2016-07-28 00:47:27,https://t.co/FN2Jz0kljw,Online Trajectory Segmentation and Summary With Applications to Visualization and Retrieval. (arXiv:1607.08188v1 [… https://t.co/FN2Jz0kljw,1,4," Abstract: Trajectory segmentation is the process of subdividing a trajectory into parts either by grouping points similar with respect to some measure of interest, or by minimizing a global objective function. Here we present a novel online algorithm for segmentation and summary, based on point density along the trajectory, and based on the nature of the naturally occurring structure of intermittent bouts of locomotive and local activity. We show an application to visualization of trajectory datasets, and discuss the use of the summary as an index allowing efficient queries which are otherwise impossible or computationally expensive, over very large datasets. "
758463848001175552,2016-07-28 00:47:27,https://t.co/fFekva5gKs,Convolutional Neural Networks Analyzed via Convolutional Sparse Coding. (arXiv:1607.08194v1 [stat.ML]) https://t.co/fFekva5gKs,3,10," Abstract: Convolutional neural networks (CNN) have led to many state-of-the-art results spanning through various fields. However, a clear and profound theoretical understanding of the forward pass, the core algorithm of CNN, is still lacking. In parallel, within the wide field of sparse approximation, Convolutional Sparse Coding (CSC) has gained increasing attention in recent years. A theoretical study of this model was recently conducted, establishing it as a reliable and stable alternative to the commonly practiced patch-based processing. Herein, we propose a novel multi-layer model, ML-CSC, in which signals are assumed to emerge from a cascade of CSC layers. This is shown to be tightly connected to CNN, so much so that the forward pass of the CNN is in fact the thresholding pursuit serving the ML-CSC model. This connection brings a fresh view to CNN, as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network, and their stable estimation, all guaranteed under simple local sparsity conditions. Lastly, identifying the weaknesses in the above pursuit scheme, we propose an alternative to the forward pass, which is connected to deconvolutional, recurrent and residual networks, and has better theoretical guarantees. "
758463846675804160,2016-07-28 00:47:27,https://t.co/qAfy9rJTBE,Channel Vector Subspace Estimation from Low-Dimensional Projections. (arXiv:1509.07469v2 [cs.IT] UPDATED) https://t.co/qAfy9rJTBE,0,4," Abstract: Massive MIMO is a variant of multiuser MIMO where the number of base-station antennas $M$ is very large (typically 100), and generally much larger than the number of spatially multiplexed data streams (typically 10). Unfortunately, the front-end A/D conversion necessary to drive hundreds of antennas, with a signal bandwidth of the order of 10 to 100 MHz, requires very large sampling bit-rate and power consumption. In order to reduce such implementation requirements, Hybrid Digital-Analog architectures have been proposed. In particular, our work in this paper is motivated by one of such schemes named Joint Spatial Division and Multiplexing (JSDM), where the downlink precoder (resp., uplink linear receiver) is split into the product of a baseband linear projection (digital) and an RF reconfigurable beamforming network (analog), such that only a reduced number $m \ll M$ of A/D converters and RF modulation/demodulation chains is needed. In JSDM, users are grouped according to the similarity of their channel dominant subspaces, and these groups are separated by the analog beamforming stage, where the multiplexing gain in each group is achieved using the digital precoder. Therefore, it is apparent that extracting the channel subspace information of the $M$-dim channel vectors from snapshots of $m$-dim projections, with $m \ll M$, plays a fundamental role in JSDM implementation. In this paper, we develop novel efficient algorithms that require sampling only $m = O(2\sqrt{M})$ specific array elements according to a coprime sampling scheme, and for a given $p \ll M$, return a $p$-dim beamformer that has a performance comparable with the best p-dim beamformer that can be designed from the full knowledge of the exact channel covariance matrix. We assess the performance of our proposed estimators both analytically and empirically via numerical simulations. "
758463845274886145,2016-07-28 00:47:26,https://t.co/PUQE1cTDGr,Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling. (arXiv:1603.02644v3 [cs.LG] UP… https://t.co/PUQE1cTDGr,2,8," Abstract: We study parameter inference in large-scale latent variable models. We first propose an unified treatment of online inference for latent variable models from a non-canonical exponential family, and draw explicit links between several previously proposed frequentist or Bayesian methods. We then propose a novel inference method for the frequentist estimation of parameters, that adapts MCMC methods to online inference of latent variable models with the proper use of local Gibbs sampling. Then, for latent Dirich-let allocation,we provide an extensive set of experiments and comparisons with existing work, where our new approach outperforms all previously proposed methods. In particular, using Gibbs sampling for latent variable inference is superior to variational inference in terms of test log-likelihoods. Moreover, Bayesian inference through variational methods perform poorly, sometimes leading to worse fits with latent variables of higher dimensionality. "
758463844020805632,2016-07-28 00:47:26,https://t.co/qXnEij83o9,Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic Differentiation. (arXiv:1603.03236v3 [cs.… https://t.co/qXnEij83o9,3,15," Abstract: Optimization on manifolds is a class of methods for optimization of an objective function, subject to constraints which are smooth, in the sense that the set of points which satisfy the constraints admits the structure of a differentiable manifold. While many optimization problems are of the described form, technicalities of differential geometry and the laborious calculation of derivatives pose a significant barrier for experimenting with these methods. We introduce Pymanopt (available at this https URL), a toolbox for optimization on manifolds, implemented in Python, that---similarly to the Manopt Matlab toolbox---implements several manifold geometries and optimization algorithms. Moreover, we lower the barriers to users further by using automated differentiation for calculating derivative information, saving users time and saving them from potential calculation and implementation errors. "
758463842737393664,2016-07-28 00:47:26,https://t.co/jy1ecSlrJr,Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016). (arXiv:1607.02531v… https://t.co/jy1ecSlrJr,2,9," Abstract: This is the Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016. Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang, and Hanna Wallach. "
758463841445478400,2016-07-28 00:47:26,https://t.co/VkykIT2ApS,Domain Adaptive Neural Networks for Object Recognition. (arXiv:1409.6041v1 [cs.CV] CROSS LISTED) https://t.co/VkykIT2ApS,1,10," Abstract: We propose a simple neural network model to deal with the domain adaptation problem in object recognition. Our model incorporates the Maximum Mean Discrepancy (MMD) measure as a regularization in the supervised learning to reduce the distribution mismatch between the source and target domains in the latent space. From experiments, we demonstrate that the MMD regularization is an effective tool to provide good domain adaptation models on both SURF features and raw image pixels of a particular image data set. We also show that our proposed model, preceded by the denoising auto-encoder pretraining, achieves better performance than recent benchmark models on the same data sets. This work represents the first study of MMD measure in the context of neural networks. "
758463839855898625,2016-07-28 00:47:25,https://t.co/A2sRBH0ULi,Domain Generalization for Object Recognition with Multi-task Autoencoders. (arXiv:1508.07680v1 [cs.CV] CROSS LISTE… https://t.co/A2sRBH0ULi,2,10," Abstract: The problem of domain generalization is to take knowledge acquired from a number of related domains where training data is available, and to then successfully apply it to previously unseen domains. We propose a new feature learning algorithm, Multi-Task Autoencoder (MTAE), that provides good generalization performance for cross-domain object recognition. Our algorithm extends the standard denoising autoencoder framework by substituting artificially induced corruption with naturally occurring inter-domain variability in the appearance of objects. Instead of reconstructing images from noisy versions, MTAE learns to transform the original image into analogs in multiple related domains. It thereby learns features that are robust to variations across domains. The learnt features are then used as inputs to a classifier. We evaluated the performance of the algorithm on benchmark image recognition datasets, where the task is to learn features from multiple datasets and to then predict the image label from unseen datasets. We found that (denoising) MTAE outperforms alternative autoencoder-based models as well as the current state-of-the-art algorithms for domain generalization. "
758463837851021313,2016-07-28 00:47:25,https://t.co/Z69WDsW87a,Scatter Component Analysis: A Unified Framework for Domain Adaptation and Domain Generalization. (arXiv:1510.04373… https://t.co/Z69WDsW87a,0,3," Abstract: This paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from (but related to) the target. Two closely related frameworks, domain adaptation and domain generalization, are concerned with such tasks, where the only difference between those frameworks is the availability of the unlabeled target data: domain adaptation can leverage unlabeled target information, while domain generalization cannot. We propose Scatter Component Analyis (SCA), a fast representation learning algorithm that can be applied to both domain adaptation and domain generalization. SCA is based on a simple geometrical measure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA finds a representation that trades between maximizing the separability of classes, minimizing the mismatch between domains, and maximizing the separability of data; each of which is quantified through scatter. The optimization problem of SCA can be reduced to a generalized eigenvalue problem, which results in a fast and exact solution. Comprehensive experiments on benchmark cross-domain object recognition datasets verify that SCA performs much faster than several state-of-the-art algorithms and also provides state-of-the-art classification accuracy in both domain adaptation and domain generalization. We also show that scatter can be used to establish a theoretical generalization bound in the case of domain adaptation. "
758102318348992513,2016-07-27 00:50:52,https://t.co/gm9flrcN6I,Deepr: A Convolutional Net for Medical Records. (arXiv:1607.07519v1 [stat.ML]) https://t.co/gm9flrcN6I,1,10," Abstract: Feature engineering remains a major bottleneck when creating predictive systems from electronic medical records. At present, an important missing element is detecting predictive regular clinical motifs from irregular episodic records. We present Deepr (short for Deep record), a new end-to-end deep learning system that learns to extract features from medical records and predicts future risk automatically. Deepr transforms a record into a sequence of discrete elements separated by coded time gaps and hospital transfers. On top of the sequence is a convolutional neural net that detects and combines predictive local clinical motifs to stratify the risk. Deepr permits transparent inspection and visualization of its inner working. We validate Deepr on hospital data to predict unplanned readmission after discharge. Deepr achieves superior accuracy compared to traditional techniques, detects meaningful clinical motifs, and uncovers the underlying structure of the disease and intervention space. "
758102317237473281,2016-07-27 00:50:51,https://t.co/grG5yPuP3j,Variational Mixture Models with Gamma or inverse-Gamma components. (arXiv:1607.07573v1 [stat.ML]) https://t.co/grG5yPuP3j,0,3," Abstract: Mixture models with Gamma and or inverse-Gamma distributed mixture components are useful for medical image tissue segmentation or as post-hoc models for regression coefficients obtained from linear regression within a Generalised Linear Modeling framework (GLM), used in this case to separate stochastic (Gaussian) noise from some kind of positive or negative ""activation"" (modeled as Gamma or inverse-Gamma distributed). To date, the most common choice in this context it is Gaussian/Gamma mixture models learned through a maximum likelihood (ML) approach; we recently extended such algorithm for mixture models with inverse-Gamma components. Here, we introduce a fully analytical Variational Bayes (VB) learning framework for both Gamma and/or inverse-Gamma components. We use synthetic and resting state fMRI data to compare the performance of the ML and VB algorithms in terms of area under the curve and computational cost. We observed that the ML Gaussian/Gamma model is very expensive specially when considering high resolution images; furthermore, these solutions are highly variable and they occasionally can overestimate the activations severely. The Bayesian Gauss-Gamma is in general the fastest algorithm but provides too dense solutions. The maximum likelihood Gaussian/inverse-Gamma is also very fast but provides in general very sparse solutions. The variational Gaussian/inverse-Gamma mixture model is the most robust and its cost is acceptable even for high resolution images. Further, the presented methodology represents an essential building block that can be directly used in more complex inference tasks, specially designed to analyse MRI-fMRI data; such models include for example analytical variational mixture models with adaptive spatial regularization or better source models for new spatial blind source separation approaches. "
758102315891060736,2016-07-27 00:50:51,https://t.co/jBpzg4PFlr,Simultaneous estimation of noise variance and number of peaks in Bayesian spectral deconvolution. (arXiv:1607.0759… https://t.co/jBpzg4PFlr,1,4," Abstract: Heuristic identification of peaks from noisy complex spectra often leads to misunderstanding physical and chemical properties of matter. In this paper, we propose a framework based on Bayesian inference, which enables us to separate multi-peak spectra into single peaks statistically and is constructed in two steps. The first step is estimating both noise variance and number of peaks as hyperparameters based on Bayes free energy, which generally is not analytically tractable. The second step is fitting the parameters of each peak function to the given spectrum by calculating the posterior density, which has a problem of local minima and saddles since multi-peak models are nonlinear and hierarchical. Our framework enables escaping from local minima or saddles by using the exchange Monte Carlo method and calculates Bayes free energy. We discuss a simulation demonstrating how efficient our framework is and show that estimating both noise variance and number of peaks prevents overfitting, overpenalizing, and misunderstanding the precision of parameter estimation. "
758102314729242624,2016-07-27 00:50:51,https://t.co/SDZALM1wSb,An Adaptive Matrix Factorization Approach for Personalized Recommender Systems. (arXiv:1607.07607v1 [cs.LG]) https://t.co/SDZALM1wSb,0,6," Abstract: Given a set $U$ of users and a set of items $I$, a dataset of recommendations can be viewed as a sparse rectangular matrix $A$ of size $|U|\times |I|$ such that $a_{u,i}$ contains the rating the user $u$ assigns to item $i$, $a_{u,i}=?$ if the user $u$ has not rated the item $i$. The goal of a recommender system is to predict replacements to the missing observations $?$ in $A$ in order to make personalized recommendations meeting the user's tastes. A promising approach is the one based on the low-rank nonnegative matrix factorization of $A$ where items and users are represented in terms of a few vectors. These vector can be used for estimating the missing evaluations and to produce new recommendations. In this paper we propose an algorithm based on the nonnegative matrix Factorization approach for predicting the missing entries. Numerical test have been performed to estimate the accuracy in predicting the missing entries and in the recommendations provided and we have compared our technique with others in the literature. We have tested the algorithm on the MovieLens databases containing ratings of users on movies. "
758102313634500609,2016-07-27 00:50:51,https://t.co/J3bcju7BON,Uniform Approximation by Neural Networks Activated by First and Second Order Ridge Splines. (arXiv:1607.07819v1 [s… https://t.co/J3bcju7BON,1,6, Abstract: We establish sup-norm error bounds for functions that are approximated by linear combinations of first and second order ridge splines and show that these bounds are near-optimal. 
758102312627867648,2016-07-27 00:50:50,https://t.co/qj4bcJTpKl,Fast Global Convergence of Online PCA. (arXiv:1607.07837v1 [math.OC]) https://t.co/qj4bcJTpKl,0,5," Abstract: We study streaming principal component analysis (PCA), that is to find the top $k$ eigenvectors of a $d\times d$ hidden matrix $\bf \Sigma$ with online vectors drawn from covariance matrix $\bf \Sigma$. We provide GLOBAL convergence for Oja's algorithm which is popularly used in practice but lacks theoretical understanding for $k>1$. We also provide a modified variant $\mathsf{Oja}^{++}$ that runs EVEN FASTER than Oja's. Our results match the information theoretic lower bound in terms of dependency on error, on eigengap, on rank $k$, and on dimension $d$, up to poly-log factors. In addition, our convergence rate can be made gap-free, that is proportional to the approximation error and independent of the eigengap. In contrast, for general rank $k$, before our work (1) it was open to design any algorithm with efficient global convergence rate; and (2) it was open to design any algorithm with (even local) gap-free convergence rate. "
758102311394762752,2016-07-27 00:50:50,https://t.co/HkE7NpCnCR,A New PAC-Bayesian Perspective on Domain Adaptation. (arXiv:1506.04573v4 [stat.ML] UPDATED) https://t.co/HkE7NpCnCR,0,5," Abstract: We study the issue of PAC-Bayesian domain adaptation: We want to learn, from a source domain, a majority vote model dedicated to a target one. Our theoretical contribution brings a new perspective by deriving an upper-bound on the target risk where the distributions' divergence---expressed as a ratio---controls the trade-off between a source error measure and the target voters' disagreement. Our bound suggests that one has to focus on regions where the source data is informative.From this result, we derive a PAC-Bayesian generalization bound, and specialize it to linear classifiers. Then, we infer a learning algorithmand perform experiments on real data. "
758102310107176961,2016-07-27 00:50:50,https://t.co/l9sepBZseQ,Gradient Estimation with Simultaneous Perturbation and Compressive Sensing. (arXiv:1511.08768v2 [stat.ML] UPDATED) https://t.co/l9sepBZseQ,0,5," Abstract: This paper aims at achieving a ""good"" estimator for the gradient of a function on a high-dimensional space. Often such functions are not sensitive in all coordinates and the gradient of the function is almost sparse. We propose a method for gradient estimation that combines ideas from Spall's Simultaneous Perturbation Stochastic Approximation with compressive sensing. The aim is to obtain ""good"" estimator without too many function evaluations. Application to estimating gradient outer product matrix as well as standard optimization problems are illustrated via simulations. "
758102309033353216,2016-07-27 00:50:49,https://t.co/q4DQhm1gmz,Model Interpolation with Trans-dimensional Random Field Language Models for Speech Recognition. (arXiv:1603.09170v… https://t.co/q4DQhm1gmz,1,4," Abstract: The dominant language models (LMs) such as n-gram and neural network (NN) models represent sentence probabilities in terms of conditionals. In contrast, a new trans-dimensional random field (TRF) LM has been recently introduced to show superior performances, where the whole sentence is modeled as a random field. In this paper, we examine how the TRF models can be interpolated with the NN models, and obtain 12.1\% and 17.9\% relative error rate reductions over 6-gram LMs for English and Chinese speech recognition respectively through log-linear combination. "
758102306923614208,2016-07-27 00:50:49,https://t.co/MmhH0SIkcI,Online Optimization with Costly and Noisy Measurements using Random Fourier Expansions. (arXiv:1603.09620v2 [cs.LG… https://t.co/MmhH0SIkcI,0,3," Abstract: This paper analyzes DONE, an online optimization algorithm that iteratively minimizes an unknown function based on costly and noisy measurements. The algorithm maintains a surrogate of the unknown function in the form of a random Fourier expansion (RFE). The surrogate is updated whenever a new measurement is available, and then used to determine the next measurement point. The algorithm is comparable to Bayesian optimization algorithms, but its computational complexity per iteration does not depend on the number of measurements. We derive several theoretical results that provide insight on how the hyper-parameters of the algorithm should be chosen. The algorithm is compared to a Bayesian optimization algorithm for a benchmark problem and three applications, namely, optical coherence tomography, optical beam-forming network tuning, and robot arm control. It is found that the DONE algorithm is significantly faster than Bayesian optimization in the discussed problems, while achieving a similar or better performance. "
758102304771964928,2016-07-27 00:50:48,https://t.co/DRvJyGWJNM,Risk Bounds for High-dimensional Ridge Function Combinations Including Neural Networks. (arXiv:1607.01434v3 [math.… https://t.co/DRvJyGWJNM,0,4," Abstract: Let $ f^{\star} $ be a function on $ \mathbb{R}^d $ satisfying a spectral norm condition. For various noise settings, we show that $ \mathbb{E}\|\hat{f} - f^{\star} \|^2 \leq v_{f^{\star}}\left(\frac{\log d}{n}\right)^{1/4} $, where $ n $ is the sample size and $ \hat{f} $ is either a penalized least squares estimator or a greedily obtained version of such using linear combinations of ramp, sinusoidal, sigmoidal or other bounded Lipschitz ridge functions. Our risk bound is effective even when the dimension $ d $ is much larger than the available sample size. For settings where the dimension is larger than the square root of the sample size this quantity is seen to improve the more familiar risk bound of $ v_{f^{\star}}\left(\frac{d\log (n/d)}{n}\right)^{1/2} $, also investigated here. "
757736758649974784,2016-07-26 00:38:15,https://t.co/JTy4uOktIQ,Interactive Learning from Multiple Noisy Labels. (arXiv:1607.06988v1 [cs.LG]) https://t.co/JTy4uOktIQ,1,7," Abstract: Interactive learning is a process in which a machine learning algorithm is provided with meaningful, well-chosen examples as opposed to randomly chosen examples typical in standard supervised learning. In this paper, we propose a new method for interactive learning from multiple noisy labels where we exploit the disagreement among annotators to quantify the easiness (or meaningfulness) of an example. We demonstrate the usefulness of this method in estimating the parameters of a latent variable classification model, and conduct experimental analyses on a range of synthetic and benchmark datasets. Furthermore, we theoretically analyze the performance of perceptron in this interactive learning framework. "
757736756858978304,2016-07-26 00:38:15,https://t.co/u6e2FLm0Vt,Community Detection in Degree-Corrected Block Models. (arXiv:1607.06993v1 [math.ST]) https://t.co/u6e2FLm0Vt,0,3," Abstract: Community detection is a central problem of network data analysis. Given a network, the goal of community detection is to partition the network nodes into a small number of clusters, which could often help reveal interesting structures. The present paper studies community detection in Degree-Corrected Block Models (DCBMs). We first derive asymptotic minimax risks of the problem for a misclassification proportion loss under appropriate conditions. The minimax risks are shown to depend on degree-correction parameters, community sizes, and average within and between community connectivities in an intuitive and interpretable way. In addition, we propose a polynomial time algorithm to adaptively perform consistent and even asymptotically optimal community detection in DCBMs. "
757736755164549120,2016-07-26 00:38:15,https://t.co/wJPSNrTN5p,Scaling Up Sparse Support Vector Machine by Simultaneous Feature and Sample Reduction. (arXiv:1607.06996v1 [stat.M… https://t.co/wJPSNrTN5p,1,3," Abstract: Sparse support vector machine (SVM) is a popular classification technique that can simultaneously learn a small set of the most interpretable features. It has achieved great success in many real-world applications. However, for large-scale problems involving a huge number of samples and extremely high-dimensional features, solving sparse SVM remains challenging. By noting that sparse SVM induces sparsities in both feature and sample spaces, we propose a novel approach---that is based on accurate estimations of the primal and dual optimums of sparse SVM---to simultaneously identify the features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identified samples and features from the training phase, which may lead to substantial savings in both memory usage and computational cost without sacrificing accuracy. To the best of our knowledge, the proposed method is the \emph{first} \emph{static} feature and sample reduction method for sparse SVM. Experiments on both synthetic and real data sets (e.g., the kddb data set with about 20 million of samples and 30 million of features) demonstrate that our approach significantly outperforms existing state-of-the-art methods and the speedup gained by our approach can be orders of magnitude. "
757736753549742080,2016-07-26 00:38:14,https://t.co/QTDmqYGjhD,Higher-Order Factorization Machines. (arXiv:1607.07195v1 [stat.ML]) https://t.co/QTDmqYGjhD,2,6," Abstract: Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy. We demonstrate the proposed approaches on four different link prediction tasks. "
757736752127827969,2016-07-26 00:38:14,https://t.co/TyZBw4AMpj,An Evolutionary Algorithm to Learn SPARQL Queries: Finding Patterns for Human Associations in DBpedia. (arXiv:1607… https://t.co/TyZBw4AMpj,3,7," Abstract: Efficient usage of the knowledge provided by the Linked Data community is often hindered by the need for domain experts to formulate the right SPARQL queries to answer questions. For new questions they have to decide which datasets are suitable and in which terminology and modelling style to phrase the SPARQL query. In this work we present an evolutionary algorithm to help with this challenging task. Given a training list of source-target node-pair examples our algorithm can learn patterns (SPARQL queries) from a SPARQL endpoint. The learned patterns can be visualised to form the basis for further investigation, or they can be used to predict target nodes for new source nodes. Amongst others, we apply our algorithm to a dataset of several hundred human associations (such as ""circle - square"") to find patterns for them in DBpedia. We show the scalability of the algorithm by running it against a SPARQL endpoint loaded with > 7.9 billion triples. Further, we use the resulting SPARQL queries to mimic human associations with a Mean Average Precision (MAP) of 39.9 % and a Recall@10 of 63.9 %. "
757736750575935489,2016-07-26 00:38:14,https://t.co/3AOH6Q5DQW,A Statistical Test for Joint Distributions Equivalence. (arXiv:1607.07270v1 [cs.LG]) https://t.co/3AOH6Q5DQW,0,1," Abstract: We provide a distribution-free test that can be used to determine whether any two joint distributions $p$ and $q$ are statistically different by inspection of a large enough set of samples. Following recent efforts from Long et al. [1], we rely on joint kernel distribution embedding to extend the kernel two-sample test of Gretton et al. [2] to the case of joint probability distributions. Our main result can be directly applied to verify if a dataset-shift has occurred between training and test distributions in a learning framework, without further assuming the shift has occurred only in the input, in the target or in the conditional distribution. "
757736748059426816,2016-07-26 00:38:13,https://t.co/aBddYijIIQ,Accelerating Stochastic Composition Optimization. (arXiv:1607.07329v1 [math.OC]) https://t.co/aBddYijIIQ,0,2," Abstract: Consider the stochastic composition optimization problem where the objective is a composition of two expected-value functions. We propose a new stochastic first-order method, namely the accelerated stochastic compositional proximal gradient (ASC-PG) method, which updates based on queries to the sampling oracle using two different timescales. The ASC-PG is the first proximal gradient method for the stochastic composition problem that can deal with nonsmooth regularization penalty. We show that the ASC-PG exhibits faster convergence than the best known algorithms, and that it achieves the optimal sample-error complexity in several important special cases. We further demonstrate the application of ASC-PG to reinforcement learning and conduct numerical experiments. "
757736746599714816,2016-07-26 00:38:13,https://t.co/Vrlh6XZv4F,Identifying Depression on Twitter. (arXiv:1607.07384v1 [cs.SI]) https://t.co/Vrlh6XZv4F,0,2," Abstract: Social media has recently emerged as a premier method to disseminate information online. Through these online networks, tens of millions of individuals communicate their thoughts, personal experiences, and social ideals. We therefore explore the potential of social media to predict, even prior to onset, Major Depressive Disorder (MDD) in online personas. We employ a crowdsourced method to compile a list of Twitter users who profess to being diagnosed with depression. Using up to a year of prior social media postings, we utilize a Bag of Words approach to quantify each tweet. Lastly, we leverage several statistical classifiers to provide estimates to the risk of depression. Our work posits a new methodology for constructing our classifier by treating social as a text-classification problem, rather than a behavioral one on social media platforms. By using a corpus of 2.5M tweets, we achieved an 81% accuracy rate in classification, with a precision score of .86. We believe that this method may be helpful in developing tools that estimate the risk of an individual being depressed, can be employed by physicians, concerned individuals, and healthcare agencies to aid in diagnosis, even possibly enabling those suffering from depression to be more proactive about recovering from their mental health. "
757736744955568128,2016-07-26 00:38:12,https://t.co/VPblVNSwUg,Symmetry-free SDP Relaxations for Affine Subspace Clustering. (arXiv:1607.07387v1 [math.OC]) https://t.co/VPblVNSwUg,0,2," Abstract: We consider clustering problems where the goal is to determine an optimal partition of a given point set in Euclidean space in terms of a collection of affine subspaces. While there is vast literature on heuristics for this kind of problem, such approaches are known to be susceptible to poor initializations and getting trapped in bad local optima. We alleviate these issues by introducing a semidefinite relaxation based on Lasserre's method of moments. While a similiar approach is known for classical Euclidean clustering problems, a generalization to our more general subspace scenario is not straightforward, due to the high symmetry of the objective function that weakens any convex relaxation. We therefore introduce a new mechanism for symmetry breaking based on covering the feasible region with polytopes. Additionally, we introduce and analyze a deterministic rounding heuristic. "
757736743592398848,2016-07-26 00:38:12,https://t.co/zSonUAfak6,Sparse Partially Linear Additive Models. (arXiv:1407.4729v2 [stat.ME] UPDATED) https://t.co/zSonUAfak6,0,2," Abstract: The generalized partially linear additive model (GPLAM) is a flexible and interpretable approach to building predictive models. It combines features in an additive manner, allowing each to have either a linear or nonlinear effect on the response. However, the choice of which features to treat as linear or nonlinear is typically assumed known. Thus, to make a GPLAM a viable approach in situations in which little is known $a~priori$ about the features, one must overcome two primary model selection challenges: deciding which features to include in the model and determining which of these features to treat nonlinearly. We introduce the sparse partially linear additive model (SPLAM), which combines model fitting and $both$ of these model selection challenges into a single convex optimization problem. SPLAM provides a bridge between the lasso and sparse additive models. Through a statistical oracle inequality and thorough simulation, we demonstrate that SPLAM can outperform other methods across a broad spectrum of statistical regimes, including the high-dimensional ($p\gg N$) setting. We develop efficient algorithms that are applied to real data sets with half a million samples and over 45,000 features with excellent predictive performance. "
757736742153838592,2016-07-26 00:38:12,https://t.co/gu5kiNOek0,RAND-WALK: A Latent Variable Model Approach to Word Embeddings. (arXiv:1502.03520v7 [cs.LG] UPDATED) https://t.co/gu5kiNOek0,0,5," Abstract: Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of~\citet{mnih2007three}. The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by~\citet{mikolov2013efficient} and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space. "
757736740916461568,2016-07-26 00:38:11,https://t.co/fIwBdAT0Up,Optimized Linear Imputation. (arXiv:1511.05309v2 [stat.ML] UPDATED) https://t.co/fIwBdAT0Up,0,2," Abstract: Often in real-world datasets, especially in high dimensional data, some feature values are missing. Since most data analysis and statistical methods do not handle gracefully missing values, the first step in the analysis requires the imputation of missing values. Indeed, there has been a long standing interest in methods for the imputation of missing values as a pre-processing step. One recent and effective approach, the IRMI stepwise regression imputation method, uses a linear regression model for each real-valued feature on the basis of all other features in the dataset. However, the proposed iterative formulation lacks convergence guarantee. Here we propose a closely related method, stated as a single optimization problem and a block coordinate-descent solution which is guaranteed to converge to a local minimum. Experiments show results on both synthetic and benchmark datasets, which are comparable to the results of the IRMI method whenever it converges. However, while in the set of experiments described here IRMI often does not converge, the performance of our methods is shown to be markedly superior in comparison with other methods. "
757736739150716928,2016-07-26 00:38:11,https://t.co/bNsFwzzYaH,Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data. (arXiv:1605.06432v2 [st… https://t.co/bNsFwzzYaH,1,19," Abstract: We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions by means of variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction. "
757736737187753984,2016-07-26 00:38:10,https://t.co/6j7rGvD6AW,Why is Posterior Sampling Better than Optimism for Reinforcement Learning. (arXiv:1607.00215v2 [stat.ML] UPDATED) https://t.co/6j7rGvD6AW,1,10," Abstract: Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\tilde{O}(H\sqrt{SAT})$ Bayesian expected regret bound for PSRL in finite-horizon episodic Markov decision processes, where $H$ is the horizon, $S$ is the number of states, $A$ is the number of actions and $T$ is the time elapsed. This improves upon the best previous bound of $\tilde{O}(H S \sqrt{AT})$ for any reinforcement learning algorithm. "
757375711925702657,2016-07-25 00:43:35,https://t.co/nyiC8lenoT,Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. (arXiv:1607.06520v1 [cs.CL]) https://t.co/nyiC8lenoT,2,7," Abstract: The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to ""debias"" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias. "
757375708834439168,2016-07-25 00:43:35,https://t.co/VrS4DZPnGT,The Landscape of Empirical Risk for Non-convex Losses. (arXiv:1607.06534v1 [stat.ML]) https://t.co/VrS4DZPnGT,2,6," Abstract: We revisit the problem of learning a noisy linear classifier by minimizing the empirical risk associated to the square loss. While the empirical risk is non-convex, we prove that its structure is remarkably simple. Namely, when the sample size is larger than $C \, d\log d$ (with $d$ the dimension, and $C$ a constant) the following happen with high probability: $(a)$ The empirical risk has a unique local minimum (which is also the global minimum); $(b)$ Gradient descent converges exponentially fast to the global minimizer, from any initialization; $(c)$ The global minimizer approaches the true parameter at nearly optimal rate. The core of our argument is to establish a uniform convergence result for the gradients and Hessians of the empirical risk. "
757375706082996224,2016-07-25 00:43:34,https://t.co/wPtpUmnGpf,Latent Variable Discovery Using Dependency Patterns. (arXiv:1607.06617v1 [cs.AI]) https://t.co/wPtpUmnGpf,0,5," Abstract: The causal discovery of Bayesian networks is an active and important research area, and it is based upon searching the space of causal models for those which can best explain a pattern of probabilistic dependencies shown in the data. However, some of those dependencies are generated by causal structures involving variables which have not been measured, i.e., latent variables. Some such patterns of dependency ""reveal"" themselves, in that no model based solely upon the observed variables can explain them as well as a model using a latent variable. That is what latent variable discovery is based upon. Here we did a search for finding them systematically, so that they may be applied in latent variable discovery in a more rigorous fashion. "
757375703130181632,2016-07-25 00:43:33,https://t.co/Y7PzsXBQcW,On Covariate Shift Adaptation via Sparse Filtering. (arXiv:1607.06781v1 [cs.LG]) https://t.co/Y7PzsXBQcW,0,7," Abstract: A major challenge in machine learning is covariate shift, i.e., the problem of training data and test data coming from different distributions. This paper studies the feasibility of tackling this problem by means of sparse filtering. We show that the sparse filtering algorithm intrinsically addresses this problem, but it has limited capacity for covariate shift adaptation. To overcome this limit, we propose a novel semi-supervised sparse filtering algorithm, named periodic sparse filtering. Our proposed algorithm is formally analyzed and empirically evaluated with an elaborated synthetic data set and real speech emotion data sets. As a result, we argue that, as an alternative methodology, feature distribution learning has enormous potential in carrying out covariate shift adaptation. "
757375700634664960,2016-07-25 00:43:33,https://t.co/PGbzXCS8sU,High-dimensional regression adjustments in randomized experiments. (arXiv:1607.06801v1 [stat.ME]) https://t.co/PGbzXCS8sU,1,5," Abstract: We study the problem of treatment effect estimation in randomized experiments with high-dimensional covariate information, and show that essentially any risk-consistent regression adjustment can be used to obtain efficient estimates of the average treatment effect. Our results considerably extend the range of settings where high-dimensional regression adjustments are guaranteed to provide valid inference about the population average treatment effect. We then propose cross-estimation, a simple method for obtaining finite-sample-unbiased treatment effect estimates that leverages high-dimensional regression adjustments. Our method can be used when the regression model is estimated using the lasso, the elastic net, subset selection, etc. Finally, we extend our analysis to allow for adaptive specification search via cross-validation, and flexible non-parametric regression adjustments with machine learning methods such as random forests or neural networks. "
757375697400791040,2016-07-25 00:43:32,https://t.co/evLkEvesbK,"Multimodal, high-dimensional, model-based, Bayesian inverse problems with applications in biomechanics. (arXiv:151… https://t.co/evLkEvesbK",1,8," Abstract: This paper is concerned with the numerical solution of model-based, Bayesian inverse problems. We are particularly interested in cases where the cost of each likelihood evaluation (forward-model call) is expensive and the number of un- known (latent) variables is high. This is the setting in many problems in com- putational physics where forward models with nonlinear PDEs are used and the parameters to be calibrated involve spatio-temporarily varying coefficients, which upon discretization give rise to a high-dimensional vector of unknowns. One of the consequences of the well-documented ill-posedness of inverse prob- lems is the possibility of multiple solutions. While such information is contained in the posterior density in Bayesian formulations, the discovery of a single mode, let alone multiple, is a formidable task. The goal of the present paper is two- fold. On one hand, we propose approximate, adaptive inference strategies using mixture densities to capture multi-modal posteriors, and on the other, to ex- tend our work in [1] with regards to effective dimensionality reduction techniques that reveal low-dimensional subspaces where the posterior variance is mostly concentrated. We validate the model proposed by employing Importance Sam- pling which confirms that the bias introduced is small and can be efficiently corrected if the analyst wishes to do so. We demonstrate the performance of the proposed strategy in nonlinear elastography where the identification of the mechanical properties of biological materials can inform non-invasive, medical di- agnosis. The discovery of multiple modes (solutions) in such problems is critical in achieving the diagnostic objectives. "
757375695194624000,2016-07-25 00:43:31,https://t.co/id6KVR5zCz,Training Input-Output Recurrent Neural Networks through Spectral Methods. (arXiv:1603.00954v4 [cs.LG] UPDATED) https://t.co/id6KVR5zCz,4,17," Abstract: We consider the problem of training input-output recurrent neural networks (RNN) for sequence labeling tasks. We propose a novel spectral approach for learning the network parameters. It is based on decomposition of the cross-moment tensor between the output and a non-linear transformation of the input, based on score functions. We guarantee consistent learning with polynomial sample and computational complexity under transparent conditions such as non-degeneracy of model parameters, polynomial activations for the neurons, and a Markovian evolution of the input sequence. We also extend our results to Bidirectional RNN which uses both previous and future information to output the label at each time point, and is employed in many NLP tasks such as POS tagging. "
757375692342435840,2016-07-25 00:43:31,https://t.co/XKsM7Mg9P3,Efficient Nonparametric Smoothness Estimation. (arXiv:1605.05785v2 [math.ST] UPDATED) https://t.co/XKsM7Mg9P3,0,3," Abstract: Sobolev quantities (norms, inner products, and distances) of probability density functions are important in the theory of nonparametric statistics, but have rarely been used in practice, partly due to a lack of practical estimators. They also include, as special cases, $L^2$ quantities which are used in many applications. We propose and analyze a family of estimators for Sobolev quantities of unknown probability density functions. We bound the bias and variance of our estimators over finite samples, finding that they are generally minimax rate-optimal. Our estimators are significantly more computationally tractable than previous estimators, and exhibit a statistical/computational trade-off allowing them to adapt to computational constraints. We also draw theoretical connections to recent work on fast two-sample testing. Finally, we empirically validate our estimators on synthetic data. "
757375688026550272,2016-07-25 00:43:30,https://t.co/4eNRR2HBFo,Optimal rates of Statistical Seriation. (arXiv:1607.02435v2 [math.ST] UPDATED) https://t.co/4eNRR2HBFo,0,3," Abstract: Given a matrix the seriation problem consists in permuting its rows in such way that all its columns have the same shape, for example, they are monotone increasing. We propose a statistical approach to this problem where the matrix of interest is observed with noise and study the corresponding minimax rate of estimation of the matrices. Specifically, when the columns are either unimodal or monotone, we show that the least squares estimator is optimal up to logarithmic factors and adapts to matrices with a certain natural structure. Finally, we propose a computationally efficient estimator in the monotonic case and study its performance both theoretically and experimentally. Our work is at the intersection of shape constrained estimation and recent work that involves permutation learning, such as graph denoising and ranking. "
756285911453802496,2016-07-22 00:33:07,https://t.co/2Y9UBvtZJr,"Supervised quantum gate ""teaching"" for quantum hardware design. (arXiv:1607.06146v1 [cs.LG]) https://t.co/2Y9UBvtZJr",1,6, Abstract: We show how to train a quantum network of pairwise interacting qubits such that its evolution implements a target quantum algorithm into a given network subset. Our strategy is inspired by supervised learning and is designed to help the physical construction of a quantum computer which operates with minimal external classical control. 
756285907091816449,2016-07-22 00:33:06,https://t.co/g11SEuiDjQ,Explaining Classification Models Built on High-Dimensional Sparse Data. (arXiv:1607.06280v1 [stat.ML]) https://t.co/g11SEuiDjQ,1,6," Abstract: Predictive modeling applications increasingly use data representing people's behavior, opinions, and interactions. Fine-grained behavior data often has different structure from traditional data, being very high-dimensional and sparse. Models built from these data are quite difficult to interpret, since they contain many thousands or even many millions of features. Listing features with large model coefficients is not sufficient, because the model coefficients do not incorporate information on feature presence, which is key when analysing sparse data. In this paper we introduce two alternatives for explaining predictive models by listing important features. We evaluate these alternatives in terms of explanation ""bang for the buck,"", i.e., how many examples' inferences are explained for a given number of features listed. The bottom line: (i) The proposed alternatives have double the bang-for-the-buck as compared to just listing the high-coefficient features, and (ii) interestingly, although they come from different sources and motivations, the two new alternatives provide strikingly similar rankings of important features. "
756285901794312193,2016-07-22 00:33:04,https://t.co/5Cy5Bz7RsO,Hierarchical Clustering of Asymmetric Networks. (arXiv:1607.06294v1 [cs.LG]) https://t.co/5Cy5Bz7RsO,0,5," Abstract: This paper considers networks where relationships between nodes are represented by directed dissimilarities. The goal is to study methods that, based on the dissimilarity structure, output hierarchical clusters, i.e., a family of nested partitions indexed by a connectivity parameter. Our construction of hierarchical clustering methods is built around the concept of admissible methods, which are those that abide by the axioms of value - nodes in a network with two nodes are clustered together at the maximum of the two dissimilarities between them - and transformation - when dissimilarities are reduced, the network may become more clustered but not less. Two particular methods, termed reciprocal and nonreciprocal clustering, are shown to provide upper and lower bounds in the space of admissible methods. Furthermore, alternative clustering methodologies and axioms are considered. In particular, modifying the axiom of value such that clustering in two-node networks occurs at the minimum of the two dissimilarities entails the existence of a unique admissible clustering method. "
756285897415528449,2016-07-22 00:33:03,https://t.co/TxOmJL6gkI,Uncovering Causality from Multivariate Hawkes Integrated Cumulants. (arXiv:1607.06333v1 [stat.ML]) https://t.co/TxOmJL6gkI,0,6," Abstract: We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each nodes of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. A consequence is that it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the third-order integrated cumulants of the process. We show on numerical experiments that our approach is indeed very robust to the shape of the kernels, and gives appealing results on the MemeTracker database. "
756285893191856129,2016-07-22 00:33:02,https://t.co/fcVRl3DYjh,Admissible Hierarchical Clustering Methods and Algorithms for Asymmetric Networks. (arXiv:1607.06335v1 [cs.LG]) https://t.co/fcVRl3DYjh,0,2," Abstract: This paper characterizes hierarchical clustering methods that abide by two previously introduced axioms -- thus, denominated admissible methods -- and proposes tractable algorithms for their implementation. We leverage the fact that, for asymmetric networks, every admissible method must be contained between reciprocal and nonreciprocal clustering, and describe three families of intermediate methods. Grafting methods exchange branches between dendrograms generated by different admissible methods. The convex combination family combines admissible methods through a convex operation in the space of dendrograms, and thirdly, the semi-reciprocal family clusters nodes that are related by strong cyclic influences in the network. Algorithms for the computation of hierarchical clusters generated by reciprocal and nonreciprocal clustering as well as the grafting, convex combination, and semi-reciprocal families are derived using matrix operations in a dioid algebra. Finally, the introduced clustering methods and algorithms are exemplified through their application to a network describing the interrelation between sectors of the United States (U.S.) economy. "
756285888095776768,2016-07-22 00:33:01,https://t.co/gEAU4s2AVm,Distributed Supervised Learning using Neural Networks. (arXiv:1607.06364v1 [stat.ML]) https://t.co/gEAU4s2AVm,2,10," Abstract: Distributed learning is the problem of inferring a function in the case where training data is distributed among multiple geographically separated sources. Particularly, the focus is on designing learning strategies with low computational requirements, in which communication is restricted only to neighboring agents, with no reliance on a centralized authority. In this thesis, we analyze multiple distributed protocols for a large number of neural network architectures. The first part of the thesis is devoted to a definition of the problem, followed by an extensive overview of the state-of-the-art. Next, we introduce different strategies for a relatively simple class of single layer neural networks, where a linear output layer is preceded by a nonlinear layer, whose weights are stochastically assigned in the beginning of the learning process. We consider both batch and sequential learning, with horizontally and vertically partitioned data. In the third part, we consider instead the more complex problem of semi-supervised distributed learning, where each agent is provided with an additional set of unlabeled training samples. We propose two different algorithms based on diffusion processes for linear support vector machines and kernel ridge regression. Subsequently, the fourth part extends the discussion to learning with time-varying data (e.g. time-series) using recurrent neural networks. We consider two different families of networks, namely echo state networks (extending the algorithms introduced in the second part), and spline adaptive filters. Overall, the algorithms presented throughout the thesis cover a wide range of possible practical applications, and lead the way to numerous future extensions, which are briefly summarized in the conclusive chapter. "
756285883112972289,2016-07-22 00:33:00,https://t.co/8pd0R49soo,Layer Normalization. (arXiv:1607.06450v1 [stat.ML]) https://t.co/8pd0R49soo,3,14," Abstract: Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques. "
756285879430352897,2016-07-22 00:32:59,https://t.co/uJ6yfciCcu,Exploiting Big Data in Logistics Risk Assessment via Bayesian Nonparametrics. (arXiv:1501.05349v2 [stat.AP] UPDATE… https://t.co/uJ6yfciCcu,0,4," Abstract: In cargo logistics, a key performance measure is transport risk, defined as the deviation of the actual arrival time from the planned arrival time. Neither earliness nor tardiness is desirable for customer and freight forwarders. In this paper, we investigate ways to assess and forecast transport risks using a half-year of air cargo data, provided by a leading forwarder on 1336 routes served by 20 airlines. Interestingly, our preliminary data analysis shows a strong multimodal feature in the transport risks, driven by unobserved events, such as cargo missing flights. To accommodate this feature, we introduce a Bayesian nonparametric model -- the probit stick-breaking process (PSBP) mixture model -- for flexible estimation of the conditional (i.e., state-dependent) density function of transport risk. We demonstrate that using simpler methods, such as OLS linear regression, can lead to misleading inferences. Our model provides a tool for the forwarder to offer customized price and service quotes. It can also generate baseline airline performance to enable fair supplier evaluation. Furthermore, the method allows us to separate recurrent risks from disruption risks. This is important, because hedging strategies for these two kinds of risks are often drastically different. "
756285875731005440,2016-07-22 00:32:58,https://t.co/L3HQNXIxPY,On the Prior Sensitivity of Thompson Sampling. (arXiv:1506.03378v2 [cs.LG] UPDATED) https://t.co/L3HQNXIxPY,0,4," Abstract: The empirically successful Thompson Sampling algorithm for stochastic bandits has drawn much interest in understanding its theoretical properties. One important benefit of the algorithm is that it allows domain knowledge to be conveniently encoded as a prior distribution to balance exploration and exploitation more effectively. While it is generally believed that the algorithm's regret is low (high) when the prior is good (bad), little is known about the exact dependence. In this paper, we fully characterize the algorithm's worst-case dependence of regret on the choice of prior, focusing on a special yet representative case. These results also provide insights into the general sensitivity of the algorithm to the choice of priors. In particular, with $p$ being the prior probability mass of the true reward-generating model, we prove $O(\sqrt{T/p})$ and $O(\sqrt{(1-p)T})$ regret upper bounds for the bad- and good-prior cases, respectively, as well as \emph{matching} lower bounds. Our proofs rely on the discovery of a fundamental property of Thompson Sampling and make heavy use of martingale theory, both of which appear novel in the literature, to the best of our knowledge. "
756285871914094592,2016-07-22 00:32:57,https://t.co/Uzaro5NxDm,Stochastic Neural Networks with Monotonic Activation Functions. (arXiv:1601.00034v3 [stat.ML] UPDATED) https://t.co/Uzaro5NxDm,2,7," Abstract: We propose a Laplace approximation that creates a stochastic unit from any smooth monotonic activation function, using only Gaussian noise. This paper investigates the application of this stochastic approximation in training a family of Restricted Boltzmann Machines (RBM) that are closely linked to Bregman divergences. This family, that we call exponential family RBM (Exp-RBM), is a subset of the exponential family Harmoniums that expresses family members through a choice of smooth monotonic non-linearity for each neuron. Using contrastive divergence along with our Gaussian approximation, we show that Exp-RBM can learn useful representations using novel stochastic units. "
756285862678302720,2016-07-22 00:32:55,https://t.co/C6tYRiBxBe,"Linear Algebraic Structure of Word Senses, with Applications to Polysemy. (arXiv:1601.03764v2 [cs.CL] UPDATED) https://t.co/C6tYRiBxBe",1,4," Abstract: Word embeddings are ubiquitous in NLP and information retrieval, but it's unclear what they represent when the word is polysemous, i.e., has multiple senses. Here it is shown that multiple word senses reside in linear superposition within the word embedding and can be recovered by simple sparse coding. The success of the method ---which applies to several embedding methods including word2vec--- is mathematically explained using the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each word sense is also accompanied by one of about 2000 discourse atoms that give a succinct description of which other words co-occur with that word sense. Discourse atoms seem of independent interest, and make the method potentially more useful than the traditional clustering-based approaches to polysemy. "
756285859041804288,2016-07-22 00:32:54,https://t.co/FxEUcFoz2u,Analysis of k-Nearest Neighbor Distances with Application to Entropy Estimation. (arXiv:1603.08578v2 [math.ST] UPD… https://t.co/FxEUcFoz2u,2,6," Abstract: Estimating entropy and mutual information consistently is important for many machine learning applications. The Kozachenko-Leonenko (KL) estimator (Kozachenko & Leonenko, 1987) is a widely used nonparametric estimator for the entropy of multivariate continuous random variables, as well as the basis of the mutual information estimator of Kraskov et al. (2004), perhaps the most widely used estimator of mutual information in this setting. Despite the practical importance of these estimators, major theoretical questions regarding their finite-sample behavior remain open. This paper proves finite-sample bounds on the bias and variance of the KL estimator, showing that it achieves the minimax convergence rate for certain classes of smooth functions. In proving these bounds, we analyze finite-sample behavior of k-nearest neighbors (k-NN) distance statistics (on which the KL estimator is based). We derive concentration inequalities for k-NN distances and a general expectation bound for statistics of k-NN distances, which may be useful for other analyses of k-NN methods. "
756285852372836352,2016-07-22 00:32:52,https://t.co/SPLV0rkLJP,A Distributed Quaternion Kalman Filter With Applications to Fly-by-Wire Systems. (arXiv:1605.05588v2 [cs.SY] UPDAT… https://t.co/SPLV0rkLJP,0,6," Abstract: The introduction of automated flight control and management systems have made possible aircraft designs that sacrifice arodynamic stability in order to incorporate stealth technology intro their shape, operate more efficiently, and are highly maneuverable. Therefore, modern flight management systems are reliant on multiple redundant sensors to monitor and control the rotations of the aircraft. To this end, a novel distributed quaternion Kalman filtering algorithm is developed for tracking the rotation and orientation of an aircraft in the three-dimensional space. The algorithm is developed to distribute computation among the sensors in a manner that forces them to consent to a unique solution while being robust to sensor and link failure, a desirable characteristic for flight management systems. In addition, the underlying quaternion-valued state space model allows to avoid problems associated with gimbal lock. The performance of the developed algorithm is verified through simulations. "
755925809861365761,2016-07-21 00:42:12,https://t.co/egG58yE5Fa,Personalization Effect on Emotion Recognition from Physiological Data: An Investigation of Performance on Differen… https://t.co/egG58yE5Fa,0,8," Abstract: This paper addresses the problem of emotion recognition from physiological signals. Features are extracted and ranked based on their effect on classification accuracy. Different classifiers are compared. The inter-subject variability and the personalization effect are thoroughly investigated, through trial-based and subject-based cross-validation. Finally, a personalized model is introduced, that would allow for enhanced emotional state prediction, based on the physiological data of subjects that exhibit a certain degree of similarity, without the requirement of further feedback. "
755925808422813696,2016-07-21 00:42:11,https://t.co/tZCZ8CMpxs,Onsager-corrected deep learning for sparse linear inverse problems. (arXiv:1607.05966v1 [cs.IT]) https://t.co/tZCZ8CMpxs,2,6," Abstract: Deep learning has gained great popularity due to its widespread success on many inference problems. We consider the application of deep learning to the sparse linear inverse problem encountered in compressive sensing, where one seeks to recover a sparse signal from a small number of noisy linear measurements. In this paper, we propose a novel neural-network architecture that decouples prediction errors across layers in the same way that the approximate message passing (AMP) algorithm decouples them across iterations: through Onsager correction. Numerical experiments suggest that our ""learned AMP"" network significantly improves upon Gregor and LeCun's ""learned ISTA"" network in both accuracy and complexity. "
755925807000907776,2016-07-21 00:42:11,https://t.co/Evajas2fn6,On the Identification and Mitigation of Weaknesses in the Knowledge Gradient Policy for Multi-Armed Bandits. (arXi… https://t.co/Evajas2fn6,0,3," Abstract: The Knowledge Gradient (KG) policy was originally proposed for online ranking and selection problems but has recently been adapted for use in online decision making in general and multi-armed bandit problems (MABs) in particular. We study its use in a class of exponential family MABs and identify weaknesses, including a propensity to take actions which are dominated with respect to both exploitation and exploration. We propose variants of KG which avoid such errors. These new policies include an index heuristic which deploys a KG approach to develop an approximation to the Gittins index. A numerical study shows this policy to perform well over a range of MABs including those for which index policies are not optimal. While KG does not make dominated actions when bandits are Gaussian, it fails to be index consistent and appears not to enjoy a performance advantage over competitor policies when arms are correlated to compensate for its greater computational demands. "
755925805709033472,2016-07-21 00:42:11,https://t.co/9cFgOTyn6u,Anomaly Detection and Localisation using Mixed Graphical Models. (arXiv:1607.05974v1 [stat.ML]) https://t.co/9cFgOTyn6u,1,7," Abstract: We propose a method that performs anomaly detection and localisation within heterogeneous data using a pairwise undirected mixed graphical model. The data are a mixture of categorical and quantitative variables, and the model is learned over a dataset that is supposed not to contain any anomaly. We then use the model over temporal data, potentially a data stream, using a version of the two-sided CUSUM algorithm. The proposed decision statistic is based on a conditional likelihood ratio computed for each variable given the others. Our results show that this function allows to detect anomalies variable by variable, and thus to localise the variables involved in the anomalies more precisely than univariate methods based on simple marginals. "
755925804119429121,2016-07-21 00:42:10,https://t.co/wmjBLi3Bhq,On the Modeling of Error Functions as High Dimensional Landscapes for Weight Initialization in Learning Networks. … https://t.co/wmjBLi3Bhq,1,3," Abstract: Next generation deep neural networks for classification hosted on embedded platforms will rely on fast, efficient, and accurate learning algorithms. Initialization of weights in learning networks has a great impact on the classification accuracy. In this paper we focus on deriving good initial weights by modeling the error function of a deep neural network as a high-dimensional landscape. We observe that due to the inherent complexity in its algebraic structure, such an error function may conform to general results of the statistics of large systems. To this end we apply some results from Random Matrix Theory to analyse these functions. We model the error function in terms of a Hamiltonian in N-dimensions and derive some theoretical results about its general behavior. These results are further used to make better initial guesses of weights for the learning algorithm. "
755925802693386248,2016-07-21 00:42:10,https://t.co/a3n0ztYfXk,Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition. (arXiv:1607.06017v1 [math.OC]) https://t.co/a3n0ztYfXk,0,5," Abstract: We study $k$-GenEV, the problem of finding the top $k$ generalized eigenvectors, and $k$-CCA, the problem of finding the top $k$ vectors in canonical-correlation analysis. We propose algorithms $\mathtt{LazyEV}$ and $\mathtt{LazyCCA}$ to solve the two problems with running times linearly dependent on the input size and on $k$. Furthermore, our algorithms are DOUBLY-ACCELERATED: our running times depend only on the square root of the matrix condition number, and on the square root of the eigengap. This is the first such result for both $k$-GenEV or $k$-CCA. We also provide the first gap-free results, which provide running times that depend on $1/\sqrt{\varepsilon}$ rather than the eigengap. "
755925801233702913,2016-07-21 00:42:10,https://t.co/Qx6HSxIAMO,Finding a sparse vector in a subspace: Linear sparsity using alternating directions. (arXiv:1412.4659v3 [cs.IT] UP… https://t.co/Qx6HSxIAMO,1,5," Abstract: Is it possible to find the sparsest vector (direction) in a generic subspace $\mathcal{S} \subseteq \mathbb{R}^p$ with $\mathrm{dim}(\mathcal{S})= n < p$? This problem can be considered a homogeneous variant of the sparse recovery problem, and finds connections to sparse dictionary learning, sparse PCA, and many other problems in signal processing and machine learning. In this paper, we focus on a **planted sparse model** for the subspace: the target sparse vector is embedded in an otherwise random subspace. Simple convex heuristics for this planted recovery problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds $O(1/\sqrt{n})$. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is $\Omega(1)$. To the best of our knowledge, this is the first practical algorithm to achieve linear scaling under the planted sparse model. Empirically, our proposed algorithm also succeeds in more challenging data models, e.g., sparse dictionary learning. "
755925799061061632,2016-07-21 00:42:09,https://t.co/5ULitOxokk,Harnessing Deep Neural Networks with Logic Rules. (arXiv:1603.06318v3 [cs.LG] UPDATED) https://t.co/5ULitOxokk,0,7," Abstract: Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems. "
755925797802770433,2016-07-21 00:42:09,https://t.co/UhZdNfnFf5,Building Better Detection with Privileged Information. (arXiv:1603.09638v2 [cs.CR] UPDATED) https://t.co/UhZdNfnFf5,1,2," Abstract: For over a quarter century, security-relevant detection has been driven by models learned from input features collected from real or simulated environments. An artifact (e.g., network event, potential malware sample, suspicious email) is deemed malicious or non-malicious based on its similarity to the learned model at run-time. However, the training of the models has been historically limited to only those features available at run time. In this paper, we consider an alternate model construction approach that trains models using forensic ""privileged"" information--features available at training time but not at runtime--to improve the accuracy and resilience of detection systems. In particular, we adapt and extend recent advances in knowledge transfer, model influence, and distillation to enable the use of forensic data in a range of security domains. Our empirical study shows that privileged information increases detection precision and recall over a system with no privileged information: we observe up to 7.7% relative decrease in detection error for fast-flux bot detection, 8.6% for malware traffic detection, 7.3% for malware classification, and 16.9% for face recognition. We explore the limitations and applications of different privileged information techniques in detection systems. Such techniques open the door to systems that can integrate forensic data directly into detection models, and therein provide a means to fully exploit the information available about past security-relevant events. "
755925794170470401,2016-07-21 00:42:08,https://t.co/KxwaPTmLIK,Higher-Order Block Term Decomposition for Spatially Folded fMRI Data. (arXiv:1607.05073v1 [cs.NA] CROSS LISTED) https://t.co/KxwaPTmLIK,0,3," Abstract: The growing use of neuroimaging technologies generates a massive amount of biomedical data that exhibit high dimensionality. Tensor-based analysis of brain imaging data has been proved quite effective in exploiting their multiway nature. The advantages of tensorial methods over matrix-based approaches have also been demonstrated in the characterization of functional magnetic resonance imaging (fMRI) data, where the spatial (voxel) dimensions are commonly grouped (unfolded) as a single way/mode of the 3-rd order array, the other two ways corresponding to time and subjects. However, such methods are known to be ineffective in more demanding scenarios, such as the ones with strong noise and/or significant overlapping of activated regions. This paper aims at investigating the possible gains from a better exploitation of the spatial dimension, through a higher- (4 or 5) order tensor modeling of the fMRI signal. In this context, and in order to increase the degrees of freedom of the modeling process, a higher-order Block Term Decomposition (BTD) is applied, for the first time in fMRI analysis. Its effectiveness is demonstrated via extensive simulation results. "
755563167162523649,2016-07-20 00:41:11,https://t.co/zwYRmYnTgv,Nested Kriging estimations for datasets with large number of observations. (arXiv:1607.05432v1 [stat.ML]) https://t.co/zwYRmYnTgv,0,5," Abstract: This work falls within the context of predicting the value of a real function f at some input locations given a limited number of observations of this function. Kriging interpolation technique (or Gaussian process regression) is often considered to tackle such problem but the method suffers from its computational burden when the number of observation points n is large. We introduce in this article nested Kriging estimators which are constructed by aggregating sub-models based on subsets of observation points. This approach is proven to have better theoretical properties than other aggregation methods that can be found in the literature. In particular, contrary to some other methods which are shown inconsistent, we prove the consistency of our proposed aggregation method. Finally, the practical interest of the proposed method is illustrated on simulated datasets and on an industrial test case with 10 4 observations in a 6-dimensional space. "
755563166042578946,2016-07-20 00:41:11,https://t.co/SQizeqJ5lu,Distribution-dependent concentration inequalities for tighter generalization bounds. (arXiv:1607.05506v1 [stat.ML]) https://t.co/SQizeqJ5lu,0,6," Abstract: We prove several distribution-dependent extensions of Hoeffding and McDiarmid's inequalities with (difference-) unbounded and hierarchically (difference-) bounded functions. For this purpose, several assumptions about the probabilistic boundedness and bounded differences are introduced. Our approaches improve the previous concentration inequalities' bounds, and achieve tight bounds in some exceptional cases where the original inequalities cannot hold. Furthermore, we discuss the potential applications of our extensions in VC dimension and Rademacher complexity. Then we obtain generalization bounds for (difference-) unbounded loss functions and tighten the existing generalization bounds. "
755563164973076481,2016-07-20 00:41:10,https://t.co/0pFt80uIVA,Combing Random Walks and Nonparametric Bayesian Topic Model for Community Detection. (arXiv:1607.05573v1 [stat.AP]) https://t.co/0pFt80uIVA,0,8," Abstract: Community detection has been an active research area for decades. Among all probabilistic models, Stochastic Block Model has been the most popular one. This paper introduces a novel probabilistic model: RW-HDP, based on random walks and Hierarchical Dirichlet Process, for community extraction. In RW-HDP, random walks conducted in a social network are treated as documents; nodes are treated as words. By using Hierarchical Dirichlet Process, a nonparametric Bayesian model, we are not only able to cluster nodes into different communities, but also determine the number of communities automatically. We use Stochastic Variational Inference for our model inference, which makes our method time efficient and can be easily extended to an online learning algorithm. "
755563163677028352,2016-07-20 00:41:10,https://t.co/oS4Rh0oFiN,Information-theoretical label embeddings for large-scale image classification. (arXiv:1607.05691v1 [cs.CV]) https://t.co/oS4Rh0oFiN,1,8," Abstract: We present a method for training multi-label, massively multi-class image classification models, that is faster and more accurate than supervision via a sigmoid cross-entropy loss (logistic regression). Our method consists in embedding high-dimensional sparse labels onto a lower-dimensional dense sphere of unit-normed vectors, and treating the classification problem as a cosine proximity regression problem on this sphere. We test our method on a dataset of 300 million high-resolution images with 17,000 labels, where it yields considerably faster convergence, as well as a 7% higher mean average precision compared to logistic regression. "
755563162431266817,2016-07-20 00:41:10,https://t.co/eBhzRL0sEk,Multi-category Angle-based Classifier Refit. (arXiv:1607.05709v1 [math.ST]) https://t.co/eBhzRL0sEk,0,2," Abstract: Classification is an important statistical learning tool. In real application, besides high prediction accuracy, it is often desirable to estimate class conditional probabilities for new observations. For traditional problems where the number of observations is large, there exist many well developed approaches. Recently, high dimensional low sample size problems are becoming increasingly popular. Margin-based classifiers, such as logistic regression, are well established methods in the literature. On the other hand, in terms of probability estimation, it is known that for binary classifiers, the commonly used methods tend to under-estimate the norm of the classification function. This can lead to biased probability estimation. Remedy approaches have been proposed in the literature. However, for the simultaneous multicategory classification framework, much less work has been done. We fill the gap in this paper. In particular, we give theoretical insights on why heavy regularization terms are often needed in high dimensional applications, and how this can lead to bias in probability estimation. To overcome this difficulty, we propose a new refit strategy for multicategory angle-based classifiers. Our new method only adds a small computation cost to the problem, and is able to attain prediction accuracy that is as good as the regular margin-based classifiers. On the other hand, the improvement of probability estimation can be very significant. Numerical results suggest that the new refit approach is highly competitive. "
755563160267059201,2016-07-20 00:41:09,https://t.co/Ygk05tYic5,On the use of Harrell's C for clinical risk prediction via random survival forests. (arXiv:1507.03092v2 [stat.ML] … https://t.co/Ygk05tYic5,0,3," Abstract: Random survival forests (RSF) are a powerful method for risk prediction of right-censored outcomes in biomedical research. RSF use the log-rank split criterion to form an ensemble of survival trees. The most common approach to evaluate the prediction accuracy of a RSF model is Harrell's concordance index for survival data ('C index'). Conceptually, this strategy implies that the split criterion in RSF is different from the evaluation criterion of interest. This discrepancy can be overcome by using Harrell's C for both node splitting and evaluation. We compare the difference between the two split criteria analytically and in simulation studies with respect to the preference of more unbalanced splits, termed end-cut preference (ECP). Specifically, we show that the log-rank statistic has a stronger ECP compared to the C index. In simulation studies and with the help of two medical data sets we demonstrate that the accuracy of RSF predictions, as measured by Harrell's C, can be improved if the log-rank statistic is replaced by the C index for node splitting. This is especially true in situations where the censoring rate or the fraction of informative continuous predictor variables is high. Conversely, log-rank splitting is preferable in noisy scenarios. Both C-based and log-rank splitting are implemented in the R~package ranger. We recommend Harrell's C as split criterion for use in smaller scale clinical studies and the log-rank split criterion for use in large-scale 'omics' studies. "
755200792064249856,2016-07-19 00:41:14,https://t.co/wQIW3Nl8Yg,Learning Unitary Operators with Help From u(n). (arXiv:1607.04903v1 [stat.ML]) https://t.co/wQIW3Nl8Yg,0,2," Abstract: A major challenge in the training of recurrent neural networks is the so-called vanishing or exploding gradient problem. The use of a norm-preserving transition operator can address this issue, but parametrization is challenging. In this work we focus on unitary operators and describe a parametrization using the Lie algebra $\mathfrak{u}(n)$ associated with the Lie group $U(n)$ of $n \times n$ unitary matrices. The exponential map provides a correspondence between these spaces, and allows us to define a unitary matrix using $n^2$ real coefficients relative to a basis of the Lie algebra. The parametrization is closed under additive updates of these coefficients, and thus provides a simple space in which to do gradient descent. We demonstrate the effectiveness of this parametrization on the problem of learning arbitrary unitary operators, comparing to several baselines and outperforming a recently-proposed lower-dimensional parametrization. This suggests a route to generalising a recently-proposed unitary recurrent neural network to arbitrary unitary matrices, solving a problem the well-known long short-term memory network was invented to address, but with a simplified and elegant network architecture. "
755200789832933376,2016-07-19 00:41:13,https://t.co/yzFjU78wL8,Geometric Mean Metric Learning. (arXiv:1607.05002v1 [stat.ML]) https://t.co/yzFjU78wL8,1,5," Abstract: We revisit the task of learning a Euclidean metric from data. We approach this problem from first principles and formulate it as a surprisingly simple optimization problem. Indeed, our formulation even admits a closed form solution. This solution possesses several very attractive properties: (i) an innate geometric appeal through the Riemannian geometry of positive definite matrices; (ii) ease of interpretability; and (iii) computational speed several orders of magnitude faster than the widely used LMNN and ITML methods. Furthermore, on standard benchmark datasets, our closed-form solution consistently attains higher classification accuracy. "
755200787555422209,2016-07-19 00:41:13,https://t.co/CC2VIZ2UEO,"A Batch, Off-Policy, Actor-Critic Algorithm for Optimizing the Average Reward. (arXiv:1607.05047v1 [stat.ML]) https://t.co/CC2VIZ2UEO",0,2, Abstract: We develop an off-policy actor-critic algorithm for learning an optimal policy from a training set composed of data from multiple individuals. This algorithm is developed with a view towards its use in mobile health. 
755200784896233472,2016-07-19 00:41:12,https://t.co/O8RxWpNeST,On the Application of Support Vector Machines to the Prediction of Propagation Losses at 169 MHz for Smart Meterin… https://t.co/O8RxWpNeST,0,3," Abstract: Recently, the need of deploying new wireless networks for smart gas metering has raised the problem of radio planning in the169 MHz band. Unluckily, software tools commonly adopted for radio planning in cellular communication systems cannot be employed to solve this problem because of the substantially lower transmission frequencies characterizing this application. In this manuscript a novel data-centric solution, based on the use of support vector machine techniques for classification and regression, is proposed. Our method requires the availability of a limited set of received signal strength measurements and the knowledge of a three-dimensional map of the propagation environment of interest, and generates both an estimate of the coverage area and a prediction of the field strength within it. Numerical results referring to different Italian villages and cities evidence that our method is able to achieve good accuracy at the price of an acceptable computational cost and of a limited effort for the acquisition of measurements in the considered environments. "
755200782983696388,2016-07-19 00:41:12,https://t.co/9QJzOtaOLu,Imitation Learning with Recurrent Neural Networks. (arXiv:1607.05241v1 [cs.CL]) https://t.co/9QJzOtaOLu,1,11," Abstract: We present a novel view that unifies two frameworks that aim to solve sequential prediction problems: learning to search (L2S) and recurrent neural networks (RNN). We point out equivalences between elements of the two frameworks. By complementing what is missing from one framework comparing to the other, we introduce a more advanced imitation learning framework that, on one hand, augments L2S s notion of search space and, on the other hand, enhances RNNs training procedure to be more robust to compounding errors arising from training on highly correlated examples. "
755200781452767233,2016-07-19 00:41:11,https://t.co/HyPKNJHKQ1,GP-select: Accelerating EM using adaptive subspace preselection. (arXiv:1412.3411v2 [stat.ML] UPDATED) https://t.co/HyPKNJHKQ1,0,2," Abstract: We propose a nonparametric procedure to achieve fast inference in generative graphical models when the number of latent states is very large. The approach is based on iterative latent variable preselection, where we alternate between learning a 'selection function' to reveal the relevant latent variables, and use this to obtain a compact approximation of the posterior distribution for EM; this can make inference possible where the number of possible latent states is e.g. exponential in the number of latent variables, whereas an exact approach would be computationally unfeasible. We learn the selection function entirely from the observed data and current EM state via Gaussian process regression. This is by contrast with earlier approaches, where selection functions were manually-designed for each problem setting. We show that our approach performs as well as these bespoke selection functions on a wide variety of inference problems: in particular, for the challenging case of a hierarchical model for object localization with occlusion, we achieve results that match a customized state-of-the-art selection method, at a far lower computational cost. "
755200779733041153,2016-07-19 00:41:11,https://t.co/wb1EoJdids,Dynamic Sum Product Networks for Tractable Inference on Sequence Data (Extended Version). (arXiv:1511.04412v2 [cs.… https://t.co/wb1EoJdids,0,7," Abstract: Sum-Product Networks (SPN) have recently emerged as a new class of tractable probabilistic graphical models. Unlike Bayesian networks and Markov networks where inference may be exponential in the size of the network, inference in SPNs is in time linear in the size of the network. Since SPNs represent distributions over a fixed set of variables only, we propose dynamic sum product networks (DSPNs) as a generalization of SPNs for sequence data of varying length. A DSPN consists of a template network that is repeated as many times as needed to model data sequences of any length. We present a local search technique to learn the structure of the template network. In contrast to dynamic Bayesian networks for which inference is generally exponential in the number of variables per time slice, DSPNs inherit the linear inference complexity of SPNs. We demonstrate the advantages of DSPNs over DBNs and other models on several datasets of sequence data. "
755200777837285376,2016-07-19 00:41:11,https://t.co/OGlsRuzxs8,Graphical Model Sketch. (arXiv:1602.03105v2 [cs.DS] UPDATED) https://t.co/OGlsRuzxs8,0,3," Abstract: Structured high-cardinality data arises in many domains, and poses a major challenge for both modeling and inference. Graphical models are a popular approach to modeling structured data but they are unsuitable for high-cardinality variables. The count-min (CM) sketch is a popular approach to estimating probabilities in high-cardinality data but it does not scale well beyond a few variables. In this work, we bring together the ideas of graphical models and count sketches; and propose and analyze several approaches to estimating probabilities in structured high-cardinality streams of data. The key idea of our approximations is to use the structure of a graphical model and approximately estimate its factors by ""sketches"", which hash high-cardinality variables using random projections. Our approximations are computationally efficient and their space complexity is independent of the cardinality of variables. Our error bounds are multiplicative and significantly improve upon those of the CM sketch, a state-of-the-art approach to estimating probabilities in streams. We evaluate our approximations on synthetic and real-world problems, and report an order of magnitude improvements over the CM sketch. "
755200773726797832,2016-07-19 00:41:10,https://t.co/bw4yzduEhZ,On the Consistency of the Likelihood Maximization Vertex Nomination Scheme: Bridging the Gap Between Maximum Likel… https://t.co/bw4yzduEhZ,0,3," Abstract: Given a graph in which a few vertices are deemed interesting a priori, the vertex nomination task is to order the remaining vertices into a nomination list such that there is a concentration of interesting vertices at the top of the list. Previous work has yielded several approaches to this problem, with theoretical results in the setting where the graph is drawn from a stochastic block model (SBM), including a vertex nomination analogue of the Bayes optimal classifier. In this paper, we prove that maximum likelihood (ML)-based vertex nomination is consistent, in the sense that the performance of the ML-based scheme asymptotically matches that of the Bayes optimal scheme. We prove theorems of this form both when model parameters are known and unknown. Additionally, we introduce and prove consistency of a related, more scalable restricted-focus ML vertex nomination scheme. Finally, we incorporate vertex and edge features into ML-based vertex nomination and briefly explore the empirical effectiveness of this approach. "
754837176639168512,2016-07-18 00:36:21,https://t.co/dtt7V6yA08,Neural Semantic Encoders. (arXiv:1607.04315v1 [cs.LG]) https://t.co/dtt7V6yA08,0,9," Abstract: We present a memory augmented neural network for natural language understanding: Neural Semantic Encoders (NSE). NSE has a variable sized encoding memory that evolves over time and maintains the understanding of input sequences through read, compose and write operations. NSE can access multiple and shared memories depending on the complexity of a task. We demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks, natural language inference, question answering, sentence classification, document sentiment analysis and machine translation where NSE achieved state-of-the-art performance when evaluated on publically available benchmarks. For example, our shared-memory model showed an encouraging result on neural machine translation, improving an attention-based baseline by approximately 1.0 BLEU. "
754837174340751360,2016-07-18 00:36:21,https://t.co/OXLRLb1JGA,Random projections of random manifolds. (arXiv:1607.04331v1 [stat.ML]) https://t.co/OXLRLb1JGA,1,12," Abstract: Interesting data often concentrate on low dimensional smooth manifolds inside a high dimensional ambient space. Random projections are a simple, powerful tool for dimensionality reduction of such data. Previous works have studied bounds on how many projections are needed to accurately preserve the geometry of these manifolds, given their intrinsic dimensionality, volume and curvature. However, such works employ definitions of volume and curvature that are inherently difficult to compute. Therefore such theory cannot be easily tested against numerical simulations to understand the tightness of the proven bounds. We instead study typical distortions arising in random projections of an ensemble of smooth Gaussian random manifolds. We find explicitly computable, approximate theoretical bounds on the number of projections required to accurately preserve the geometry of these manifolds. Our bounds, while approximate, can only be violated with a probability that is exponentially small in the ambient dimension, and therefore they hold with high probability in cases of practical interest. Moreover, unlike previous work, we test our theoretical bounds against numerical experiments on the actual geometric distortions that typically occur for random projections of random smooth manifolds. We find our bounds are tighter than previous results by several orders of magnitude. "
754837171773829121,2016-07-18 00:36:20,https://t.co/92zRMIy4QO,Neural Tree Indexers for Text Understanding. (arXiv:1607.04492v1 [cs.CL]) https://t.co/92zRMIy4QO,2,6," Abstract: Neural networks with recurrent or recursive architecture have shown promising results on various natural language processing (NLP) tasks. The recurrent and recursive architectures have their own strength and limitations. The recurrent networks process input text sequentially and model the conditional transition between word tokens. In contrast, the recursive networks explicitly model the compositionality and the recursive structure of natural language. Current recursive architecture is based on syntactic tree, thus limiting its practical applicability in different NLP applications. In this paper, we introduce a class of tree structured model, Neural Tree Indexers (NTI) that provides a middle ground between the sequential RNNs and the syntactic tree-based recursive models. NTI constructs a full n-ary tree by processing the input text with its node function in a bottom-up fashion. Attention mechanism can then be applied to both structure and different forms of node function. We demonstrated the effectiveness and the flexibility of a binary-tree model of NTI, showing the model achieved the state-of-the-art performance on three different NLP tasks: natural language inference, answer sentence selection, and sentence classification. "
754837170196779008,2016-07-18 00:36:20,https://t.co/cqZ0NHThZq,Spectral Echolocation via the Wave Embedding. (arXiv:1607.04566v1 [stat.ML]) https://t.co/cqZ0NHThZq,0,4," Abstract: Spectral embedding uses eigenfunctions of the discrete Laplacian on a weighted graph to obtain coordinates for an embedding of an abstract data set into Euclidean space. We propose a new pre-processing step of first using the eigenfunctions to simulate a low-frequency wave moving over the data and using both position as well as change in time of the wave to obtain a refined metric to which classical methods of dimensionality reduction can then applied. This is motivated by the behavior of waves, symmetries of the wave equation and the hunting technique of bats. It is shown to be effective in practice and also works for other partial differential equations -- the method yields improved results even for the classical heat equation. "
754837168493821952,2016-07-18 00:36:19,https://t.co/yirElogAmn,Analyzing features learned for Offline Signature Verification using Deep CNNs. (arXiv:1607.04573v1 [cs.CV]) https://t.co/yirElogAmn,0,3," Abstract: Research on Offline Handwritten Signature Verification explored a large variety of handcrafted feature extractors, ranging from graphology, texture descriptors to interest points. In spite of advancements in the last decades, performance of such systems is still far from optimal when we test the systems against skilled forgeries - signature forgeries that target a particular individual. In previous research, we proposed a formulation of the problem to learn features from data (signature images) in a Writer-Independent format, using Deep Convolutional Neural Networks (CNNs), seeking to improve performance on the task. In this research, we push further the performance of such method, exploring a range of architectures, and obtaining a large improvement in state-of-the-art performance on the GPDS dataset, the largest publicly available dataset on the task. In the GPDS-160 dataset, we obtained an Equal Error Rate of 2.74%, compared to 6.97% in the best result published in literature (that used a combination of multiple classifiers). We also present a visual analysis of the feature space learned by the model, and an analysis of the errors made by the classifier. Our analysis shows that the model is very effective in separating signatures that have a different global appearance, while being particularly vulnerable to forgeries that very closely resemble genuine signatures, even if their line quality is bad, which is the case of slowly-traced forgeries. "
754837167290085376,2016-07-18 00:36:19,https://t.co/kDuwZ1h4jR,Learning from Conditional Distributions via Dual Kernel Embeddings. (arXiv:1607.04579v1 [cs.LG]) https://t.co/kDuwZ1h4jR,1,5," Abstract: In many machine learning problems, such as policy evaluation in reinforcement learning and learning with invariance, each data point $x$ itself is a conditional distribution $p(z|x)$, and we want to learn a function $f$ which links these conditional distributions to target values $y$. The learning problem becomes very challenging when we only have limited samples or in the extreme case only one sample from each conditional distribution $p(z|x)$. Commonly used approaches either assume that $z$ is independent of $x$, or require an overwhelmingly large sample size from each conditional distribution. To address these challenges, we propose a novel approach which reformulates the original problem into a min-max optimization problem. In the new view, we only need to deal with the kernel embedding of the joint distribution $p(z,x)$ which is easy to estimate. Furthermore, we design an efficient learning algorithm based on mirror descent stochastic approximation, and establish the sample complexity for learning from conditional distributions. Finally, numerical experiments in both synthetic and real data show that our method can significantly improve over the previous state-of-the-arts. "
754837165624979456,2016-07-18 00:36:19,https://t.co/KueDQmL3Oz,Optimally-Weighted Herding is Bayesian Quadrature. (arXiv:1204.1664v3 [stat.ML] UPDATED) https://t.co/KueDQmL3Oz,1,5, Abstract: Herding and kernel herding are deterministic methods of choosing samples which summarise a probability distribution. A related task is choosing samples for estimating integrals using Bayesian quadrature. We show that the criterion minimised when selecting samples in kernel herding is equivalent to the posterior variance in Bayesian quadrature. We then show that sequential Bayesian quadrature can be viewed as a weighted version of kernel herding which achieves performance superior to any other weighted herding method. We demonstrate empirically a rate of convergence faster than O(1/N). Our results also imply an upper bound on the empirical error of the Bayesian quadrature estimate. 
754837162462511104,2016-07-18 00:36:18,https://t.co/BImJXOQkkM,Adversarial Feature Learning. (arXiv:1605.09782v2 [cs.LG] UPDATED) https://t.co/BImJXOQkkM,4,16," Abstract: The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to ""linearize semantics"" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning. "
753750924988907520,2016-07-15 00:39:59,https://t.co/MKHLuFgOIi,Ensemble preconditioning for Markov chain Monte Carlo simulation. (arXiv:1607.03954v1 [stat.ME]) https://t.co/MKHLuFgOIi,0,3," Abstract: We describe parallel Markov chain Monte Carlo methods that propagate a collective ensemble of paths, with local covariance information calculated from neighboring replicas. The use of collective dynamics eliminates multiplicative noise and stabilizes the dynamics thus providing a practical approach to difficult anisotropic sampling problems in high dimensions. Numerical experiments with model problems demonstrate that dramatic potential speedups, compared to various alternative schemes, are attainable. "
753750921105014784,2016-07-15 00:39:58,https://t.co/dRnQabnQFE,Estimating and Controlling the False Discovery Rate for the PC Algorithm Using Edge-Specific P-Values. (arXiv:1607… https://t.co/dRnQabnQFE,0,2," Abstract: The PC algorithm allows investigators to estimate a complete partially directed acyclic graph (CPDAG) from a finite dataset, but few groups have investigated strategies for estimating and controlling the false discovery rate (FDR) of the edges in the CPDAG. In this paper, we introduce PC with p-values (PC-p), a fast algorithm which robustly computes edge-specific p-values and then estimates and controls the FDR across the edges. PC-p specifically uses the p-values returned by many conditional independence tests to upper bound the p-values of more complex edge-specific hypothesis tests. The algorithm then estimates and controls the FDR using the bounded p-values and the Benjamini-Yekutieli FDR procedure. Modifications to the original PC algorithm also help PC-p accurately compute the upper bounds despite non-zero Type II error rates. Experiments show that PC-p yields more accurate FDR estimation and control across the edges in a variety of CPDAGs compared to alternative methods. "
753750917007216640,2016-07-15 00:39:57,https://t.co/btlSwOZXH5,Dynamic Question Ordering in Online Surveys. (arXiv:1607.04209v1 [stat.OT]) https://t.co/btlSwOZXH5,0,1," Abstract: Online surveys have the potential to support adaptive questions, where later questions depend on earlier responses. Past work has taken a rule-based approach, uniformly across all respondents. We envision a richer interpretation of adaptive questions, which we call dynamic question ordering (DQO), where question order is personalized. Such an approach could increase engagement, and therefore response rate, as well as imputation quality. We present a DQO framework to improve survey completion and imputation. In the general survey-taking setting, we want to maximize survey completion, and so we focus on ordering questions to engage the respondent and collect hopefully all information, or at least the information that most characterizes the respondent, for accurate imputations. In another scenario, our goal is to provide a personalized prediction. Since it is possible to give reasonable predictions with only a subset of questions, we are not concerned with motivating users to answer all questions. Instead, we want to order questions to get information that reduces prediction uncertainty, while not being too burdensome. We illustrate this framework with an example of providing energy estimates to prospective tenants. We also discuss DQO for national surveys and consider connections between our statistics-based question-ordering approach and cognitive survey methodology. "
753750914092130304,2016-07-15 00:39:56,https://t.co/CqQuzKo06O,Fifty Shades of Ratings: How to Benefit from a Negative Feedback in Top-N Recommendations Tasks. (arXiv:1607.04228… https://t.co/CqQuzKo06O,1,3," Abstract: Conventional collaborative filtering techniques treat a top-n recommendations problem as a task of generating a list of the most relevant items. This formulation, however, disregards an opposite - avoiding recommendations with completely irrelevant items. Due to that bias, standard algorithms, as well as commonly used evaluation metrics, become insensitive to negative feedback. In order to resolve this problem we propose to treat user feedback as a categorical variable and model it with users and items in a ternary way. We employ a third-order tensor factorization technique and implement a higher order folding-in method to support online recommendations. The method is equally sensitive to entire spectrum of user ratings and is able to accurately predict relevant items even from a negative only feedback. Our method may partially eliminate the need for complicated rating elicitation process as it provides means for personalized recommendations from the very beginning of an interaction with a recommender system. We also propose a modification of standard metrics which helps to reveal unwanted biases and account for sensitivity to a negative feedback. Our model achieves state-of-the-art quality in standard recommendation tasks while significantly outperforming other methods in the cold-start ""no-positive-feedback"" scenarios. "
753750910946406400,2016-07-15 00:39:55,https://t.co/BQvssZghjm,Optimally-Weighted Herding is Bayesian Quadrature. (arXiv:1408.2049v2 [cs.LG] UPDATED) https://t.co/BQvssZghjm,0,3, Abstract: Herding and kernel herding are deterministic methods of choosing samples which summarise a probability distribution. A related task is choosing samples for estimating integrals using Bayesian quadrature. We show that the criterion minimised when selecting samples in kernel herding is equivalent to the posterior variance in Bayesian quadrature. We then show that sequential Bayesian quadrature can be viewed as a weighted version of kernel herding which achieves performance superior to any other weighted herding method. We demonstrate empirically a rate of convergence faster than O(1/N). Our results also imply an upper bound on the empirical error of the Bayesian quadrature estimate. 
753750908039794688,2016-07-15 00:39:55,https://t.co/OzcTtDtZFB,Causality on Cross-Sectional Data: Stable Specification Search in Constrained Structural Equation Modeling. (arXiv… https://t.co/OzcTtDtZFB,0,3," Abstract: Causal modeling has long been an attractive topic for many researchers and in recent decades there has seen a surge in theoretical development and discovery algorithms. Generally discovery algorithms can be divided into two approaches: constraint-based and score-based. The constraint-based approach is able to detect common causes of the observed variables but the use of independence tests makes it less reliable. The score-based approach produces a result that is easier to interpret as it also measures the reliability of the inferred causal relationships, but it is unable to detect common confounders of the observed variables. A drawback of both score-based and constrained-based approaches is the inherent instability in structure estimation. With finite samples small changes in the data can lead to completely different optimal structures. The present work introduces a new hypothesis-free score-based causal discovery algorithm, called stable specification search, that is robust for finite samples based on recent advances in stability selection using subsampling and selection algorithms. Structure search is performed over Structural Equation Models. Our approach uses exploratory search but allows incorporation of prior background knowledge. We validated our approach on one simulated data set, which we compare to the known ground truth, and two real-world data sets for Chronic Fatigue Syndrome and Attention Deficit Hyperactivity Disorder, which we compare to earlier medical studies. The results on the simulated data set show significant improvement over alternative approaches and the results on the real-word data sets show consistency with the hypothesis driven models constructed by medical experts. "
753750901773467648,2016-07-15 00:39:53,https://t.co/etrv3n9oAm,Automatic Generation of Probabilistic Programming from Time Series Data. (arXiv:1607.00710v2 [stat.ML] UPDATED) https://t.co/etrv3n9oAm,1,4," Abstract: Probabilistic programming languages represent complex data with intermingled models in a few lines of code. Efficient inference algorithms in probabilistic programming languages make possible to build unified frameworks to compute interesting probabilities of various large, real-world problems. When the structure of model is given, constructing a probabilistic program is rather straightforward. Thus, main focus have been to learn the best model parameters and compute marginal probabilities. In this paper, we provide a new perspective to build expressive probabilistic program from continue time series data when the structure of model is not given. The intuition behind of our method is to find a descriptive covariance structure of time series data in nonparametric Gaussian process regression. We report that such descriptive covariance structure efficiently derives a probabilistic programming description accurately. "
753391192700772352,2016-07-14 00:50:32,https://t.co/1rj4Benuk1,Natural brain-information interfaces: Recommending information by relevance inferred from human brain signals. (ar… https://t.co/1rj4Benuk1,4,6," Abstract: Finding relevant information from large document collections such as the World Wide Web is a common task in our daily lives. Estimation of a user's interest or search intention is necessary to recommend and retrieve relevant information from these collections. We introduce a brain-information interface used for recommending information by relevance inferred directly from brain signals. In experiments, participants were asked to read Wikipedia documents about a selection of topics while their EEG was recorded. Based on the prediction of word relevance, the individual's search intent was modeled and successfully used for retrieving new, relevant documents from the whole English Wikipedia corpus. The results show that the users' interests towards digital content can be modeled from the brain signals evoked by reading. The introduced brain-relevance paradigm enables the recommendation of information without any explicit user interaction, and may be applied across diverse information-intensive applications. "
753391188644888576,2016-07-14 00:50:31,https://t.co/NtPeo9999P,Fast Sampling for Strongly Rayleigh Measures with Application to Determinantal Point Processes. (arXiv:1607.03559v… https://t.co/NtPeo9999P,0,2," Abstract: In this note we consider sampling from (non-homogeneous) strongly Rayleigh probability measures. As an important corollary, we obtain a fast mixing Markov Chain sampler for Determinantal Point Processes. "
753391182915534848,2016-07-14 00:50:29,https://t.co/VfcPi9Z2t6,Effects of Additional Data on Bayesian Clustering. (arXiv:1607.03574v1 [stat.ML]) https://t.co/VfcPi9Z2t6,0,8," Abstract: Hierarchical probabilistic models, such as mixture models, are used for cluster analysis. These models have two types of variables: observable and latent. In cluster analysis, the latent variable is estimated, and it is expected that additional information will improve the accuracy of the estimation of the latent variable. Many proposed learning methods are able to use additional data; these include semi-supervised learning and transfer learning. However, from a statistical point of view, a complex probabilistic model that encompasses both the initial and additional data might be less accurate due to having a higher-dimensional parameter. The present paper presents a theoretical analysis of the accuracy of such a model and clarifies which factor has the greatest effect on its accuracy, the advantages of obtaining additional data, and the disadvantages of increasing the complexity. "
753391179723579393,2016-07-14 00:50:29,https://t.co/ZusbJopw3q,Multiple-Instance Logistic Regression with LASSO Penalty. (arXiv:1607.03615v1 [stat.ML]) https://t.co/ZusbJopw3q,0,4," Abstract: In this work, we consider a manufactory process which can be described by a multiple-instance logistic regression model. In order to compute the maximum likelihood estimation of the unknown coefficient, an expectation-maximization algorithm is proposed, and the proposed modeling approach can be extended to identify the important covariates by adding the coefficient penalty term into the likelihood function. In addition to essential technical details, we demonstrate the usefulness of the proposed method by simulations and real examples. "
753391176246501377,2016-07-14 00:50:28,https://t.co/RklQZXGmB8,Learning Shallow Detection Cascades for Wearable Sensor-Based Mobile Health Applications. (arXiv:1607.03730v1 [sta… https://t.co/RklQZXGmB8,0,2," Abstract: The field of mobile health aims to leverage recent advances in wearable on-body sensing technology and smart phone computing capabilities to develop systems that can monitor health states and deliver just-in-time adaptive interventions. However, existing work has largely focused on analyzing collected data in the off-line setting. In this paper, we propose a novel approach to learning shallow detection cascades developed explicitly for use in a real-time wearable-phone or wearable-phone-cloud systems. We apply our approach to the problem of cigarette smoking detection from a combination of wrist-worn actigraphy data and respiration chest band data using two and three stage cascades. "
753391172563963904,2016-07-14 00:50:27,https://t.co/A6171d898p,Kernel Density Estimation for Dynamical Systems. (arXiv:1607.03792v1 [stat.ML]) https://t.co/A6171d898p,0,7," Abstract: We study the density estimation problem with observations generated by certain dynamical systems that admit a unique underlying invariant Lebesgue density. Observations drawn from dynamical systems are not independent and moreover, usual mixing concepts may not be appropriate for measuring the dependence among these observations. By employing the $\mathcal{C}$-mixing concept to measure the dependence, we conduct statistical analysis on the consistency and convergence of the kernel density estimator. Our main results are as follows: First, we show that with properly chosen bandwidth, the kernel density estimator is universally consistent under $L_1$-norm; Second, we establish convergence rates for the estimator with respect to several classes of dynamical systems under $L_1$-norm. In the analysis, the density function $f$ is only assumed to be H\""{o}lder continuous which is a weak assumption in the literature of nonparametric density estimation and also more realistic in the dynamical system context. Last but not least, we prove that the same convergence rates of the estimator under $L_\infty$-norm and $L_1$-norm can be achieved when the density function is H\""{o}lder continuous, compactly supported and bounded. The bandwidth selection problem of the kernel density estimator for dynamical system is also discussed in our study via numerical simulations. "
753391168017367040,2016-07-14 00:50:26,https://t.co/ckCKQztBHE,Homotopy Smoothing for Non-Smooth Problems with Lower Complexity than $O(1/\epsilon)$. (arXiv:1607.03815v1 [math.O… https://t.co/ckCKQztBHE,1,5," Abstract: In this paper, we develop a novel {\bf ho}moto{\bf p}y {\bf s}moothing (HOPS) algorithm for solving a family of non-smooth problems that is composed of a non-smooth term with an explicit max-structure and a smooth term or a simple non-smooth term whose proximal mapping is easy to compute. The best known iteration complexity for solving such non-smooth optimization problems is $O(1/\epsilon)$ without any assumption on the strong convexity. In this work, we will show that the proposed HOPS achieved a lower iteration complexity of $\widetilde O(1/\epsilon^{1-\theta})$\footnote{$\widetilde O()$ suppresses a logarithmic factor.} with $\theta\in(0,1]$ capturing the local sharpness of the objective function around the optimal solutions. To the best of our knowledge, this is the lowest iteration complexity achieved so far for the considered non-smooth optimization problems without strong convexity assumption. The HOPS algorithm employs Nesterov's smoothing technique and Nesterov's accelerated gradient method and runs in stages, which gradually decreases the smoothing parameter in a stage-wise manner until it yields a sufficiently good approximation of the original function. We show that HOPS enjoys a linear convergence for many well-known non-smooth problems (e.g., empirical risk minimization with a piece-wise linear loss function and $\ell_1$ norm regularizer, finding a point in a polyhedron, cone programming, etc). Experimental results verify the effectiveness of HOPS in comparison with Nesterov's smoothing algorithm and the primal-dual style of first-order methods. "
753391164028514304,2016-07-14 00:50:25,https://t.co/7UQ20uMuPs,Feature Extraction and Automated Classification of Heartbeats by Machine Learning. (arXiv:1607.03822v1 [stat.ML]) https://t.co/7UQ20uMuPs,1,10," Abstract: We present algorithms for the detection of a class of heart arrhythmias with the goal of eventual adoption by practicing cardiologists. In clinical practice, detection is based on a small number of meaningful features extracted from the heartbeat cycle. However, techniques proposed in the literature use high dimensional vectors consisting of morphological, and time based features for detection. Using electrocardiogram (ECG) signals, we found smaller subsets of features sufficient to detect arrhythmias with high accuracy. The features were found by an iterative step-wise feature selection method. We depart from common literature in the following aspects: 1. As opposed to a high dimensional feature vectors, we use a small set of features with meaningful clinical interpretation, 2. we eliminate the necessity of short-duration patient-specific ECG data to append to the global training data for classification 3. We apply semi-parametric classification procedures (in an ensemble framework) for arrhythmia detection, and 4. our approach is based on a reduced sampling rate of ~ 115 Hz as opposed to 360 Hz in standard literature. "
753391158693330945,2016-07-14 00:50:24,https://t.co/iq2X4OJpNR,Safe Policy Improvement by Minimizing Robust Baseline Regret. (arXiv:1607.03842v1 [stat.ML]) https://t.co/iq2X4OJpNR,0,5," Abstract: An important problem in sequential decision-making under uncertainty is to use limited data to compute a safe policy, i.e., a policy that is guaranteed to perform at least as well as a given baseline strategy. In this paper, we develop and analyze a new model-based approach to compute a safe policy when we have access to an inaccurate dynamics model of the system with known accuracy guarantees. Our proposed robust method uses this (inaccurate) model to directly minimize the (negative) regret w.r.t. the baseline policy. Contrary to the existing approaches, minimizing the regret allows one to improve the baseline policy in states with accurate dynamics and seamlessly fall back to the baseline policy, otherwise. We show that our formulation is NP-hard and propose an approximate algorithm. Our empirical results on several domains show that even this relatively simple approximate algorithm can significantly outperform standard approaches. "
753391154582921216,2016-07-14 00:50:23,https://t.co/f8CUjfqrmU,Fitting a Simplicial Complex using a Variation of $k$-means. (arXiv:1607.03849v1 [cs.LG]) https://t.co/f8CUjfqrmU,0,4," Abstract: We give a simple and effective two stage algorithm for approximating a point cloud $\mathcal{S}\subset\mathbb{R}^m$ by a simplicial complex $K$. The first stage is an iterative fitting procedure that generalizes k-means clustering, while the second stage involves deleting redundant simplices. A form of dimension reduction of $\mathcal{S}$ is obtained as a consequence. "
753391149684056064,2016-07-14 00:50:22,https://t.co/5axY5whyxj,Sequential Design for Ranking Response Surfaces. (arXiv:1509.00980v2 [stat.ML] UPDATED) https://t.co/5axY5whyxj,0,3," Abstract: We propose and analyze sequential design methods for the problem of ranking several response surfaces. Namely, given $L \ge 2$ response surfaces over a continuous input space $\cal X$, the aim is to efficiently find the index of the minimal response across the entire $\cal X$. The response surfaces are not known and have to be noisily sampled one-at-a-time. This setting is motivated by stochastic control applications and requires joint experimental design both in space and response-index dimensions. To generate sequential design heuristics we investigate stepwise uncertainty reduction approaches, as well as sampling based on posterior classification complexity. We also make connections between our continuous-input formulation and the discrete framework of pure regret in multi-armed bandits. To model the response surfaces we utilize kriging surrogates. Several numerical examples using both synthetic data and an epidemics control problem are provided to illustrate our approach and the efficacy of respective adaptive designs. "
753391145594617856,2016-07-14 00:50:21,https://t.co/xIT1KSXCVK,UBL: an R package for Utility-based Learning. (arXiv:1604.08079v2 [cs.MS] UPDATED) https://t.co/xIT1KSXCVK,0,5," Abstract: This document describes the R package UBL that allows the use of several methods for handling utility-based learning problems. Classification and regression problems that assume non-uniform costs and/or benefits pose serious challenges to predictive analytic tasks. In the context of meteorology, finance, medicine, ecology, among many other, specific domain information concerning the preference bias of the users must be taken into account to enhance the models predictive performance. To deal with this problem, a large number of techniques was proposed by the research community for both classification and regression tasks. The main goal of UBL package is to facilitate the utility-based predictive analytic task by providing a set of methods to deal with this type of problems in the R environment. It is a versatile tool that provides mechanisms to handle both regression and classification (binary and multiclass) tasks. Moreover, UBL package allows the user to specify his domain preferences, but it also provides some automatic methods that try to infer those preference bias from the domain, considering some common known settings. "
753391140267823104,2016-07-14 00:50:19,https://t.co/NGZIGOHlsz,The Quality of the Covariance Selection Through Detection Problem and AUC Bounds. (arXiv:1605.05776v2 [cs.IT] UPDA… https://t.co/NGZIGOHlsz,0,4," Abstract: We consider the problem of quantifying the quality of a model selection problem for a graphical model. We discuss this by formulating the problem as a detection problem. Model selection problems usually minimize a distance between the original distribution and the model distribution. For the special case of Gaussian distributions, the model selection problem simplifies to the covariance selection problem which is widely discussed in literature by Dempster [2] where the likelihood criterion is maximized or equivalently the Kullback-Leibler (KL) divergence is minimized to compute the model covariance matrix. While this solution is optimal for Gaussian distributions in the sense of the KL divergence, it is not optimal when compared with other information divergences and criteria such as Area Under the Curve (AUC). In this paper, we analytically compute upper and lower bounds for the AUC and discuss the quality of model selection problem using the AUC and its bounds as an accuracy measure in detection problem. We define the correlation approximation matrix (CAM) and show that analytical computation of the KL divergence, the AUC and its bounds only depend on the eigenvalues of CAM. We also show the relationship between the AUC, the KL divergence and the ROC curve by optimizing with respect to the ROC curve. In the examples provided, we pick tree structures as the simplest graphical models. We perform simulations on fully-connected graphs and compute the tree structured models by applying the widely used Chow-Liu algorithm [3]. Examples show that the quality of tree approximation models are not good in general based on information divergences, the AUC and its bounds when the number of nodes in the graphical model is large. We show both analytically and by simulations that the 1-AUC for the tree approximation model decays exponentially as the dimension of graphical model increases. "
753391135847026688,2016-07-14 00:50:18,https://t.co/JnvUtHqD9h,Asymptotically exact conditional inference in deep generative models and differentiable simulators. (arXiv:1605.07… https://t.co/JnvUtHqD9h,2,8, Abstract: Many generative models can be expressed as a differentiable function of random inputs drawn from some simple probability density. This framework includes both deep generative architectures such as Variational Autoencoders and a large class of 'likelihood-free' simulator models. We present a method for performing efficient MCMC inference in such models when conditioning on observations of the model output. For some models this offers an asymptotically exact inference method where Approximate Bayesian Computation might otherwise be employed. We use the intuition that inference corresponds to integrating a density across the manifold corresponding to the set of inputs consistent with the observed outputs. This motivates the use of a constrained variant of Hamiltonian Monte Carlo which leverages the smooth geometry of the manifold to coherently move between inputs exactly consistent with observations. We validate the method by performing inference tasks in a diverse set of models. 
753391123897384960,2016-07-14 00:50:15,https://t.co/EzkIXcmFGL,Statistical Inference for Algorithmic Leveraging. (arXiv:1606.01473v2 [stat.AP] UPDATED) https://t.co/EzkIXcmFGL,0,4," Abstract: The age of big data has produced data sets that are computationally expensive to analyze. To deal with such large-scale data sets, the method of algorithmic leveraging proposes that we sample according to some special distribution, rescale the data, and then perform analysis on the smaller sample. Ma, Mahoney, and Yu (2015) provides a framework to determine the statistical properties of algorithmic leveraging in the context of estimating the regression coefficients in a linear model with a fixed number of predictors. In this paper, we discuss how to perform statistical inference on regression coefficients estimated using algorithmic leveraging. In particular, we show how to construct confidence intervals for each estimated coefficient and present an efficient algorithm for doing so when the error variance is known. Through simulations, we confirm that our procedure controls the type I errors of significance tests for the regression coefficients and show that it has good power for those tests. "
753025785947123716,2016-07-13 00:38:32,https://t.co/dcTmdtLCAs,How to calculate partition functions using convex programming hierarchies: provable bounds for variational methods… https://t.co/dcTmdtLCAs,3,8," Abstract: We consider the problem of approximating partition functions for Ising models. We make use of recent tools in combinatorial optimization: the Sherali-Adams and Lasserre convex programming hierarchies, in combination with variational methods to get algorithms for calculating partition functions in these families. These techniques give new, non-trivial approximation guarantees for the partition function beyond the regime of correlation decay. They also generalize some classical results from statistical physics about the Curie-Weiss ferromagnetic Ising model, as well as provide a partition function counterpart of classical results about max-cut on dense graphs \cite{arora1995polynomial}. With this, we connect techniques from two apparently disparate research areas -- optimization and counting/partition function approximations. (i.e. \#-P type of problems). Furthermore, we design to the best of our knowledge the first provable, convex variational methods. Though in the literature there are a host of convex versions of variational methods \cite{wainwright2003tree, wainwright2005new, heskes2006convexity, meshi2009convexifying}, they come with no guarantees (apart from some extremely special cases, like e.g. the graph has a single cycle \cite{weiss2000correctness}). We consider dense and low threshold rank graphs, and interestingly, the reason our approach works on these types of graphs is because local correlations propagate to global correlations -- completely the opposite of algorithms based on correlation decay. In the process we design novel entropy approximations based on the low-order moments of a distribution. Our proof techniques are very simple and generic, and likely to be applicable to many other settings other than Ising models. "
753025782402969601,2016-07-13 00:38:31,https://t.co/5OGKl73s3O,On Deterministic Conditions for Subspace Clustering under Missing Data. (arXiv:1607.03191v1 [cs.IT]) https://t.co/5OGKl73s3O,0,3," Abstract: In this paper we present deterministic conditions for success of sparse subspace clustering (SSC) under missing data, when data is assumed to come from a Union of Subspaces (UoS) model. We consider two algorithms, which are variants of SSC with entry-wise zero-filling that differ in terms of the optimization problems used to find affinity matrix for spectral clustering. For both the algorithms, we provide deterministic conditions for any pattern of missing data such that perfect clustering can be achieved. We provide extensive sets of simulation results for clustering as well as completion of data at missing entries, under the UoS model. Our experimental results indicate that in contrast to the full data case, accurate clustering does not imply accurate subspace identification and completion, indicating the natural order of relative hardness of these problems. "
753025778686787585,2016-07-13 00:38:30,https://t.co/x2rxqwayqg,Qualitative Judgement of Research Impact: Domain Taxonomy as a Fundamental Framework for Judgement of the Quality … https://t.co/x2rxqwayqg,0,3," Abstract: The appeal of metric evaluation of research impact has attracted considerable interest in recent times. Although the public at large and administrative bodies are much interested in the idea, scientists and other researchers are much more cautious, insisting that metrics are but an auxiliary instrument to the qualitative peer-based judgement. The goal of this article is to propose availing of such a well positioned construct as domain taxonomy as a tool for directly assessing the scope and quality of research. We first show how taxonomies can be used to analyse the scope and perspectives of a set of research projects or papers. Then we proceed to define a research team or researcher's rank by those nodes in the hierarchy that have been created or significantly transformed by the results of the researcher. An experimental test of the approach in the data analysis domain is described. Although the concept of taxonomy seems rather simplistic to describe all the richness of a research domain, its changes and use can be made transparent and subject to open discussions. "
753025774660313088,2016-07-13 00:38:29,https://t.co/wnnmx5IbjH,Rapid Prediction of Player Retention in Free-to-Play Mobile Games. (arXiv:1607.03202v1 [stat.ML]) https://t.co/wnnmx5IbjH,0,3," Abstract: Predicting and improving player retention is crucial to the success of mobile Free-to-Play games. This paper explores the problem of rapid retention prediction in this context. Heuristic modeling approaches are introduced as a way of building simple rules for predicting short-term retention. Compared to common classification algorithms, our heuristic-based approach achieves reasonable and comparable performance using information from the first session, day, and week of player activity. "
753025769656479745,2016-07-13 00:38:28,https://t.co/qXoUXkZ1im,Information Projection and Approximate Inference for Structured Sparse Variables. (arXiv:1607.03204v1 [stat.ML]) https://t.co/qXoUXkZ1im,1,8," Abstract: Approximate inference via information projection has been recently introduced as a general-purpose approach for efficient probabilistic inference given sparse variables. This manuscript goes beyond classical sparsity by proposing efficient algorithms for approximate inference via information projection that are applicable to any structure on the set of variables that admits enumeration using a \emph{matroid}. We show that the resulting information projection can be reduced to combinatorial submodular optimization subject to matroid constraints. Further, leveraging recent advances in submodular optimization, we provide an efficient greedy algorithm with strong optimization-theoretic guarantees. The class of probabilistic models that can be expressed in this way is quite broad and, as we show, includes group sparse regression, group sparse principal components analysis and sparse canonical correlation analysis, among others. Moreover, empirical results on simulated data and high dimensional neuroimaging data highlight the superior performance of the information projection approach as compared to established baselines for a range of probabilistic models. "
753025765348872192,2016-07-13 00:38:27,https://t.co/fWdSBLs8N1,From Dependence to Causation. (arXiv:1607.03300v1 [stat.ML]) https://t.co/fWdSBLs8N1,2,6," Abstract: Machine learning is the science of discovering statistical dependencies in data, and the use of those dependencies to perform predictions. During the last decade, machine learning has made spectacular progress, surpassing human performance in complex tasks such as object recognition, car driving, and computer gaming. However, the central role of prediction in machine learning avoids progress towards general-purpose artificial intelligence. As one way forward, we argue that causal inference is a fundamental component of human intelligence, yet ignored by learning algorithms. Causal inference is the problem of uncovering the cause-effect relationships between the variables of a data generating system. Causal structures provide understanding about how these systems behave under changing, unseen environments. In turn, knowledge about these causal dynamics allows to answer ""what if"" questions, describing the potential responses of the system under hypothetical manipulations and interventions. Thus, understanding cause and effect is one step from machine learning towards machine reasoning and machine intelligence. But, currently available causal inference algorithms operate in specific regimes, and rely on assumptions that are difficult to verify in practice. This thesis advances the art of causal inference in three different ways. First, we develop a framework for the study of statistical dependence based on copulas and random features. Second, we build on this framework to interpret the problem of causal inference as the task of distribution classification, yielding a family of novel causal inference algorithms. Third, we discover causal structures in convolutional neural network features using our algorithms. The algorithms presented in this thesis are scalable, exhibit strong theoretical guarantees, and achieve state-of-the-art performance in a variety of real-world benchmarks. "
753025761322405888,2016-07-13 00:38:26,https://t.co/pS44tMwoEN,Predicting the evolution of stationary graph signals. (arXiv:1607.03313v1 [stat.ML]) https://t.co/pS44tMwoEN,0,6," Abstract: An emerging way of tackling the dimensionality issues arising in the modeling of a multivariate process is to assume that the inherent data structure can be captured by a graph. Nevertheless, though state-of-the-art graph-based methods have been successful for many learning tasks, they do not consider time-evolving signals and thus are not suitable for prediction. Based on the recently introduced joint stationarity framework for time-vertex processes, this letter considers multivariate models that exploit the graph topology so as to facilitate the prediction. The resulting method yields similar accuracy to the joint (time-graph) mean-squared error estimator but at lower complexity, and outperforms purely time-based methods. "
753025757279035392,2016-07-13 00:38:25,https://t.co/nW7rv7BusZ,Approximate maximum entropy principles via Goemans-Williamson with applications to provable variational methods. (… https://t.co/nW7rv7BusZ,3,5," Abstract: The well known maximum-entropy principle due to Jaynes, which states that given mean parameters, the maximum entropy distribution matching them is in an exponential family, has been very popular in machine learning due to its ""Occam's razor"" interpretation. Unfortunately, calculating the potentials in the maximum-entropy distribution is intractable \cite{bresler2014hardness}. We provide computationally efficient versions of this principle when the mean parameters are pairwise moments: we design distributions that approximately match given pairwise moments, while having entropy which is comparable to the maximum entropy distribution matching those moments. We additionally provide surprising applications of the approximate maximum entropy principle to designing provable variational methods for partition function calculations for Ising models without any assumptions on the potentials of the model. More precisely, we show that in every temperature, we can get approximation guarantees for the log-partition function comparable to those in the low-temperature limit, which is the setting of optimization of quadratic forms over the hypercube. \cite{alon2006approximating} "
753025753374162944,2016-07-13 00:38:24,https://t.co/swC5rkKUTe,Statistical power and measurement bias in multisite resting-state fMRI connectivity. (arXiv:1607.03392v1 [q-bio.QM… https://t.co/swC5rkKUTe,1,4," Abstract: Connectivity studies using resting-state functional magnetic resonance imaging are increasingly pooling data acquired at multiple sites. While this may allow investigators to speed up recruitment or increase sample size, multisite studies also potentially introduce systematic biases in connectivity measures across sites. In this work, we measure the inter-site bias in connectivity and its impact on our ability to detect individual and group differences. Our study was based on real multisite fMRI datasets collected in N=345 young, healthy subjects across 8 scanning sites with 3T scanners and heterogeneous scanning protocols. We first empirically show that typical functional networks were reliably found at the group level in all sites, and that the amplitude of the inter-site bias was small to moderate, with a Cohen's effect size below 0.5 on average across brain connections. We then implemented a series of Monte-Carlo simulations, based on real data, to evaluate the impact of the multisite bias on detection power in statistical tests comparing two groups (with and without the effect) using a general linear model, as well as on the prediction of group labels with a support-vector machine. As a reference, we also implemented the same simulations with fMRI data collected at a single site using an identical sample size. Simulations revealed that using data from heterogeneous sites only slightly decreased our ability to detect changes compared to a monosite study with the GLM, and had a more serious impact on prediction accuracy. However, the deleterious effect of multisite data pooling tended to decrease as the total sample size increased, to a point where differences between monosite and multisite simulations were small with N=120 subjects. Taken together, our results support the feasibility of multisite studies in rs-fMRI provided sample size is large enough. "
753025748815020033,2016-07-13 00:38:23,https://t.co/aeuVhFANRD,Learning in Quantum Control: High-Dimensional Global Optimization for Noisy Quantum Dynamics. (arXiv:1607.03428v1 … https://t.co/aeuVhFANRD,2,5," Abstract: Quantum control is valuable for various quantum technologies such as high-fidelity gates for universal quantum computing, adaptive quantum-enhanced metrology, and ultra-cold atom manipulation. Although supervised machine learning and reinforcement learning are widely used for optimizing control parameters in classical systems, quantum control for parameter optimization is mainly pursued via gradient-based greedy algorithms. Although the quantum fitness landscape is often compatible with greedy algorithms, sometimes greedy algorithms yield poor results, especially for large-dimensional quantum systems. We employ differential evolution algorithms to circumvent the stagnation problem of non-convex optimization. We improve quantum control fidelity for noisy system by averaging over the objective function. To reduce computational cost, we introduce heuristics for early termination of runs and for adaptive selection of search subspaces. Our implementation is massively parallel and vectorized to reduce run time even further. We demonstrate our methods with two examples, namely quantum phase estimation and quantum gate design, for which we achieve superior fidelity and scalability than obtained using greedy algorithms. "
753025745044312064,2016-07-13 00:38:22,https://t.co/2GxwbjRfnr,Incomplete Pivoted QR-based Dimensionality Reduction. (arXiv:1607.03456v1 [cs.LG]) https://t.co/2GxwbjRfnr,0,3," Abstract: High-dimensional big data appears in many research fields such as image recognition, biology and collaborative filtering. Often, the exploration of such data by classic algorithms is encountered with difficulties due to `curse of dimensionality' phenomenon. Therefore, dimensionality reduction methods are applied to the data prior to its analysis. Many of these methods are based on principal components analysis, which is statistically driven, namely they map the data into a low-dimension subspace that preserves significant statistical properties of the high-dimensional data. As a consequence, such methods do not directly address the geometry of the data, reflected by the mutual distances between multidimensional data point. Thus, operations such as classification, anomaly detection or other machine learning tasks may be affected. This work provides a dictionary-based framework for geometrically driven data analysis that includes dimensionality reduction, out-of-sample extension and anomaly detection. It embeds high-dimensional data in a low-dimensional subspace. This embedding preserves the original high-dimensional geometry of the data up to a user-defined distortion rate. In addition, it identifies a subset of landmark data points that constitute a dictionary for the analyzed dataset. The dictionary enables to have a natural extension of the low-dimensional embedding to out-of-sample data points, which gives rise to a distortion-based criterion for anomaly detection. The suggested method is demonstrated on synthetic and real-world datasets and achieves good results for classification, anomaly detection and out-of-sample tasks. "
753025741521125377,2016-07-13 00:38:21,https://t.co/kdMC59SYrx,Even Faster SVD Decomposition Yet Without Agonizing Pain. (arXiv:1607.03463v1 [math.OC]) https://t.co/kdMC59SYrx,2,4," Abstract: We study k-SVD that is to obtain the first k singular vectors of a matrix $A$ approximately. Recently, a few breakthroughs have been discovered on k-SVD: Musco and Musco [1] provided the first gap-free theorem for the block Krylov method, Shamir [2] discovered the first variance-reduction stochastic method, and Bhojanapalli et al. [3] provided the fastest $O(\mathsf{nnz}(A) + \mathsf{poly}(1/\varepsilon))$-type of algorithm using alternating minimization. In this paper, put forward a new framework for SVD and improve the above breakthroughs. We obtain faster gap-free convergence rate outperforming [1], we obtain the first accelerated AND stochastic method outperforming [2]. In the $O(\mathsf{nnz}(A) + \mathsf{poly}(1/\varepsilon))$ running-time regime, we outperform [3] in certain parameter regimes without even using alternating minimization. "
753025737683243012,2016-07-13 00:38:21,https://t.co/fgw3TOfUV9,Nystrom Method for Approximating the GMM Kernel. (arXiv:1607.03475v1 [stat.ML]) https://t.co/fgw3TOfUV9,0,2," Abstract: The GMM (generalized min-max) kernel was recently proposed (Li, 2016) as a measure of data similarity and was demonstrated effective in machine learning tasks. In order to use the GMM kernel for large-scale datasets, the prior work resorted to the (generalized) consistent weighted sampling (GCWS) to convert the GMM kernel to linear kernel. We call this approach as ``GMM-GCWS''. In the machine learning literature, there is a popular algorithm which we call ``RBF-RFF''. That is, one can use the ``random Fourier features'' (RFF) to convert the ``radial basis function'' (RBF) kernel to linear kernel. It was empirically shown in (Li, 2016) that RBF-RFF typically requires substantially more samples than GMM-GCWS in order to achieve comparable accuracies. The Nystrom method is a general tool for computing nonlinear kernels, which again converts nonlinear kernels into linear kernels. We apply the Nystrom method for approximating the GMM kernel, a strategy which we name as ``GMM-NYS''. In this study, our extensive experiments on a set of fairly large datasets confirm that GMM-NYS is also a strong competitor of RBF-RFF. "
753025732851425280,2016-07-13 00:38:19,https://t.co/4RR1CW9ksP,Local identifiability of $l_1$-minimization dictionary learning: a sufficient and almost necessary condition. (arX… https://t.co/4RR1CW9ksP,0,2," Abstract: We study the theoretical properties of learning a dictionary from $N$ signals $\mathbf x_i\in \mathbb R^K$ for $i=1,...,N$ via $l_1$-minimization. We assume that $\mathbf x_i$'s are $i.i.d.$ random linear combinations of the $K$ columns from a complete (i.e., square and invertible) reference dictionary $\mathbf D_0 \in \mathbb R^{K\times K}$. Here, the random linear coefficients are generated from either the $s$-sparse Gaussian model or the Bernoulli-Gaussian model. First, for the population case, we establish a sufficient and almost necessary condition for the reference dictionary $\mathbf D_0$ to be locally identifiable, i.e., a local minimum of the expected $l_1$-norm objective function. Our condition covers both sparse and dense cases of the random linear coefficients and significantly improves the sufficient condition by Gribonval and Schnass (2010). In addition, we show that for a complete $\mu$-coherent reference dictionary, i.e., a dictionary with absolute pairwise column inner-product at most $\mu\in[0,1)$, local identifiability holds even when the random linear coefficient vector has up to $O(\mu^{-2})$ nonzeros on average. Moreover, our local identifiability results also translate to the finite sample case with high probability provided that the number of signals $N$ scales as $O(K\log K)$. "
753025728359370753,2016-07-13 00:38:18,https://t.co/bs9G2tQuEa,The Impact of Estimation: A New Method for Clustering and Trajectory Estimation in Patient Flow Modeling. (arXiv:1… https://t.co/bs9G2tQuEa,0,3," Abstract: The ability to accurately forecast and control inpatient census, and thereby workloads, is a critical and longstanding problem in hospital management. Majority of current literature focuses on optimal scheduling of inpatients, but largely ignores the process of accurate estimation of the trajectory of patients throughout the treatment and recovery process. The result is that current scheduling models are optimizing based on inaccurate input data. We developed a Clustering and Scheduling Integrated (CSI) approach to capture patient flows through a network of hospital services. CSI functions by clustering patients into groups based on similarity of trajectory using a novel Semi-Markov model (SMM)-based clustering scheme proposed in this paper, as opposed to clustering by admit type or condition as in previous literature. The methodology is validated by simulation and then applied to real patient data from a partner hospital where we see it outperforms current methods. Further, we demonstrate that extant optimization methods achieve significantly better results on key hospital performance measures under CSI, compared with traditional estimation approaches, increasing elective admissions by 97% and utilization by 22% compared to 30% and 8% using traditional estimation techniques. From a theoretical standpoint, the SMM-clustering is a novel approach applicable to any temporal-spatial stochastic data that is prevalent in many industries and application areas. "
753025725343666176,2016-07-13 00:38:18,https://t.co/gNKSrDa53n,Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural Networks. (arXiv:1606.04393v2 [cs.CV] UPDATED) https://t.co/gNKSrDa53n,2,10," Abstract: Taking inspiration from biological evolution, we explore the idea of ""Can deep neural networks evolve naturally over successive generations into highly efficient deep neural networks?"" by introducing the notion of synthesizing new highly efficient, yet powerful deep neural networks over successive generations via an evolutionary process from ancestor deep neural networks. The architectural traits of ancestor deep neural networks are encoded using synaptic probability models, which can be viewed as the `DNA' of these networks. New descendant networks with differing network architectures are synthesized based on these synaptic probability models from the ancestor networks and computational environmental factor models, in a random manner to mimic heredity, natural selection, and random mutation. These offspring networks are then trained into fully functional networks, like one would train a newborn, and have more efficient, more diverse network architectures than their ancestor networks, while achieving powerful modeling capabilities. Experimental results for the task of visual saliency demonstrated that the synthesized `evolved' offspring networks can achieve state-of-the-art performance while having network architectures that are significantly more efficient (with a staggering $\sim$48-fold decrease in synapses by the fourth generation) compared to the original ancestor network. "
753025713586962433,2016-07-13 00:38:15,https://t.co/EN3ASABncX,"European Union regulations on algorithmic decision-making and a ""right to explanation"". (arXiv:1606.08813v2 [stat.… https://t.co/EN3ASABncX",2,7," Abstract: We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which ""significantly affect"" users. The law will also effectively create a ""right to explanation,"" whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation. "
752664909066928128,2016-07-12 00:44:32,https://t.co/uJmYSPmGpb,Pseudo-Marginal Hamiltonian Monte Carlo. (arXiv:1607.02516v1 [stat.ME]) https://t.co/uJmYSPmGpb,5,11," Abstract: Bayesian inference in the presence of an intractable likelihood function is computationally challenging. When following a Markov chain Monte Carlo (MCMC) approach to approximate the posterior distribution in this context, one typically either uses MCMC schemes which target the joint posterior of the parameters and some auxiliary latent variables or pseudo-marginal Metropolis-Hastings (MH) schemes which mimic a MH algorithm targeting the marginal posterior of the parameters by approximating unbiasedly the intractable likelihood. In scenarios where the parameters and auxiliary variables are strongly correlated under the posterior and/or this posterior is multimodal, Gibbs sampling or Hamiltonian Monte Carlo (HMC) will perform poorly and the pseudo-marginal MH algorithm, as any other MH scheme, will be inefficient for high dimensional parameters. We propose here an original MCMC algorithm, termed pseudo-marginal HMC, which approximates the HMC algorithm targeting the marginal posterior of the parameters. We demonstrate through experiments that pseudo-marginal HMC can outperform significantly both standard HMC and pseudo-marginal MH schemes. "
752664906340638720,2016-07-12 00:44:32,https://t.co/jy1ecSlrJr,Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016). (arXiv:1607.02531v… https://t.co/jy1ecSlrJr,15,19," Abstract: This is the Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016. Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang, and Hanna Wallach. "
752664904612638721,2016-07-12 00:44:31,https://t.co/hNpaTpzcns,Adversarial examples in the physical world. (arXiv:1607.02533v1 [cs.CV]) https://t.co/hNpaTpzcns,3,8," Abstract: Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera. "
752664902649647104,2016-07-12 00:44:31,https://t.co/0Zuceuo0tI,Beating level-set methods for 3D seismic data interpolation: a primal-dual alternating approach. (arXiv:1607.02624… https://t.co/0Zuceuo0tI,0,4," Abstract: Acquisition cost is a crucial bottleneck for seismic workflows, and low-rank formulations for data interpolation allow practitioners to `fill in' data volumes from critically subsampled data acquired in the field. Tremendous size of seismic data volumes required for seismic processing remains a major challenge for these techniques. We propose a new approach to solve residual constrained formulations for interpolation. We represent the data volume using matrix factors, and build a block-coordinate algorithm with constrained convex subproblems that are solved with a primal-dual splitting scheme. The new approach is competitive with state of the art level-set algorithms that interchange the role of objectives with constraints. We use the new algorithm to successfully interpolate a large scale 5D seismic data volume, generated from the geologically complex synthetic 3D Compass velocity model, where 80% of the data has been removed. "
752664900934180864,2016-07-12 00:44:30,https://t.co/Kj4WPWv7kI,Combining multiple resolutions into hierarchical representations for kernel-based image classification. (arXiv:160… https://t.co/Kj4WPWv7kI,0,6," Abstract: Geographic object-based image analysis (GEOBIA) framework has gained increasing interest recently. Following this popular paradigm, we propose a novel multiscale classification approach operating on a hierarchical image representation built from two images at different resolutions. They capture the same scene with different sensors and are naturally fused together through the hierarchical representation, where coarser levels are built from a Low Spatial Resolution (LSR) or Medium Spatial Resolution (MSR) image while finer levels are generated from a High Spatial Resolution (HSR) or Very High Spatial Resolution (VHSR) image. Such a representation allows one to benefit from the context information thanks to the coarser levels, and subregions spatial arrangement information thanks to the finer levels. Two dedicated structured kernels are then used to perform machine learning directly on the constructed hierarchical representation. This strategy overcomes the limits of conventional GEOBIA classification procedures that can handle only one or very few pre-selected scales. Experiments run on an urban classification task show that the proposed approach can highly improve the classification accuracy w.r.t. conventional approaches working on a single scale. "
752664899004817409,2016-07-12 00:44:30,https://t.co/EKZkzrXAcN,Classifier Risk Estimation under Limited Labeling Resources. (arXiv:1607.02665v1 [cs.LG]) https://t.co/EKZkzrXAcN,0,2," Abstract: In this paper we propose strategies for estimating performance of a classifier when labels cannot be obtained for the whole test set. The number of test instances which can be labeled is very small compared to the whole test data size. The goal then is to obtain a precise estimate of classifier performance using as little labeling resource as possible. Specifically, we try to answer, how to select a subset of the large test set for labeling such that the performance of a classifier estimated on this subset is as close as possible to the one on the whole test set. We propose strategies based on stratified sampling for selecting this subset. We show that these strategies can reduce the variance in estimation of classifier accuracy by a significant amount compared to simple random sampling (over 65% in several cases). Hence, our proposed methods are much more precise compared to random sampling for accuracy estimation under restricted labeling resources. The reduction in number of samples required (compared to random sampling) to estimate the classifier accuracy with only 1% error is high as 60% in some cases. "
752664896664469504,2016-07-12 00:44:29,https://t.co/btEfKZMPpF,Sparse additive Gaussian process with soft interactions. (arXiv:1607.02670v1 [stat.ML]) https://t.co/btEfKZMPpF,2,5," Abstract: Additive nonparametric regression models provide an attractive tool for variable selection in high dimensions when the relationship between the response and predictors is complex. They offer greater flexibility compared to parametric non-linear regression models and better interpretability and scalability than the non-parametric regression models. However, achieving sparsity simultaneously in the number of nonparametric components as well as in the variables within each nonparametric component poses a stiff computational challenge. In this article, we develop a novel Bayesian additive regression model using a combination of hard and soft shrinkages to separately control the number of additive components and the variables within each component. An efficient algorithm is developed to select the importance variables and estimate the interaction network. Excellent performance is obtained in simulated and real data examples. "
752664894462386181,2016-07-12 00:44:29,https://t.co/itdhCEBMOQ,Convex Relaxation for Community Detection with Covariates. (arXiv:1607.02675v1 [stat.ME]) https://t.co/itdhCEBMOQ,4,6," Abstract: Community detection in networks is an important problem in many applied areas. In this paper, we investigate this in the presence of node covariates. Recently, an emerging body of theoretical work has been focused on leveraging information from both the edges in the network and the node covariates to infer community memberships. However, so far the role of the network and that of the covariates have not been examined closely. In essence, in most parameter regimes, one of the sources of information provides enough information to infer the hidden cluster labels, thereby making the other source redundant. To our knowledge, this is the first work which shows that when the network and the covariates carry ""orthogonal"" pieces of information about the cluster memberships, one can get asymptotically consistent clustering by using them both, while each of them fails individually. "
752664891824152576,2016-07-12 00:44:28,https://t.co/0owHc0YAo8,Bayesian quantile additive regression trees. (arXiv:1607.02676v1 [stat.ML]) https://t.co/0owHc0YAo8,0,2," Abstract: Ensemble of regression trees have become popular statistical tools for the estimation of conditional mean given a set of predictors. However, quantile regression trees and their ensembles have not yet garnered much attention despite the increasing popularity of the linear quantile regression model. This work proposes a Bayesian quantile additive regression trees model that shows very good predictive performance illustrated using simulation studies and real data applications. Further extension to tackle binary classification problems is also considered. "
752664889450192896,2016-07-12 00:44:28,https://t.co/Wfvm9u5DzJ,Magnetic Hamiltonian Monte Carlo. (arXiv:1607.02738v1 [stat.ML]) https://t.co/Wfvm9u5DzJ,0,8," Abstract: Hamiltonian Monte Carlo (HMC) exploits Hamiltonian dynamics to construct efficient proposals for Markov chain Monte Carlo (MCMC). In this paper, we present a generalization of HMC which exploits \textit{non-canonical} Hamiltonian dynamics. We refer to this algorithm as magnetic HMC, since in 3 dimensions a subset of the dynamics map onto the mechanics of a charged particle coupled to a magnetic field. We establish a theoretical basis for the use of non-canonical Hamiltonian dynamics in MCMC, and construct a symplectic, leapfrog-like integrator allowing for the implementation of magnetic HMC. Finally, we exhibit several examples where these non-canonical dynamics can lead to improved mixing of magnetic HMC relative to ordinary HMC. "
752664887357280258,2016-07-12 00:44:27,https://t.co/2wC74LyJDw,An Improved Convergence Analysis of Cyclic Block Coordinate Descent-type Methods for Strongly Convex Minimization.… https://t.co/2wC74LyJDw,0,4," Abstract: The cyclic block coordinate descent-type (CBCD-type) methods have shown remarkable computational performance for solving strongly convex minimization problems. Typical applications include many popular statistical machine learning methods such as elastic-net regression, ridge penalized logistic regression, and sparse additive regression. Existing optimization literature has shown that the CBCD-type methods attain iteration complexity of $O(p\cdot\log(1/\epsilon))$, where $\epsilon$ is a pre-specified accuracy of the objective value, and $p$ is the number of blocks. However, such iteration complexity explicitly depends on $p$, and therefore is at least $p$ times worse than those of gradient descent methods. To bridge this theoretical gap, we propose an improved convergence analysis for the CBCD-type methods. In particular, we first show that for a family of quadratic minimization problems, the iteration complexity of the CBCD-type methods matches that of the gradient descent methods in term of dependency on $p$ (up to a $\log^2 p$ factor). Thus our complexity bounds are sharper than the existing bounds by at least a factor of $p/\log^2p$. We also provide a lower bound to confirm that our improved complexity bounds are tight (up to a $\log^2 p$ factor) if the largest and smallest eigenvalues of the Hessian matrix do not scale with $p$. Finally, we generalize our analysis to other strongly convex minimization problems beyond quadratic ones "
752664885218213888,2016-07-12 00:44:27,https://t.co/g9ePzD49yS,Bounds on the Number of Measurements for Reliable Compressive Classification. (arXiv:1607.02801v1 [cs.IT]) https://t.co/g9ePzD49yS,2,4," Abstract: This paper studies the classification of high-dimensional Gaussian signals from low-dimensional noisy, linear measurements. In particular, it provides upper bounds (sufficient conditions) on the number of measurements required to drive the probability of misclassification to zero in the low-noise regime, both for random measurements and designed ones. Such bounds reveal two important operational regimes that are a function of the characteristics of the source: i) when the number of classes is less than or equal to the dimension of the space spanned by signals in each class, reliable classification is possible in the low-noise regime by using a one-vs-all measurement design; ii) when the dimension of the spaces spanned by signals in each class is lower than the number of classes, reliable classification is guaranteed in the low-noise regime by using a simple random measurement design. Simulation results both with synthetic and real data show that our analysis is sharp, in the sense that it is able to gauge the number of measurements required to drive the misclassification probability to zero in the low-noise regime. "
752664883427237888,2016-07-12 00:44:26,https://t.co/EvVR5LPZTe,Mapping distributional to model-theoretic semantic spaces: a baseline. (arXiv:1607.02802v1 [cs.CL]) https://t.co/EvVR5LPZTe,0,2," Abstract: Word embeddings have been shown to be useful across state-of-the-art systems in many natural language processing tasks, ranging from question answering systems to dependency parsing. (Herbelot and Vecchi, 2015) explored word embeddings and their utility for modeling language semantics. In particular, they presented an approach to automatically map a standard distributional semantic space onto a set-theoretic model using partial least squares regression. We show in this paper that a simple baseline achieves a +51% relative improvement compared to their model on one of the two datasets they used, and yields competitive results on the second dataset. "
752664881443241986,2016-07-12 00:44:26,https://t.co/39NeJUIXNv,Minimum Description Length Principle in Supervised Learning with Application to Lasso. (arXiv:1607.02914v1 [cs.IT]) https://t.co/39NeJUIXNv,0,3," Abstract: The minimum description length (MDL) principle in supervised learning is studied. One of the most important theories for the MDL principle is Barron and Cover's theory (BC theory), which gives a mathematical justification of the MDL principle. The original BC theory, however, can be applied to supervised learning only approximately and limitedly. Though Barron et al. recently succeeded in removing a similar approximation in case of unsupervised learning, their idea cannot be essentially applied to supervised learning in general. To overcome this issue, an extension of BC theory to supervised learning is proposed. The derived risk bound has several advantages inherited from the original BC theory. First, the risk bound holds for finite sample size. Second, it requires remarkably few assumptions. Third, the risk bound has a form of redundancy of the two-stage code for the MDL procedure. Hence, the proposed extension gives a mathematical justification of the MDL principle to supervised learning like the original BC theory. As an important example of application, new risk and (probabilistic) regret bounds of lasso with random design are derived. The derived risk bound holds for any finite sample size $n$ and feature number $p$ even if $n\ll p$ without boundedness of features in contrast to the past work. Behavior of the regret bound is investigated by numerical simulations. We believe that this is the first extension of BC theory to general supervised learning with random design without approximation. "
752664878993866753,2016-07-12 00:44:25,https://t.co/xfXKPbuAlC,From Behavior to Sparse Graphical Games: Efficient Recovery of Equilibria. (arXiv:1607.02959v1 [cs.GT]) https://t.co/xfXKPbuAlC,0,3," Abstract: In this paper we study the problem of exact recovery of the pure-strategy Nash equilibria (PSNE) set of a graphical game from noisy observations of joint actions of the players alone. We consider sparse linear influence games --- a parametric class of graphical games with linear payoffs, and represented by directed graphs of n nodes (players) and in-degree of at most k. We present an $\ell_1$-regularized logistic regression based algorithm for recovering the PSNE set exactly, that is both computationally efficient --- i.e. runs in polynomial time --- and statistically efficient --- i.e. has logarithmic sample complexity. Specifically, we show that the sufficient number of samples required for exact PSNE recovery scales as $\mathcal{O}(\mathrm{poly}(k) \log n)$. We also validate our theoretical results using synthetic experiments. "
752664876414304256,2016-07-12 00:44:25,https://t.co/HuwHy7MkoE,Retrospective Causal Inference with Machine Learning Ensembles: An Application to Anti-Recidivism Policies in Colo… https://t.co/HuwHy7MkoE,2,4," Abstract: We present new methods to estimate causal effects retrospectively from micro data with the assistance of a machine learning ensemble. This approach overcomes two important limitations in conventional methods like regression modeling or matching: (i) ambiguity about the pertinent retrospective counterfactuals and (ii) potential misspecification, overfitting, and otherwise bias-prone or inefficient use of a large identifying covariate set in the estimation of causal effects. Our method targets the analysis toward a well defined ``retrospective intervention effect'' (RIE) based on hypothetical population interventions and applies a machine learning ensemble that allows data to guide us, in a controlled fashion, on how to use a large identifying covariate set. We illustrate with an analysis of policy options for reducing ex-combatant recidivism in Colombia. "
752664873402761217,2016-07-12 00:44:24,https://t.co/TL52bFEHjw,Learning a metric for class-conditional KNN. (arXiv:1607.03050v1 [cs.LG]) https://t.co/TL52bFEHjw,0,2," Abstract: Naive Bayes Nearest Neighbour (NBNN) is a simple and effective framework which addresses many of the pitfalls of K-Nearest Neighbour (KNN) classification. It has yielded competitive results on several computer vision benchmarks. Its central tenet is that during NN search, a query is not compared to every example in a database, ignoring class information. Instead, NN searches are performed within each class, generating a score per class. A key problem with NN techniques, including NBNN, is that they fail when the data representation does not capture perceptual (e.g.~class-based) similarity. NBNN circumvents this by using independent engineered descriptors (e.g.~SIFT). To extend its applicability outside of image-based domains, we propose to learn a metric which captures perceptual similarity. Similar to how Neighbourhood Components Analysis optimizes a differentiable form of KNN classification, we propose ""Class Conditional"" metric learning (CCML), which optimizes a soft form of the NBNN selection rule. Typical metric learning algorithms learn either a global or local metric. However, our proposed method can be adjusted to a particular level of locality by tuning a single parameter. An empirical evaluation on classification and retrieval tasks demonstrates that our proposed method clearly outperforms existing learned distance metrics across a variety of image and non-image datasets. "
752664871527997441,2016-07-12 00:44:23,https://t.co/AwSDOfwB9K,Proximal Quasi-Newton Methods for Convex Optimization. (arXiv:1607.03081v1 [cs.NA]) https://t.co/AwSDOfwB9K,0,3," Abstract: In [19], a general, inexact, efficient proximal quasi-Newton algorithm for composite optimization problems has been proposed and a sublinear global convergence rate has been established. In this paper, we analyze the convergence properties of this method, both in the exact and inexact setting, in the case when the objective function is strongly convex. We also investigate a practical variant of this method by establishing a simple stopping criterion for the subproblem optimization. Furthermore, we consider an accelerated variant, based on FISTA [1], to the proximal quasi-Newton algorithm. A similar accelerated method has been considered in [7], where the convergence rate analysis relies on very strong impractical assumptions. We present a modified analysis while relaxing these assumptions and perform a practical comparison of the accelerated proximal quasi- Newton algorithm and the regular one. Our analysis and computational results show that acceleration may not bring any benefit in the quasi-Newton setting. "
752664869921587200,2016-07-12 00:44:23,https://t.co/9ASQH6PXOv,Kernel-based methods for bandit convex optimization. (arXiv:1607.03084v1 [cs.LG]) https://t.co/9ASQH6PXOv,2,5," Abstract: We consider the adversarial convex bandit problem and we build the first $\mathrm{poly}(T)$-time algorithm with $\mathrm{poly}(n) \sqrt{T}$-regret for this problem. To do so we introduce three new ideas in the derivative-free optimization literature: (i) kernel methods, (ii) a generalization of Bernoulli convolutions, and (iii) a new annealing schedule for exponential weights (with increasing learning rate). The basic version of our algorithm achieves $\tilde{O}(n^{9.5} \sqrt{T})$-regret, and we show that a simple variant of this algorithm can be run in $\mathrm{poly}(n \log(T))$-time per step at the cost of an additional $\mathrm{poly}(n) T^{o(1)}$ factor in the regret. These results improve upon the $\tilde{O}(n^{11} \sqrt{T})$-regret and $\exp(\mathrm{poly}(T))$-time result of the first two authors, and the $\log(T)^{\mathrm{poly}(n)} \sqrt{T}$-regret and $\log(T)^{\mathrm{poly}(n)}$-time result of Hazan and Li. Furthermore we conjecture that another variant of the algorithm could achieve $\tilde{O}(n^{1.5} \sqrt{T})$-regret, and moreover that this regret is unimprovable (the current best lower bound being $\Omega(n \sqrt{T})$ and it is achieved with linear functions). For the simpler situation of zeroth order stochastic convex optimization this corresponds to the conjecture that the optimal query complexity is of order $n^3 / \epsilon^2$. "
752664868264738816,2016-07-12 00:44:23,https://t.co/bOVNVIH1Ou,Avoiding pathologies in very deep networks. (arXiv:1402.5836v3 [stat.ML] UPDATED) https://t.co/bOVNVIH1Ou,2,8," Abstract: Choosing appropriate architectures and regularization strategies for deep networks is crucial to good predictive performance. To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions. Specifically, we study the deep Gaussian process, a type of infinitely-wide, deep neural network. We show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, retaining only a single degree of freedom in the limit. We propose an alternate network architecture which does not suffer from this pathology. We also examine deep covariance functions, obtained by composing infinitely many feature transforms. Lastly, we characterize the class of models obtained by performing dropout on Gaussian processes. "
752664866570264580,2016-07-12 00:44:22,https://t.co/Xhf2TzQml2,"An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives. (arXiv:1506.… https://t.co/Xhf2TzQml2",1,5," Abstract: We consider a contextual version of multi-armed bandit problem with global knapsack constraints. In each round, the outcome of pulling an arm is a scalar reward and a resource consumption vector, both dependent on the context, and the global knapsack constraints require the total consumption for each resource to be below some pre-fixed budget. The learning agent competes with an arbitrary set of context-dependent policies. This problem was introduced by Badanidiyuru et al. (2014), who gave a computationally inefficient algorithm with near-optimal regret bounds for it. We give a computationally efficient algorithm for this problem with slightly better regret bounds, by generalizing the approach of Agarwal et al. (2014) for the non-constrained version of the problem. The computational time of our algorithm scales logarithmically in the size of the policy space. This answers the main open question of Badanidiyuru et al. (2014). We also extend our results to a variant where there are no knapsack constraints but the objective is an arbitrary Lipschitz concave function of the sum of outcome vectors. "
752664865051926530,2016-07-12 00:44:22,https://t.co/2KXeao0fPp,Persistence Images: A Stable Vector Representation of Persistent Homology. (arXiv:1507.06217v3 [cs.CG] UPDATED) https://t.co/2KXeao0fPp,0,5," Abstract: Many datasets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a dataset. A useful representation of this homological information is a persistence diagram (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finite-dimensional vector representation which we call a persistence image (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The discriminatory power of PIs is compared against existing methods, showing significant performance gains. We explore the use of PIs with vector-based machine learning tools, such as linear sparse support vector machines, which identify features containing discriminating topological information. Finally, high accuracy inference of parameter values from the dynamic output of a discrete dynamical system (the linked twist map) and a partial differential equation (the anisotropic Kuramoto-Sivashinsky equation) provide a novel application of the discriminatory power of PIs. "
752664863613353985,2016-07-12 00:44:21,https://t.co/JTLrMq2SY3,Linear Contextual Bandits with Knapsacks. (arXiv:1507.06738v2 [cs.LG] UPDATED) https://t.co/JTLrMq2SY3,0,3," Abstract: We consider the linear contextual bandit problem with resource consumption, in addition to reward generation. In each round, the outcome of pulling an arm is a reward as well as a vector of resource consumptions. The expected values of these outcomes depend linearly on the context of that arm. The budget/capacity constraints require that the total consumption doesn't exceed the budget for each resource. The objective is once again to maximize the total reward. This problem turns out to be a common generalization of classic linear contextual bandits (linContextual), bandits with knapsacks (BwK), and the online stochastic packing problem (OSPP). We present algorithms with near-optimal regret bounds for this problem. Our bounds compare favorably to results on the unstructured version of the problem where the relation between the contexts and the outcomes could be arbitrary, but the algorithm only competes against a fixed set of policies accessible through an optimization oracle. We combine techniques from the work on linContextual, BwK, and OSPP in a nontrivial manner while also tackling new difficulties that are not present in any of these special cases. "
752664861767766016,2016-07-12 00:44:21,https://t.co/K6asaK3Oxo,MultiView Diffusion Maps. (arXiv:1508.05550v5 [cs.LG] UPDATED) https://t.co/K6asaK3Oxo,0,4," Abstract: In this study we consider learning a reduced dimensionality representation from datasets obtained under multiple views. Such multiple views of datasets can be obtained, for example, when the same underlying process is observed using several different modalities, or measured with different instrumentation. Our goal is to effectively exploit the availability of such multiple views for various purposes, such as non-linear embedding, manifold learning, spectral clustering, anomaly detection and non-linear system identification. Our proposed method exploits the intrinsic relation within each view, as well as the mutual relations between views. We do this by defining a cross-view model, in which an implied Random Walk process between objects is restrained to hop between the different views. Our method is robust to scaling of each dataset, and is insensitive to small structural changes in the data. Within this framework, we define new diffusion distances and analyze the spectra of the implied kernels. We demonstrate the applicability of the proposed approach on both artificial and real data sets. "
752664859360235521,2016-07-12 00:44:20,https://t.co/QRt8QsYb7d,Stationary signal processing on graphs. (arXiv:1601.02522v4 [cs.DS] UPDATED) https://t.co/QRt8QsYb7d,0,3," Abstract: Graphs are a central tool in machine learning and information processing as they allow to conveniently capture the structure of complex datasets. In this context, it is of high importance to develop flexible models of signals defined over graphs or networks. In this paper, we generalize the traditional concept of wide sense stationarity to signals defined over the vertices of arbitrary weighted undirected graphs. We show that stationarity is intimately linked to statistical invariance under a localization operator reminiscent of translation. We prove that stationary graph signals are characterized by a well-defined Power Spectral Density that can be efficiently estimated even for large graphs. We leverage this new concept to derive Wiener-type estimation procedures of noisy and partially observed signals and illustrate the performance of this new model for denoising and regression. "
752664857137278979,2016-07-12 00:44:20,https://t.co/baeQzSs7WD,Collaborative Learning of Stochastic Bandits over a Social Network. (arXiv:1602.08886v2 [cs.LG] UPDATED) https://t.co/baeQzSs7WD,0,6," Abstract: We consider a collaborative online learning paradigm, wherein a group of agents connected through a social network are engaged in playing a stochastic multi-armed bandit game. Each time an agent takes an action, the corresponding reward is instantaneously observed by the agent, as well as its neighbours in the social network. We perform a regret analysis of various policies in this collaborative learning setting. A key finding of this paper is that natural extensions of widely-studied single agent learning policies to the network setting need not perform well in terms of regret. In particular, we identify a class of non-altruistic and individually consistent policies, and argue by deriving regret lower bounds that they are liable to suffer a large regret in the networked setting. We also show that the learning performance can be substantially improved if the agents exploit the structure of the network, and develop a simple learning algorithm based on dominating sets of the network. Specifically, we first consider a star network, which is a common motif in hierarchical social networks, and show analytically that the hub agent can be used as an information sink to expedite learning and improve the overall regret. We also derive networkwide regret bounds for the algorithm applied to general networks. We conduct numerical experiments on a variety of networks to corroborate our analytical results. "
752664854121635840,2016-07-12 00:44:19,https://t.co/vIq5o9hga1,A First Order Free Lunch for SQRT-Lasso. (arXiv:1605.07950v2 [cs.LG] UPDATED) https://t.co/vIq5o9hga1,0,3," Abstract: Many statistical machine learning techniques sacrifice convenient computational structures to gain estimation robustness and modeling flexibility. In this paper, we study this fundamental tradeoff through a SQRT-Lasso problem for sparse linear regression and sparse precision matrix estimation in high dimensions. We explain how novel optimization techniques help address these computational challenges. Particularly, we propose a pathwise iterative smoothing shrinkage thresholding algorithm for solving the SQRT-Lasso optimization problem. We further provide a novel model-based perspective for analyzing the smoothing optimization framework, which allows us to establish a nearly linear convergence (R-linear convergence) guarantee for our proposed algorithm. This implies that solving the SQRT-Lasso optimization is almost as easy as solving the Lasso optimization. Moreover, we show that our proposed algorithm can also be applied to sparse precision matrix estimation, and enjoys good computational properties. Numerical experiments are provided to support our theory. "
752302145089724416,2016-07-11 00:43:03,https://t.co/i1ZFIt8sig,Single-Channel Multi-Speaker Separation using Deep Clustering. (arXiv:1607.02173v1 [cs.LG]) https://t.co/i1ZFIt8sig,1,8," Abstract: Deep clustering is a recently introduced deep learning architecture that uses discriminatively trained embeddings as the basis for clustering. It was recently applied to spectrogram segmentation, resulting in impressive results on speaker-independent multi-speaker separation. In this paper we extend the baseline system with an end-to-end signal approximation objective that greatly improves performance on a challenging speech separation. We first significantly improve upon the baseline system performance by incorporating better regularization, larger temporal context, and a deeper architecture, culminating in an overall improvement in signal to distortion ratio (SDR) of 10.3 dB compared to the baseline of 6.0 dB for two-speaker separation, as well as a 7.1 dB SDR improvement for three-speaker separation. We then extend the model to incorporate an enhancement layer to refine the signal estimates, and perform end-to-end training through both the clustering and enhancement stages to maximize signal fidelity. We evaluate the results using automatic speech recognition. The new signal approximation objective, combined with end-to-end training, produces unprecedented performance, reducing the word error rate (WER) from 89.1% down to 30.8%. This represents a major advancement towards solving the cocktail party problem. "
752302143378456576,2016-07-11 00:43:02,https://t.co/uI9K7tMLES,Whole-brain substitute CT generation using Markov random field mixture models. (arXiv:1607.02188v1 [stat.AP]) https://t.co/uI9K7tMLES,0,3," Abstract: Computed tomography (CT) equivalent information is needed for attenuation correction in PET imaging and for dose planning in radiotherapy. Prior work has shown that Gaussian mixture models can be used to generate a substitute CT (s-CT) image from a specific set of MRI modalities. This work introduces a more flexible class of mixture models for s-CT generation, that incorporates spatial dependency in the data through a Markov random field prior on the latent field of class memberships associated with a mixture model. Furthermore, the mixture distributions are extended from Gaussian to normal inverse Gaussian (NIG), allowing heavier tails and skewness. The amount of data needed to train a model for s-CT generation is of the order of 100 million voxels. The computational efficiency of the parameter estimation and prediction methods are hence paramount, especially when spatial dependency is included in the models. A stochastic Expectation Maximization (EM) gradient algorithm is proposed in order to tackle this challenge. The advantages of the spatial model and NIG distributions are evaluated with a cross-validation study based on data from 14 patients. The study show that the proposed model enhances the predictive quality of the s-CT images by reducing the mean absolute error with 17.9%. Also, the distribution of CT values conditioned on the MR images are better explained by the proposed model as evaluated using continuous ranked probability scores. "
752302141897838592,2016-07-11 00:43:02,https://t.co/6ftP3i3mL5,Convergence rates of Kernel Conjugate Gradient for random design regression. (arXiv:1607.02387v1 [math.ST]) https://t.co/6ftP3i3mL5,0,6," Abstract: We prove statistical rates of convergence for kernel-based least squares regression from i.i.d. data using a conjugate gradient algorithm, where regularization against overfitting is obtained by early stopping. This method is related to Kernel Partial Least Squares, a regression method that combines supervised dimensionality reduction with least squares projection. Following the setting introduced in earlier related literature, we study so-called ""fast convergence rates"" depending on the regularity of the target regression function (measured by a source condition in terms of the kernel integral operator) and on the effective dimensionality of the data mapped into the kernel space. We obtain upper bounds, essentially matching known minimax lower bounds, for the $\mathcal{L}^2$ (prediction) norm as well as for the stronger Hilbert norm, if the true regression function belongs to the reproducing kernel Hilbert space. If the latter assumption is not fulfilled, we obtain similar convergence rates for appropriate norms, provided additional unlabeled data are available. "
752302139632979968,2016-07-11 00:43:01,https://t.co/MwENovjjvz,Lower Bounds on Active Learning for Graphical Model Selection. (arXiv:1607.02413v1 [cs.IT]) https://t.co/MwENovjjvz,0,4," Abstract: We consider the problem of estimating the underlying graph associated with a Markov random field, with the added twist that the decoding algorithm can iteratively choose which subsets of nodes to sample based on the previous samples, i.e., active learning. Considering both Ising and Gaussian models, we provide algorithm-independent lower bounds for high-probability recovery within the class of degree-bounded graphs. Our main results are minimax lower bounds for the active setting that match the best known lower bounds for the passive setting, which are known to be tight in several cases. Our analysis is based on a novel variation of Fano's inequality for the active learning setting. While we consider graph ensembles that are similar or identical to those considered for the passive setting, we require different techniques to analyze them, with the key challenge being bounding a mutual information quantity associated with observed subsets of the nodes, as opposed to full observations. "
752302136562778112,2016-07-11 00:43:01,https://t.co/4eNRR2HBFo,Optimal rates of Statistical Seriation. (arXiv:1607.02435v1 [math.ST]) https://t.co/4eNRR2HBFo,0,6," Abstract: Given a matrix the seriation problem consists in permuting its rows in such way that all its columns have the same shape, for example, they are monotone increasing. We propose a statistical approach to this problem where the matrix of interest is observed with noise and study the corresponding minimax rate of estimation of the matrices. Specifically, when the columns are either unimodal or monotone, we show that the least squares estimator is optimal up to logarithmic factors and adapts to matrices with a certain natural structure. Finally, we propose a computationally efficient estimator in the monotonic case and study its performance both theoretically and experimentally. Our work is at the intersection of shape constrained estimation and recent work that involves permutation learning, such as graph denoising and ranking. "
752302133630889984,2016-07-11 00:43:00,https://t.co/UfhBZgoE1d,Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in Social Good Applications. (arXiv:1607.024… https://t.co/UfhBZgoE1d,1,11," Abstract: This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York. "
752302130619441153,2016-07-11 00:42:59,https://t.co/FAQTRRs0Kz,Interpretable Classification Models for Recidivism Prediction. (arXiv:1503.07810v6 [stat.ML] UPDATED) https://t.co/FAQTRRs0Kz,0,5," Abstract: We investigate a long-debated question, which is how to create predictive models of recidivism that are sufficiently accurate, transparent, and interpretable to use for decision-making. This question is complicated as these models are used to support different decisions, from sentencing, to determining release on probation, to allocating preventative social services. Each use case might have an objective other than classification accuracy, such as a desired true positive rate (TPR) or false positive rate (FPR). Each (TPR, FPR) pair is a point on the receiver operator characteristic (ROC) curve. We use popular machine learning methods to create models along the full ROC curve on a wide range of recidivism prediction problems. We show that many methods (SVM, Ridge Regression) produce equally accurate models along the full ROC curve. However, methods that designed for interpretability (CART, C5.0) cannot be tuned to produce models that are accurate and/or interpretable. To handle this shortcoming, we use a new method known as SLIM (Supersparse Linear Integer Models) to produce accurate, transparent, and interpretable models along the full ROC curve. These models can be used for decision-making for many different use cases, since they are just as accurate as the most powerful black-box machine learning models, but completely transparent, and highly interpretable. "
752302127863717890,2016-07-11 00:42:59,https://t.co/ekgVBMw9xS,Bayesian Estimation of Multidimensional Latent Variables and Its Asymptotic Accuracy. (arXiv:1510.01003v2 [stat.ML… https://t.co/ekgVBMw9xS,1,11," Abstract: Hierarchical learning models, such as mixture models and Bayesian networks, are widely employed for unsupervised learning tasks, such as clustering analysis. They consist of observable and hidden variables, which represent the given data and their hidden generation process, respectively. It has been pointed out that conventional statistical analysis is not applicable to these models, because redundancy of the latent variable produces singularities in the parameter space. In recent years, a method based on algebraic geometry has allowed us to analyze the accuracy of predicting observable variables when using Bayesian estimation. However, how to analyze latent variables has not been sufficiently studied, even though one of the main issues in unsupervised learning is to determine how accurately the latent variable is estimated. A previous study proposed a method that can be used when the range of the latent variable is redundant compared with the model generating data. The present paper extends that method to the situation in which the latent variables have redundant dimensions. We formulate new error functions and derive their asymptotic forms. Calculation of the error functions is demonstrated in two-layered Bayesian networks. "
752302125506527232,2016-07-11 00:42:58,https://t.co/llOcY64Qtv,MERLiN: Mixture Effect Recovery in Linear Networks. (arXiv:1512.01255v2 [stat.ME] UPDATED) https://t.co/llOcY64Qtv,0,3," Abstract: Causal inference concerns the identification of cause-effect relationships between variables, e.g. establishing whether a stimulus affects activity in a certain brain region. The observed variables themselves often do not constitute meaningful causal variables, however, and linear combinations need to be considered. In electroencephalographic studies, for example, one is not interested in establishing cause-effect relationships between electrode signals (the observed variables), but rather between cortical signals (the causal variables) which can be recovered as linear combinations of electrode signals. We introduce MERLiN (Mixture Effect Recovery in Linear Networks), a family of causal inference algorithms that implement a novel means of constructing causal variables from non-causal variables. We demonstrate through application to EEG data how the basic MERLiN algorithm can be extended for application to different (neuroimaging) data modalities. Given an observed linear mixture, the algorithms can recover a causal variable that is a linear effect of another given variable. That is, MERLiN allows us to recover a cortical signal that is affected by activity in a certain brain region, while not being a direct effect of the stimulus. The Python/Matlab implementation for all presented algorithms is available on this https URL "
752302122448855041,2016-07-11 00:42:57,https://t.co/SaR6VBF8XX,On the Difficulty of Selecting Ising Models with Approximate Recovery. (arXiv:1602.03647v2 [cs.IT] UPDATED) https://t.co/SaR6VBF8XX,0,5," Abstract: In this paper, we consider the problem of estimating the underlying graph associated with an Ising model given a number of independent and identically distributed samples. We adopt an \emph{approximate recovery} criterion that allows for a number of missed edges or incorrectly-included edges, in contrast with the widely-studied exact recovery problem. Our main results provide information-theoretic lower bounds on the sample complexity for graph classes imposing constraints on the number of edges, maximal degree, and other properties. We identify a broad range of scenarios where, either up to constant factors or logarithmic factors, our lower bounds match the best known lower bounds for the exact recovery criterion, several of which are known to be tight or near-tight. Hence, in these cases, approximate recovery has a similar difficulty to exact recovery in the minimax sense. Our bounds are obtained via a modification of Fano's inequality for handling the approximate recovery criterion, along with suitably-designed ensembles of graphs that can broadly be classed into two categories: (i) Those containing graphs that contain several isolated edges or cliques and are thus difficult to distinguish from the empty graph; (ii) Those containing graphs for which certain groups of nodes are highly correlated, thus making it difficult to determine precisely which edges connect them. We support our theoretical results on these ensembles with numerical experiments. "
752302115515752448,2016-07-11 00:42:56,https://t.co/CYdES70yI6,An Empirical-Bayes Score for Discrete Bayesian Networks. (arXiv:1605.03884v2 [stat.ML] UPDATED) https://t.co/CYdES70yI6,0,4," Abstract: Bayesian network structure learning is often performed in a Bayesian setting, by evaluating candidate structures using their posterior probabilities for a given data set. Score-based algorithms then use those posterior probabilities as an objective function and return the maximum a posteriori network as the learned model. For discrete Bayesian networks, the canonical choice for a posterior score is the Bayesian Dirichlet equivalent uniform (BDeu) marginal likelihood with a uniform (U) graph prior (Heckerman et al., 1995). Its favourable theoretical properties descend from assuming a uniform prior both on the space of the network structures and on the space of the parameters of the network. In this paper, we revisit the limitations of these assumptions; and we introduce an alternative set of assumptions and the resulting score: the Bayesian Dirichlet sparse (BDs) empirical Bayes marginal likelihood with a marginal uniform (MU) graph prior. We evaluate its performance in an extensive simulation study, showing that MU+BDs is more accurate than U+BDeu both in learning the structure of the network and in predicting new observations, while not being computationally more complex to estimate. "
751216267126796288,2016-07-08 00:48:09,https://t.co/fjB6SZlIaL,Nesterov's Accelerated Gradient and Momentum as approximations to Regularised Update Descent. (arXiv:1607.01981v1 … https://t.co/fjB6SZlIaL,1,7," Abstract: We present a unifying framework for adapting the update direction in gradient-based iterative optimization methods. As natural special cases we re-derive classical momentum and Nesterov's accelerated gradient method, lending a new intuitive interpretation to the latter algorithm. We show that a new algorithm, which we term Regularised Gradient Descent, can converge more quickly than either Nesterov's algorithm or the classical momentum algorithm. "
751216265000325121,2016-07-08 00:48:09,https://t.co/HZEUKKLwLV,Kernel Bayesian Inference with Posterior Regularization. (arXiv:1607.02011v1 [stat.ML]) https://t.co/HZEUKKLwLV,2,8," Abstract: We propose a vector-valued regression problem whose solution is equivalent to the reproducing kernel Hilbert space (RKHS) embedding of the Bayesian posterior distribution. This equivalence provides a new understanding of kernel Bayesian inference. Moreover, the optimization problem induces a new regularization for the posterior embedding estimator, which is faster and has comparable performance to the squared regularization in kernel Bayes' rule. This regularization coincides with a former thresholding approach used in kernel POMDPs whose consistency remains to be established. Our theoretical work solves this open problem and provides consistency analysis in regression settings. Based on our optimizational formulation, we propose a flexible Bayesian posterior regularization framework which for the first time enables us to put regularization at the distribution level. We apply this method to nonparametric state-space filtering tasks with extremely nonlinear dynamics and show performance gains over all other baselines. "
751216262722809856,2016-07-08 00:48:08,https://t.co/BqsyQaIjIO,Mini-Batch Spectral Clustering. (arXiv:1607.02024v1 [stat.ML]) https://t.co/BqsyQaIjIO,1,2," Abstract: The cost of computing the spectrum of Laplacian matrices hinders the application of spectral clustering to large data sets. While approximations recover computational tractability, they can potentially affect clustering performance. This paper proposes a practical approach to learn spectral clustering based on adaptive stochastic gradient optimization. Crucially, the proposed approach recovers the exact spectrum of Laplacian matrices in the limit of the iterations, and the cost of each iteration is linear in the number of samples. Extensive experimental validation on data sets with up to half a million samples demonstrate its scalability and its ability to outperform state-of-the-art approximate methods to learn spectral clustering for a given computational budget. "
751216259790954496,2016-07-08 00:48:07,https://t.co/4fiopaqdwx,A characterization of product-form exchangeable feature probability functions. (arXiv:1607.02066v1 [math.PR]) https://t.co/4fiopaqdwx,0,2," Abstract: We characterize the class of exchangeable feature allocations assigning probability $V_{n,k}\prod_{l=1}^{k}W_{m_{l}}U_{n-m_{l}}$ to a feature allocation of $n$ individuals, displaying $k$ features with counts $(m_{1},\ldots,m_{k})$ for these features. Each element of this class is parametrized by a countable matrix $V$ and two sequences $U$ and $W$ of non-negative weights. Moreover, a consistency condition is imposed to guarantee that the distribution for feature allocations of $n-1$ individuals is recovered from that of $n$ individuals, when the last individual is integrated out. In Theorem 1.1, we prove that the only members of this class satisfying the consistency condition are mixtures of the Indian Buffet Process over its mass parameter $\gamma$ and mixtures of the Beta--Bernoulli model over its dimensionality parameter $N$. Hence, we provide a characterization of these two models as the only, up to randomization of the parameters, consistent exchangeable feature allocations having the required product form. "
751216257471578112,2016-07-08 00:48:07,https://t.co/vx3KIt3EFv,A Classification Framework for Partially Observed Dynamical Systems. (arXiv:1607.02085v1 [stat.ML]) https://t.co/vx3KIt3EFv,0,2," Abstract: We present a general framework for classifying partially observed dynamical systems based on the idea of learning in the model space. In contrast to the existing approaches using model point estimates to represent individual data items, we employ posterior distributions over models, thus taking into account in a principled manner the uncertainty due to both the generative (observational and/or dynamic noise) and observation (sampling in time) processes. We evaluate the framework on two testbeds - a biological pathway model and a stochastic double-well system. Crucially, we show that the classifier performance is not impaired when the model class used for inferring posterior distributions is much more simple than the observation-generating model class, provided the reduced complexity inferential model class captures the essential characteristics needed for the given classification task. "
751216255588265988,2016-07-08 00:48:06,https://t.co/JqaygUxBDF,Predicting and Understanding Law-Making with Machine Learning. (arXiv:1607.02109v1 [cs.CL]) https://t.co/JqaygUxBDF,0,3," Abstract: Out of nearly 70,000 bills introduced in the U.S. Congress from 2001 to 2015, only 2,513 were enacted. We developed a machine learning approach to forecasting the probability that any bill will become law. Starting in 2001 with the 107th Congress, we trained models on data from previous Congresses, predicted all bills in the current Congress, and repeated until the 113th Congress served as the test. For prediction we scored each sentence of a bill with a language model that embeds legislative vocabulary into a semantic-laden vector space. This language representation enables our investigation into which words increase the probability of enactment for any topic. To test the relative importance of text and context, we compared the text model to a context-only model that uses variables such as whether the bill's sponsor is in the majority party. To test the effect of changes to bills after their introduction on our ability to predict their final outcome, we compared using the bill text and meta-data available at the time of introduction with using the most recent data. At the time of introduction context-only predictions outperform text-only, and with the newest data text-only outperforms context-only. Combining text and context always performs best. We conducted a global sensitivity analysis on the combined model to determine important factors predicting enactment. "
751216253453434880,2016-07-08 00:48:06,https://t.co/XIfsl8RYh2,Efficient Reinforcement Learning in Deterministic Systems with Value Function Generalization. (arXiv:1307.4847v4 [… https://t.co/XIfsl8RYh2,0,8," Abstract: We consider the problem of reinforcement learning over episodes of a finite-horizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efficient exploration and value function generalization. We establish that when the true value function lies within a given hypothesis class, OCP selects optimal actions over all but at most K episodes, where K is the eluder dimension of the given hypothesis class. We establish further efficiency and asymptotic performance guarantees that apply even if the true value function does not lie in the given hypothesis class, for the special case where the hypothesis class is the span of pre-specified indicator functions over disjoint sets. We also discuss the computational complexity of OCP and present computational results involving two illustrative examples. "
751216249623969792,2016-07-08 00:48:05,https://t.co/vHOGPqD8bh,"Dealing with a large number of classes -- Likelihood, Discrimination or Ranking?. (arXiv:1606.06959v2 [stat.ML] UP… https://t.co/vHOGPqD8bh",0,6," Abstract: We consider training probabilistic classifiers in the case of a large number of classes. The number of classes is assumed too large to perform exact normalisation over all classes. To account for this we consider a simple approach that directly approximates the likelihood. We show that this simple approach works well on toy problems and is competitive with recently introduced alternative non-likelihood based approximations. Furthermore, we relate this approach to a simple ranking objective. This leads us to suggest a specific setting for the optimal threshold in the ranking objective. "
750851766229561350,2016-07-07 00:39:45,https://t.co/QHlLrDA64U,An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality in Machine Learning. (arXiv:1607.01400v1 … https://t.co/QHlLrDA64U,0,3," Abstract: We propose a clustering-based iterative algorithm to solve certain optimization problems in machine learning, where we start the algorithm by aggregating the original data, solving the problem on aggregated data, and then in subsequent steps gradually disaggregate the aggregated data. We apply the algorithm to common machine learning problems such as the least absolute deviation regression problem, support vector machines, and semi-supervised support vector machines. We derive model-specific data aggregation and disaggregation procedures. We also show optimality, convergence, and the optimality gap of the approximated solution in each iteration. A computational study is provided. "
750851763922665472,2016-07-07 00:39:45,https://t.co/23KxH8BRUO,Algorithms for Generalized Cluster-wise Linear Regression. (arXiv:1607.01417v1 [stat.ML]) https://t.co/23KxH8BRUO,1,6," Abstract: Cluster-wise linear regression (CLR), a clustering problem intertwined with regression, is to find clusters of entities such that the overall sum of squared errors from regressions performed over these clusters is minimized, where each cluster may have different variances. We generalize the CLR problem by allowing each entity to have more than one observation, and refer to it as generalized CLR. We propose an exact mathematical programming based approach relying on column generation, a column generation based heuristic algorithm that clusters predefined groups of entities, a metaheuristic genetic algorithm with adapted Lloyd's algorithm for K-means clustering, a two-stage approach, and a modified algorithm of Sp{\""a}th \cite{Spath1979} for solving generalized CLR. We examine the performance of our algorithms on a stock keeping unit (SKU) clustering problem employed in forecasting halo and cannibalization effects in promotions using real-world retail data from a large supermarket chain. In the SKU clustering problem, the retailer needs to cluster SKUs based on their seasonal effects in response to promotions. The seasonal effects are the results of regressions with predictors being promotion mechanisms and seasonal dummies performed over clusters generated. We compare the performance of all proposed algorithms for the SKU problem with real-world and synthetic data. "
750851761703907328,2016-07-07 00:39:44,https://t.co/DRvJyGWJNM,Risk Bounds for High-dimensional Ridge Function Combinations Including Neural Networks. (arXiv:1607.01434v1 [math.… https://t.co/DRvJyGWJNM,0,4," Abstract: Let $ f^{\star} $ be a function on $ \mathbb{R}^d $ satisfying a spectral norm condition. For various noise settings, we show that $ \mathbb{E}\|\hat{f} - f^{\star} \|^2 \leq v_{f^{\star}}\left(\frac{\log d}{n}\right)^{1/4} $, where $ n $ is the sample size and $ \hat{f} $ is either a penalized least squares estimator or a greedily obtained version of such using linear combinations of ramp, sinusoidal, sigmoidal or other bounded Lipschitz ridge functions. Our risk bound is effective even when the dimension $ d $ is much larger than the available sample size. For settings where the dimension is larger than the square root of the sample size this quantity is seen to improve the more familiar risk bound of $ v_{f^{\star}}\left(\frac{d\log (n/d)}{n}\right)^{1/2} $, also investigated here. "
750851759988416512,2016-07-07 00:39:44,https://t.co/wiQ6Kn2bjq,An optimal learning method for developing personalized treatment regimes. (arXiv:1607.01462v1 [stat.ML]) https://t.co/wiQ6Kn2bjq,2,5," Abstract: A treatment regime is a function that maps individual patient information to a recommended treatment, hence explicitly incorporating the heterogeneity in need for treatment across individuals. Patient responses are dichotomous and can be predicted through an unknown relationship that depends on the patient information and the selected treatment. The goal is to find the treatments that lead to the best patient responses on average. Each experiment is expensive, forcing us to learn the most from each experiment. We adopt a Bayesian approach both to incorporate possible prior information and to update our treatment regime continuously as information accrues, with the potential to allow smaller yet more informative trials and for patients to receive better treatment. By formulating the problem as contextual bandits, we introduce a knowledge gradient policy to guide the treatment assignment by maximizing the expected value of information, for which an approximation method is used to overcome computational challenges. We provide a detailed study on how to make sequential medical decisions under uncertainty to reduce health care costs on a real world knee replacement dataset. We use clustering and LASSO to deal with the intrinsic sparsity in health datasets. We show experimentally that even though the problem is sparse, through careful selection of physicians (versus picking them at random), we can significantly improve the success rates. "
750851758386245633,2016-07-07 00:39:44,https://t.co/tZZDN4lhfV,Bayesian nonparametrics for Sparse Dynamic Networks. (arXiv:1607.01624v1 [stat.ML]) https://t.co/tZZDN4lhfV,1,10," Abstract: We propose a Bayesian nonparametric prior for time-varying networks. To each node of the network is associated a positive parameter, modeling the sociability of that node. Sociabilities are assumed to evolve over time, and are modeled via a dynamic point process model. The model is able to (a) capture smooth evolution of the interaction between nodes, allowing edges to appear/disappear over time (b) capture long term evolution of the sociabilities of the nodes (c) and yield sparse graphs, where the number of edges grows subquadratically with the number of nodes. The evolution of the sociabilities is described by a tractable time-varying gamma process. We provide some theoretical insights into the model and apply it to three real world datasets. "
750851756960149504,2016-07-07 00:39:43,https://t.co/w7Zb25oYQT,Tensor Decomposition for Signal Processing and Machine Learning. (arXiv:1607.01668v1 [stat.ML]) https://t.co/w7Zb25oYQT,4,17," Abstract: Tensors or multi-way arrays are functions of three or more indices $(i,j,k,\cdots)$ -- similar to matrices (two-way arrays), which are functions of two indices $(r,c)$ for (row,column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning. "
750851754586177536,2016-07-07 00:39:43,https://t.co/fDejxTOsKT,"Graphons, mergeons, and so on!. (arXiv:1607.01718v1 [stat.ML]) https://t.co/fDejxTOsKT",0,5," Abstract: In this work we develop a theory of hierarchical clustering for graphs. Our modeling assumption is that graphs are sampled from a graphon, which is a powerful and general model for generating graphs and analyzing large networks. Graphons are a far richer class of graph models than stochastic blockmodels, the primary setting for recent progress in the statistical theory of graph clustering. We define what it means for an algorithm to produce the ""correct"" clustering, give sufficient conditions in which a method is statistically consistent, and provide an explicit algorithm satisfying these properties. "
750851752463831045,2016-07-07 00:39:42,https://t.co/zsJqyVEqTH,Measuring dependence powerfully and equitably. (arXiv:1505.02213v3 [stat.ME] UPDATED) https://t.co/zsJqyVEqTH,0,2," Abstract: Given a high-dimensional data set we often wish to find the strongest relationships within it. A common strategy is to evaluate a measure of dependence on every variable pair and retain the highest-scoring pairs for follow-up. This strategy works well if the statistic used is equitable [Reshef et al. 2015a], i.e., if, for some measure of noise, it assigns similar scores to equally noisy relationships regardless of relationship type (e.g., linear, exponential, periodic). In this paper, we introduce and characterize a population measure of dependence called MIC*. We show three ways that MIC* can be viewed: as the population value of MIC, a highly equitable statistic from [Reshef et al. 2011], as a canonical ""smoothing"" of mutual information, and as the supremum of an infinite sequence defined in terms of optimal one-dimensional partitions of the marginals of the joint distribution. Based on this theory, we introduce an efficient approach for computing MIC* from the density of a pair of random variables, and we define a new consistent estimator MICe for MIC* that is efficiently computable. In contrast, there is no known polynomial-time algorithm for computing the original equitable statistic MIC. We show through simulations that MICe has better bias-variance properties than MIC. We then introduce and prove the consistency of a second statistic, TICe, that is a trivial side-product of the computation of MICe and whose goal is powerful independence testing rather than equitability. We show in simulations that MICe and TICe have good equitability and power against independence respectively. The analyses here complement a more in-depth empirical evaluation of several leading measures of dependence [Reshef et al. 2015b] that shows state-of-the-art performance for MICe and TICe. "
750851750119211008,2016-07-07 00:39:42,https://t.co/KL1oXazRq7,Variational Gram Functions: Convex Analysis and Optimization. (arXiv:1507.04734v2 [math.OC] UPDATED) https://t.co/KL1oXazRq7,0,4," Abstract: We propose a new class of convex penalty functions, called \emph{variational Gram functions} (VGFs), that can promote pairwise relations, such as orthogonality, among a set of vectors in a vector space. These functions can serve as regularizers in convex optimization problems arising from hierarchical classification, multitask learning, and estimating vectors with disjoint supports, among other applications. We study necessary and sufficient conditions under which a VGF is convex, and give a characterization of its subdifferential. We show how to compute its proximal operator, and discuss efficient optimization algorithms for regularized loss minimization problems where the loss admits a simple variational representation and the regularizer is a VGF. We also establish a general representer theorem for such learning problems. Lastly, numerical experiments on a hierarchical classification problem are presented to demonstrate the effectiveness of VGFs and the associated optimization algorithms. "
750851748131143681,2016-07-07 00:39:41,https://t.co/LGodUGuDaK,Sacrificing information for the greater good: how to select photometric bands for optimal accuracy. (arXiv:1511.05… https://t.co/LGodUGuDaK,1,3," Abstract: Large-scale surveys make huge amounts of photometric data available. Because of the sheer amount of objects, spectral data cannot be obtained for all of them. Therefore it is important to devise techniques for reliably estimating physical properties of objects from photometric information alone. These estimates are needed to automatically identify interesting objects worth a follow-up investigation as well as to produce the required data for a statistical analysis of the space covered by a survey. We argue that machine learning techniques are suitable to compute these estimates accurately and efficiently. This study promotes a feature selection algorithm, which selects the most informative magnitudes and colours for a given task of estimating physical quantities from photometric data alone. Using k nearest neighbours regression, a well-known non-parametric machine learning method, we show that using the found features significantly increases the accuracy of the estimations compared to using standard features and standard methods. We illustrate the usefulness of the approach by estimating specific star formation rates (sSFRs) and redshifts (photo-z's) using only the broad-band photometry from the Sloan Digital Sky Survey (SDSS). For estimating sSFRs, we demonstrate that our method produces better estimates than traditional spectral energy distribution (SED) fitting. For estimating photo-z's, we show that our method produces more accurate photo-z's than the method employed by SDSS. The study highlights the general importance of performing proper model selection to improve the results of machine learning systems and how feature selection can provide insights into the predictive relevance of particular input features. "
750851746025631745,2016-07-07 00:39:41,https://t.co/ivhLkhQmoN,Computationally Feasible Near-Optimal Subset Selection for Linear Regression under Measurement Constraints. (arXiv… https://t.co/ivhLkhQmoN,2,5," Abstract: Computationally feasible and statistically near-optimal subset selection strategies are derived to select a small portion of design (data) points in a linear regression model $y=X\beta+\varepsilon$ to reduce measurement cost and data efficiency. We consider two subset selection algorithms for estimating model coefficients $\beta$: the first algorithm is a random subsampling based method that achieves optimal statistical performance with a small $(1+\epsilon)$ relative factor under the with replacement model, and an $O(\log k)$ multiplicative factor under the without replacement model, with $k$ denoting the measurement budget. The second algorithm is fully deterministic and achieves $(1+\epsilon)$ relative approximation under the without replacement model, at the cost of slightly worse dependency of $k$ on the number of variables (data dimension) in the linear regression model. Finally, we show how our method could be extended to the corresponding prediction problem and also remark on interpretable sampling (selection) of data points under random design frameworks. "
750851743496429568,2016-07-07 00:39:40,https://t.co/PXLLipFjQP,Identifiability Assumptions and Algorithm for Directed Graphical Models with Feedback. (arXiv:1602.04418v2 [stat.M… https://t.co/PXLLipFjQP,1,5," Abstract: Directed graphical models provide a useful framework for modeling causal or directional relationships for multivariate data. Prior work has largely focused on identifiability and search algorithms for directed acyclic graphical (DAG) models. In many applications, feedback naturally arises and directed graphical models that permit cycles occur. In this paper we address the issue of identifiability for general directed cyclic graphical (DCG) models satisfying the Markov assumption. In particular, in addition to the faithfulness assumption which has already been introduced for cyclic models, we introduce two new identifiability assumptions, one based on selecting the model with the fewest edges and the other based on selecting the DCG model that entails the maximum number of d-separation rules. We provide theoretical results comparing these assumptions which show that: (1) selecting models with the largest number of d-separation rules is strictly weaker than the faithfulness assumption; (2) unlike for DAG models, selecting models with the fewest edges does not necessarily result in a milder assumption than the faithfulness assumption. We also provide connections between our two new principles and minimality assumptions. We use our identifiability assumptions to develop search algorithms for small-scale DCG models. Our simulation study supports our theoretical results, showing that the algorithms based on our two new principles generally out-perform algorithms based on the faithfulness assumption in terms of selecting the true skeleton for DCG models. "
750851740954664960,2016-07-07 00:39:39,https://t.co/wzNULPvAPw,Uniform {\varepsilon}-Stability of Distributed Nonlinear Filtering over DNAs: Gaussian-Finite HMMs. (arXiv:1602.04… https://t.co/wzNULPvAPw,0,2," Abstract: In this work, we study stability of distributed filtering of Markov chains with finite state space, partially observed in conditionally Gaussian noise. We consider a nonlinear filtering scheme over a Distributed Network of Agents (DNA), which relies on the distributed evaluation of the likelihood part of the centralized nonlinear filter and is based on a particular specialization of the Alternating Direction Method of Multipliers (ADMM) for fast average consensus. Assuming the same number of consensus steps between any two consecutive noisy measurements for each sensor in the network, we fully characterize a minimal number of such steps, such that the distributed filter remains uniformly stable with a prescribed accuracy level, {\varepsilon} \in (0,1], within a finite operational horizon, T, and across all sensors. Stability is in the sense of the \ell_1-norm between the centralized and distributed versions of the posterior at each sensor, and at each time within T. Roughly speaking, our main result shows that uniform {\varepsilon}-stability of the distributed filtering process depends only loglinearly on T and (roughly) the size of the network, and only logarithmically on 1/{\varepsilon}. If this total loglinear bound is fulfilled, any additional consensus iterations will incur a fully quantified further exponential decay in the consensus error. Our bounds are universal, in the sense that they are independent of the particular structure of the Gaussian Hidden Markov Model (HMM) under consideration. "
750851738773622784,2016-07-07 00:39:39,https://t.co/dW6L2ME8f2,EEG-informed attended speaker extraction from recorded speech mixtures with application in neuro-steered hearing p… https://t.co/dW6L2ME8f2,0,3," Abstract: OBJECTIVE: We aim to extract and denoise the attended speaker in a noisy, two-speaker acoustic scenario, relying on microphone array recordings from a binaural hearing aid, which are complemented with electroencephalography (EEG) recordings to infer the speaker of interest. METHODS: In this study, we propose a modular processing flow that first extracts the two speech envelopes from the microphone recordings, then selects the attended speech envelope based on the EEG, and finally uses this envelope to inform a multi-channel speech separation and denoising algorithm. RESULTS: Strong suppression of interfering (unattended) speech and background noise is achieved, while the attended speech is preserved. Furthermore, EEG-based auditory attention detection (AAD) is shown to be robust to the use of noisy speech signals. CONCLUSIONS: Our results show that AAD-based speaker extraction from microphone array recordings is feasible and robust, even in noisy acoustic environments, and without access to the clean speech signals to perform EEG-based AAD. SIGNIFICANCE: Current research on AAD always assumes the availability of the clean speech signals, which limits the applicability in real settings. We have extended this research to detect the attended speaker even when only microphone recordings with noisy speech mixtures are available. This is an enabling ingredient for new brain-computer interfaces and effective filtering schemes in neuro-steered hearing prostheses. Here, we provide a first proof of concept for EEG-informed attended speaker extraction and denoising. "
750851734344437760,2016-07-07 00:39:38,https://t.co/QdEWX0dTKn,A sequential Monte Carlo approach to Thompson sampling for Bayesian optimization. (arXiv:1604.00169v2 [stat.ML] UP… https://t.co/QdEWX0dTKn,1,9," Abstract: Bayesian optimization through Gaussian process regression is an effective method of optimizing an unknown function for which every measurement is expensive. It approximates the objective function and then recommends a new measurement point to try out. This recommendation is usually selected by optimizing a given acquisition function. After a sufficient number of measurements, a recommendation about the maximum is made. However, a key realization is that the maximum of a Gaussian process is not a deterministic point, but a random variable with a distribution of its own. This distribution cannot be calculated analytically. Our main contribution is a sequential Monte Carlo approach towards approximating this maximum distribution. Subsequently, by taking samples from this distribution, we enable Thompson sampling to be applied to (armed-bandit) optimization problems with a continuous input space. All this is done without requiring the optimization of a nonlinear acquisition function. Experiments have shown that the resulting optimization method has a competitive performance at keeping the cumulative regret limited. "
750490129664512001,2016-07-06 00:42:45,https://t.co/TjNAGDCME1,Accelerate Stochastic Subgradient Method by Leveraging Local Error Bound. (arXiv:1607.01027v1 [math.OC]) https://t.co/TjNAGDCME1,1,8," Abstract: In this paper, we propose two accelerated stochastic {\bf subgradient} methods for stochastic non-strongly convex optimization problems by leveraging a generic local error bound condition. The novelty of the proposed methods lies at smartly leveraging the recent historical solution to tackle the variance in the stochastic subgradient. The key idea of both methods is to iteratively solve the original problem approximately in a local region around a recent historical solution with size of the local region gradually decreasing as the solution approaches the optimal set. The difference of the two methods lies at how to construct the local region. The first method uses an explicit ball constraint and the second method uses an implicit regularization approach. For both methods, we establish the improved iteration complexity in a high probability for achieving an $\epsilon$-optimal solution. Besides the improved order of iteration complexity with a high probability, the proposed algorithms also enjoy a logarithmic dependence on the distance of the initial solution to the optimal set. When applied to the $\ell_1$ regularized polyhedral loss minimization (e.g., hinge loss, absolute loss), the proposed stochastic methods have a logarithmic iteration complexity. "
750490127252746240,2016-07-06 00:42:44,https://t.co/oOcAflw6Ni,Bootstrap Model Aggregation for Distributed Statistical Learning. (arXiv:1607.01036v1 [stat.ML]) https://t.co/oOcAflw6Ni,2,5," Abstract: In distributed, or privacy-preserving learning, we are often given a set of probabilistic models estimated from different local repositories, and asked to combine them into a single model that gives efficient statistical estimation. A simple method is to linearly average the parameters of the local models, which, however, tends to be degenerate or not applicable on non-convex models, or models with different parameter dimensions. One more practical strategy is to generate bootstrap samples from the local models, and then learn a joint model based on the combined bootstrap set. Unfortunately, the bootstrap procedure introduces additional noise and can significantly deteriorate the performance. In this work, we propose two variance reduction methods to correct the bootstrap noise, including a weighted M-estimator that is both statistically efficient and practically powerful. Both theoretical and empirical analysis is provided to demonstrate our methods. "
750490124392206338,2016-07-06 00:42:43,https://t.co/nYPbOK9TOA,How to Evaluate the Quality of Unsupervised Anomaly Detection Algorithms?. (arXiv:1607.01152v1 [stat.ML]) https://t.co/nYPbOK9TOA,0,12," Abstract: When sufficient labeled data are available, classical criteria based on Receiver Operating Characteristic (ROC) or Precision-Recall (PR) curves can be used to compare the performance of un-supervised anomaly detection algorithms. However , in many situations, few or no data are labeled. This calls for alternative criteria one can compute on non-labeled data. In this paper, two criteria that do not require labels are empirically shown to discriminate accurately (w.r.t. ROC or PR based criteria) between algorithms. These criteria are based on existing Excess-Mass (EM) and Mass-Volume (MV) curves, which generally cannot be well estimated in large dimension. A methodology based on feature sub-sampling and aggregating is also described and tested, extending the use of these criteria to high-dimensional datasets and solving major drawbacks inherent to standard EM and MV curves. "
750490122479632384,2016-07-06 00:42:43,https://t.co/TSG284Vm0g,Machine Learning for Antimicrobial Resistance. (arXiv:1607.01224v1 [stat.ML]) https://t.co/TSG284Vm0g,1,2," Abstract: Biological datasets amenable to applied machine learning are more available today than ever before, yet they lack adequate representation in the Data-for-Good community. Here we present a work in progress case study performing analysis on antimicrobial resistance (AMR) using standard ensemble machine learning techniques and note the successes and pitfalls such work entails. Broadly, applied machine learning (AML) techniques are well suited to AMR, with classification accuracies ranging from mid-90% to low- 80% depending on sample size. Additionally, these techniques prove successful at identifying gene regions known to be associated with the AMR phenotype. We believe that the extensive amount of biological data available, the plethora of problems presented, and the global impact of such work merits the consideration of the Data- for-Good community. "
750490120625717248,2016-07-06 00:42:42,https://t.co/ferr2Jfb3y,Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization. (arXiv:1607.01231v1 [math.OC]) https://t.co/ferr2Jfb3y,4,12," Abstract: In this paper we study stochastic quasi-Newton methods for nonconvex stochastic optimization, where we assume that noisy information about the gradients of the objective function is available via a stochastic first-order oracle ($\SFO$). We propose a general framework for such methods, for which we prove almost sure convergence to stationary points and analyze its worst-case iteration complexity. When a randomly chosen iterate is returned as the output of such an algorithm, we prove that in the worst-case, the $\SFO$-calls complexity is $O(\epsilon^{-2})$ to ensure that the expectation of the squared norm of the gradient is smaller than the given accuracy tolerance $\epsilon$. We also propose a specific algorithm, namely a stochastic damped L-BFGS (SdLBFGS) method, that falls under the proposed framework. Moreover, we incorporate the SVRG variance reduction technique into the proposed SdLBFGS method, and analyze its $\SFO$-calls complexity. Numerical results on a nonconvex binary classification problem using SVM, and a multiclass classification problem using neural networks are reported. "
750490118855782400,2016-07-06 00:42:42,https://t.co/99pZenRM02,Minimum Message Length based Mixture Modelling using Bivariate von Mises Distributions with Applications to Bioinf… https://t.co/99pZenRM02,0,2," Abstract: The modelling of empirically observed data is commonly done using mixtures of probability distributions. In order to model angular data, directional probability distributions such as the bivariate von Mises (BVM) is typically used. The critical task involved in mixture modelling is to determine the optimal number of component probability distributions. We employ the Bayesian information-theoretic principle of minimum message length (MML) to distinguish mixture models by balancing the trade-off between the model's complexity and its goodness-of-fit to the data. We consider the problem of modelling angular data resulting from the spatial arrangement of protein structures using BVM distributions. The main contributions of the paper include the development of the mixture modelling apparatus along with the MML estimation of the parameters of the BVM distribution. We demonstrate that statistical inference using the MML framework supersedes the traditional methods and offers a mechanism to objectively determine models that are of practical significance. "
750490116628570112,2016-07-06 00:42:41,https://t.co/8bU0XMtSXl,Learning Discriminative Features using Encoder-Decoder type Deep Neural Nets. (arXiv:1607.01354v1 [cs.LG]) https://t.co/8bU0XMtSXl,2,9," Abstract: As machine learning is applied to an increasing variety of complex problems, which are defined by high dimensional and complex data sets, the necessity for task oriented feature learning grows in importance. With the advancement of Deep Learning algorithms, various successful feature learning techniques have evolved. In this paper, we present a novel way of learning discriminative features by training Deep Neural Nets which have Encoder or Decoder type architecture similar to an Autoencoder. We demonstrate that our approach can learn discriminative features which can perform better at pattern classification tasks when the number of training samples is relatively small in size. "
750490114292326401,2016-07-06 00:42:41,https://t.co/bw4yzduEhZ,On the Consistency of the Likelihood Maximization Vertex Nomination Scheme: Bridging the Gap Between Maximum Likel… https://t.co/bw4yzduEhZ,1,2," Abstract: Given a graph in which a few vertices are deemed interesting a priori, the vertex nomination task is to order the remaining vertices into a nomination list such that there is a concentration of interesting vertices at the top of the list. Previous work has yielded several approaches to this problem, with theoretical results in the setting where the graph is drawn from a stochastic block model (SBM), including a vertex nomination analogue of the Bayes optimal classifier. In this paper, we prove that maximum likelihood (ML)-based vertex nomination is consistent, in the sense that the performance of the ML-based scheme asymptotically matches that of the Bayes optimal scheme. We prove theorems of this form both when model parameters are known and unknown. Additionally, we introduce and prove consistency of a related, more scalable restricted-focus ML vertex nomination scheme. Finally, we incorporate vertex and edge features into ML-based vertex nomination and briefly explore the empirical effectiveness of this approach. "
750490111842914305,2016-07-06 00:42:40,https://t.co/uFsa4flLGi]),Efficient Estimation in the Tails of Gaussian Copulas. (arXiv:1607.01375v1 [https://t.co/uFsa4flLGi]) https://t.co/tYUYiryZ09,1,2,INDEXERROR
750490107510153217,2016-07-06 00:42:39,https://t.co/G77nZ508Lv,One-Shot Session Recommendation Systems with Combinatorial Items. (arXiv:1607.01381v1 [stat.ML]) https://t.co/G77nZ508Lv,2,7," Abstract: In recent years, content recommendation systems in large websites (or \emph{content providers}) capture an increased focus. While the type of content varies, e.g.\ movies, articles, music, advertisements, etc., the high level problem remains the same. Based on knowledge obtained so far on the user, recommend the most desired content. In this paper we present a method to handle the well known user-cold-start problem in recommendation systems. In this scenario, a recommendation system encounters a new user and the objective is to present items as relevant as possible with the hope of keeping the user's session as long as possible. We formulate an optimization problem aimed to maximize the length of this initial session, as this is believed to be the key to have the user come back and perhaps register to the system. In particular, our model captures the fact that a single round with low quality recommendation is likely to terminate the session. In such a case, we do not proceed to the next round as the user leaves the system, possibly never to seen again. We denote this phenomenon a \emph{One-Shot Session}. Our optimization problem is formulated as an MDP where the action space is of a combinatorial nature as we recommend in each round, multiple items. This huge action space presents a computational challenge making the straightforward solution intractable. We analyze the structure of the MDP to prove monotone and submodular like properties that allow a computationally efficient solution via a method denoted by \emph{Greedy Value Iteration} (G-VI). "
750490105031319552,2016-07-06 00:42:39,https://t.co/ZWAOgnid92,Sequential Dimensionality Reduction for Extracting Localized Features. (arXiv:1505.06957v2 [cs.CV] UPDATED) https://t.co/ZWAOgnid92,2,6," Abstract: Linear dimensionality reduction techniques are powerful tools for image analysis as they allow the identification of important features in a data set. In particular, nonnegative matrix factorization (NMF) has become very popular as it is able to extract sparse, localized and easily interpretable features by imposing an additive combination of nonnegative basis elements. Nonnegative matrix underapproximation (NMU) is a closely related technique that has the advantage to identify features sequentially. In this paper, we propose a variant of NMU that is particularly well suited for image analysis as it incorporates the spatial information, that is, it takes into account the fact that neighboring pixels are more likely to be contained in the same features, and favors the extraction of localized features by looking for sparse basis elements. We show that our new approach competes favorably with comparable state-of-the-art techniques on synthetic, facial and hyperspectral image data sets. "
750490102695157760,2016-07-06 00:42:38,https://t.co/khqQe8rzzm,Building Ensembles of Adaptive Nested Dichotomies with Random-Pair Selection. (arXiv:1604.01854v2 [stat.ML] UPDATE… https://t.co/khqQe8rzzm,0,2," Abstract: A system of nested dichotomies is a method of decomposing a multi-class problem into a collection of binary problems. Such a system recursively splits the set of classes into two subsets, and trains a binary classifier to distinguish between each subset. Even though ensembles of nested dichotomies with random structure have been shown to perform well in practice, using a more sophisticated class subset selection method can be used to improve classification accuracy. We investigate an approach to this problem called random-pair selection, and evaluate its effectiveness compared to other published methods of subset selection. We show that our method outperforms other methods in many cases when forming ensembles of nested dichotomies, and is at least on par in all other cases. "
750490099700367360,2016-07-06 00:42:37,https://t.co/zHBDOJqNyt,Solving Systems of Random Quadratic Equations via Truncated Amplitude Flow. (arXiv:1605.08285v2 [stat.ML] UPDATED) https://t.co/zHBDOJqNyt,0,2," Abstract: This paper puts forth a new algorithm, termed \emph{truncated amplitude flow} (TAF), to recover an unknown $n$-dimensional real-/complex-valued vector $\bm{x}$ from $m$ quadratic equations of the form $y_i=|\langle\bm{a}_i,\bm{x}\rangle|^2$. This problem is known to be \emph{NP-hard} in general. We prove that as soon as the number of equations $m$ is on the order of the number of unknowns $n$, TAF recovers the solution exactly (up to a global unimodular constant) with high probability and complexity growing linearly with the time required to read the data. Our method adopts the \emph{amplitude-based} cost function and proceeds in two stages: In stage one, we introduce an \emph{orthogonality-promoting} initialization that is obtained with a few simple power iterations. Stage two refines the initial estimate by successive updates of scalable \emph{truncated generalized gradient iterations}. The former is in sharp contrast to existing spectral initializations, while the latter handles the rather challenging nonconvex and nonsmooth amplitude-based cost function. In particular for real-valued vectors, our gradient truncation rule provably eliminates the erroneously estimated signs with high probability to markedly improve upon its untruncated version. Numerical tests demonstrate that our initialization method returns more accurate and robust estimates relative to its spectral counterparts. Furthermore, even under the same initialization, our amplitude-based refinement outperforms Wirtinger-based alternatives, corroborating the superior performance of TAF over state-of-the-art algorithms. "
750490095141187584,2016-07-06 00:42:36,https://t.co/jqQS6hSr0i,Stochastic Portfolio Theory: A Machine Learning Perspective. (arXiv:1605.02654v1 [q-fin.PM] CROSS LISTED) https://t.co/jqQS6hSr0i,4,9," Abstract: In this paper we propose a novel application of Gaussian processes (GPs) to financial asset allocation. Our approach is deeply rooted in Stochastic Portfolio Theory (SPT), a stochastic analysis framework introduced by Robert Fernholz that aims at flexibly analysing the performance of certain investment strategies in stock markets relative to benchmark indices. In particular, SPT has exhibited some investment strategies based on company sizes that, under realistic assumptions, outperform benchmark indices with probability 1 over certain time horizons. Galvanised by this result, we consider the inverse problem that consists of learning (from historical data) an optimal investment strategy based on any given set of trading characteristics, and using a user-specified optimality criterion that may go beyond outperforming a benchmark index. Although this inverse problem is of the utmost interest to investment management practitioners, it can hardly be tackled using the SPT framework. We show that our machine learning approach learns investment strategies that considerably outperform existing SPT strategies in the US stock market. "
750126458924851200,2016-07-05 00:37:39,https://t.co/qGiuFSoIi3,Decoding the Encoding of Functional Brain Networks: an fMRI Classification Comparison of Non-negative Matrix Facto… https://t.co/qGiuFSoIi3,0,3," Abstract: Brain networks in fMRI are typically identified using spatial independent component analysis (ICA), yet mathematical constraints such as sparse coding and positivity both provide alternate biologically-plausible frameworks for generating brain networks. Non-negative Matrix Factorization (NMF) would suppress negative BOLD signal by enforcing positivity. Spatial sparse coding algorithms ($L1$ Regularized Learning and K-SVD) would impose local specialization and a discouragement of multitasking, where the total observed activity in a single voxel originates from a restricted number of possible brain networks. The assumptions of independence, positivity, and sparsity to encode task-related brain networks are compared; the resulting brain networks for different constraints are used as basis functions to encode the observed functional activity at a given time point. These encodings are decoded using machine learning to compare both the algorithms and their assumptions, using the time series weights to predict whether a subject is viewing a video, listening to an audio cue, or at rest, in 304 fMRI scans from 51 subjects. For classifying cognitive activity, the sparse coding algorithm of $L1$ Regularized Learning consistently outperformed 4 variations of ICA across different numbers of networks and noise levels (p$<$0.001). The NMF algorithms, which suppressed negative BOLD signal, had the poorest accuracy. Within each algorithm, encodings using sparser spatial networks (containing more zero-valued voxels) had higher classification accuracy (p$<$0.001). The success of sparse coding algorithms may suggest that algorithms which enforce sparse coding, discourage multitasking, and promote local specialization may capture better the underlying source processes than those which allow inexhaustible local processes such as ICA. "
750126457435877377,2016-07-05 00:37:38,https://t.co/z0UPvlZzm7,A Greedy Approach to Adapting the Trace Parameter for Temporal Difference Learning. (arXiv:1607.00446v1 [cs.AI]) https://t.co/z0UPvlZzm7,0,1," Abstract: One of the main obstacles to broad application of reinforcement learning methods is the parameter sensitivity of our core learning algorithms. In many large-scale applications, online computation and function approximation represent key strategies in scaling up reinforcement learning algorithms. In this setting, we have effective and reasonably well understood algorithms for adapting the learning-rate parameter, online during learning. Such meta-learning approaches can improve robustness of learning and enable specialization to current task, improving learning speed. For temporal-difference learning algorithms which we study here, there is yet another parameter, $\lambda$, that similarly impacts learning speed and stability in practice. Unfortunately, unlike the learning-rate parameter, $\lambda$ parametrizes the objective function that temporal-difference methods optimize. Different choices of $\lambda$ produce different fixed-point solutions, and thus adapting $\lambda$ online and characterizing the optimization is substantially more complex than adapting the learning-rate parameter. There are no meta-learning method for $\lambda$ that can achieve (1) incremental updating, (2) compatibility with function approximation, and (3) maintain stability of learning under both on and off-policy sampling. In this paper we contribute a novel objective function for optimizing $\lambda$ as a function of state rather than time. We derive a new incremental, linear complexity $\lambda$-adaption algorithm that does not require offline batch updating or access to a model of the world, and present a suite of experiments illustrating the practicality of our new algorithm in three different settings. Taken together, our contributions represent a concrete step towards black-box application of temporal-difference learning methods in real world problems. "
750126455988817920,2016-07-05 00:37:38,https://t.co/hzsMZNGkWJ,Alzheimer's Disease Diagnostics by Adaptation of 3D Convolutional Network. (arXiv:1607.00455v1 [cs.LG]) https://t.co/hzsMZNGkWJ,3,7," Abstract: Early diagnosis, playing an important role in preventing progress and treating the Alzheimer\{'}s disease (AD), is based on classification of features extracted from brain images. The features have to accurately capture main AD-related variations of anatomical brain structures, such as, e.g., ventricles size, hippocampus shape, cortical thickness, and brain volume. This paper proposed to predict the AD with a deep 3D convolutional neural network (3D-CNN), which can learn generic features capturing AD biomarkers and adapt to different domain datasets. The 3D-CNN is built upon a 3D convolutional autoencoder, which is pre-trained to capture anatomical shape variations in structural brain MRI scans. Fully connected upper layers of the 3D-CNN are then fine-tuned for each task-specific AD classification. Experiments on the CADDementia MRI dataset with no skull-stripping preprocessing have shown our 3D-CNN outperforms several conventional classifiers by accuracy. Abilities of the 3D-CNN to generalize the features learnt and adapt to other domains have been validated on the ADNI dataset. "
750126454357258241,2016-07-05 00:37:38,https://t.co/umC13Ys5K2,Group Sparse Regularization for Deep Neural Networks. (arXiv:1607.00485v1 [stat.ML]) https://t.co/umC13Ys5K2,0,5," Abstract: In this paper, we consider the joint task of simultaneously optimizing (i) the weights of a deep neural network, (ii) the number of neurons for each hidden layer, and (iii) the subset of active input features (i.e., feature selection). While these problems are generally dealt with separately, we present a simple regularized formulation allowing to solve all three of them in parallel, using standard optimization routines. Specifically, we extend the group Lasso penalty (originated in the linear regression literature) in order to impose group-level sparsity on the network's connections, where each group is defined as the set of outgoing weights from a unit. Depending on the specific case, the weights can be related to an input variable, to a hidden neuron, or to a bias unit, thus performing simultaneously all the aforementioned tasks in order to obtain a compact network. We perform an extensive experimental evaluation, by comparing with classical weight decay and Lasso penalties. We show that a sparse version of the group Lasso penalty is able to achieve competitive performances, while at the same time resulting in extremely compact networks with a smaller number of input features. We evaluate both on a toy dataset for handwritten digit recognition, and on multiple realistic large-scale classification problems. "
750126452927062017,2016-07-05 00:37:37,https://t.co/eBWlIeYP1E,Double-detector for Sparse Signal Detection from One Bit Compressed Sensing Measurements. (arXiv:1607.00494v1 [cs.… https://t.co/eBWlIeYP1E,0,0," Abstract: This letter presents the sparse vector signal detection from one bit compressed sensing measurements, in contrast to the previous works which deal with scalar signal detection. In this letter, available results are extended to the vector case and the GLRT detector and the optimal quantizer design are obtained. Also, a double-detector scheme is introduced in which a sensor level threshold detector is integrated into network level GLRT to improve the performance. The detection criteria of oracle and clairvoyant detectors are also derived. Simulation results show that with careful design of the threshold detector, the overall detection performance of double-detector scheme would be better than the sign-GLRT proposed in [1] and close to oracle and clairvoyant detectors. Also, the proposed detector is applied to spectrum sensing and the results are near the well known energy detector which uses the real valued data while the proposed detector only uses the sign of the data. "
750126450938961922,2016-07-05 00:37:37,https://t.co/mCMQjGMXwJ,Approximate Joint Matrix Triangularization. (arXiv:1607.00514v1 [cs.NA]) https://t.co/mCMQjGMXwJ,0,0," Abstract: We consider the problem of approximate joint triangularization of a set of noisy jointly diagonalizable real matrices. Approximate joint triangularizers are commonly used in the estimation of the joint eigenstructure of a set of matrices, with applications in signal processing, linear algebra, and tensor decomposition. By assuming the input matrices to be perturbations of noise-free, simultaneously diagonalizable ground-truth matrices, the approximate joint triangularizers are expected to be perturbations of the exact joint triangularizers of the ground-truth matrices. We provide a priori and a posteriori perturbation bounds on the `distance' between an approximate joint triangularizer and its exact counterpart. The a priori bounds are theoretical inequalities that involve functions of the ground-truth matrices and noise matrices, whereas the a posteriori bounds are given in terms of observable quantities that can be computed from the input matrices. From a practical perspective, the problem of finding the best approximate joint triangularizer of a set of noisy matrices amounts to solving a nonconvex optimization problem. We show that, under a condition on the noise level of the input matrices, it is possible to find a good initial triangularizer such that the solution obtained by any local descent-type algorithm has certain global guarantees. Finally, we discuss the application of approximate joint matrix triangularization to canonical tensor decomposition and we derive novel estimation error bounds. "
750126448967639040,2016-07-05 00:37:36,https://t.co/t77JwlhZhn,Alzheimer's Disease Diagnostics by a Deeply Supervised Adaptable 3D Convolutional Network. (arXiv:1607.00556v1 [cs… https://t.co/t77JwlhZhn,0,3," Abstract: Early diagnosis, playing an important role in preventing progress and treating the Alzheimer's disease (AD), is based on classification of features extracted from brain images. The features have to accurately capture main AD-related variations of anatomical brain structures, such as, e.g., ventricles size, hippocampus shape, cortical thickness, and brain volume. This paper proposes to predict the AD with a deep 3D convolutional neural network (3D-CNN), which can learn generic features capturing AD biomarkers and adapt to different domain datasets. The 3D-CNN is built upon a 3D convolutional autoencoder, which is pre-trained to capture anatomical shape variations in structural brain MRI scans. Fully connected upper layers of the 3D-CNN are then fine-tuned for each task-specific AD classification. Experiments on the \emph{ADNI} MRI dataset with no skull-stripping preprocessing have shown our 3D-CNN outperforms several conventional classifiers by accuracy and robustness. Abilities of the 3D-CNN to generalize the features learnt and adapt to other domains have been validated on the \emph{CADDementia} dataset. "
750126446543339521,2016-07-05 00:37:36,https://t.co/WKbn7WIF7A,Sub-sampled Newton Methods with Non-uniform Sampling. (arXiv:1607.00559v1 [math.OC]) https://t.co/WKbn7WIF7A,0,1," Abstract: We consider the problem of finding the minimizer of a convex function $F: \mathbb R^d \rightarrow \mathbb R$ of the form $F(w) := \sum_{i=1}^n f_i(w) + R(w)$ where a low-rank factorization of $\nabla^2 f_i(w)$ is readily available. We consider the regime where $n \gg d$. As second-order methods prove to be effective in finding the minimizer to a high-precision, in this work, we propose randomized Newton-type algorithms that exploit \textit{non-uniform} sub-sampling of $\{\nabla^2 f_i(w)\}_{i=1}^{n}$, as well as inexact updates, as means to reduce the computational complexity. Two non-uniform sampling distributions based on {\it block norm squares} and {\it block partial leverage scores} are considered in order to capture important terms among $\{\nabla^2 f_i(w)\}_{i=1}^{n}$. We show that at each iteration non-uniformly sampling at most $\mathcal O(d \log d)$ terms from $\{\nabla^2 f_i(w)\}_{i=1}^{n}$ is sufficient to achieve a linear-quadratic convergence rate in $w$ when a suitable initial point is provided. In addition, we show that our algorithms achieve a lower computational complexity and exhibit more robustness and better dependence on problem specific quantities, such as the condition number, compared to similar existing methods, especially the ones based on uniform sampling. Finally, we empirically demonstrate that our methods are at least twice as fast as Newton's methods with ridge logistic regression on several real datasets. "
750126444165169157,2016-07-05 00:37:35,https://t.co/3fImHqfNBO,Rademacher Complexity Bounds for a Penalized Multiclass Semi-supervised Algorithm. (arXiv:1607.00567v1 [stat.ML]) https://t.co/3fImHqfNBO,0,1," Abstract: We propose Rademacher complexity bounds for multiclass classifiers trained with a two-step semi-supervised model. In the first step, the algorithm partitions the partially labeled data and then identifies dense clusters containing $\kappa$ predominant classes using the labeled training examples such that the proportion of their non-predominant classes is below a fixed threshold. In the second step, a classifier is trained by minimizing a margin empirical loss over the labeled training set and a penalization term measuring the disability of the learner to predict the $\kappa$ predominant classes of the identified clusters. The resulting data-dependent generalization error bound involves the margin distribution of the classifier, the stability of the clustering technique used in the first step and Rademacher complexity terms corresponding to partially labeled training data. Our theoretical result exhibit convergence rates extending those proposed in the literature for the binary case, and experimental results on different multiclass classification problems show empirical evidence that supports the theory. "
750126441782730752,2016-07-05 00:37:35,https://t.co/6WCSHg5CnP,node2vec: Scalable Feature Learning for Networks. (arXiv:1607.00653v1 [cs.SI]) https://t.co/6WCSHg5CnP,4,11," Abstract: Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks. "
750126439022915584,2016-07-05 00:37:34,https://t.co/GvJZtxBUaY,Unsupervised Learning of 3D Structure from Images. (arXiv:1607.00662v1 [cs.CV]) https://t.co/GvJZtxBUaY,8,26," Abstract: A key goal of computer vision is to recover the underlying 3D structure from 2D observations of the world. In this paper we learn strong deep generative models of 3D structures, and recover these structures from 3D and 2D images via probabilistic inference. We demonstrate high-quality samples and report log-likelihoods on several datasets, including ShapeNet [2], and establish the first benchmarks in the literature. We also show how these models and their inference networks can be trained end-to-end from 2D images. This demonstrates for the first time the feasibility of learning to infer 3D representations of the world in a purely unsupervised manner. "
750126436611129344,2016-07-05 00:37:33,https://t.co/yYQm2y9J4O,Understanding the Energy and Precision Requirements for Online Learning. (arXiv:1607.00669v1 [stat.ML]) https://t.co/yYQm2y9J4O,0,1," Abstract: It is well-known that the precision of data, hyperparameters, and internal representations employed in learning systems directly impacts its energy, throughput, and latency. The precision requirements for the training algorithm are also important for systems that learn on-the-fly. Prior work has shown that the data and hyperparameters can be quantized heavily without incurring much penalty in classification accuracy when compared to floating point implementations. These works suffer from two key limitations. First, they assume uniform precision for the classifier and for the training algorithm and thus miss out on the opportunity to further reduce precision. Second, prior works are empirical studies. In this article, we overcome both these limitations by deriving analytical lower bounds on the precision requirements of the commonly employed stochastic gradient descent (SGD) on-line learning algorithm in the specific context of a support vector machine (SVM). Lower bounds on the data precision are derived in terms of the the desired classification accuracy and precision of the hyperparameters used in the classifier. Additionally, lower bounds on the hyperparameter precision in the SGD training algorithm are obtained. These bounds are validated using both synthetic and the UCI breast cancer dataset. Additionally, the impact of these precisions on the energy consumption of a fixed-point SVM with on-line training is studied. "
750126434409181184,2016-07-05 00:37:33,https://t.co/tEtq1282Vg,Variational limits of k-NN graph based functionals on data clouds. (arXiv:1607.00696v1 [math.ST]) https://t.co/tEtq1282Vg,0,2," Abstract: We consider i.i.d. samples $x_1, \dots, x_n$ from a measure $\nu$ with density supported on a bounded Euclidean domain $D \subseteq R^d $ where $d\geq3$. A graph on the point cloud is obtained by connecting two points if one of them is among the $k$-nearest neighbors of the other. Our goal is to study consistency of graph based procedures to clustering, classification and dimensionality reduction by studying the variational convergence of the graph total variation associated to such $k$-NN graph. We prove that provided $k:=k_n$ scales like $n \gg k_n \gg \log(n)$, then the $\Gamma$-convergence of the graph total variation towards an appropriate weighted total variation is guaranteed. "
750126431707996160,2016-07-05 00:37:32,https://t.co/5ZSlGEvJa3,A Semi-supervised learning approach to enhance health care Community-based Question Answering: A case study in alc… https://t.co/5ZSlGEvJa3,0,3," Abstract: Community-based Question Answering (CQA) sites play an important role in addressing health information needs. However, a significant number of posted questions remain unanswered. Automatically answering the posted questions can provide a useful source of information for online health communities. In this study, we developed an algorithm to automatically answer health-related questions based on past questions and answers (QA). We also aimed to understand information embedded within online health content that are good features in identifying valid answers. Our proposed algorithm uses information retrieval techniques to identify candidate answers from resolved QA. In order to rank these candidates, we implemented a semi-supervised leaning algorithm that extracts the best answer to a question. We assessed this approach on a curated corpus from Yahoo! Answers and compared against a rule-based string similarity baseline. On our dataset, the semi-supervised learning algorithm has an accuracy of 86.2%. UMLS-based (health-related) features used in the model enhance the algorithm's performance by proximately 8 %. A reasonably high rate of accuracy is obtained given that the data is considerably noisy. Important features distinguishing a valid answer from an invalid answer include text length, number of stop words contained in a test question, a distance between the test question and other questions in the corpus as well as a number of overlapping health-related terms between questions. Overall, our automated QA system based on historical QA pairs is shown to be effective according to the data set in this case study. It is developed for general use in the health care domain which can also be applied to other CQA sites. "
750126429619224576,2016-07-05 00:37:32,https://t.co/etrv3n9oAm,Automatic Generation of Probabilistic Programming from Time Series Data. (arXiv:1607.00710v1 [stat.ML]) https://t.co/etrv3n9oAm,2,9," Abstract: Probabilistic programming languages represent complex data with intermingled models in a few lines of code. Efficient inference algorithms in probabilistic programming languages make possible to build unified frameworks to compute interesting probabilities of various large, real-world problems. When the structure of model is given, constructing a probabilistic program is rather straightforward. Thus, main focus have been to learn the best model parameters and compute marginal probabilities. In this paper, we provide a new perspective to build expressive probabilistic program from continue time series data when the structure of model is not given. The intuition behind of our method is to find a descriptive covariance structure of time series data in nonparametric Gaussian process regression. We report that such descriptive covariance structure efficiently derives a probabilistic programming description accurately. "
750126427811479553,2016-07-05 00:37:31,https://t.co/OqfQMrhBGl,A Residual Bootstrap for High-Dimensional Regression with Near Low-Rank Designs. (arXiv:1607.00743v1 [math.ST]) https://t.co/OqfQMrhBGl,1,2," Abstract: We study the residual bootstrap (RB) method in the context of high-dimensional linear regression. Specifically, we analyze the distributional approximation of linear contrasts $c^{\top} (\hat{\beta}_{\rho}-\beta)$, where $\hat{\beta}_{\rho}$ is a ridge-regression estimator. When regression coefficients are estimated via least squares, classical results show that RB consistently approximates the laws of contrasts, provided that $p\ll n$, where the design matrix is of size $n\times p$. Up to now, relatively little work has considered how additional structure in the linear model may extend the validity of RB to the setting where $p/n\asymp 1$. In this setting, we propose a version of RB that resamples residuals obtained from ridge regression. Our main structural assumption on the design matrix is that it is nearly low rank --- in the sense that its singular values decay according to a power-law profile. Under a few extra technical assumptions, we derive a simple criterion for ensuring that RB consistently approximates the law of a given contrast. We then specialize this result to study confidence intervals for mean response values $X_i^{\top} \beta$, where $X_i^{\top}$ is the $i$th row of the design. More precisely, we show that conditionally on a Gaussian design with near low-rank structure, RB simultaneously approximates all of the laws $X_i^{\top}(\hat{\beta}_{\rho}-\beta)$, $i=1,\dots,n$. This result is also notable as it imposes no sparsity assumptions on $\beta$. Furthermore, since our consistency results are formulated in terms of the Mallows (Kantorovich) metric, the existence of a limiting distribution is not required. "
750126426309943300,2016-07-05 00:37:31,https://t.co/K5t9z8iigh,Tracking Dynamic Point Processes on Networks. (arXiv:1409.0031v3 [stat.ML] UPDATED) https://t.co/K5t9z8iigh,0,2," Abstract: Cascading chains of events are a salient feature of many real-world social, biological, and financial networks. In social networks, social reciprocity accounts for retaliations in gang interactions, proxy wars in nation-state conflicts, or Internet memes shared via social media. Neuron spikes stimulate or inhibit spike activity in other neurons. Stock market shocks can trigger a contagion of volatility throughout a financial network. In these and other examples, only individual events associated with network nodes are observed, usually without knowledge of the underlying dynamic relationships between nodes. This paper addresses the challenge of tracking how events within such networks stimulate or influence future events. The proposed approach is an online learning framework well-suited to streaming data, using a multivariate Hawkes point process model to encapsulate autoregressive features of observed events within the social network. Recent work on online learning in dynamic environments is leveraged not only to exploit the dynamics within the underlying network, but also to track the network structure as it evolves. Regret bounds and experimental results demonstrate that the proposed method performs nearly as well as an oracle or batch algorithm. "
750126424607092736,2016-07-05 00:37:30,https://t.co/B92tzPhXMA,Graphical Exponential Screening. (arXiv:1503.02698v2 [stat.ML] UPDATED) https://t.co/B92tzPhXMA,0,2," Abstract: In high dimensions we propose and analyze an aggregation estimator of the precision matrix for Gaussian graphical models. This estimator, called graphical Exponential Screening (gES), linearly combines a suitable set of individual estimators with different underlying graphs, and balances the estimation error and sparsity. We study the risk of this aggregation estimator and show that it is comparable to that of the best estimator based on a single graph, chosen by an oracle. Numerical performance of our method is investigated using both simulated and real datasets, in comparison with some state-of-art estimation procedures. "
750126423105478657,2016-07-05 00:37:30,https://t.co/TpfEYPfST1,Tight Risk Bounds for Multi-Class Margin Classifiers. (arXiv:1507.03040v3 [stat.ML] UPDATED) https://t.co/TpfEYPfST1,0,2," Abstract: We consider a problem of risk estimation for large-margin multi-class classifiers. We propose a novel risk bound for the multi-class classification problem. The bound involves the marginal distribution of the classifier and the Rademacher complexity of the hypothesis class. We prove that our bound is tight in the number of classes. Finally, we compare our bound with the related ones and provide a simplified version of the bound for the multi-class classification with kernel based hypotheses. "
750126421331312640,2016-07-05 00:37:30,https://t.co/Iera7vyKw2,Adaptive Smoothing Algorithms for Nonsmooth Composite Convex Minimization. (arXiv:1509.00106v5 [math.OC] UPDATED) https://t.co/Iera7vyKw2,0,2," Abstract: We propose an adaptive smoothing algorithm based on Nesterov's smoothing technique in \cite{Nesterov2005c} for solving ""fully"" nonsmooth composite convex optimization problems. Our method combines both Nesterov's accelerated proximal gradient scheme and a new homotopy strategy for smoothness parameter. By an appropriate choice of smoothing functions, we develop a new algorithm that has the $\mathcal{O}\left(\frac{1}{\varepsilon}\right)$-worst-case iteration-complexity while preserves the same complexity-per-iteration as in Nesterov's method and allows one to automatically update the smoothness parameter at each iteration. Then, we customize our algorithm to solve four special cases that cover various applications. We also specify our algorithm to solve constrained convex optimization problems and show its convergence guarantee on a primal sequence of iterates. We demonstrate our algorithm through three numerical examples and compare it with other related algorithms. "
750126419234156545,2016-07-05 00:37:29,https://t.co/XYq84pOUlj,On the complexity of switching linear regression. (arXiv:1510.06920v2 [stat.ML] UPDATED) https://t.co/XYq84pOUlj,0,3," Abstract: This technical note extends recent results on the computational complexity of globally minimizing the error of piecewise-affine models to the related problem of minimizing the error of switching linear regression models. In particular, we show that, on the one hand the problem is NP-hard, but on the other hand, it admits a polynomial-time algorithm with respect to the number of data points for any fixed data dimension and number of modes. "
750126417602547712,2016-07-05 00:37:29,https://t.co/26lKMXYbgK,Synthesis of Gaussian Trees with Correlation Sign Ambiguity: An Information Theoretic Approach. (arXiv:1601.06403v… https://t.co/26lKMXYbgK,0,1," Abstract: In latent Gaussian trees the pairwise correlation signs between the variables are intrinsically unrecoverable. Such information is vital since it completely determines the direction in which two variables are associated. In this work, we resort to information theoretical approaches to achieve two fundamental goals: First, we quantify the amount of information loss due to unrecoverable sign information. Second, we show the importance of such information in determining the maximum achievable rate region, in which the observed output vector can be synthesized, given its probability density function. In particular, we model the graphical model as a communication channel and propose a new layered encoding framework to synthesize observed data using upper layer Gaussian inputs and independent Bernoulli correlation sign inputs from each layer. We find the achievable rate region for the rate tuples of multi-layer latent Gaussian messages to synthesize the desired observables. "
750126415723495429,2016-07-05 00:37:28,https://t.co/37LGJ6YEYW,Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks. (arXiv:1… https://t.co/37LGJ6YEYW,0,3," Abstract: While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- Internal Covariate Shift-- the current solution has certain drawbacks. Specifically, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate for validation due to shifting parameter values (especially during initial training epochs). Also, BN cannot be used with batch-size 1 during training. We address these drawbacks by proposing a non-adaptive normalization technique for removing internal covariate shift, that we call Normalization Propagation. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers. "
750126413752270848,2016-07-05 00:37:28,https://t.co/4gV3BEPvug,Delta divergence: A novel decision cognizant measure of classifier incongruence. (arXiv:1604.04451v2 [cs.LG] UPDAT… https://t.co/4gV3BEPvug,0,3," Abstract: Disagreement between two classifiers regarding the class membership of an observation in pattern recognition can be indicative of an anomaly and its nuance. As in general classifiers base their decision on class aposteriori probabilities, the most natural approach to detecting classifier incongruence is to use divergence. However, existing divergences are not particularly suitable to gauge classifier incongruence. In this paper, we postulate the properties that a divergence measure should satisfy and propose a novel divergence measure, referred to as Delta divergence. In contrast to existing measures, it is decision cognizant. The focus in Delta divergence on the dominant hypotheses has a clutter reducing property, the significance of which grows with increasing number of classes. The proposed measure satisfies other important properties such as symmetry, and independence of classifier confidence. The relationship of the proposed divergence to some baseline measures is demonstrated experimentally, showing its superiority. "
750126411457921024,2016-07-05 00:37:27,https://t.co/DbIo9EX9TV,Forest Floor Visualizations of Random Forests. (arXiv:1605.09196v3 [stat.ML] UPDATED) https://t.co/DbIo9EX9TV,4,6," Abstract: We propose a novel methodology, forest floor, to visualize and interpret random forest (RF) models. RF is a popular and useful tool for non-linear multi-variate classification and regression, which yields a good trade-off between robustness (low variance) and adaptiveness (low bias). Direct interpretation of a RF model is difficult, as the explicit ensemble model of hundreds of deep trees is complex. Nonetheless, it is possible to visualize a RF model fit by its mapping from feature space to prediction space. Hereby the user is first presented with the overall geometrical shape of the model structure, and when needed one can zoom in on local details. Dimensional reduction by projection is used to visualize high dimensional shapes. The traditional method to visualize RF model structure, partial dependence plots, achieve this by averaging multiple parallel projections. We suggest to first use feature contributions, a method to decompose trees by splitting features, and then subsequently perform projections. The advantages of forest floor over partial dependence plots is that interactions are not masked by averaging. As a consequence, it is possible to locate interactions, which are not visualized in a given projection. Furthermore, we introduce: a goodness-of-visualization measure, use of colour gradients to identify interactions and an out-of-bag cross validated variant of feature contributions. "
750126405971763200,2016-07-05 00:37:26,https://t.co/wB8W9yBOLI,Online and Differentially-Private Tensor Decomposition. (arXiv:1606.06237v2 [stat.ML] UPDATED) https://t.co/wB8W9yBOLI,0,2," Abstract: In this paper, we resolve many of the key algorithmic questions regarding robustness, memory efficiency, and differential privacy of tensor decomposition. We propose simple variants of the tensor power method which enjoy these strong properties. We present the first guarantees for online tensor power method which has a linear memory requirement. Moreover, we present a noise calibrated tensor power method with efficient privacy guarantees. At the heart of all these guarantees lies a careful perturbation analysis derived in this paper which improves up on the existing results significantly. "
749789679067824129,2016-07-04 02:19:24,https://t.co/uFsa4flLGi]),The Simulator: An Engine to Streamline Simulations. (arXiv:1607.00021v1 [https://t.co/uFsa4flLGi]) https://t.co/LFyd48gAcF,0,3,INDEXERROR
749789675607449600,2016-07-04 02:19:23,https://t.co/vZENmICoUd,Ballpark Learning: Estimating Labels from Rough Group Comparisons. (arXiv:1607.00034v1 [stat.ML]) https://t.co/vZENmICoUd,1,3," Abstract: We are interested in estimating individual labels given only coarse, aggregated signal over the data points. In our setting, we receive sets (""bags"") of unlabeled instances with constraints on label proportions. We relax the unrealistic assumption of known label proportions, made in previous work; instead, we assume only to have upper and lower bounds, and constraints on bag differences. We motivate the problem, propose an intuitive formulation and algorithm, and apply our methods to real-world scenarios. Across several domains, we show how using only proportion constraints and no labeled examples, we can achieve surprisingly high accuracy. In particular, we demonstrate how to predict income level using rough stereotypes and how to perform sentiment analysis using very little information. We also apply our method to guide exploratory analysis, recovering geographical differences in twitter dialect. "
749789670989594624,2016-07-04 02:19:22,https://t.co/xZOTA0KhHE,Geometric Learning and Topological Inference with Biobotic Networks: Convergence Analysis. (arXiv:1607.00051v1 [cs… https://t.co/xZOTA0KhHE,1,4," Abstract: In this study, we present and analyze a framework for geometric and topological estimation for mapping of unknown environments. We consider agents mimicking motion behaviors of cyborg insects, known as biobots, and exploit coordinate-free local interactions among them to infer geometric and topological information about the environment, under minimal sensing and localization constraints. Local interactions are used to create a graphical representation referred to as the encounter graph. A metric is estimated over the encounter graph of the agents in order to construct a geometric point cloud using manifold learning techniques. Topological data analysis (TDA), in particular persistent homology, is used in order to extract topological features of the space and a classification method is proposed to infer robust features of interest (e.g. existence of obstacles). We examine the asymptotic behavior of the proposed metric in terms of the convergence to the geodesic distances in the underlying manifold of the domain, and provide stability analysis results for the topological persistence. The proposed framework and its convergences and stability analysis are demonstrated through numerical simulations and experiments. "
749789666644226048,2016-07-04 02:19:21,https://t.co/R9OlvhOXXf,Unsupervised Learning with Imbalanced Data via Structure Consolidation Latent Variable Model. (arXiv:1607.00067v1 … https://t.co/R9OlvhOXXf,3,5," Abstract: Unsupervised learning on imbalanced data is challenging because, when given imbalanced data, current model is often dominated by the major category and ignores the categories with small amount of data. We develop a latent variable model that can cope with imbalanced data by dividing the latent space into a shared space and a private space. Based on Gaussian Process Latent Variable Models, we propose a new kernel formulation that enables the separation of latent space and derives an efficient variational inference method. The performance of our model is demonstrated with an imbalanced medical image dataset. "
749789662915465216,2016-07-04 02:19:20,https://t.co/Xuk9rj0mGR,An Operator Theoretic Approach to Nonparametric Mixture Models. (arXiv:1607.00071v1 [stat.ML]) https://t.co/Xuk9rj0mGR,1,6," Abstract: When estimating finite mixture models, it is common to make assumptions on the mixture components, such as parametric assumptions. In this work, we make no distributional assumptions on the mixture components and instead assume that observations from the mixture model are grouped, such that observations in the same group are known to be drawn from the same mixture component. We precisely characterize the number of observations $n$ per group needed for the mixture model to be identifiable, as a function of the number $m$ of mixture components. In addition to our assumption-free analysis, we also study the settings where the mixture components are either linearly independent or jointly irreducible. Furthermore, our analysis considers two kinds of identifiability -- where the mixture model is the simplest one explaining the data, and where it is the only one. As an application of these results, we precisely characterize identifiability of multinomial mixture models. Our analysis relies on an operator-theoretic framework that associates mixture models in the grouped-sample setting with certain infinite-dimensional tensors. Based on this framework, we introduce general spectral algorithms for recovering the mixture components and illustrate their use on a synthetic data set. "
749789658872176641,2016-07-04 02:19:19,https://t.co/bA5iqJyVBU,Multi-class classification: mirror descent approach. (arXiv:1607.00076v1 [math.OC]) https://t.co/bA5iqJyVBU,1,5, Abstract: We consider the problem of multi-class classification and a stochastic opti- mization approach to it. We derive risk bounds for stochastic mirror descent algorithm and provide examples of set geometries that make the use of the algorithm efficient in terms of error in k. 
749789655260856320,2016-07-04 02:19:18,https://t.co/1dvjLAydYv,Provable Symmetric Nonnegative Matrix Factorization for Overlapping Clustering. (arXiv:1607.00084v1 [stat.ML]) https://t.co/1dvjLAydYv,0,2," Abstract: The problem of finding overlapping communities in networks has gained much attention recently. Algorithmic approaches often employ non-negative matrix factorization (NMF) or variants, while model-based approaches (such as the widely used mixed-membership stochastic blockmodel, or MMSB) assume a distribution over communities for each node and run standard inference techniques to recover these parameters. However, few of these approaches have provable consistency guarantees. We investigate the use of the symmetric NMF (or SNMF) for the MMSB model, and provide conditions under which an optimal SNMF algorithm can recover the MMSB parameters consistently. Since we are unaware of general-purpose optimal SNMF algorithms, we develop an SNMF variant, called GeoNMF, designed specifically for the MMSB model. GeoNMF is provably consistent, and experiments on both simulated and real-world datasets show its accuracy. "
749789651351773184,2016-07-04 02:19:17,https://t.co/MDjDQbQzJy,Randomized block proximal damped Newton method for composite self-concordant minimization. (arXiv:1607.00101v1 [ma… https://t.co/MDjDQbQzJy,0,3," Abstract: In this paper we consider the composite self-concordant (CSC) minimization problem, which minimizes the sum of a self-concordant function $f$ and a (possibly nonsmooth) proper closed convex function $g$. The CSC minimization is the cornerstone of the path-following interior point methods for solving a broad class of convex optimization problems. It has also found numerous applications in machine learning. The proximal damped Newton (PDN) methods have been well studied in the literature for solving this problem that enjoy a nice iteration complexity. Given that at each iteration these methods typically require evaluating or accessing the Hessian of $f$ and also need to solve a proximal Newton subproblem, the cost per iteration can be prohibitively high when applied to large-scale problems. Inspired by the recent success of block coordinate descent methods, we propose a randomized block proximal damped Newton (RBPDN) method for solving the CSC minimization. Compared to the PDN methods, the computational cost per iteration of RBPDN is usually significantly lower. The computational experiment on a class of regularized logistic regression problems demonstrate that RBPDN is indeed promising in solving large-scale CSC minimization problems. The convergence of RBPDN is also analyzed in the paper. In particular, we show that RBPDN is globally convergent when $g$ is Lipschitz continuous. It is also shown that RBPDN enjoys a local linear convergence. Moreover, we show that for a class of $g$ including the case where $g$ is Lipschitz differentiable, RBPDN enjoys a global linear convergence. As a striking consequence, it shows that the classical damped Newton methods [22,40] and the PDN [31] for such $g$ are globally linearly convergent, which was previously unknown in the literature. Moreover, this result can be used to sharpen the existing iteration complexity of these methods. "
749789648159924224,2016-07-04 02:19:17,https://t.co/tCR7No307M,Combining Gradient Boosting Machines with Collective Inference to Predict Continuous Values. (arXiv:1607.00110v1 [… https://t.co/tCR7No307M,3,10," Abstract: Gradient boosting of regression trees is a competitive procedure for learning predictive models of continuous data that fits the data with an additive non-parametric model. The classic version of gradient boosting assumes that the data is independent and identically distributed. However, relational data with interdependent, linked instances is now common and the dependencies in such data can be exploited to improve predictive performance. Collective inference is one approach to exploit relational correlation patterns and significantly reduce classification error. However, much of the work on collective learning and inference has focused on discrete prediction tasks rather than continuous. %target values has not got that attention in terms of collective inference. In this work, we investigate how to combine these two paradigms together to improve regression in relational domains. Specifically, we propose a boosting algorithm for learning a collective inference model that predicts a continuous target variable. In the algorithm, we learn a basic relational model, collectively infer the target values, and then iteratively learn relational models to predict the residuals. We evaluate our proposed algorithm on a real network dataset and show that it outperforms alternative boosting methods. However, our investigation also revealed that the relational features interact together to produce better predictions. "
749789644800270337,2016-07-04 02:19:16,https://t.co/aTOy3RCxPl,Deep Learning with Differential Privacy. (arXiv:1607.00133v1 [stat.ML]) https://t.co/aTOy3RCxPl,6,8," Abstract: Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality. "
749789641566552064,2016-07-04 02:19:15,https://t.co/TCZrd2rjnD,Missing Data Estimation in High-Dimensional Datasets: A Swarm Intelligence-Deep Neural Network Approach. (arXiv:16… https://t.co/TCZrd2rjnD,1,5," Abstract: In this paper, we examine the problem of missing data in high-dimensional datasets by taking into consideration the Missing Completely at Random and Missing at Random mechanisms, as well as theArbitrary missing pattern. Additionally, this paper employs a methodology based on Deep Learning and Swarm Intelligence algorithms in order to provide reliable estimates for missing data. The deep learning technique is used to extract features from the input data via an unsupervised learning approach by modeling the data distribution based on the input. This deep learning technique is then used as part of the objective function for the swarm intelligence technique in order to estimate the missing data after a supervised fine-tuning phase by minimizing an error function based on the interrelationship and correlation between features in the dataset. The investigated methodology in this paper therefore has longer running times, however, the promising potential outcomes justify the trade-off. Also, basic knowledge of statistics is presumed. "
749789638097862656,2016-07-04 02:19:14,https://t.co/eWXAF2DRso,Efficient and Consistent Robust Time Series Analysis. (arXiv:1607.00146v1 [cs.LG]) https://t.co/eWXAF2DRso,1,5," Abstract: We study the problem of robust time series analysis under the standard auto-regressive (AR) time series model in the presence of arbitrary outliers. We devise an efficient hard thresholding based algorithm which can obtain a consistent estimate of the optimal AR model despite a large fraction of the time series points being corrupted. Our algorithm alternately estimates the corrupted set of points and the model parameters, and is inspired by recent advances in robust regression and hard-thresholding methods. However, a direct application of existing techniques is hindered by a critical difference in the time-series domain: each point is correlated with all previous points rendering existing tools inapplicable directly. We show how to overcome this hurdle using novel proof techniques. Using our techniques, we are also able to provide the first efficient and provably consistent estimator for the robust regression problem where a standard linear observation model with white additive noise is corrupted arbitrarily. We illustrate our methods on synthetic datasets and show that our methods indeed are able to consistently recover the optimal parameters despite a large fraction of points being corrupted. "
749789634587131904,2016-07-04 02:19:13,https://t.co/X8XqMXtZiE,LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection. (arXiv:1607.00148v1 [cs.AI]) https://t.co/X8XqMXtZiE,6,10," Abstract: Mechanical devices such as engines, vehicles, aircrafts, etc., are typically instrumented with numerous sensors to capture the behavior and health of the machine. However, there are often external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable. For instance, manual controls and/or unmonitored environmental conditions or load may lead to inherently unpredictable time-series. Detecting anomalies in such scenarios becomes challenging using standard approaches based on mathematical models that rely on stationarity, or prediction models that utilize prediction errors to detect anomalies. We propose a Long Short Term Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) that learns to reconstruct 'normal' time-series behavior, and thereafter uses reconstruction error to detect anomalies. We experiment with three publicly available quasi predictable time-series datasets: power demand, space shuttle, and ECG, and two real-world engine datasets with both predictive and unpredictable behavior. We show that EncDec-AD is robust and can detect anomalies from predictable, unpredictable, periodic, aperiodic, and quasi-periodic time-series. Further, we show that EncDec-AD is able to detect anomalies from short time-series (length as small as 30) as well as long time-series (length as large as 500). "
749789631479152640,2016-07-04 02:19:13,https://t.co/6j7rGvD6AW,Why is Posterior Sampling Better than Optimism for Reinforcement Learning. (arXiv:1607.00215v1 [stat.ML]) https://t.co/6j7rGvD6AW,3,9," Abstract: Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\tilde{O}(H\sqrt{SAT})$ Bayesian expected regret bound for PSRL in finite-horizon episodic Markov decision processes, where $H$ is the horizon, $S$ is the number of states, $A$ is the number of actions and $T$ is the time elapsed. This improves upon the best previous bound of $\tilde{O}(H S \sqrt{AT})$ for any reinforcement learning algorithm. "
749789628727717890,2016-07-04 02:19:12,https://t.co/DHaVnDrbXa,A new analytical approach to consistency and overfitting in regularized empirical risk minimization. (arXiv:1607.0… https://t.co/DHaVnDrbXa,0,5," Abstract: This work considers the problem of binary classification: given training data $x_1, \dots, x_n$ from a certain population, together with associated labels $y_1,\dots, y_n \in \left\{0,1 \right\}$, determine the best label for an element $x$ not among the training data. More specifically, this work considers a variant of the regularized empirical risk functional which is defined intrinsically to the observed data and does not depend on the underlying population. Tools from modern analysis are used to obtain a concise proof of asymptotic consistency as regularization parameters are taken to zero at rates related to the size of the sample. These analytical tools give a new framework for understanding overfitting and underfitting, and rigorously connect the notion of overfitting with a loss of compactness. "
749789626215370753,2016-07-04 02:19:11,https://t.co/559sVhiUPO,Meaningful Models: Utilizing Conceptual Structure to Improve Machine Learning Interpretability. (arXiv:1607.00279v… https://t.co/559sVhiUPO,5,9," Abstract: The last decade has seen huge progress in the development of advanced machine learning models; however, those models are powerless unless human users can interpret them. Here we show how the mind's construction of concepts and meaning can be used to create more interpretable machine learning models. By proposing a novel method of classifying concepts, in terms of 'form' and 'function', we elucidate the nature of meaning and offer proposals to improve model understandability. As machine learning begins to permeate daily life, interpretable models may serve as a bridge between domain-expert authors and non-expert users. "
749789624298504192,2016-07-04 02:19:11,https://t.co/Qyie4dyXRX,A multilevel framework for sparse optimization with application to inverse covariance estimation and logistic regr… https://t.co/Qyie4dyXRX,0,3," Abstract: Solving l1 regularized optimization problems is common in the fields of computational biology, signal processing and machine learning. Such l1 regularization is utilized to find sparse minimizers of convex functions. A well-known example is the LASSO problem, where the l1 norm regularizes a quadratic function. A multilevel framework is presented for solving such l1 regularized sparse optimization problems efficiently. We take advantage of the expected sparseness of the solution, and create a hierarchy of problems of similar type, which is traversed in order to accelerate the optimization process. This framework is applied for solving two problems: (1) the sparse inverse covariance estimation problem, and (2) l1-regularized logistic regression. In the first problem, the inverse of an unknown covariance matrix of a multivariate normal distribution is estimated, under the assumption that it is sparse. To this end, an l1 regularized log-determinant optimization problem needs to be solved. This task is challenging especially for large-scale datasets, due to time and memory limitations. In the second problem, the l1-regularization is added to the logistic regression classification objective to reduce overfitting to the data and obtain a sparse model. Numerical experiments demonstrate the efficiency of the multilevel framework in accelerating existing iterative solvers for both of these problems. "
749789620381085697,2016-07-04 02:19:10,https://t.co/xo3b0fvlXT,Convergence Rate of Frank-Wolfe for Non-Convex Objectives. (arXiv:1607.00345v1 [math.OC]) https://t.co/xo3b0fvlXT,0,7," Abstract: We give a simple proof that the Frank-Wolfe algorithm obtains a stationary point at a rate of $O(1/\sqrt{t})$ on non-convex objectives with a Lipschitz continuous gradient. Our analysis is affine invariant and is the first, to the best of our knowledge, giving a similar rate to what was already proven for projected gradient methods (though on slightly different measures of stationarity). "
749789616841060357,2016-07-04 02:19:09,https://t.co/uoPCLXHziR,A scaled Bregman theorem with applications. (arXiv:1607.00360v1 [cs.LG]) https://t.co/uoPCLXHziR,0,2," Abstract: Bregman divergences play a central role in the design and analysis of a range of machine learning algorithms. This paper explores the use of Bregman divergences to establish reductions between such algorithms and their analyses. We present a new scaled isodistortion theorem involving Bregman divergences (scaled Bregman theorem for short) which shows that certain ""Bregman distortions'"" (employing a potentially non-convex generator) may be exactly re-written as a scaled Bregman divergence computed over transformed data. Admissible distortions include geodesic distances on curved manifolds and projections or gauge-normalisation, while admissible data include scalars, vectors and matrices. Our theorem allows one to leverage to the wealth and convenience of Bregman divergences when analysing algorithms relying on the aforementioned Bregman distortions. We illustrate this with three novel applications of our theorem: a reduction from multi-class density ratio to class-probability estimation, a new adaptive projection free yet norm-enforcing dual norm mirror descent algorithm, and a reduction from clustering on flat manifolds to clustering on curved manifolds. Experiments on each of these domains validate the analyses and suggest that the scaled Bregman theorem might be a worthy addition to the popular handful of Bregman divergence properties that have been pervasive in machine learning. "
749789613498204160,2016-07-04 02:19:08,https://t.co/MHgy2t5Bsz,Visualizing the Effects of a Changing Distance on Data Using Continuous Embeddings. (arXiv:1311.1911v3 [stat.ML] U… https://t.co/MHgy2t5Bsz,1,5," Abstract: Most Machine Learning (ML) methods, from clustering to classification, rely on a distance function to describe relationships between datapoints. For complex datasets it is hard to avoid making some arbitrary choices when defining a distance function. To compare images, one must choose a spatial scale, for signals, a temporal scale. The right scale is hard to pin down and it is preferable when results do not depend too tightly on the exact value one picked. Topological data analysis seeks to address this issue by focusing on the notion of neighbourhood instead of distance. It is shown that in some cases a simpler solution is available. It can be checked how strongly distance relationships depend on a hyperparameter using dimensionality reduction. A variant of dynamical multi-dimensional scaling (MDS) is formulated, which embeds datapoints as curves. The resulting algorithm is based on the Concave-Convex Procedure (CCCP) and provides a simple and efficient way of visualizing changes and invariances in distance patterns as a hyperparameter is varied. A variant to analyze the dependence on multiple hyperparameters is also presented. A cMDS algorithm that is straightforward to implement, use and extend is provided. To illustrate the possibilities of cMDS, cMDS is applied to several real-world data sets. "
749789608620195841,2016-07-04 02:19:07,https://t.co/YSr2LJUq0b,Adaptive Ensemble Learning with Confidence Bounds. (arXiv:1512.07446v2 [cs.LG] UPDATED) https://t.co/YSr2LJUq0b,1,5," Abstract: Extracting actionable intelligence from distributed, heterogeneous, correlated and high-dimensional data sources requires run-time processing and learning both locally and globally. In the last decade, a large number of meta-learning techniques have been proposed in which local learners make online predictions based on their locally-collected data instances, and feed these predictions to an ensemble learner, which fuses them and issues a global prediction. However, most of these works do not provide performance guarantees or, when they do, these guarantees are asymptotic. None of these existing works provide confidence estimates about the issued predictions or rate of learning guarantees for the ensemble learner. In this paper, we provide a systematic ensemble learning method called Hedged Bandits, which comes with both long run (asymptotic) and short run (rate of learning) performance guarantees. Moreover, our approach yields performance guarantees with respect to the optimal local prediction strategy, and is also able to adapt its predictions in a data-driven manner. We illustrate the performance of Hedged Bandits in the context of medical informatics and show that it outperforms numerous online and offline ensemble learning methods. "
749764222821756928,2016-07-04 00:38:15,https://t.co/XcpN10f3jm,A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model Evaluation. (arXiv:1602.03253v2 [stat.ML] UPDAT… https://t.co/XcpN10f3jm,0,3," Abstract: We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein's identity with the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly. "
749764220984684544,2016-07-04 00:38:14,https://t.co/A6Ld3CYUQT,Deep Exploration via Bootstrapped DQN. (arXiv:1602.04621v2 [cs.LG] UPDATED) https://t.co/A6Ld3CYUQT,2,11," Abstract: Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games. "
749764218044448768,2016-07-04 00:38:14,https://t.co/vkHzVO1QQg,ROCS-Derived Features for Virtual Screening. (arXiv:1606.01822v2 [stat.ML] UPDATED) https://t.co/vkHzVO1QQg,0,2," Abstract: Rapid overlay of chemical structures (ROCS) is a standard tool for the calculation of 3D shape and chemical (""color"") similarity. ROCS uses unweighted sums to combine many aspects of similarity, yielding parameter-free models for virtual screening. In this report, we decompose the ROCS color force field into ""color components"" and ""color atom overlaps"", novel color similarity features that can be weighted in a system-specific manner by machine learning algorithms. In cross-validation experiments, these additional features significantly improve virtual screening performance (ROC AUC scores) relative to standard ROCS. "
748680558067249153,2016-07-01 00:52:09,https://t.co/7iTBYiXWct,Learning without Forgetting. (arXiv:1606.09282v1 [cs.CV]) https://t.co/7iTBYiXWct,1,14," Abstract: When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning as standard practice for improved new task performance. "
748680554858545152,2016-07-01 00:52:08,https://t.co/L2vykCWvOP,Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. (arXiv:1606.09375v1 [cs.LG]) https://t.co/L2vykCWvOP,1,11," Abstract: In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs. "
748680551473741830,2016-07-01 00:52:07,https://t.co/s2uHDWa7W7,Asymptotically Optimal Algorithms for Multiple Play Bandits with Partial Feedback. (arXiv:1606.09388v1 [stat.ML]) https://t.co/s2uHDWa7W7,0,7," Abstract: We study a variant of the multi-armed bandit problem with multiple plays in which the user wishes to sample the m out of k arms with the highest expected rewards, but at any given time can only sample l $\le$ m arms. When l = m, Thompson sampling was recently shown to be asymptotically efficient. We derive an asymptotic regret lower bound for any uniformly efficient algorithm in our new setting where may be less than m. We then establish the asymptotic optimality of Thompson sampling for Bernoulli rewards, where our proof technique differs from earlier methods even when l = m. We also prove the asymptotic optimality of an algorithm based on upper confidence bounds, KL-CUCB, for single-parameter exponential families and bounded, finitely supported rewards, a result which is new for all values of l. "
748680547992494080,2016-07-01 00:52:07,https://t.co/zumaDmD6wy,Vote-boosting ensembles. (arXiv:1606.09458v1 [cs.LG]) https://t.co/zumaDmD6wy,0,4," Abstract: Vote-boosting is a sequential ensemble learning method in which individual classifiers are built on different weighted versions of the training data. To build a new classifier, the weight of each training instance is determined as a function of the disagreement rate of the current ensemble predictions for that particular instance. Experiments using the symmetric beta distribution as the emphasis function and different base learners are used to illustrate the properties and to analyze the performance of these types of ensembles. In classification problems with low or no class-label noise, when simple base learners are used, vote-boosting behaves as if it were an interpolation between bagging and standard boosting (e.g. AdaBoost), depending on the value of the shape parameter of the beta distribution. In terms of predictive accuracy the best results, which are comparable or better than random forests, are obtained with vote-boosting ensembles of random trees. "
748680545228492800,2016-07-01 00:52:06,https://t.co/xaODwEVfkW,A Model Explanation System: Latest Updates and Extensions. (arXiv:1606.09517v1 [stat.ML]) https://t.co/xaODwEVfkW,0,6," Abstract: We propose a general model explanation system (MES) for ""explaining"" the output of black box classifiers. This paper describes extensions to Turner (2015), which is referred to frequently in the text. We use the motivating example of a classifier trained to detect fraud in a credit card transaction history. The key aspect is that we provide explanations applicable to a single prediction, rather than provide an interpretable set of parameters. We focus on explaining positive predictions (alerts). However, the presented methodology is symmetrically applicable to negative predictions. "
748680541688459269,2016-07-01 00:52:05,https://t.co/koZXxrBplK,A Permutation-based Model for Crowd Labeling: Optimal Estimation and Robustness. (arXiv:1606.09632v1 [cs.LG]) https://t.co/koZXxrBplK,1,3," Abstract: The aggregation and denoising of crowd labeled data is a task that has gained increased significance with the advent of crowdsourcing platforms and massive datasets. In this paper, we propose a permutation-based model for crowd labeled data that is a significant generalization of the common Dawid-Skene model, and introduce a new error metric by which to compare different estimators. Working in a high-dimensional non-asymptotic framework that allows both the number of workers and tasks to scale, we derive optimal rates of convergence for the permutation-based model. We show that the permutation-based model offers significant robustness in estimation due to its richness, while surprisingly incurring only a small additional statistical penalty as compared to the Dawid-Skene model. Finally, we propose a computationally-efficient method, called the OBI-WAN estimator, that is uniformly optimal over a class intermediate between the permutation-based and the Dawid-Skene models, and is uniformly consistent over the entire permutation-based model class. In contrast, the guarantees for estimators available in prior literature are sub-optimal over the original Dawid-Skene model. "
748680539025080321,2016-07-01 00:52:04,https://t.co/YbNdV1QVxq,An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes. (arXiv:1509.01520v3 [cs.CV]… https://t.co/YbNdV1QVxq,0,6," Abstract: Object tracking is an ubiquitous problem that appears in many applications such as remote sensing, audio processing, computer vision, human-machine interfaces, human-robot interaction, etc. Although thoroughly investigated in computer vision, tracking a time-varying number of persons remains a challenging open problem. In this paper, we propose an on-line variational Bayesian model for multi-person tracking from cluttered visual observations provided by person detectors. The contributions of this paper are the followings. First, we propose a variational Bayesian framework for tracking an unknown and varying number of persons. Second, our model results in a variational expectation-maximization (VEM) algorithm with closed-form expressions for the posterior distributions of the latent variables and for the estimation of the model parameters. Third, the proposed model exploits observations from multiple detectors, and it is therefore multimodal by nature. Finally, we propose to embed both object-birth and object-visibility processes in an effort to robustly handle person appearances and disappearances over time. Evaluated on classical multiple person tracking datasets, our method shows competitive results with respect to state-of-the-art multiple-object tracking models, such as the probability hypothesis density (PHD) filter among others. "
748680536160362496,2016-07-01 00:52:04,https://t.co/3OyaLDujBe,Ensemble Robustness of Deep Learning Algorithms. (arXiv:1602.02389v3 [cs.LG] UPDATED) https://t.co/3OyaLDujBe,3,7," Abstract: The question why deep learning algorithms perform so well in practice has attracted increasing research interest. However, most of well-established approaches, such as hypothesis capacity, robustness or sparseness, have not provided complete explanations, due to the high complexity of the deep learning algorithms and their inherent randomness. In this work, we introduce a new approach~\textendash~ensemble robustness~\textendash~towards characterizing the generalization performance of generic deep learning algorithms. Ensemble robustness concerns robustness of the \emph{population} of the hypotheses that may be output by a learning algorithm. Through the lens of ensemble robustness, we reveal that a stochastic learning algorithm can generalize well as long as its sensitiveness to adversarial perturbation is bounded in average, or equivalently, the performance variance of the algorithm is small. Quantifying ensemble robustness of various deep learning algorithms may be difficult analytically. However, extensive simulations for seven common deep learning algorithms for different network architectures provide supporting evidence for our claims. Furthermore, our work explains the good performance of several published deep learning algorithms. "
748680534260322304,2016-07-01 00:52:03,https://t.co/lOIycdftKx,Parsimonious modeling with Information Filtering Networks. (arXiv:1602.07349v2 [cs.IT] UPDATED) https://t.co/lOIycdftKx,0,2," Abstract: We introduce a methodology to construct parsimonious probabilistic models. This method makes use of Information Filtering Networks to produce a robust estimate of the global sparse inverse covariance from a simple sum of local inverse covariances computed on small sub-parts of the network. Being based on local and low-dimensional inversions, this method is computationally very efficient and statistically robust even for the estimation of inverse covariance of high-dimensional, noisy and short time-series. Applied to financial data our method results computationally more efficient than state-of-the-art methodologies such as Glasso producing, in a fraction of the computation time, models that can have equivalent or better performances but with a sparser inference structure. We also discuss performances with sparse factor models where we notice that relative performances decrease with the number of factors. The local nature of this approach allows us to perform computations in parallel and provides a tool for dynamical adaptation by partial updating when the properties of some variables change without the need of recomputing the whole model. This makes this approach particularly suitable to handle big datasets with large numbers of variables. Examples of practical application for forecasting, stress testing and risk allocation in financial systems are also provided. "
748680532209332224,2016-07-01 00:52:03,https://t.co/ouMUL9iKEO,Character-based Neural Machine Translation. (arXiv:1603.00810v3 [cs.CL] UPDATED) https://t.co/ouMUL9iKEO,0,4," Abstract: Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task. "
748680530179260421,2016-07-01 00:52:02,https://t.co/6L4kghGm6N,Cascading Bandits for Large-Scale Recommendation Problems. (arXiv:1603.05359v2 [cs.LG] UPDATED) https://t.co/6L4kghGm6N,1,5," Abstract: Most recommender systems recommend a list of items. The user examines the list, from the first item to the last, and often chooses the first attractive item and does not examine the rest. This type of user behavior can be modeled by the cascade model. In this work, we study cascading bandits, an online learning variant of the cascade model where the goal is to recommend $K$ most attractive items from a large set of $L$ candidate items. We propose two algorithms for solving this problem, which are based on the idea of linear generalization. The key idea in our solutions is that we learn a predictor of the attraction probabilities of items from their features, as opposing to learning the attraction probability of each item independently as in the existing work. This results in practical learning algorithms whose regret does not depend on the number of items $L$. We bound the regret of one algorithm and comprehensively evaluate the other on a range of recommendation problems. The algorithm performs well and outperforms all baselines. "
748680524454105088,2016-07-01 00:52:01,https://t.co/Rbyc1Rz5I2,Predicting Student Dropout in Higher Education. (arXiv:1606.06364v2 [stat.ML] UPDATED) https://t.co/Rbyc1Rz5I2,5,4," Abstract: Each year, roughly 30% of first-year students at US baccalaureate institutions do not return for their second year and over $9 billion is spent educating these students. Yet, little quantitative research has analyzed the causes and possible remedies for student attrition. Here, we describe initial efforts to model student dropout using the largest known dataset on higher education attrition, which tracks over 32,500 students' demographics and transcript records at one of the nation's largest public universities. Our results highlight several early indicators of student attrition and show that dropout can be accurately predicted even when predictions are based on a single term of academic transcript data. These results highlight the potential for machine learning to have an impact on student retention and success while pointing to several promising directions for future work. "
748317395761176576,2016-06-30 00:49:04,https://t.co/TeSaJeIcr7,Tracking Switched Dynamic Network Topologies from Information Cascades. (arXiv:1606.08882v1 [stat.ML]) https://t.co/TeSaJeIcr7,0,4," Abstract: Contagions such as the spread of popular news stories, or infectious diseases, propagate in cascades over dynamic networks with unobservable topologies. However, ""social signals"" such as product purchase time, or blog entry timestamps are measurable, and implicitly depend on the underlying topology, making it possible to track it over time. Interestingly, network topologies often ""jump"" between discrete states that may account for sudden changes in the observed signals. The present paper advocates a switched dynamic structural equation model to capture the topology-dependent cascade evolution, as well as the discrete states driving the underlying topologies. Conditions under which the proposed switched model is identifiable are established. Leveraging the edge sparsity inherent to social networks, a recursive $\ell_1$-norm regularized least-squares estimator is put forth to jointly track the states and network topologies. An efficient first-order proximal-gradient algorithm is developed to solve the resulting optimization problem. Numerical experiments on both synthetic data and real cascades measured over the span of one year are conducted, and test results corroborate the efficacy of the advocated approach. "
748317392292446209,2016-06-30 00:49:04,https://t.co/Ee7tHrJMYR,Alternating Estimation for Structured High-Dimensional Multi-Response Models. (arXiv:1606.08957v1 [stat.ML]) https://t.co/Ee7tHrJMYR,0,2," Abstract: We consider learning high-dimensional multi-response linear models with structured parameters. By exploiting the noise correlations among responses, we propose an alternating estimation (AltEst) procedure to estimate the model parameters based on the generalized Dantzig selector. Under suitable sample size and resampling assumptions, we show that the error of the estimates generated by AltEst, with high probability, converges linearly to certain minimum achievable level, which can be tersely expressed by a few geometric measures, such as Gaussian width of sets related to the parameter structure. To the best of our knowledge, this is the first non-asymptotic statistical guarantee for such AltEst-type algorithm applied to estimation problem with general structures. "
748317388920262656,2016-06-30 00:49:03,https://t.co/YvzYvAsggR,Non-linear Label Ranking for Large-scale Prediction of Long-Term User Interests. (arXiv:1606.08963v1 [cs.AI]) https://t.co/YvzYvAsggR,1,4," Abstract: We consider the problem of personalization of online services from the viewpoint of ad targeting, where we seek to find the best ad categories to be shown to each user, resulting in improved user experience and increased advertisers' revenue. We propose to address this problem as a task of ranking the ad categories depending on a user's preference, and introduce a novel label ranking approach capable of efficiently learning non-linear, highly accurate models in large-scale settings. Experiments on a real-world advertising data set with more than 3.2 million users show that the proposed algorithm outperforms the existing solutions in terms of both rank loss and top-K retrieval performance, strongly suggesting the benefit of using the proposed model on large-scale ranking problems. "
748317386093256704,2016-06-30 00:49:02,https://t.co/9XNmhvcdSf,Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach. (arXiv:1606.09066v1 [stat.ML]) https://t.co/9XNmhvcdSf,2,4," Abstract: Tree ensembles such as random forests and boosted trees are renowned for their high prediction performance; however, their interpretability is critically limited. One way of interpreting a complex tree ensemble is to obtain its simplified representation, which is formalized as a model selection problem: Given a complex tree ensemble, we want to obtain the simplest representation that is essentially equivalent to the original one. To this end, we derive a Bayesian model selection algorithm. Our approach has three appealing features: the prediction performance is maintained, the coverage is sufficiently large, and the computation is reasonably feasible. Our synthetic data experiment and real world data applications show that complicated tree ensembles are approximated reasonably as interpretable. "
748317382255476736,2016-06-30 00:49:01,https://t.co/bnjILwmkhs,Accelerated first-order primal-dual proximal methods for linearly constrained composite convex programming. (arXiv… https://t.co/bnjILwmkhs,0,3," Abstract: Motivated by big data applications, first-order methods have been extremely popular in recent years. However, naive gradient methods generally converge slowly. Hence, much efforts have been made to accelerate various first-order methods. This paper proposes two accelerated methods towards solving structured linearly constrained convex programming, for which we assume composite convex objective. The first method is the accelerated linearized augmented Lagrangian method (LALM). At each update to the primal variable, it allows linearization to the differentiable function and also the augmented term, and thus it enables easy subproblems. Assuming merely weak convexity, we show that LALM owns $O(1/t)$ convergence if parameters are kept fixed during all the iterations and can be accelerated to $O(1/t^2)$ if the parameters are adapted, where $t$ is the number of total iterations. The second method is the accelerated linearized alternating direction method of multipliers (LADMM). In addition to the composite convexity, it further assumes two-block structure on the objective. Different from classic ADMM, our method allows linearization to the objective and also augmented term to make the update simple. Assuming strong convexity on one block variable, we show that LADMM also enjoys $O(1/t^2)$ convergence with adaptive parameters. This result is a significant improvement over that in [Goldstein et. al, SIIMS'14], which requires strong convexity on both block variables and no linearization to the objective or augmented term. Numerical experiments are performed on quadratic programming, image denoising, and support vector machine. The proposed accelerated methods are compared to nonaccelerated ones and also existing accelerated methods. The results demonstrate the validness of acceleration and superior performance of the proposed methods over existing ones. "
748317377838854144,2016-06-30 00:49:00,https://t.co/GA0apNwkUS,Optimising The Input Window Alignment in CD-DNN Based Phoneme Recognition for Low Latency Processing. (arXiv:1606.… https://t.co/GA0apNwkUS,1,3," Abstract: We present a systematic analysis on the performance of a phonetic recogniser when the window of input features is not symmetric with respect to the current frame. The recogniser is based on Context Dependent Deep Neural Networks (CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the latency of the system by reducing the number of future feature frames required to estimate the current output. Our tests performed on the TIMIT database show that the performance does not degrade when the input window is shifted up to 5 frames in the past compared to common practice (no future frame). This corresponds to improving the latency by 50 ms in our settings. Our tests also show that the best results are not obtained with the symmetric window commonly employed, but with an asymmetric window with eight past and two future context frames, although this observation should be confirmed on other data sets. The reduction in latency suggested by our results is critical for specific applications such as real-time lip synchronisation for tele-presence, but may also be beneficial in general applications to improve the lag in human-machine spoken interaction. "
748317375414558720,2016-06-30 00:49:00,https://t.co/6LVnKfbHXo,Disease Trajectory Maps. (arXiv:1606.09184v1 [stat.ML]) https://t.co/6LVnKfbHXo,0,2," Abstract: Medical researchers are coming to appreciate that many diseases are in fact complex, heterogeneous syndromes composed of subpopulations that express different variants of a related complication. Time series data extracted from individual electronic health records (EHR) offer an exciting new way to study subtle differences in the way these diseases progress over time. In this paper, we focus on answering two questions that can be asked using these databases of time series. First, we want to understand whether there are individuals with similar disease trajectories and whether there are a small number of degrees of freedom that account for differences in trajectories across the population. Second, we want to understand how important clinical outcomes are associated with disease trajectories. To answer these questions, we propose the Disease Trajectory Map (DTM), a novel probabilistic model that learns low-dimensional representations of sparse and irregularly sampled time series. We propose a stochastic variational inference algorithm for learning the DTM that allows the model to scale to large modern medical datasets. To demonstrate the DTM, we analyze data collected on patients with the complex autoimmune disease, scleroderma. We find that DTM learns meaningful representations of disease trajectories and that the representations are significantly associated with important clinical outcomes. "
748317372117884928,2016-06-30 00:48:59,https://t.co/O9jOo9IMKa,Zero Shot Learning for Semantic Boundary Detection - How Far Can We Get?. (arXiv:1606.09187v1 [cs.CV]) https://t.co/O9jOo9IMKa,0,4," Abstract: Semantic boundary and edge detection aims at simultaneously detecting object edge pixels in images and assigning class labels to them. Systematic training of predictors for this task requires the labeling of edges in images which is a particularly tedious task. We propose a novel strategy for solving this task in an almost zero-shot manner by relying on conventional whole image neural net classifiers that were trained using large bounding boxes. Our method performs the following two steps at test time. First it predicts the class labels by applying the trained whole image network to the test images. Second it computes pixel-wise scores from the obtained predictions by applying backprop gradients as well as recent visualization algorithms such as deconvolution and layer-wise relevance propagation. We show that high pixel-wise scores are indicative for the location of semantic boundaries, which suggests that the semantic boundary problem can be approached without using edge labels during the training phase. "
748317368749809664,2016-06-30 00:48:58,https://t.co/06MY04saW2,A Semi-Definite Programming approach to low dimensional embedding for unsupervised clustering. (arXiv:1606.09190v1… https://t.co/06MY04saW2,1,5, Abstract: This paper proposes a variant of the method of Gu\'edon and Verhynin for estimating the cluster matrix in the Mixture of Gaussians framework via Semi-Definite Programming. A clustering oriented embedding is deduced from this estimate. The procedure is suitable for very high dimensional data because it is based on pairwise distances only. Theoretical garantees are provided and an eigenvalue optimisation approach is proposed for computing the embedding. The performance of the method is illustrated via Monte Carlo experiements and comparisons with other embeddings from the literature. 
748317364563902464,2016-06-30 00:48:57,https://t.co/zHGeBRgdke,Small coherence implies the weak Null Space Property. (arXiv:1606.09193v1 [math.ST]) https://t.co/zHGeBRgdke,0,2," Abstract: In the Compressed Sensing community, it is well known that given a matrix $X \in \mathbb R^{n\times p}$ with $\ell_2$ normalized columns, the Restricted Isometry Property (RIP) implies the Null Space Property (NSP). It is also well known that a small Coherence $\mu$ implies a weak RIP, i.e. the singular values of $X_T$ lie between $1-\delta$ and $1+\delta$ for ""most"" index subsets $T \subset \{1,\ldots,p\}$ with size governed by $\mu$ and $\delta$. In this short note, we show that a small Coherence implies a weak Null Space Property, i.e. $\Vert h_T\Vert_2 \le C \ \Vert h_{T^c}\Vert_1/\sqrt{s}$ for most $T \subset \{1,\ldots,p\}$ with cardinality $|T|\le s$. We moreover prove some singular value perturbation bounds that may also prove useful for other applications. "
748317360851927040,2016-06-30 00:48:56,https://t.co/JZKir8mEM9,Tighter bounds lead to improved classifiers. (arXiv:1606.09202v1 [cs.LG]) https://t.co/JZKir8mEM9,0,3," Abstract: The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints. "
748317357215522816,2016-06-30 00:48:55,https://t.co/Q1XnRBxPXr,A bag-of-paths framework for network data analysis. (arXiv:1302.6766v2 [stat.ML] UPDATED) https://t.co/Q1XnRBxPXr,0,3," Abstract: This work develops a generic framework, called the bag-of-paths (BoP), for link and network data analysis. The central idea is to assign a probability distribution on the set of paths in a network. More precisely, a Gibbs-Boltzmann distribution is defined over a bag of paths in a network, that is, on a representation that considers all paths independently. We show that, under this distribution, the probability of drawing a path connecting two nodes can easily be computed in closed form by simple matrix inversion. This probability captures a notion of relatedness between nodes of the graph: two nodes are considered as highly related when they are connected by many, preferably low-cost, paths. As an application, two families of distances between nodes are derived from the BoP probabilities. Interestingly, the second distance family interpolates between the shortest path distance and the resistance distance. In addition, it extends the Bellman-Ford formula for computing the shortest path distance, in order to integrate sub-optimal paths by simply replacing the minimum operator by the soft minimum operator, therefore providing an optimal trade-off between exploitation and exploration. Experimental results on semi-supervised classification show that both of the new distance families are competitive with other state-of-the-art approaches. In addition to the distance measures studied in this paper, the bag-of-paths framework enables straightforward computation of many other relevant network measures. "
748317353117626368,2016-06-30 00:48:54,https://t.co/P6pK4s5CSc,Blind Source Separation Algorithms Using Hyperbolic and Givens Rotations for High-Order QAM Constellations. (arXiv… https://t.co/P6pK4s5CSc,0,3," Abstract: This paper addresses the problem of blind demixing of instantaneous mixtures in a multiple-input multiple-output communication system. The main objective is to present efficient blind source separation (BSS) algorithms dedicated to moderate or high-order QAM constellations. Four new iterative batch BSS algorithms are presented dealing with the multimodulus (MM) and alphabet matched (AM) criteria. For the optimization of these cost functions, iterative methods of Givens and hyperbolic rotations are used. A pre-whitening operation is also utilized to reduce the complexity of design problem. It is noticed that the designed algorithms using Givens rotations gives satisfactory performance only for large number of samples. However, for small number of samples, the algorithms designed by combining both Givens and hyperbolic rotations compensate for the ill-whitening that occurs in this case and thus improves the performance. Two algorithms dealing with the MM criterion are presented for moderate order QAM signals such as 16-QAM. The other two dealing with the AM criterion are presented for high-order QAM signals. These methods are finally compared with the state of art batch BSS algorithms in terms of signal-to-interference and noise ratio, symbol error rate and convergence rate. Simulation results show that the proposed methods outperform the contemporary batch BSS algorithms. "
748317350298947584,2016-06-30 00:48:54,https://t.co/R1JLmb8X26,Mapping Heritability of Large-Scale Brain Networks with a Billion Connections {\em via} Persistent Homology. (arXi… https://t.co/R1JLmb8X26,0,3," Abstract: In many human brain network studies, we do not have sufficient number (n) of images relative to the number (p) of voxels due to the prohibitively expensive cost of scanning enough subjects. Thus, brain network models usually suffer the small-n large-p problem. Such a problem is often remedied by sparse network models, which are usually solved numerically by optimizing L1-penalties. Unfortunately, due to the computational bottleneck associated with optimizing L1-penalties, it is not practical to apply such methods to construct large-scale brain networks at the voxel-level. In this paper, we propose a new scalable sparse network model using cross-correlations that bypass the computational bottleneck. Our model can build sparse brain networks at the voxel level with p > 25000. Instead of using a single sparse parameter that may not be optimal in other studies and datasets, the computational speed gain enables us to analyze the collection of networks at every possible sparse parameter in a coherent mathematical framework via persistent homology. The method is subsequently applied in determining the extent of heritability on a functional brain network at the voxel-level for the first time using twin fMRI. "
748317345865732096,2016-06-30 00:48:52,https://t.co/Bt0Q1fjxH5,Universal Collaboration Strategies for Signal Detection: A Sparse Learning Approach. (arXiv:1601.06201v2 [cs.LG] U… https://t.co/Bt0Q1fjxH5,1,6," Abstract: This paper considers the problem of high dimensional signal detection in a large distributed network whose nodes can collaborate with their one-hop neighboring nodes (spatial collaboration). We assume that only a small subset of nodes communicate with the Fusion Center (FC). We design optimal collaboration strategies which are universal for a class of deterministic signals. By establishing the equivalence between the collaboration strategy design problem and sparse PCA, we solve the problem efficiently and evaluate the impact of collaboration on detection performance. "
748317342472495104,2016-06-30 00:48:52,https://t.co/UZ6pPlB5ZV,Nested Mini-Batch K-Means. (arXiv:1602.02934v4 [stat.ML] UPDATED) https://t.co/UZ6pPlB5ZV,0,2," Abstract: A new algorithm is proposed which accelerates the mini-batch k-means algorithm of Sculley (2010) by using the distance bounding approach of Elkan (2003). We argue that, when incorporating distance bounds into a mini-batch algorithm, already used data should preferentially be reused. To this end we propose using nested mini-batches, whereby data in a mini-batch at iteration t is automatically reused at iteration t+1. Using nested mini-batches presents two difficulties. The first is that unbalanced use of data can bias estimates, which we resolve by ensuring that each data sample contributes exactly once to centroids. The second is in choosing mini-batch sizes, which we address by balancing premature fine-tuning of centroids with redundancy induced slow-down. Experiments show that the resulting nmbatch algorithm is very effective, often arriving within 1% of the empirical minimum 100 times earlier than the standard mini-batch algorithm. "
748317339297386496,2016-06-30 00:48:51,https://t.co/OrFGN83WGZ,Sub-Gaussian estimators of the mean of a random matrix with heavy-tailed entries. (arXiv:1605.07129v3 [math.ST] UP… https://t.co/OrFGN83WGZ,0,2," Abstract: Estimation of the covariance matrix has attracted a lot of attention of the statistical research community over the years, partially due to important applications such as Principal Component Analysis. However, frequently used empirical covariance estimator (and its modifications) is very sensitive to outliers in the data. As P. J. Huber wrote in 1964, ""...This raises a question which could have been asked already by Gauss, but which was, as far as I know, only raised a few years ago (notably by Tukey): what happens if the true distribution deviates slightly from the assumed normal one? As is now well known, the sample mean then may have a catastrophically bad performance..."" Motivated by this question, we develop a new estimator of the (element-wise) mean of a random matrix, which includes covariance estimation problem as a special case. Assuming that the entries of a matrix possess only finite second moment, this new estimator admits sub-Gaussian or sub-exponential concentration around the unknown mean in the operator norm. We will explain the key ideas behind our construction, as well as applications to covariance estimation and matrix completion problems. "
748317335711260672,2016-06-30 00:48:50,https://t.co/YWu1Kj3VUo,Network Lasso Optimization For Smart City Ride Share Prediction. (arXiv:1606.03276v2 [cs.CY] UPDATED) https://t.co/YWu1Kj3VUo,1,3," Abstract: Ride sharing has important implications in terms of environmental, social and individual goals by reducing carbon footprints, fostering social interactions and economizing commuter costs. The ride sharing systems that are commonly available lack adaptive and scalable techniques that can simultaneously learn from the large scale data and predict in real-time dynamic fashion. In this paper, we study such a problem towards a smart city initiative, where a generic ride sharing system is conceived capable of making predictions about ride share opportunities based on the historically recorded data while satisfying real-time ride requests. Underpinning the system is an application of a powerful machine learning convex optimization framework called Network Lasso that uses the Alternate Direction Method of Multipliers (ADMM) optimization for learning and dynamic prediction. We propose an application of a robust and scalable unified optimization framework within the ride sharing case-study. The application of Network Lasso framework is capable of jointly optimizing and clustering different rides based on their spatial and model similarity. The prediction from the framework clusters new ride requests, making accurate price prediction based on the clusters, detecting hidden correlations in the data and allowing fast convergence due to the network topology. We provide an empirical evaluation of the application of ADMM network Lasso on real trip record and simulated data, proving their effectiveness since the mean squared error of the algorithm's prediction is minimized on the test rides. "
748317328652238848,2016-06-30 00:48:48,https://t.co/zbno8xGRh2,"Dialog state tracking, a machine reading approach using a memory-enhanced neural network. (arXiv:1606.04052v3 [cs.… https://t.co/zbno8xGRh2",1,4," Abstract: In an end-to-end dialog system, the aim of dialog state tracking is to accurately estimate a compact representation of the current dialog status from a sequence of noisy observations produced by the speech recognition and the natural language understanding modules. This paper introduces a novel method of dialog state tracking based on the general paradigm of machine reading and proposes to solve it using an End-to-End Memory Network, MemN2N, a memory-enhanced neural network architecture. We evaluate the proposed approach on the second Dialog State Tracking Challenge (DSTC-2) dataset. The corpus has been converted for the occasion in order to frame the hidden state variable inference as a question-answering task based on a sequence of utterances extracted from a dialog. We show that the proposed tracker gives encouraging results. Then, we propose to extend the DSTC-2 dataset with specific reasoning capabilities requirement like counting, list maintenance, yes-no question answering and indefinite knowledge management. Finally, we present encouraging results using our proposed MemN2N based tracking model. "
747953665068113922,2016-06-29 00:43:44,https://t.co/OAzImO3Vl6,Anomaly detection in video with Bayesian nonparametrics. (arXiv:1606.08455v1 [stat.ML]) https://t.co/OAzImO3Vl6,2,12, Abstract: A novel dynamic Bayesian nonparametric topic model for anomaly detection in video is proposed in this paper. Batch and online Gibbs samplers are developed for inference. The paper introduces a new abnormality measure for decision making. The proposed method is evaluated on both synthetic and real data. The comparison with a non-dynamic model shows the superiority of the proposed dynamic one in terms of the classification performance for anomaly detection. 
747953661976973312,2016-06-29 00:43:43,https://t.co/vHyIcdYcKp,Dynamic Hierarchical Dirichlet Process for Abnormal Behaviour Detection in Video. (arXiv:1606.08476v1 [stat.ML]) https://t.co/vHyIcdYcKp,3,11," Abstract: This paper proposes a novel dynamic Hierarchical Dirichlet Process topic model that considers the dependence between successive observations. Conventional posterior inference algorithms for this kind of models require processing of the whole data through several passes. It is computationally intractable for massive or sequential data. We design the batch and online inference algorithms, based on the Gibbs sampling, for the proposed model. It allows to process sequential data, incrementally updating the model by a new observation. The model is applied to abnormal behaviour detection in video sequences. A new abnormality measure is proposed for decision making. The proposed method is compared with the method based on the non- dynamic Hierarchical Dirichlet Process, for which we also derive the online Gibbs sampler and the abnormality measure. The results with synthetic and real data show that the consideration of the dynamics in a topic model improves the classification performance for abnormal behaviour detection. "
747953659196149760,2016-06-29 00:43:43,https://t.co/wl3ybgvaHg,A Learning Algorithm for Relational Logistic Regression: Preliminary Results. (arXiv:1606.08531v1 [cs.AI]) https://t.co/wl3ybgvaHg,2,4," Abstract: Relational logistic regression (RLR) is a representation of conditional probability in terms of weighted formulae for modelling multi-relational data. In this paper, we develop a learning algorithm for RLR models. Learning an RLR model from data consists of two steps: 1- learning the set of formulae to be used in the model (a.k.a. structure learning) and learning the weight of each formula (a.k.a. parameter learning). For structure learning, we deploy Schmidt and Murphy's hierarchical assumption: first we learn a model with simple formulae, then more complex formulae are added iteratively only if all their sub-formulae have proven effective in previous learned models. For parameter learning, we convert the problem into a non-relational learning problem and use an off-the-shelf logistic regression learning algorithm from Weka, an open-source machine learning tool, to learn the weights. We also indicate how hidden features about the individuals can be incorporated into RLR to boost the learning performance. We compare our learning algorithm to other structure and parameter learning algorithms in the literature, and compare the performance of RLR models to standard logistic regression and RDN-Boost on a modified version of the MovieLens data-set. "
747953655874207744,2016-06-29 00:43:42,https://t.co/Qo91cP34LS,A Local Density-Based Approach for Local Outlier Detection. (arXiv:1606.08538v1 [cs.AI]) https://t.co/Qo91cP34LS,0,2," Abstract: This paper presents a simple but effective density-based outlier detection approach with the local kernel density estimation (KDE). A Relative Density-based Outlier Score (RDOS) is introduced to measure the local outlierness of objects, in which the density distribution at the location of an object is estimated with a local KDE method based on extended nearest neighbors of the object. Instead of using only $k$ nearest neighbors, we further consider reverse nearest neighbors and shared nearest neighbors of an object for density distribution estimation. Some theoretical properties of the proposed RDOS including its expected value and false alarm probability are derived. A comprehensive experimental study on both synthetic and real-life data sets demonstrates that our approach is more effective than state-of-the-art outlier detection methods. "
747953652153843712,2016-06-29 00:43:41,https://t.co/hFVPAbVKju,Automatic Variational ABC. (arXiv:1606.08549v1 [stat.ML]) https://t.co/hFVPAbVKju,1,9," Abstract: Approximate Bayesian Computation (ABC) is a framework for performing likelihood-free posterior inference for simulation models. Stochastic Variational inference (SVI) is an appealing alternative to the inefficient sampling approaches commonly used in ABC. However, SVI is highly sensitive to the variance of the gradient estimators, and this problem is exacerbated by approximating the likelihood. We draw upon recent advances in variance reduction for SV and likelihood-free inference using deterministic simulations to produce low variance gradient estimators of the variational lower-bound. By then exploiting automatic differentiation libraries we can avoid nearly all model-specific derivations. We demonstrate performance on three problems and compare to existing SVI algorithms. Our results demonstrate the correctness and efficiency of our algorithm. "
747953649133957121,2016-06-29 00:43:40,https://t.co/lf18xO928J,Estimating the class prior and posterior from noisy positives and unlabeled data. (arXiv:1606.08561v1 [stat.ML]) https://t.co/lf18xO928J,0,4," Abstract: We develop a classification algorithm for estimating posterior distributions from positive-unlabeled data, that is robust to noise in the positive labels and effective for high-dimensional data. In recent years, several algorithms have been proposed to learn from positive-unlabeled data; however, many of these contributions remain theoretical, performing poorly on real high-dimensional data that is typically contaminated with noise. We build on this previous work to develop two practical classification algorithms that explicitly model the noise in the positive labels and utilize univariate transforms built on discriminative classifiers. We prove that these univariate transforms preserve the class prior, enabling estimation in the univariate space and avoiding kernel density estimation for high-dimensional data. The theoretical development and both parametric and nonparametric algorithms proposed here constitutes an important step towards wide-spread use of robust classification algorithms for positive-unlabeled data. "
747953646286016512,2016-06-29 00:43:40,https://t.co/4Ml9fEbrlx,Learning Generative ConvNet with Continuous Latent Factors by Alternating Back-Propagation. (arXiv:1606.08571v1 [s… https://t.co/4Ml9fEbrlx,3,8," Abstract: This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a non-linear generalization of factor analysis. In this model, the mapping from the latent factors to the observed vector is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates between the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dynamics or gradient descent. (2) Learning back-propagation, which updates the parameters given the inferred latent factors by gradient descent. The gradient computations in both steps are powered by back-propagation, and they share most of their code in common. We show that the alternating back-propagation algorithm can learn realistic generator models of natural images, video sequences, and sounds. Moreover, it can also be used to learn from incomplete or indirect training data. "
747953643337433089,2016-06-29 00:43:39,https://t.co/XtXKJyWe76,Unsupervised Relational Representation Learning via Clustering: Preliminary Results. (arXiv:1606.08658v1 [stat.ML]) https://t.co/XtXKJyWe76,0,2," Abstract: The goal of unsupervised representation learning methods is to learn a new representation of the original data, such that it makes a certain classification task easier to solve. Since their introduction in late 2000s, these methods initiated a revolution within machine learning. In this paper we present an unsupervised representation learning method for relational data. The proposed approach uses a clustering procedure to learn a new representation. Moreover, we introduce an adaptive clustering method, capable of addressing multiple interpretations of similarity in relational context. Finally, we experimentally evaluate the proposed approach. The preliminary results show the promise of the approach, as the models learned on the new representation often achieve better performance and are less complex than the ones learned on the original data representation. "
747953640032374787,2016-06-29 00:43:38,https://t.co/LhfXnnv8ds,Theory reconstruction: a representation learning view on predicate invention. (arXiv:1606.08660v1 [stat.ML]) https://t.co/LhfXnnv8ds,1,4," Abstract: With this positional paper we present a representation learning view on predicate invention. The intention of this proposal is to bridge the relational and deep learning communities on the problem of predicate invention. We propose a theory reconstruction approach, a formalism that extends autoencoder approach to representation learning to the relational settings. Our intention is to start a discussion to define a unifying framework for predicate invention and theory revision. "
747953637318660097,2016-06-29 00:43:38,https://t.co/eHEPw5qR6T,Reviving Threshold-Moving: a Simple Plug-in Bagging Ensemble for Binary and Multiclass Imbalanced Data. (arXiv:160… https://t.co/eHEPw5qR6T,1,3," Abstract: Class imbalance presents a major hurdle in the application of data mining methods. A common practice to deal with it is to create ensembles of classifiers that learn from resampled balanced data. For example, bagged decision trees combined with random undersampling (RUS) or the synthetic minority oversampling technique (SMOTE). However, most of the resampling methods entail asymmetric changes to the examples of different classes, which in turn can introduce its own biases in the model. Furthermore, those methods require a performance measure to be specified a priori before learning. An alternative is to use a so-called threshold-moving method that a posteriori changes the decision threshold of a model to counteract the imbalance, thus has a potential to adapt to the performance measure of interest. Surprisingly, little attention has been paid to the potential of combining bagging ensemble with threshold-moving. In this paper, we present probability thresholding bagging (PT-bagging), a versatile plug-in method that fills this gap. Contrary to usual rebalancing practice, our method preserves the natural class distribution of the data resulting in well calibrated posterior probabilities. We also extend the proposed method to handle multiclass data. The method is validated on binary and multiclass benchmark data sets. We perform analyses that provide insights into the proposed method. "
747953634915323904,2016-06-29 00:43:37,https://t.co/HVMsxSk0le,Modeling Industrial ADMET Data with Multitask Networks. (arXiv:1606.08793v1 [stat.ML]) https://t.co/HVMsxSk0le,0,2," Abstract: Deep learning methods such as multitask neural networks have recently been applied to ligand-based virtual screening and other drug discovery applications. Using a set of industrial ADMET datasets, we compare neural networks to standard baseline models and analyze multitask learning effects with both random cross-validation and a more relevant temporal validation scheme. We confirm that multitask learning can provide modest benefits over single-task models and show that smaller datasets tend to benefit more than larger datasets from multitask learning. Additionally, we find that adding massive amounts of side information is not guaranteed to improve performance relative to simpler multitask learning. Our results emphasize that multitask effects are highly dataset-dependent, suggesting the use of dataset-specific models to maximize overall performance. "
747953631664685056,2016-06-29 00:43:36,https://t.co/EN3ASABncX,"EU regulations on algorithmic decision-making and a ""right to explanation"". (arXiv:1606.08813v1 [stat.ML]) https://t.co/EN3ASABncX",8,5," Abstract: We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which ""significantly affect"" users. The law will also effectively create a ""right to explanation,"" whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation. "
747953629429121024,2016-06-29 00:43:36,https://t.co/ysi80DLuQK,Multi-View Kernel Consensus For Data Analysis and Signal Processing. (arXiv:1606.08819v1 [cs.LG]) https://t.co/ysi80DLuQK,0,3," Abstract: The input data features set for many data driven tasks is high-dimensional while the intrinsic dimension of the data is low. Data analysis methods aim to uncover the underlying low dimensional structure imposed by the low dimensional hidden parameters by utilizing distance metrics that consider the set of attributes as a single monolithic set. However, the transformation of the low dimensional phenomena into the measured high dimensional observations might distort the distance metric, This distortion can effect the desired estimated low dimensional geometric structure. In this paper, we suggest to utilize the redundancy in the attribute domain by partitioning the attributes into multiple subsets we call views. The proposed methods utilize the agreement also called consensus between different views to extract valuable geometric information that unifies multiple views about the intrinsic relationships among several different observations. This unification enhances the information that a single view or a simple concatenations of views provides. "
747953626442768384,2016-06-29 00:43:35,https://t.co/2pMJmvJ722,Active Ranking from Pairwise Comparisons and the Futility of Parametric Assumptions. (arXiv:1606.08842v1 [cs.LG]) https://t.co/2pMJmvJ722,1,5," Abstract: We consider sequential or active ranking of a set of n items based on noisy pairwise comparisons. Items are ranked according to the probability that a given item beats a randomly chosen item, and ranking refers to partitioning the items into sets of pre-specified sizes according to their scores. This notion of ranking includes as special cases the identification of the top-k items and the total ordering of the items. We first analyze a sequential ranking algorithm that counts the number of comparisons won, and uses these counts to decide whether to stop, or to compare another pair of items, chosen based on confidence intervals specified by the data collected up to that point. We prove that this algorithm succeeds in recovering the ranking using a number of comparisons that is optimal up to logarithmic factors. This guarantee does not require any structural properties of the underlying pairwise probability matrix, unlike a significant body of past work on pairwise ranking based on parametric models such as the Thurstone or Bradley-Terry-Luce models. It has been a long-standing open question as to whether or not imposing these parametric assumptions allows for improved ranking algorithms. For stochastic comparison models, in which the pairwise probabilities are bounded away from zero, our second contribution is to resolve this issue by proving a lower bound for parametric models. This shows, perhaps surprisingly, that these popular parametric modeling choices offer at most logarithmic gains for stochastic comparisons. "
747953622613377024,2016-06-29 00:43:34,https://t.co/x5XhMMuir4,A Variational Approximations-DIC Rubric for Parameter Estimation and Mixture Model Selection Within a Family Setti… https://t.co/x5XhMMuir4,0,4," Abstract: Mixture model-based clustering has become an increasingly popular data analysis technique since its introduction fifty years ago, and is now commonly utilized within the family setting. Families of mixture models arise when the component parameters, usually the component covariance matrices, are decomposed and a number of constraints are imposed. Within the family setting, we need to choose the member of the family, i.e., the appropriate covariance structure, in addition to the number of mixture components. To date, the Bayesian information criterion (BIC) has proved most effective for model selection, and the expectation-maximization (EM) algorithm is usually used for parameter estimation. To date, this EM-BIC rubric has monopolized the literature on families of mixture models. We deviate from this rubric, using variational Bayes approximations for parameter estimation and the deviance information criterion for model selection. The variational Bayes approach alleviates some of the computational complexities associated with the EM algorithm by constructing a tight lower bound on the complex marginal likelihood and maximizing this lower bound by minimizing the associated Kullback-Leibler divergence. We use this approach on the most famous family of Gaussian mixture models within the literature and real and simulated data are used to compare our approach to the EM-BIC rubric. "
747953620277166084,2016-06-29 00:43:34,https://t.co/PQ8vD5GRRD,Bootstrap-Based Regularization for Low-Rank Matrix Estimation. (arXiv:1410.8275v3 [stat.ME] UPDATED) https://t.co/PQ8vD5GRRD,0,5," Abstract: We develop a flexible framework for low-rank matrix estimation that allows us to transform noise models into regularization schemes via a simple bootstrap algorithm. Effectively, our procedure seeks an autoencoding basis for the observed matrix that is stable with respect to the specified noise model; we call the resulting procedure a stable autoencoder. In the simplest case, with an isotropic noise model, our method is equivalent to a classical singular value shrinkage estimator. For non-isotropic noise models, e.g., Poisson noise, the method does not reduce to singular value shrinkage, and instead yields new estimators that perform well in experiments. Moreover, by iterating our stable autoencoding scheme, we can automatically generate low-rank estimates without specifying the target rank as a tuning parameter. "
747953617525682177,2016-06-29 00:43:33,https://t.co/IAfgYJ711b,Expectation propagation for continuous time stochastic processes. (arXiv:1512.06098v2 [stat.ML] UPDATED) https://t.co/IAfgYJ711b,0,6, Abstract: We consider the inverse problem of reconstructing the posterior measure over the trajec- tories of a diffusion process from discrete time observations and continuous time constraints. We cast the problem in a Bayesian framework and derive approximations to the posterior distributions of single time marginals using variational approximate inference. We then show how the approximation can be extended to a wide class of discrete-state Markov jump pro- cesses by making use of the chemical Langevin equation. Our empirical results show that the proposed method is computationally efficient and provides good approximations for these classes of inverse problems. 
747953614509969408,2016-06-29 00:43:32,https://t.co/XXgjDceuvS,ABC random forests for Bayesian parameter inference. (arXiv:1605.05537v2 [stat.ME] UPDATED) https://t.co/XXgjDceuvS,0,7," Abstract: Approximate Bayesian Computation (ABC) has grown into a standard methodology to handle Bayesian inference in models associated with intractable likelihood functions. Most ABC implementations require the selection of a summary statistic as the data itself is too large to be compared to simulated realisations from the assumed model. The dimension of this statistic is generally quite large. Furthermore, the tolerance level that governs the acceptance or rejection of parameter values needs to be calibrated and the range of calibration techniques available so far is mostly based on asymptotic arguments. We propose here to conduct Bayesian inference based on an arbitrarily large vector of summary statistics without imposing a selection of the relevant components and bypassing the derivation of a tolerance. The approach relies on the random forest methodology of Breiman (2001) when applied to regression. We advocate the derivation of a new random forest for each component of the parameter vector. Correlations between parameter components are handled by separate random forests. When compared with standard ABC solutions, this technology offers significant gains in terms of robustness to the choice of the summary statistics and of computing time. All methods designed here have been added to the abcrf R library. "
747953608839270400,2016-06-29 00:43:31,https://t.co/0H1gQODa3V,On estimating a mixture on graphons. (arXiv:1606.02401v2 [stat.ML] UPDATED) https://t.co/0H1gQODa3V,1,5," Abstract: Community detection, which focuses on clustering nodes or detecting communities in (mostly) a single network, is a problem of considerable practical interest and has received a great deal of attention in the research community. While being able to cluster within a network is important, there are emerging needs to be able to cluster multiple networks. This is largely motivated by the routine collection of network data that are generated from potentially different populations, such as brain networks of subjects from different disease groups, genders, or biological networks generated under different experimental conditions, etc. We propose a simple and general framework for clustering multiple networks based on a mixture model on graphons. Our clustering method employs graphon estimation as a first step and performs spectral clustering on the matrix of distances between estimated graphons. This is illustrated through both simulated and real data sets, and theoretical justification of the algorithm is given in terms of consistency. "
747612479203704832,2016-06-28 02:07:59,https://t.co/kUXD47fimB,Modeling Group Dynamics Using Probabilistic Tensor Decompositions. (arXiv:1606.07840v1 [stat.ML]) https://t.co/kUXD47fimB,0,17," Abstract: We propose a probabilistic modeling framework for learning the dynamic patterns in the collective behaviors of social agents and developing profiles for different behavioral groups, using data collected from multiple information sources. The proposed model is based on a hierarchical Bayesian process, in which each observation is a finite mixture of an set of latent groups and the mixture proportions (i.e., group probabilities) are drawn randomly. Each group is associated with some distributions over a finite set of outcomes. Moreover, as time evolves, the structure of these groups also changes; we model the change in the group structure by a hidden Markov model (HMM) with a fixed transition probability. We present an efficient inference method based on tensor decompositions and the expectation-maximization (EM) algorithm for parameter estimation. "
747591636302045185,2016-06-28 00:45:10,https://t.co/lNblaAAtqg,Probabilistic Forecasting and Simulation of Electricity Markets via Online Dictionary Learning. (arXiv:1606.07855v… https://t.co/lNblaAAtqg,0,5," Abstract: The problem of probabilistic forecasting and online simulation of real-time electricity market with stochastic generation and demand is considered. By exploiting the parametric structure of the direct current optimal power flow, a new technique based on online dictionary learning (ODL) is proposed. The ODL approach incorporates real-time measurements and historical traces to produce forecasts of joint and marginal probability distributions of future locational marginal prices, power flows, and dispatch levels, conditional on the system state at the time of forecasting. Compared with standard Monte Carlo simulation techniques, the ODL approach offers several orders of magnitude improvement in computation time, making it feasible for online forecasting of market operations. Numerical simulations on large and moderate size power systems illustrate its performance and complexity features and its potential as a tool for system operators. "
747591633605136384,2016-06-28 00:45:09,https://t.co/uFsa4flLGi]),Large-Scale Kernel Methods for Independence Testing. (arXiv:1606.07892v1 [https://t.co/uFsa4flLGi]) https://t.co/rJKRDa2m0e,1,4,INDEXERROR
747591631327617024,2016-06-28 00:45:09,https://t.co/tGEw8hl2UJ,Fast Methods for Recovering Sparse Parameters in Linear Low Rank Models. (arXiv:1606.08009v1 [cs.LG]) https://t.co/tGEw8hl2UJ,0,5," Abstract: In this paper, we investigate the recovery of a sparse weight vector (parameters vector) from a set of noisy linear combinations. However, only partial information about the matrix representing the linear combinations is available. Assuming a low-rank structure for the matrix, one natural solution would be to first apply a matrix completion on the data, and then to solve the resulting compressed sensing problem. In big data applications such as massive MIMO and medical data, the matrix completion step imposes a huge computational burden. Here, we propose to reduce the computational cost of the completion task by ignoring the columns corresponding to zero elements in the sparse vector. To this end, we employ a technique to initially approximate the support of the sparse vector. We further propose to unify the partial matrix completion and sparse vector recovery into an augmented four-step problem. Simulation results reveal that the augmented approach achieves the best performance, while both proposed methods outperform the natural two-step technique with substantially less computational requirements. "
747591629155012608,2016-06-28 00:45:08,https://t.co/EYXpWw5gAY,Discriminating sample groups with multi-way data. (arXiv:1606.08046v1 [stat.ME]) https://t.co/EYXpWw5gAY,0,2," Abstract: High-dimensional linear classifiers, such as the support vector machine (SVM) and distance weighted discrimination (DWD), are commonly used in biomedical research to distinguish groups of subjects based on a large number of features. However, their use is limited to applications where a single vector of features is measured for each subject. In practice data are often multi-way, or measured over multiple dimensions. For example, metabolite abundance may be measured over multiple regions or tissues, or gene expression may be measured over multiple time points, for the same subjects. We propose a framework for linear classification of high-dimensional multi-way data, in which coefficients can be factorized into weights that are specific to each dimension. More generally, the coefficients for each measurement in a multi-way dataset are assumed to have low-rank structure. This framework extends existing classification techniques, and we have implemented multi-way versions of SVM and DWD. We describe informative simulation results, and apply multi-way DWD to data for two very different clinical research studies. The first study uses metabolite magnetic resonance spectroscopy data over multiple brain regions to compare patients with and without spinocerebellar ataxia, the second uses publicly available gene expression time-course data to compare treatment responses for patients with multiple sclerosis. Our method improves performance and simplifies interpretation over naive applications of full rank linear classification to multi-way data. An R package is available at this https URL . "
747591625807888384,2016-06-28 00:45:07,https://t.co/BaAkB9Jan7,Enhancing Transparency and Control when Drawing Data-Driven Inferences about Individuals. (arXiv:1606.08063v1 [sta… https://t.co/BaAkB9Jan7,2,2," Abstract: Recent studies have shown that information disclosed on social network sites (such as Facebook) can be used to predict personal characteristics with surprisingly high accuracy. In this paper we examine a method to give online users transparency into why certain inferences are made about them by statistical models, and control to inhibit those inferences by hiding (""cloaking"") certain personal information from inference. We use this method to examine whether such transparency and control would be a reasonable goal by assessing how difficult it would be for users to actually inhibit inferences. Applying the method to data from a large collection of real users on Facebook, we show that a user must cloak only a small portion of her Facebook Likes in order to inhibit inferences about their personal characteristics. However, we also show that in response a firm could change its modeling of users to make cloaking more difficult. "
747591623354134528,2016-06-28 00:45:07,https://t.co/9Fi463vDUB,Cyberbullying Identification Using Participant-Vocabulary Consistency. (arXiv:1606.08084v1 [cs.SI]) https://t.co/9Fi463vDUB,2,4," Abstract: With the rise of social media, people can now form relationships and communities easily regardless of location, race, ethnicity, or gender. However, the power of social media simultaneously enables harmful online behavior such as harassment and bullying. Cyberbullying is a serious social problem, making it an important topic in social network analysis. Machine learning methods can potentially help provide better understanding of this phenomenon, but they must address several key challenges: the rapidly changing vocabulary involved in cyber- bullying, the role of social network structure, and the scale of the data. In this study, we propose a model that simultaneously discovers instigators and victims of bullying as well as new bullying vocabulary by starting with a corpus of social interactions and a seed dictionary of bullying indicators. We formulate an objective function based on participant-vocabulary consistency. We evaluate this approach on Twitter and Ask.fm data sets and show that the proposed method can detect new bullying vocabulary as well as victims and bullies. "
747591620472733696,2016-06-28 00:45:06,https://t.co/wgLr32BXAe,The Dependent Random Measures with Independent Increments in Mixture Models. (arXiv:1606.08105v1 [stat.ML]) https://t.co/wgLr32BXAe,0,3," Abstract: When observations are organized into groups where commonalties exist amongst them, the dependent random measures can be an ideal choice for modeling. One of the propositions of the dependent random measures is that the atoms of the posterior distribution are shared amongst groups, and hence groups can borrow information from each other. When normalized dependent random measures prior with independent increments are applied, we can derive appropriate exchangeable probability partition function (EPPF), and subsequently also deduce its inference algorithm given any mixture model likelihood. We provide all necessary derivation and solution to this framework. For demonstration, we used mixture of Gaussians likelihood in combination with a dependent structure constructed by linear combinations of CRMs. Our experiments show superior performance when using this framework, where the inferred values including the mixing weights and the number of clusters both respond appropriately to the number of completely random measure used. "
747591617465434113,2016-06-28 00:45:05,https://t.co/0f1O0gkZFK,Out-of-Sample Extension for Dimensionality Reduction of Noisy Time Series. (arXiv:1606.08282v1 [stat.ML]) https://t.co/0f1O0gkZFK,0,5," Abstract: This paper proposes an out-of-sample extension framework for a global manifold learning algorithm (Isomap) that uses temporal information in out-of-sample points in order to make the embedding more robust to noise and artifacts. Given a set of noise-free training data and its embedding, the proposed framework extends the embedding for a noisy time series. This is achieved by adding a spatio-temporal compactness term to the optimization objective of the embedding. To the best of our knowledge, this is the first method for out-of-sample extension of manifold embeddings that leverages timing information available for the extension set. Experimental results demonstrate that our out-of-sample extension algorithm renders a more robust and accurate embedding of sequentially ordered image data in the presence of various noise and artifacts when compared to other timing-aware embeddings. "
747591614755897344,2016-06-28 00:45:05,https://t.co/kUCs7BEMq1,Interpreting extracted rules from ensemble of trees: Application to computer-aided diagnosis of breast MRI. (arXiv… https://t.co/kUCs7BEMq1,1,2, Abstract: High predictive performance and ease of use and interpretability are important requirements for the applicability of a computer-aided diagnosis (CAD) to human reading studies. We propose a CAD system specifically designed to be more comprehensible to the radiologist reviewing screening breast MRI studies. Multiparametric imaging features are combined to produce a CAD system for differentiating cancerous and non-cancerous lesions. The complete system uses a rule-extraction algorithm to present lesion classification results in an easy to understand graph visualization. 
747591612428083200,2016-06-28 00:45:04,https://t.co/N8eWX4zCzI,Efficient Bayesian Learning in Social Networks with Gaussian Estimators. (arXiv:1002.0747v3 [stat.AP] UPDATED) https://t.co/N8eWX4zCzI,2,4," Abstract: We consider a group of Bayesian agents who try to estimate a state of the world $\theta$ through interaction on a social network. Each agent $v$ initially receives a private measurement of $\theta$: a number $S_v$ picked from a Gaussian distribution with mean $\theta$ and standard deviation one. Then, in each discrete time iteration, each reveals its estimate of $\theta$ to its neighbors, and, observing its neighbors' actions, updates its belief using Bayes' Law. This process aggregates information efficiently, in the sense that all the agents converge to the belief that they would have, had they access to all the private measurements. We show that this process is computationally efficient, so that each agent's calculation can be easily carried out. We also show that on any graph the process converges after at most $2N \cdot D$ steps, where $N$ is the number of agents and $D$ is the diameter of the network. Finally, we show that on trees and on distance transitive-graphs the process converges after $D$ steps, and that it preserves privacy, so that agents learn very little about the private signal of most other agents, despite the efficient aggregation of information. Our results extend those in an unpublished manuscript of the first and last authors. "
747591610058285057,2016-06-28 00:45:04,https://t.co/LTqDlOf6PT,Two New Approaches to Compressed Sensing Exhibiting Both Robust Sparse Recovery and the Grouping Effect. (arXiv:14… https://t.co/LTqDlOf6PT,0,4," Abstract: In this paper we introduce a new optimization formulation for sparse regression and compressed sensing, called CLOT (Combined L-One and Two), wherein the regularizer is a convex combination of the $\ell_1$- and $\ell_2$-norms. This formulation differs from the Elastic Net (EN) formulation, in which the regularizer is a convex combination of the $\ell_1$- and $\ell_2$-norm squared. This seemingly simple modification has fairly significant consequences. In particular, it is shown in this paper that the EN formulation \textit{does not achieve} robust recovery of sparse vectors in the context of compressed sensing, whereas the new CLOT formulation does so. Also, like EN but unlike LASSO, the CLOT formulation achieves the grouping effect, wherein coefficients of highly correlated columns of the measurement (or design) matrix are assigned roughly comparable values. It is noteworthy that LASSO does not have the grouping effect and EN (as shown here) does not achieve robust sparse recovery. Therefore the CLOT formulation combines the best features of both LASSO (robust sparse recovery) and EN (grouping effect). The CLOT formulation is a special case of another one called SGL (Sparse Group LASSO) which was introduced into the literature previously, but without any analysis of either the grouping effect or robust sparse recovery. It is shown here that SGL achieves robust sparse recovery, and also achieves a version of the grouping effect in that coefficients of highly correlated columns of the measurement (or design) matrix are assigned roughly comparable values, if the columns belong to the same group. oughly comparable values, if the columns belong to the same group. "
747591607881433089,2016-06-28 00:45:03,https://t.co/gVrywYfQCv,Global Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation. (arXiv:1506.07405v3 [cs.N… https://t.co/gVrywYfQCv,0,8," Abstract: It has been observed in a variety of contexts that gradient descent methods have great success in solving low-rank matrix factorization problems, despite the relevant problem formulation being non-convex. We tackle a particular instance of this scenario, where we seek the $d$-dimensional subspace spanned by a streaming data matrix. We apply the natural first order incremental gradient descent method, constraining the gradient method to the Grassmannian. In this paper, we propose an adaptive step size scheme that is greedy for the noiseless case, that maximizes the improvement of our metric of convergence at each data index $t$, and yields an expected improvement for the noisy case. We show that, with noise-free data, this method converges from any random initialization to the global minimum of the problem. For noisy data, we provide the expected convergence rate of the proposed algorithm per iteration. "
747591605419388928,2016-06-28 00:45:02,https://t.co/k0y3ZNRZMm,Admissibility of a posterior predictive decision rule. (arXiv:1507.06350v4 [stat.ML] UPDATED) https://t.co/k0y3ZNRZMm,0,2, Abstract: Recent decades have seen an interest in prediction problems for which Bayesian methodology has been extremely useful. Sampling from or approximating the posterior predictive distribution in a Bayesian model allows one to make inferential statements about potentially observable random quantities given observed data. The purpose of this note is to use statistical decision theory as a basis to justify the use of a posterior predictive distribution for making a point prediction. 
747591602974130176,2016-06-28 00:45:02,https://t.co/2Iqr1Wzf7h,Quantifying the information of the prior and likelihood in parametric Bayesian modeling. (arXiv:1511.01214v8 [stat… https://t.co/2Iqr1Wzf7h,0,4," Abstract: I suggest using a pair of metrics to quantify the information of the prior and likelihood functions within a parametric Bayesian model, one of which is closely related to the reference priors of Berger and Bernardo (Bernardo 1979, Berger and Bernardo 2009) and information measure introduced by Lindley (Lindley 1956). A Monte Carlo algorithm to estimate these metrics is developed and their properties are explored via a combination of theoretical results, simulations, and applications to public medical data sets. This combination of theoretical, empirical, and computational support provides evidence that these metrics may be useful diagnostic tools when performing a Bayesian analysis. "
747591599895515137,2016-06-28 00:45:01,https://t.co/uQitjFMES0,A Sparse Linear Model and Significance Test for Individual Consumption Prediction. (arXiv:1511.01853v2 [stat.ML] U… https://t.co/uQitjFMES0,0,3," Abstract: Accurate prediction of user consumption is a key part not also in understanding consumer flexibility and behavior patterns, but in the design of robust and efficient energy saving programs as well. Existing prediction methods usually have high relative errors that can be larger than 40\%. In this paper, we propose a method to improve prediction accuracy of individual users by exploring sparsity in historical data and leveraging predictive relationship between different users. Sparsity is captured by popular least absolute shrinkage and selection (LASSO) estimator, while user selection is formulated as an optimal hypothesis testing problem and solved via a covariance test. We provide extensive simulation against well-known techniques such as support vector machine (SVM), principle component analysis (PCA) and random forest (RF) using real world data. Simulation results demonstrate that our proposed methods are operationally efficient, interpretable, and achieves optimal prediction performance. "
747591597974519809,2016-06-28 00:45:01,https://t.co/WQOLQkMd1u,Functional archetype and archetypoid analysis. (arXiv:1601.06911v2 [stat.ME] UPDATED) https://t.co/WQOLQkMd1u,0,3," Abstract: Archetype and archetypoid analysis can be extended to functional data. Each function is represented as a mixture of actual observations (functional archetypoids) or functional archetypes, which are a mixture of observations in the data set. Well-known Canadian temperature data are used to illustrate the analysis developed. Computational methods are proposed for performing these analyses, based on the coefficients of a basis. Unlike a previous attempt to compute functional archetypes, which was only valid for an orthogonal basis, the proposed methodology can be used for any basis. It is computationally less demanding than the simple approach of discretizing the functions. Multivariate functional archetype and archetypoid analysis are also introduced and applied in an interesting problem about the study of human development around the world over the last 50 years. These tools can contribute to the understanding of a functional data set, as in the multivariate case. "
747591595751505920,2016-06-28 00:45:00,https://t.co/ezDXEEcv6f,Hyperparameter optimization with approximate gradient. (arXiv:1602.02355v5 [stat.ML] UPDATED) https://t.co/ezDXEEcv6f,0,4," Abstract: Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of L2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods. "
747591592656109568,2016-06-28 00:44:59,https://t.co/8LviylK8kH,Further properties of the forward-backward envelope with applications to difference-of-convex programming. (arXiv:… https://t.co/8LviylK8kH,0,3," Abstract: In this paper, we further study the forward-backward envelope first introduced in [28] and [30] for problems whose objective is the sum of a proper closed convex function and a twice continuously differentiable possibly nonconvex function with Lipschitz continuous gradient. We derive sufficient conditions on the original problem for the corresponding forward-backward envelope to be a level-bounded and Kurdyka-{\L}ojasiewicz function with an exponent of $\frac12$; these results are important for the efficient minimization of the forward-backward envelope by classical optimization algorithms. In addition, we demonstrate how to minimize some difference-of-convex regularized least squares problems by minimizing a suitably constructed forward-backward envelope. Our preliminary numerical results on randomly generated instances of large-scale $\ell_{1-2}$ regularized least squares problems [37] illustrate that an implementation of this approach with a limited-memory BFGS scheme usually outperforms standard first-order methods such as the nonmonotone proximal gradient method in [35]. "
747591588424089600,2016-06-28 00:44:58,https://t.co/9BEQ20nUkT,On the expressive power of deep neural networks. (arXiv:1606.05336v2 [stat.ML] UPDATED) https://t.co/9BEQ20nUkT,0,7," Abstract: We study the expressive power of deep neural networks before and after training. Considering neural nets after random initialization, we show that three natural measures of expressivity all display an exponential dependence on the depth of the network. We prove, theoretically and experimentally, that all of these measures are in fact related to a fourth quantity, trajectory length. This quantity grows exponentially in the depth of the network, and is responsible for the depth sensitivity observed. These results translate to consequences for networks during and after training. The connection of all expressivity measures to trajectory length suggests that parameters earlier in the network have greater influence on the expressive power -- in particular, given a layer, its influence on expressivity is determined by the remaining depth of the network after that layer. This is verified with experiments on MNIST and CIFAR-10. We also find that the training process decreases depth sensitivity for real and synthetic data, but at different rates. "
747229493690011648,2016-06-27 00:46:08,https://t.co/xbAT2RtxFK,NN-grams: Unifying neural network and n-gram language models for Speech Recognition. (arXiv:1606.07470v1 [cs.CL]) https://t.co/xbAT2RtxFK,1,15," Abstract: We present NN-grams, a novel, hybrid language model integrating n-grams and neural networks (NN) for speech recognition. The model takes as input both word histories as well as n-gram counts. Thus, it combines the memorization capacity and scalability of an n-gram model with the generalization ability of neural networks. We report experiments where the model is trained on 26B words. NN-grams are efficient at run-time since they do not include an output soft-max layer. The model is trained using noise contrastive estimation (NCE), an approach that transforms the estimation problem of neural networks into one of binary classification between data samples and noise samples. We present results with noise samples derived from either an n-gram distribution or from speech recognition lattices. NN-grams outperforms an n-gram model on an Italian speech recognition dictation task. "
747229489021739008,2016-06-27 00:46:07,https://t.co/4buHfKyR5r,Interactive Semantic Featuring for Text Classification. (arXiv:1606.07545v1 [cs.CL]) https://t.co/4buHfKyR5r,0,5," Abstract: In text classification, dictionaries can be used to define human-comprehensible features. We propose an improvement to dictionary features called smoothed dictionary features. These features recognize document contexts instead of n-grams. We describe a principled methodology to solicit dictionary features from a teacher, and present results showing that models built using these human-comprehensible features are competitive with models trained with Bag of Words features. "
747229483829202944,2016-06-27 00:46:06,https://t.co/iDJJzoz59M,Multipartite Ranking-Selection of Low-Dimensional Instances by Supervised Projection to High-Dimensional Space. (a… https://t.co/iDJJzoz59M,0,1," Abstract: Pruning of redundant or irrelevant instances of data is a key to every successful solution for pattern recognition. In this paper, we present a novel ranking-selection framework for low-length but highly correlated instances. Instead of working in the low-dimensional instance space, we learn a supervised projection to high-dimensional space spanned by the number of classes in the dataset under study. Imposing higher distinctions via exposing the notion of labels to the instances, lets to deploy one versus all ranking for each individual classes and selecting quality instances via adaptive thresholding of the overall scores. To prove the efficiency of our paradigm, we employ it for the purpose of texture understanding which is a hard recognition challenge due to high similarity of texture pixels and low dimensionality of their color features. Our experiments show considerable improvements in recognition performance over other local descriptors on several publicly available datasets. "
747229478288560128,2016-06-27 00:46:05,https://t.co/fQKLACMle2,Regression Trees and Random forest based feature selection for malaria risk exposure prediction. (arXiv:1606.07578… https://t.co/fQKLACMle2,1,7," Abstract: This paper deals with prediction of anopheles number, the main vector of malaria risk, using environmental and climate variables. The variables selection is based on an automatic machine learning method using regression trees, and random forests combined with stratified two levels cross validation. The minimum threshold of variables importance is accessed using the quadratic distance of variables importance while the optimal subset of selected variables is used to perform predictions. Finally the results revealed to be qualitatively better, at the selection, the prediction , and the CPU time point of view than those obtained by GLM-Lasso method. "
747229473502822400,2016-06-27 00:46:03,https://t.co/FX0Gy68EX9,Should one minimize the expected Bellman residual or maximize the mean value?. (arXiv:1606.07636v1 [cs.LG]) https://t.co/FX0Gy68EX9,1,8," Abstract: This paper aims at theoretically and empirically comparing two standard optimization criterion for Reinforcement Learning: i) maximization of the mean value (predominant approach in policy search algorithms) and ii) minimization of the Bellman residual (mainly used in approximate dynamic programming). For doing so, we introduce a new policy search algorithm based on the minimization of the residual $\|T_* v_\pi - v_\pi\|_{1,\nu}$. We prove that it enjoys a performance bound that is better than the sole known bound for maximizing the mean value and that matches the best known bounds in approximate dynamic programming. We also conduct experiments on randomly generated generic Markov decision processes to compare both approaches empirically. It turns out that the Bellman residual is a good optimization proxy only if there is a good match between the sampling distribution and the discounted state occupancy distribution induced by the optimal policy, while maximizing the mean value is rather insensitive to this issue. "
747229467760787456,2016-06-27 00:46:02,https://t.co/QpCXFWupsd,Gamblets for opening the complexity-bottleneck of implicit schemes for hyperbolic and parabolic ODEs/PDEs with rou… https://t.co/QpCXFWupsd,0,2, Abstract: Implicit schemes are popular methods for the integration of time dependent PDEs such as hyperbolic and parabolic PDEs. However the necessity to solve corresponding linear systems at each time step constitutes a complexity bottleneck in their application to PDEs with rough coefficients. We present a generalization of gamblets introduced in arXiv:1503.03467 enabling the resolution of these implicit systems in near-linear complexity and provide rigorous a-priori error bounds on the resulting numerical approximations of hyperbolic and parabolic PDEs. These generalized gamblets induce a multiresolution decomposition of the solution space that is adapted to both the underlying (hyperbolic and parabolic) PDE (and the system of ODEs resulting from space discretization) and to the time-steps of the numerical scheme. 
747229463545585664,2016-06-27 00:46:01,https://t.co/E0iqCbl7Ox,Harnessing the Power of the Crowd to Increase Capacity for Data Science in the Social Sector. (arXiv:1606.07781v1 … https://t.co/E0iqCbl7Ox,2,3," Abstract: We present three case studies of organizations using a data science competition to answer a pressing question. The first is in education where a nonprofit that creates smart school budgets wanted to automatically tag budget line items. The second is in public health, where a low-cost, nonprofit women's health care provider wanted to understand the effect of demographic and behavioral questions on predicting which services a woman would need. The third and final example is in government innovation: using online restaurant reviews from Yelp, competitors built models to forecast which restaurants were most likely to have hygiene violations when visited by health inspectors. Finally, we reflect on the unique benefits of the open, public competition model. "
747229458776592386,2016-06-27 00:46:00,https://t.co/rPDiCxqqW9,Wide &amp; Deep Learning for Recommender Systems. (arXiv:1606.07792v1 [cs.LG]) https://t.co/rPDiCxqqW9,1,11," Abstract: Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow. "
747229453567270912,2016-06-27 00:45:59,https://t.co/Gsf5Vj04as,Estimating a common covariance matrix for network meta-analysis of gene expression datasets in diffuse large B-cel… https://t.co/Gsf5Vj04as,0,3," Abstract: Estimating gene networks in combination with posthoc analysis based on data from malignant tissue is a major challenge in cancer systems biology as it allows us to improve our understanding of disease pathology and eventually identify new drug targets. Motivated by the need for improving the inherently unstable covariance estimation compounded by noisy gene expression data, we present a hierarchical random covariance model applied as a meta-analysis of gene networks across eleven large-scale gene expression studies of diffuse large B-cell lymphoma (DLBCL). The approach was inspired by traditional meta-analysis using random effects models and we derive and compare basic properties and estimators of the model. Simple inference and interpretation of an introduced parameter measuring the inter-class homogeneity is suggested. The methods are generally applicable where multiple classes are present and believed to share a common covariance matrix of interest that is obscured by class-dependent noise. As such, it provides a basis for meta- or integrative analysis of covariance matrices where the classes are formed by datasets. In a posthoc analysis of the estimated common covariance matrix for the DLBCL data we were able to identify biologically meaningful gene networks of prognostic value. Of particular interest was the identification of a network with the S100 family of calcium-binding proteins as central players which further fuels the indications that knock down of these proteins may improve the immunotherapy strategies and outcome of lymphoma patients. "
747229448756420608,2016-06-27 00:45:58,https://t.co/8VGSCutDo3,Bounding and Minimizing Counterfactual Error. (arXiv:1606.03976v2 [stat.ML] UPDATED) https://t.co/8VGSCutDo3,0,2," Abstract: There is intense interest in applying machine learning to problems of causal inference in healthcare, economics, education, and other fields. In particular, individual-level causal inference has applications such as precision medicine and personalized advertising. We give a new theoretical analysis and family of algorithms for estimating individual treatment effect (ITE) from observational data. The algorithm itself learns a ""balanced"" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distance. Experiments on real and simulated data show the new algorithms match or outperform state-of-the-art methods. "
747229437729587200,2016-06-27 00:45:55,https://t.co/IHR3TYzAmu,Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis. (arXiv:1606.04316v2 [s… https://t.co/IHR3TYzAmu,1,12," Abstract: The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better - more sound and useful - alternatives for it. "
746140434322309121,2016-06-24 00:38:36,https://t.co/rSCMMUBmpg,Finite Sample Prediction and Recovery Bounds for Ordinal Embedding. (arXiv:1606.07081v1 [stat.ML]) https://t.co/rSCMMUBmpg,0,2," Abstract: The goal of ordinal embedding is to represent items as points in a low-dimensional Euclidean space given a set of constraints in the form of distance comparisons like ""item $i$ is closer to item $j$ than item $k$"". Ordinal constraints like this often come from human judgments. To account for errors and variation in judgments, we consider the noisy situation in which the given constraints are independently corrupted by reversing the correct constraint with some probability. This paper makes several new contributions to this problem. First, we derive prediction error bounds for ordinal embedding with noise by exploiting the fact that the rank of a distance matrix of points in $\mathbb{R}^d$ is at most $d+2$. These bounds characterize how well a learned embedding predicts new comparative judgments. Second, we investigate the special case of a known noise model and study the Maximum Likelihood estimator. Third, knowledge of the noise model enables us to relate prediction errors to embedding accuracy. This relationship is highly non-trivial since we show that the linear map corresponding to distance comparisons is non-invertible, but there exists a nonlinear map that is invertible. Fourth, two new algorithms for ordinal embedding are proposed and evaluated in experiments. "
746140432506228736,2016-06-24 00:38:36,https://t.co/xxuL9Mn2Hj,Visualizing Dynamics: from t-SNE to SEMI-MDPs. (arXiv:1606.07112v1 [stat.ML]) https://t.co/xxuL9Mn2Hj,2,8," Abstract: Deep Reinforcement Learning (DRL) is a trending field of research, showing great promise in many challenging problems such as playing Atari, solving Go and controlling robots. While DRL agents perform well in practice we are still missing the tools to analayze their performance and visualize the temporal abstractions that they learn. In this paper, we present a novel method that automatically discovers an internal Semi Markov Decision Process (SMDP) model in the Deep Q Network's (DQN) learned representation. We suggest a novel visualization method that represents the SMDP model by a directed graph and visualize it above a t-SNE map. We show how can we interpret the agent's policy and give evidence for the hierarchical state aggregation that DQNs are learning automatically. Our algorithm is fully automatic, does not require any domain specific knowledge and is evaluated by a novel likelihood based evaluation criteria. "
746140430610407424,2016-06-24 00:38:35,https://t.co/1InRjHNeza,Explainable Restricted Boltzmann Machines for Collaborative Filtering. (arXiv:1606.07129v1 [stat.ML]) https://t.co/1InRjHNeza,5,5," Abstract: Most accurate recommender systems are black-box models, hiding the reasoning behind their recommendations. Yet explanations have been shown to increase the user's trust in the system in addition to providing other benefits such as scrutability, meaning the ability to verify the validity of recommendations. This gap between accuracy and transparency or explainability has generated an interest in automated explanation generation methods. Restricted Boltzmann Machines (RBM) are accurate models for CF that also lack interpretability. In this paper, we focus on RBM based collaborative filtering recommendations, and further assume the absence of any additional data source, such as item content or user attributes. We thus propose a new Explainable RBM technique that computes the top-n recommendation list from items that are explainable. Experimental results show that our method is effective in generating accurate and explainable recommendations. "
746140428395745280,2016-06-24 00:38:35,https://t.co/3YNSJbSMbx,Fast robustness quantification with variational Bayes. (arXiv:1606.07153v1 [stat.ML]) https://t.co/3YNSJbSMbx,1,4," Abstract: Bayesian hierarchical models are increasing popular in economics. When using hierarchical models, it is useful not only to calculate posterior expectations, but also to measure the robustness of these expectations to reasonable alternative prior choices. We use variational Bayes and linear response methods to provide fast, accurate posterior means and robustness measures with an application to measuring the effectiveness of microcredit in the developing world. "
746140426613198848,2016-06-24 00:38:34,https://t.co/m5XHxdutNE,Interpretable Machine Learning Models for the Digital Clock Drawing Test. (arXiv:1606.07163v1 [stat.ML]) https://t.co/m5XHxdutNE,1,4," Abstract: The Clock Drawing Test (CDT) is a rapid, inexpensive, and popular neuropsychological screening tool for cognitive conditions. The Digital Clock Drawing Test (dCDT) uses novel software to analyze data from a digitizing ballpoint pen that reports its position with considerable spatial and temporal precision, making possible the analysis of both the drawing process and final product. We developed methodology to analyze pen stroke data from these drawings, and computed a large collection of features which were then analyzed with a variety of machine learning techniques. The resulting scoring systems were designed to be more accurate than the systems currently used by clinicians, but just as interpretable and easy to use. The systems also allow us to quantify the tradeoff between accuracy and interpretability. We created automated versions of the CDT scoring systems currently used by clinicians, allowing us to benchmark our models, which indicated that our machine learning models substantially outperformed the existing scoring systems. "
746140424729944066,2016-06-24 00:38:34,https://t.co/7BFUbhZXul,PAC-Bayesian Theorems for Multiview Learning. (arXiv:1606.07240v1 [stat.ML]) https://t.co/7BFUbhZXul,0,7," Abstract: We tackle the issue of multiview learning which aims to take advantages of multiple represen-tations/views of the data. In this context, many machine learning algorithms exist. However, the majority of the theoretical studies focus on learning with exactly two representations. In this paper, we propose a general PAC-Bayesian theory for multiview learning with more than two views. We focus our study to binary classification models that take the form of a majority vote. We derive PAC-Bayesian generalization bounds allowing to consider different relations between empirical and true risks by taking into account a notion of diversity of the voters and views, and that can be naturally extended to semi-supervised learning. "
746140422557278208,2016-06-24 00:38:34,https://t.co/XBXSxFda1Y,Algorithmic Composition of Melodies with Deep Recurrent Neural Networks. (arXiv:1606.07251v1 [stat.ML]) https://t.co/XBXSxFda1Y,1,8," Abstract: A big challenge in algorithmic composition is to devise a model that is both easily trainable and able to reproduce the long-range temporal dependencies typical of music. Here we investigate how artificial neural networks can be trained on a large corpus of melodies and turned into automated music composers able to generate new melodies coherent with the style they have been trained on. We employ gated recurrent unit networks that have been shown to be particularly efficient in learning complex sequential activations with arbitrary long time lags. Our model processes rhythm and melody in parallel while modeling the relation between these two features. Using such an approach, we were able to generate interesting complete melodies or suggest possible continuations of a melody fragment that is coherent with the characteristics of the fragment itself. "
746140420804050945,2016-06-24 00:38:33,https://t.co/EYqdFkJSN4,Semi-supervised Inference: General Theory and Estimation of Means. (arXiv:1606.07268v1 [stat.ME]) https://t.co/EYqdFkJSN4,0,5," Abstract: We propose a general semi-supervised inference framework focused on the estimation of the population mean. We consider both the ideal semi-supervised setting where infinitely many unlabeled samples are available, as well as the ordinary semi-supervised setting in which only a finite number of unlabeled samples is available. As usual in semi-supervised settings, there exists an unlabeled sample of covariate vectors and a labeled sample consisting of covariate vectors along with real-valued responses (""labels""). Otherwise the formulation is ""assumption-lean"" in that no major conditions are imposed on the statistical or functional form of the data. Estimators are proposed along with corresponding confidence intervals for the population mean. Theoretical analysis on both the asymptotic behavior and $\ell_2$-risk for the proposed procedures are given. Surprisingly, the proposed estimators, based on a simple form of the least squares method, outperform the ordinary sample mean. The method is further extended to a nonparametric setting, in which the oracle rate can be achieved asymptotically. The proposed estimators are further illustrated by simulation studies and a real data example involving estimation of the homeless population. "
746140419508047872,2016-06-24 00:38:33,https://t.co/lFby08I8vZ,Multiclass feature learning for hyperspectral image classification: sparse and hierarchical solutions. (arXiv:1606… https://t.co/lFby08I8vZ,0,1," Abstract: In this paper, we tackle the question of discovering an effective set of spatial filters to solve hyperspectral classification problems. Instead of fixing a priori the filters and their parameters using expert knowledge, we let the model find them within random draws in the (possibly infinite) space of possible filters. We define an active set feature learner that includes in the model only features that improve the classifier. To this end, we consider a fast and linear classifier, multiclass logistic classification, and show that with a good representation (the filters discovered), such a simple classifier can reach at least state of the art performances. We apply the proposed active set learner in four hyperspectral image classification problems, including agricultural and urban classification at different resolutions, as well as multimodal data. We also propose a hierarchical setting, which allows to generate more complex banks of features that can better describe the nonlinearities present in the data. "
746140417444450304,2016-06-24 00:38:32,https://t.co/FQxtV7nfF6,A review of undirected and acyclic directed Gaussian Markov model selection and estimation. (arXiv:1606.07282v1 [s… https://t.co/FQxtV7nfF6,0,3," Abstract: Markov models lie at the interface between statistical independence in a probability distribution and graph separation properties. We review model selection and estimation in directed and undirected Markov models with Gaussian parametrization, emphasizing the main similarities and differences. These two model types are foundationally similar but not equivalent, as we highlight. We report existing results with a unified notation and terminology, taking into account literature from both the artificial intelligence and statistics research communities, which first developed these models. Finally, we point out the main active research areas and open problems now existing with regard to these traditional, albeit rich, Markov models. "
746140416018362369,2016-06-24 00:38:32,https://t.co/FTVc24ie7y,Identifying individual facial expressions by deconstructing a neural network. (arXiv:1606.07285v1 [cs.CV]) https://t.co/FTVc24ie7y,0,3," Abstract: This paper focuses on the problem of explaining predictions of psychological attributes such as attractiveness, happiness, confidence and intelligence from face photographs using deep neural networks. Since psychological attribute datasets typically suffer from small sample sizes, we apply transfer learning with two base models to avoid overfitting. These models were trained on an age and gender prediction task, respectively. Using a novel explanation method we extract heatmaps that highlight the parts of the image most responsible for the prediction. We further observe that the explanation method provides important insights into the nature of features of the base model, which allow one to assess the aptitude of the base model for a given transfer learning task. Finally, we observe that the multiclass model is more feature rich than its binary counterpart. The experimental evaluation is performed on the 2222 images from the 10k US faces dataset containing psychological attribute labels as well as on a subset of KDEF images. "
746140414248366080,2016-06-24 00:38:32,https://t.co/mpr1COtV3F,Non-convex regularization in remote sensing. (arXiv:1606.07289v1 [stat.ML]) https://t.co/mpr1COtV3F,0,2," Abstract: In this paper, we study the effect of different regularizers and their implications in high dimensional image classification and sparse linear unmixing. Although kernelization or sparse methods are globally accepted solutions for processing data in high dimensions, we present here a study on the impact of the form of regularization used and its parametrization. We consider regularization via traditional squared (2) and sparsity-promoting (1) norms, as well as more unconventional nonconvex regularizers (p and Log Sum Penalty). We compare their properties and advantages on several classification and linear unmixing tasks and provide advices on the choice of the best regularizer for the problem at hand. Finally, we also provide a fully functional toolbox for the community. "
746140412662910976,2016-06-24 00:38:31,https://t.co/Cub8O30pRT,Explaining Predictions of Non-Linear Classifiers in NLP. (arXiv:1606.07298v1 [cs.CL]) https://t.co/Cub8O30pRT,0,5," Abstract: Layer-wise relevance propagation (LRP) is a recently proposed technique for explaining predictions of complex non-linear classifiers in terms of input variables. In this paper, we apply LRP for the first time to natural language processing (NLP). More precisely, we use it to explain the predictions of a convolutional neural network (CNN) trained on a topic categorization task. Our analysis highlights which words are relevant for a specific prediction of the CNN. We compare our technique to standard sensitivity analysis, both qualitatively and quantitatively, using a ""word deleting"" perturbation experiment, a PCA analysis, and various visualizations. All experiments validate the suitability of LRP for explaining the CNN predictions, which is also in line with results reported in recent image classification studies. "
746140410335092736,2016-06-24 00:38:31,https://t.co/3v2xkPq2aX,Unsupervised preprocessing for Tactile Data. (arXiv:1606.07312v1 [cs.RO]) https://t.co/3v2xkPq2aX,0,3," Abstract: Tactile information is important for gripping, stable grasp, and in-hand manipulation, yet the complexity of tactile data prevents widespread use of such sensors. We make use of an unsupervised learning algorithm that transforms the complex tactile data into a compact, latent representation without the need to record ground truth reference data. These compact representations can either be used directly in a reinforcement learning based controller or can be used to calibrate the tactile sensor to physical quantities with only a few datapoints. We show the quality of our latent representation by predicting important features and with a simple control task. "
746140407982071809,2016-06-24 00:38:30,https://t.co/OS6oz0jU7F,DropNeuron: Simplifying the Structure of Deep Neural Networks. (arXiv:1606.07326v1 [cs.CV]) https://t.co/OS6oz0jU7F,0,9," Abstract: Deep learning using multi-layer neural networks (NNs) architecture manifests superb power in modern machine learning systems. The trained Deep Neural Networks (DNNs) are typically large. The question we would like to address is whether it is possible to simplify the NN during training process to achieve a reasonable performance within an acceptable computational time. We presented a novel approach of optimising a deep neural network through regularisation of net- work architecture. We proposed regularisers which support a simple mechanism of dropping neurons during a network training process. The method supports the construction of a simpler deep neural networks with compatible performance with its simplified version. As a proof of concept, we evaluate the proposed method with examples including sparse linear regression, deep autoencoder and convolutional neural network. The valuations demonstrate excellent performance. The code for this work can be found in this http URL "
746140405373206528,2016-06-24 00:38:29,https://t.co/v0LEMDzWv9,Parallel SGD: When does averaging help?. (arXiv:1606.07365v1 [stat.ML]) https://t.co/v0LEMDzWv9,1,5," Abstract: Consider a number of workers running SGD independently on the same pool of data and averaging the models every once in a while -- a common but not well understood practice. We study model averaging as a variance-reducing mechanism and describe two ways in which the frequency of averaging affects convergence. For convex objectives, we show the benefit of frequent averaging depends on the gradient variance envelope. For non-convex objectives, we illustrate that this benefit depends on the presence of multiple globally optimal points. We complement our findings with multicore experiments on both synthetic and real data. "
746140402441388033,2016-06-24 00:38:29,https://t.co/H4cC2233Vk,Personalized Prognostic Models for Oncology: A Machine Learning Approach. (arXiv:1606.07369v1 [stat.AP]) https://t.co/H4cC2233Vk,0,3," Abstract: We have applied a little-known data transformation to subsets of the Surveillance, Epidemiology, and End Results (SEER) publically available data of the National Cancer Institute (NCI) to make it suitable input to standard machine learning classifiers. This transformation properly treats the right-censored data in the SEER data and the resulting Random Forest and Multi-Layer Perceptron models predict full survival curves. Treating the 6, 12, and 60 months points of the resulting survival curves as 3 binary classifiers, the 18 resulting classifiers have AUC values ranging from .765 to .885. Further evidence that the models have generalized well from the training data is provided by the extremely high levels of agreement between the random forest and neural network models predictions on the 6, 12, and 60 month binary classifiers. "
746140399694151680,2016-06-24 00:38:28,https://t.co/gyRM2HyiQg,The combinatorial structure of beta negative binomial processes. (arXiv:1401.0062v4 [math.ST] UPDATED) https://t.co/gyRM2HyiQg,0,2," Abstract: We characterize the combinatorial structure of conditionally-i.i.d. sequences of negative binomial processes with a common beta process base measure. In Bayesian nonparametric applications, such processes have served as models for latent multisets of features underlying data. Analogously, random subsets arise from conditionally-i.i.d. sequences of Bernoulli processes with a common beta process base measure, in which case the combinatorial structure is described by the Indian buffet process. Our results give a count analogue of the Indian buffet process, which we call a negative binomial Indian buffet process. As an intermediate step toward this goal, we provide a construction for the beta negative binomial process that avoids a representation of the underlying beta process base measure. We describe the key Markov kernels needed to use a NB-IBP representation in a Markov Chain Monte Carlo algorithm targeting a posterior distribution. "
746140397634621440,2016-06-24 00:38:28,https://t.co/XgsdAT3ioY,A Unified Theory of Confidence Regions and Testing for High Dimensional Estimating Equations. (arXiv:1510.08986v2 … https://t.co/XgsdAT3ioY,0,5," Abstract: We propose a new inferential framework for constructing confidence regions and testing hypotheses in statistical models specified by a system of high dimensional estimating equations. We construct an influence function by projecting the fitted estimating equations to a sparse direction obtained by solving a large-scale linear program. Our main theoretical contribution is to establish a unified Z-estimation theory of confidence regions for high dimensional problems. Different from existing methods, all of which require the specification of the likelihood or pseudo-likelihood, our framework is likelihood-free. As a result, our approach provides valid inference for a broad class of high dimensional constrained estimating equation problems, which are not covered by existing methods. Such examples include, noisy compressed sensing, instrumental variable regression, undirected graphical models, discriminant analysis and vector autoregressive models. We present detailed theoretical results for all these examples. Finally, we conduct thorough numerical simulations, and a real dataset analysis to back up the developed theoretical results. "
746140395206283264,2016-06-24 00:38:27,https://t.co/CxDS8gEVE1,Signed Support Recovery for Single Index Models in High-Dimensions. (arXiv:1511.02270v2 [math.ST] UPDATED) https://t.co/CxDS8gEVE1,0,2," Abstract: In this paper we study the support recovery problem for single index models $Y=f(\boldsymbol{X}^{\intercal} \boldsymbol{\beta},\varepsilon)$, where $f$ is an unknown link function, $\boldsymbol{X}\sim N_p(0,\mathbb{I}_{p})$ and $\boldsymbol{\beta}$ is an $s$-sparse unit vector such that $\boldsymbol{\beta}_{i}\in \{\pm\frac{1}{\sqrt{s}},0\}$. In particular, we look into the performance of two computationally inexpensive algorithms: (a) the diagonal thresholding sliced inverse regression (DT-SIR) introduced by Lin et al. (2015); and (b) a semi-definite programming (SDP) approach inspired by Amini & Wainwright (2008). When $s=O(p^{1-\delta})$ for some $\delta>0$, we demonstrate that both procedures can succeed in recovering the support of $\boldsymbol{\beta}$ as long as the rescaled sample size $\kappa=\frac{n}{s\log(p-s)}$ is larger than a certain critical threshold. On the other hand, when $\kappa$ is smaller than a critical value, any algorithm fails to recover the support with probability at least $\frac{1}{2}$ asymptotically. In other words, we demonstrate that both DT-SIR and the SDP approach are optimal (up to a scalar) for recovering the support of $\boldsymbol{\beta}$ in terms of sample size. We provide extensive simulations, as well as a real dataset application to help verify our theoretical observations. "
746140392609955841,2016-06-24 00:38:26,https://t.co/Pzs2TFVDny,L1-Regularized Least Squares for Support Recovery of High Dimensional Single Index Models with Gaussian Designs. (… https://t.co/Pzs2TFVDny,0,3," Abstract: It is known that for a certain class of single index models (SIMs) $Y = f(\boldsymbol{X}_{p \times 1}^\intercal\boldsymbol{\beta}_0, \varepsilon)$, support recovery is impossible when $\boldsymbol{X} \sim \mathcal{N}(0, \mathbb{I}_{p \times p})$ and a model complexity adjusted sample size is below a critical threshold. Recently, optimal algorithms based on Sliced Inverse Regression (SIR) were suggested. These algorithms work provably under the assumption that the design $\boldsymbol{X}$ comes from an i.i.d. Gaussian distribution. In the present paper we analyze algorithms based on covariance screening and least squares with $L_1$ penalization (i.e. LASSO) and demonstrate that they can also enjoy optimal (up to a scalar) rescaled sample size in terms of support recovery, albeit under slightly different assumptions on $f$ and $\varepsilon$ compared to the SIR based algorithms. Furthermore, we show more generally, that LASSO succeeds in recovering the signed support of $\boldsymbol{\beta}_0$ if $\boldsymbol{X} \sim \mathcal{N}(0, \boldsymbol{\Sigma})$, and the covariance $\boldsymbol{\Sigma}$ satisfies the irrepresentable condition. Our work extends existing results on the support recovery of LASSO for the linear model, to a more general class of SIMs. "
746140390659600390,2016-06-24 00:38:26,https://t.co/o59WJAsqpi,RSG: Beating SG without Smoothness and/or Strong Convexity. (arXiv:1512.03107v10 [math.OC] UPDATED) https://t.co/o59WJAsqpi,0,4," Abstract: In this paper, we study the efficiency of a {\bf R}estarted {\bf S}ub{\bf G}radient (RSG) method that periodically restarts the standard subgradient method (SG). We show that, when applied to a broad class of convex optimization problems, RSG method can find an $\epsilon$-optimal solution with a low complexity than SG method. In particular, we first show that RSG can reduce the dependence of SG's iteration complexity on the distance between the initial solution and the optimal set to that between the $\epsilon$-level set and the optimal set. In addition, we show the advantages of RSG over SG in solving three different families of convex optimization problems. (a) For the problems whose epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with local quadratic growth property, RSG has an $O(\frac{1}{\epsilon}\log(\frac{1}{\epsilon}))$ iteration complexity. (c) For the problems that admit a local Kurdyka-\L ojasiewicz property with a power constant of $\beta\in[0,1)$, RSG has an $O(\frac{1}{\epsilon^{2\beta}}\log(\frac{1}{\epsilon}))$ iteration complexity. On the contrary, with only the standard analysis, the iteration complexity of SG is known to be $O(\frac{1}{\epsilon^2})$ for these three classes of problems. The novelty of our analysis lies at exploiting the lower bound of the first-order optimality residual at the $\epsilon$-level set. It is this novelty that allows us to explore the local properties of functions (e.g., local quadratic growth property, local Kurdyka-\L ojasiewicz property, more generally local error bounds) to develop the improved convergence of RSG. We demonstrate the effectiveness of the proposed algorithms on several machine learning tasks including regression and classification. "
746140388776419328,2016-06-24 00:38:25,https://t.co/sa43vsEPnG,Online Active Linear Regression via Thresholding. (arXiv:1602.02845v3 [stat.ML] UPDATED) https://t.co/sa43vsEPnG,0,2," Abstract: We consider the problem of online active learning to collect data for regression modeling. Specifically, we consider a decision maker with a limited experimentation budget who must efficiently learn an underlying linear population model. Our main contribution is a novel threshold-based algorithm for selection of most informative observations; we characterize its performance and fundamental lower bounds. We extend the algorithm and its guarantees to sparse linear regression in high-dimensional settings. Simulations suggest the algorithm is remarkably robust: it provides significant benefits over passive random sampling in real-world datasets that exhibit high nonlinearity and high dimensionality --- significantly reducing both the mean and variance of the squared error. "
746140383265103873,2016-06-24 00:38:24,https://t.co/AHjtXTiJHd,Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors. (arXiv:1603.04733v5 [stat.ML] … https://t.co/AHjtXTiJHd,2,9," Abstract: We introduce a variational Bayesian neural network where the parameters are governed via a probability distribution on random matrices. Specifically, we employ a matrix variate Gaussian \cite{gupta1999matrix} parameter posterior distribution where we explicitly model the covariance among the input and output dimensions of each layer. Furthermore, with approximate covariance matrices we can achieve a more efficient way to represent those correlations that is also cheaper than fully factorized parameter posteriors. We further show that with the ""local reprarametrization trick"" \cite{kingma2015variational} on this posterior distribution we arrive at a Gaussian Process \cite{rasmussen2006gaussian} interpretation of the hidden units in each layer and we, similarly with \cite{gal2015dropout}, provide connections with deep Gaussian processes. We continue in taking advantage of this duality and incorporate ""pseudo-data"" \cite{snelson2005sparse} in our model, which in turn allows for more efficient sampling while maintaining the properties of the original model. The validity of the proposed approach is verified through extensive experiments. "
745780279093170178,2016-06-23 00:47:29,https://t.co/vHOGPqD8bh,"Dealing with a large number of classes -- Likelihood, Discrimination or Ranking?. (arXiv:1606.06959v1 [stat.ML]) https://t.co/vHOGPqD8bh",1,8," Abstract: We consider training probabilistic classifiers in the case of a large number of classes. The number of classes is assumed too large to perform exact normalisation over all classes. To account for this we consider a simple approach that directly approximates the likelihood. We show that this simple approach works well on toy problems and is competitive with recently introduced alternative non-likelihood based approximations. Furthermore, we relate this approach to a simple ranking objective. This leads us to suggest a specific setting for the optimal threshold in the ranking objective. "
745780276257783808,2016-06-23 00:47:28,https://t.co/iJjAFc858s,Towards stationary time-vertex signal processing. (arXiv:1606.06962v1 [cs.LG]) https://t.co/iJjAFc858s,0,2," Abstract: Graph-based methods for signal processing have shown promise for the analysis of data exhibiting irregular structure, such as those found in social, transportation, and sensor networks. Yet, though these systems are often dynamic, state-of-the-art methods for signal processing on graphs ignore the dimension of time, treating successive graph signals independently or taking a global average. To address this shortcoming, this paper considers the statistical analysis of time-varying graph signals. We introduce a novel definition of joint (time-vertex) stationarity, which generalizes the classical definition of time stationarity and the more recent definition appropriate for graphs. Joint stationarity gives rise to a scalable Wiener optimization framework for joint denoising, semi-supervised learning, or more generally inversing a linear operator, that is provably optimal. Experimental results on real weather data demonstrate that taking into account graph and time dimensions jointly can yield significant accuracy improvements in the reconstruction effort. "
745780273686667264,2016-06-23 00:47:27,https://t.co/kpXJehMU0p,Robust Identifiability in Sparse Dictionary Learning. (arXiv:1606.06997v1 [stat.ML]) https://t.co/kpXJehMU0p,0,4," Abstract: Sparse coding or sparse dictionary learning methods have exposed underlying sparse structure in many kinds of natural data. Here, we generalize previous results guaranteeing when the learned dictionary and sparse codes are unique up to inherent permutation and scaling ambiguities. We show that these solutions are robust to the addition of measurement noise provided the data samples are sufficiently diverse. Central to our proofs is a useful lemma in combinatorial matrix theory which allows us to derive bounds on the number of samples necessary to guarantee uniqueness. We also provide probabilistic extensions to our robust identifiability theorem and an extension to the case where only an upper bound on the number of dictionary elements is known a priori. Our results help to inform the interpretation of sparse structure learned from data; whenever the conditions to one of our robust identifiability theorems are met, any sparsity-constrained algorithm that succeeds in approximately reconstructing the data well enough recovers the original dictionary and sparse codes up to an error commensurate with the noise. We discuss applications of this result to smoothed analysis, communication theory, and applied physics and engineering. "
745780269806948352,2016-06-23 00:47:26,https://t.co/wVUkaZceeZ,Efficient Attack Graph Analysis through Approximate Inference. (arXiv:1606.07025v1 [cs.CR]) https://t.co/wVUkaZceeZ,0,4," Abstract: Attack graphs provide compact representations of the attack paths that an attacker can follow to compromise network resources by analysing network vulnerabilities and topology. These representations are a powerful tool for security risk assessment. Bayesian inference on attack graphs enables the estimation of the risk of compromise to the system's components given their vulnerabilities and interconnections, and accounts for multi-step attacks spreading through the system. Whilst static analysis considers the risk posture at rest, dynamic analysis also accounts for evidence of compromise, e.g. from SIEM software or forensic investigation. However, in this context, exact Bayesian inference techniques do not scale well. In this paper we show how Loopy Belief Propagation - an approximate inference technique - can be applied to attack graphs, and that it scales linearly in the number of nodes for both static and dynamic analysis, making such analyses viable for larger networks. We experiment with different topologies and network clustering on synthetic Bayesian attack graphs with thousands of nodes to show that the algorithm's accuracy is acceptable and converge to a stable solution. We compare sequential and parallel versions of Loopy Belief Propagation with exact inference techniques for both static and dynamic analysis, showing the advantages of approximate inference techniques to scale to larger attack graphs. "
745780266820575232,2016-06-23 00:47:26,https://t.co/EsFJuGt8v5,Ancestral Causal Inference. (arXiv:1606.07035v1 [cs.LG]) https://t.co/EsFJuGt8v5,0,5," Abstract: Constraint-based causal discovery from limited data is a notoriously difficult challenge due to the many borderline independence test decisions. Several approaches to improve the reliability of the predictions by exploiting redundancy in the independence information have been proposed recently. Though promising, existing approaches can still be greatly improved in terms of accuracy and scalability. We present a novel method that reduces the combinatorial explosion of the search space by using a more coarse-grained representation of causal information, drastically reducing computation time. Additionally, we propose a method to score causal predictions based on their confidence. Crucially, our implementation also allows one to easily combine observational and interventional data and to incorporate various types of available background knowledge. We prove soundness and asymptotic consistency of our method and demonstrate that it can outperform the state-of-the-art on synthetic data, achieving a speedup of several orders of magnitude. We illustrate its practical feasibility by applying it on a challenging protein data set. "
745780263930716164,2016-06-23 00:47:25,https://t.co/TsEM9fMdKJ,Toward Interpretable Topic Discovery via Anchored Correlation Explanation. (arXiv:1606.07043v1 [stat.ML]) https://t.co/TsEM9fMdKJ,2,6," Abstract: Many predictive tasks, such as diagnosing a patient based on their medical chart, are ultimately defined by the decisions of human experts. Unfortunately, encoding experts' knowledge is often time consuming and expensive. We propose a simple way to use fuzzy and informal knowledge from experts to guide discovery of interpretable latent topics in text. The underlying intuition of our approach is that latent factors should be informative about both correlations in the data and a set of relevance variables specified by an expert. Mathematically, this approach is a combination of the information bottleneck and Total Correlation Explanation (CorEx). We give a preliminary evaluation of Anchored CorEx, showing that it produces more coherent and interpretable topics on two distinct corpora. "
745780261800054787,2016-06-23 00:47:24,https://t.co/oUHYfqTef3,Risk-consistency of cross-validation with lasso-type procedures. (arXiv:1308.0810v3 [math.ST] UPDATED) https://t.co/oUHYfqTef3,0,4," Abstract: The lasso and related sparsity inducing algorithms have been the target of substantial theoretical and applied research. Correspondingly, many results are known about their behavior for a fixed or optimally chosen tuning parameter specified up to unknown constants. In practice, however, this oracle tuning parameter is inaccessible so one must use the data to select one. Common statistical practice is to use a variant of cross-validation for this task. However, little is known about the theoretical properties of the resulting predictions with such data-dependent methods. We consider the high-dimensional setting with random design wherein the number of predictors $p$ grows with the number of observations $n$. Under typical assumptions on the data generating process, similar to those in the literature, we recover oracle rates up to a log factor when choosing the tuning parameter with cross-validation. Under weaker conditions, when the true model is not necessarily linear, we show that the lasso remains risk consistent relative to its linear oracle. We also generalize these results to the group lasso and square-root lasso and investigate the predictive and model selection performance of cross-validation via simulation. "
745780259304443904,2016-06-23 00:47:24,https://t.co/TC4jo7vH96,Active Algorithms For Preference Learning Problems with Multiple Populations. (arXiv:1603.04118v2 [stat.ML] UPDATE… https://t.co/TC4jo7vH96,0,6," Abstract: In this paper we model the problem of learning preferences of a population as an active learning problem. We propose an algorithm can adaptively choose pairs of items to show to users coming from a heterogeneous population, and use the obtained reward to decide which pair of items to show next. We provide computationally efficient algorithms with provable sample complexity guarantees for this problem in both the noiseless and noisy cases. In the process of establishing sample complexity guarantees for our algorithms, we establish new results using a Nystr{\""o}m-like method which can be of independent interest. We supplement our theoretical results with experimental comparisons. "
745780256133550080,2016-06-23 00:47:23,https://t.co/t9qEj0M07l,Extreme Stochastic Variational Inference: Distributed and Asynchronous. (arXiv:1605.09499v2 [stat.ML] UPDATED) https://t.co/t9qEj0M07l,0,14," Abstract: We propose extreme stochastic variational inference (ESVI), which allows multiple processors to simultaneously and asynchronously perform variational inference updates. Moreover, by using a classic owner computes paradigm, our algorithm can be made lock-free. ESVI exhibits data and model parallelism, that is, each processor only needs access to a subset of the data and a subset of the parameters. In our experiments we show that our new algorithm outperforms a straightforward strategy for parallelizing variational inference, which requires bulk synchronization after every iteration. "
745415475736223744,2016-06-22 00:37:53,https://t.co/59T14WLf4J,Twitter as a Source of Global Mobility Patterns for Social Good. (arXiv:1606.06343v1 [cs.SI]) https://t.co/59T14WLf4J,4,9," Abstract: Data on human spatial distribution and movement is essential for understanding and analyzing social systems. However existing sources for this data are lacking in various ways; difficult to access, biased, have poor geographical or temporal resolution, or are significantly delayed. In this paper, we describe how geolocation data from Twitter can be used to estimate global mobility patterns and address these shortcomings. These findings will inform how this novel data source can be harnessed to address humanitarian and development efforts. "
745415473295032324,2016-06-22 00:37:52,https://t.co/C6cLQzfBxJ,Visualizing textual models with in-text and word-as-pixel highlighting. (arXiv:1606.06352v1 [stat.ML]) https://t.co/C6cLQzfBxJ,1,7," Abstract: We explore two techniques which use color to make sense of statistical text models. One method uses in-text annotations to illustrate a model's view of particular tokens in particular documents. Another uses a high-level, ""words-as-pixels"" graphic to display an entire corpus. Together, these methods offer both zoomed-in and zoomed-out perspectives into a model's understanding of text. We show how these interconnected methods help diagnose a classifier's poor performance on Twitter slang, and make sense of a topic model on historical political texts. "
745415471781003264,2016-06-22 00:37:52,https://t.co/UdIH5SS9u1,Complex Embeddings for Simple Link Prediction. (arXiv:1606.06357v1 [cs.AI]) https://t.co/UdIH5SS9u1,0,2," Abstract: In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks. "
745415470304595968,2016-06-22 00:37:51,https://t.co/wFyYZcc9AX,A Probabilistic Generative Grammar for Semantic Parsing. (arXiv:1606.06361v1 [cs.CL]) https://t.co/wFyYZcc9AX,0,6," Abstract: We present a framework that couples the syntax and semantics of natural language sentences in a generative model, in order to develop a semantic parser that jointly infers the syntactic, morphological, and semantic representations of a given sentence under the guidance of background knowledge. To generate a sentence in our framework, a semantic statement is first sampled from a prior, such as from a set of beliefs in a knowledge base. Given this semantic statement, a grammar probabilistically generates the output sentence. A joint semantic-syntactic parser is derived that returns the $k$-best semantic and syntactic parses for a given sentence. The semantic prior is flexible, and can be used to incorporate background knowledge during parsing, in ways unlike previous semantic parsing approaches. For example, semantic statements corresponding to beliefs in a knowledge base can be given higher prior probability, type-correct statements can be given somewhat lower probability, and beliefs outside the knowledge base can be given lower probability. The construction of our grammar invokes a novel application of hierarchical Dirichlet processes (HDPs), which in turn, requires a novel and efficient inference approach. We present experimental results showing, for a simple grammar, that our parser outperforms a state-of-the-art CCG semantic parser and scales to knowledge bases with millions of beliefs. "
745415468945645568,2016-06-22 00:37:51,https://t.co/Rbyc1Rz5I2,Predicting Student Dropout in Higher Education. (arXiv:1606.06364v1 [stat.ML]) https://t.co/Rbyc1Rz5I2,1,6," Abstract: Each year, roughly 30% of first-year students at US baccalaureate institutions do not return for their second year and over $9 billion is spent educating these students. Yet, little quantitative research has analyzed the causes and possible remedies for student attrition. Here, we describe initial efforts to model student dropout using the largest known dataset on higher education attrition, which tracks over 32,500 students' demographics and transcript records at one of the nation's largest public universities. Our results highlight several early indicators of student attrition and show that dropout can be accurately predicted even when predictions are based on a single term of academic transcript data. These results highlight the potential for machine learning to have an impact on student retention and success while pointing to several promising directions for future work. "
745415467679031297,2016-06-22 00:37:51,https://t.co/zmBCLKHx2L,FSMJ: Feature Selection with Maximum Jensen-Shannon Divergence for Text Categorization. (arXiv:1606.06366v1 [stat.… https://t.co/zmBCLKHx2L,1,5," Abstract: In this paper, we present a new wrapper feature selection approach based on Jensen-Shannon (JS) divergence, termed feature selection with maximum JS-divergence (FSMJ), for text categorization. Unlike most existing feature selection approaches, the proposed FSMJ approach is based on real-valued features which provide more information for discrimination than binary-valued features used in conventional approaches. We show that the FSMJ is a greedy approach and the JS-divergence monotonically increases when more features are selected. We conduct several experiments on real-life data sets, compared with the state-of-the-art feature selection approaches for text categorization. The superior performance of the proposed FSMJ approach demonstrates its effectiveness and further indicates its wide potential applications on data mining. "
745415466068357120,2016-06-22 00:37:50,https://t.co/N47yE5EBHB,Kernel-based Generative Learning in Distortion Feature Space. (arXiv:1606.06377v1 [stat.ML]) https://t.co/N47yE5EBHB,1,8," Abstract: This paper presents a novel kernel-based generative classifier which is defined in a distortion subspace using polynomial series expansion, named Kernel-Distortion (KD) classifier. An iterative kernel selection algorithm is developed to steadily improve classification performance by repeatedly removing and adding kernels. The experimental results on character recognition application not only show that the proposed generative classifier performs better than many existing classifiers, but also illustrate that it has different recognition capability compared to the state-of-the-art discriminative classifier - deep belief network. The recognition diversity indicates that a hybrid combination of the proposed generative classifier and the discriminative classifier could further improve the classification performance. Two hybrid combination methods, cascading and stacking, have been implemented to verify the diversity and the improvement of the proposed classifier. "
745415464596045824,2016-06-22 00:37:50,https://t.co/4JPhIW6so4,Social-sparsity brain decoders: faster spatial sparsity. (arXiv:1606.06439v1 [stat.ML]) https://t.co/4JPhIW6so4,0,2," Abstract: Spatially-sparse predictors are good models for brain decoding: they give accurate predictions and their weight maps are interpretable as they focus on a small number of regions. However, the state of the art, based on total variation or graph-net, is computationally costly. Here we introduce sparsity in the local neighborhood of each voxel with social-sparsity, a structured shrinkage operator. We find that, on brain imaging classification problems, social-sparsity performs almost as well as total-variation models and better than graph-net, for a fraction of the computational cost. It also very clearly outlines predictive regions. We give details of the model and the algorithm. "
745415462956109826,2016-06-22 00:37:50,https://t.co/mJzsk0RpuK,An artificial neural network to find correlation patterns among an arbitrary number of variables. (arXiv:1606.0656… https://t.co/mJzsk0RpuK,1,6," Abstract: Methods to find correlation among variables are of interest to many disciplines, including statistics, machine learning, (big) data mining and neurosciences. Parameters that measure correlation between two variables are of limited utility when used with multiple variables. In this work, I propose a simple criterion to measure correlation among an arbitrary number of variables, based on a data set. The central idea is to i) design a function of the variables that can take different forms depending on a set of parameters, ii) calculate the difference between a statistics associated to the function computed on the data set and the same statistics computed on a randomised version of the data set, called ""scrambled"" data set, and iii) optimise the parameters to maximise this difference. Many such functions can be organised in layers, which can in turn be stacked one on top of the other, forming a neural network. The function parameters are searched with an enhanced genetic algortihm called POET and the resulting method is tested on a cancer gene data set. The method may have potential implications for some issues that affect the field of neural networks, such as overfitting, the need to process huge amounts of data for training and the presence of ""adversarial examples"". "
745415461530124288,2016-06-22 00:37:49,https://t.co/fs3hQdtVYg,On the consistency of inversion-free parameter estimation for Gaussian random fields. (arXiv:1601.03822v2 [math.ST… https://t.co/fs3hQdtVYg,0,6," Abstract: Gaussian random fields are a powerful tool for modeling environmental processes. For high dimensional samples, classical approaches for estimating the covariance parameters require highly challenging and massive computations, such as the evaluation of the Cholesky factorization or solving linear systems. Recently, Anitescu, Chen and Stein \cite{M.Anitescu} proposed a fast and scalable algorithm which does not need such burdensome computations. The main focus of this article is to study the asymptotic behavior of the algorithm of Anitescu et al. (ACS) for regular and irregular grids in the increasing domain setting. Consistency, minimax optimality and asymptotic normality of this algorithm are proved under mild differentiability conditions on the covariance function. Despite the fact that ACS's method entails a non-concave maximization, our results hold for any stationary point of the objective function. A numerical study is presented to evaluate the efficiency of this algorithm for large data sets. "
745415459219054592,2016-06-22 00:37:49,https://t.co/BOiPA4EBTD,A Theoretical Analysis of Deep Neural Networks for Texture Classification. (arXiv:1605.02699v2 [cs.CV] UPDATED) https://t.co/BOiPA4EBTD,2,10," Abstract: We investigate the use of Deep Neural Networks for the classification of image datasets where texture features are important for generating class-conditional discriminative representations. To this end, we first derive the size of the feature space for some standard textural features extracted from the input dataset and then use the theory of Vapnik-Chervonenkis dimension to show that hand-crafted feature extraction creates low-dimensional representations which help in reducing the overall excess error rate. As a corollary to this analysis, we derive for the first time upper bounds on the VC dimension of Convolutional Neural Network as well as Dropout and Dropconnect networks and the relation between excess error rate of Dropout and Dropconnect networks. The concept of intrinsic dimension is used to validate the intuition that texture-based datasets are inherently higher dimensional as compared to handwritten digits or other object recognition datasets and hence more difficult to be shattered by neural networks. We then derive the mean distance from the centroid to the nearest and farthest sampling points in an n-dimensional manifold and show that the Relative Contrast of the sample data vanishes as dimensionality of the underlying vector space tends to infinity. "
745056140917084160,2016-06-21 00:50:01,https://t.co/GQ8bwBY9Vz,Interpretability in Linear Brain Decoding. (arXiv:1606.05672v1 [stat.ML]) https://t.co/GQ8bwBY9Vz,1,4," Abstract: Improving the interpretability of brain decoding approaches is of primary interest in many neuroimaging studies. Despite extensive studies of this type, at present, there is no formal definition for interpretability of brain decoding models. As a consequence, there is no quantitative measure for evaluating the interpretability of different brain decoding methods. In this paper, we present a simple definition for interpretability of linear brain decoding models. Then, we propose to combine the interpretability and the performance of the brain decoding into a new multi-objective criterion for model selection. Our preliminary results on the toy data show that optimizing the hyper-parameters of the regularized linear classifier based on the proposed criterion results in more informative linear models. The presented definition provides the theoretical background for quantitative evaluation of interpretability in linear brain decoding. "
745056139793039360,2016-06-21 00:50:00,https://t.co/0UNEMUwy4E,Using Visual Analytics to Interpret Predictive Machine Learning Models. (arXiv:1606.05685v1 [stat.ML]) https://t.co/0UNEMUwy4E,1,7," Abstract: It is commonly believed that increasing the interpretability of a machine learning model may decrease its predictive power. However, inspecting input-output relationships of those models using visual analytics, while treating them as black-box, can help to understand the reasoning behind outcomes without sacrificing predictive quality. We identify a space of possible solutions and provide two examples of where such techniques have been successfully used in practice. "
745056138475888641,2016-06-21 00:50:00,https://t.co/6nmIgZeFLe,Structured Stochastic Linear Bandits. (arXiv:1606.05693v1 [stat.ML]) https://t.co/6nmIgZeFLe,0,7," Abstract: The stochastic linear bandit problem proceeds in rounds where at each round the algorithm selects a vector from a decision set after which it receives a noisy linear loss parameterized by an unknown vector. The goal in such a problem is to minimize the (pseudo) regret which is the difference between the total expected loss of the algorithm and the total expected loss of the best fixed vector in hindsight. In this paper, we consider settings where the unknown parameter has structure, e.g., sparse, group sparse, low-rank, which can be captured by a norm, e.g., $L_1$, $L_{(1,2)}$, nuclear norm. We focus on constructing confidence ellipsoids which contain the unknown parameter across all rounds with high-probability. We show the radius of such ellipsoids depend on the Gaussian width of sets associated with the norm capturing the structure. Such characterization leads to tighter confidence ellipsoids and, therefore, sharper regret bounds compared to bounds in the existing literature which are based on the ambient dimensionality. "
745056135342751744,2016-06-21 00:49:59,https://t.co/JqkUk3BPfq,An Efficient Large-scale Semi-supervised Multi-label Classifier Capable of Handling Missing labels. (arXiv:1606.05… https://t.co/JqkUk3BPfq,0,6," Abstract: Multi-label classification has received considerable interest in recent years. Multi-label classifiers have to address many problems including: handling large-scale datasets with many instances and a large set of labels, compensating missing label assignments in the training set, considering correlations between labels, as well as exploiting unlabeled data to improve prediction performance. To tackle datasets with a large set of labels, embedding-based methods have been proposed which seek to represent the label assignments in a low-dimensional space. Many state-of-the-art embedding-based methods use a linear dimensionality reduction to represent the label assignments in a low-dimensional space. However, by doing so, these methods actually neglect the tail labels - labels that are infrequently assigned to instances. We propose an embedding-based method that non-linearly embeds the label vectors using an stochastic approach, thereby predicting the tail labels more accurately. Moreover, the proposed method have excellent mechanisms for handling missing labels, dealing with large-scale datasets, as well as exploiting unlabeled data. With the best of our knowledge, our proposed method is the first multi-label classifier that simultaneously addresses all of the mentioned challenges. Experiments on real-world datasets show that our method outperforms stateof-the-art multi-label classifiers by a large margin, in terms of prediction performance, as well as training time. "
745056133115682816,2016-06-21 00:49:59,https://t.co/XVZEiQtJMs,Interpretable Two-level Boolean Rule Learning for Classification. (arXiv:1606.05798v1 [stat.ML]) https://t.co/XVZEiQtJMs,1,3," Abstract: As a contribution to interpretable machine learning research, we develop a novel optimization framework for learning accurate and sparse two-level Boolean rules. We consider rules in both conjunctive normal form (AND-of-ORs) and disjunctive normal form (OR-of-ANDs). A principled objective function is proposed to trade classification accuracy and interpretability, where we use Hamming loss to characterize accuracy and sparsity to characterize interpretability. We propose efficient procedures to optimize these objectives based on linear programming (LP) relaxation, block coordinate descent, and alternating minimization. Experiments show that our new algorithms provide very good tradeoffs between accuracy and interpretability. "
745056131819659268,2016-06-21 00:49:58,https://t.co/wYiG6QOp62,Building an Interpretable Recommender via Loss-Preserving Transformation. (arXiv:1606.05819v1 [stat.ML]) https://t.co/wYiG6QOp62,1,5," Abstract: We propose a method for building an interpretable recommender system for personalizing online content and promotions. Historical data available for the system consists of customer features, provided content (promotions), and user responses. Unlike in a standard multi-class classification setting, misclassification costs depend on both recommended actions and customers. Our method transforms such a data set to a new set which can be used with standard interpretable multi-class classification algorithms. The transformation has the desirable property that minimizing the standard misclassification penalty in this new space is equivalent to minimizing the custom cost function. "
745056130175401984,2016-06-21 00:49:58,https://t.co/9ueKU42dWo,Tight Performance Bounds for Compressed Sensing With Group Sparsity. (arXiv:1606.05889v1 [stat.ML]) https://t.co/9ueKU42dWo,0,4," Abstract: Compressed sensing refers to the recovery of a high-dimensional but sparse vector using a small number of linear measurements. Minimizing the $\ell_1$-norm is among the more popular approaches for compressed sensing. A recent paper by Cai and Zhang has provided the ""best possible"" bounds for $\ell_1$-norm minimization to achieve robust sparse recovery (a formal statement of compressed sensing). In some applications, ""group sparsity"" is more natural than conventional sparsity. In this paper we present sufficient conditions for $\ell_1$-norm minimization to achieve robust group sparse recovery. When specialized to conventional sparsity, these conditions reduce to the known ""best possible"" bounds proved earlier by Cai and Zhang. This is achieved by stating and proving a group robust null space property, which is a new result even for conventional sparsity. We also derive bounds for the $\ell_p$-norm of the residual error between the true vector and its approximation, for all $p \in [1,2]$. These bounds are new even for conventional sparsity and of course also for group sparsity, because previously error bounds were available only for the $\ell_2$-norm. "
745056127868493825,2016-06-21 00:49:58,https://t.co/AoZMKGQSCQ,Clustering with a Reject Option: Interactive Clustering as Bayesian Prior Elicitation. (arXiv:1606.05896v1 [stat.M… https://t.co/AoZMKGQSCQ,1,3," Abstract: A good clustering can help a data analyst to explore and understand a data set, but what constitutes a good clustering may depend on domain-specific and application-specific criteria. These criteria can be difficult to formalize, even when it is easy for an analyst to know a good clustering when they see one. We present a new approach to interactive clustering for data exploration called TINDER, based on a particularly simple feedback mechanism, in which an analyst can reject a given clustering and request a new one, which is chosen to be different from the previous clustering while fitting the data well. We formalize this interaction in a Bayesian framework as a method for prior elicitation, in which each different clustering is produced by a prior distribution that is modified to discourage previously rejected clusterings. We show that TINDER successfully produces a diverse set of clusterings, each of equivalent quality, that are much more diverse than would be obtained by randomized restarts. "
745056125586907136,2016-06-21 00:49:57,https://t.co/Qfh9Rt0wWu,Tutorial on Variational Autoencoders. (arXiv:1606.05908v1 [stat.ML]) https://t.co/Qfh9Rt0wWu,0,24," Abstract: In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed. "
745056124450267136,2016-06-21 00:49:57,https://t.co/8ifv9CgSfE,Graph based manifold regularized deep neural networks for automatic speech recognition. (arXiv:1606.05925v1 [stat.… https://t.co/8ifv9CgSfE,0,7," Abstract: Deep neural networks (DNNs) have been successfully applied to a wide variety of acoustic modeling tasks in recent years. These include the applications of DNNs either in a discriminative feature extraction or in a hybrid acoustic modeling scenario. Despite the rapid progress in this area, a number of challenges remain in training DNNs. This paper presents an effective way of training DNNs using a manifold learning based regularization framework. In this framework, the parameters of the network are optimized to preserve underlying manifold based relationships between speech feature vectors while minimizing a measure of loss between network outputs and targets. This is achieved by incorporating manifold based locality constraints in the objective criterion of DNNs. Empirical evidence is provided to demonstrate that training a network with manifold constraints preserves structural compactness in the hidden layers of the network. Manifold regularization is applied to train bottleneck DNNs for feature extraction in hidden Markov model (HMM) based speech recognition. The experiments in this work are conducted on the Aurora-2 spoken digits and the Aurora-4 read news large vocabulary continuous speech recognition tasks. The performance is measured in terms of word error rate (WER) on these tasks. It is shown that the manifold regularized DNNs result in up to 37% reduction in WER relative to standard DNNs. "
745056123078619137,2016-06-21 00:49:56,https://t.co/OJuR74BaE5,Continuum directions for supervised dimension reduction. (arXiv:1606.05988v1 [stat.ME]) https://t.co/OJuR74BaE5,0,3," Abstract: We consider dimension reduction of multivariate data under the existence of various types of auxiliary information. We propose a criterion that provides a series of orthogonal directional vectors, that form a basis for dimension reduction. The proposed method can be thought of as an extension from the continuum regression, and the resulting basis is called continuum directions. We show that these directions continuously bridge the principal component, mean difference and linear discriminant directions, thus ranging from unsupervised to fully supervised dimension reduction. With a presence of binary supervision data, the proposed directions can be directly used for a two-group classification. Numerical studies show that the proposed method works well in high-dimensional settings where the variance of the first principal component is much larger than the rest. "
745056120406827012,2016-06-21 00:49:56,https://t.co/pFSqLNsu6N,Quantifying and Reducing Stereotypes in Word Embeddings. (arXiv:1606.06121v1 [cs.CL]) https://t.co/pFSqLNsu6N,1,2," Abstract: Machine learning algorithms are optimized to model statistical properties of the training data. If the input data reflects stereotypes and biases of the broader society, then the output of the learning algorithm also captures these stereotypes. In this paper, we initiate the study of gender stereotypes in {\em word embedding}, a popular framework to represent text data. As their use becomes increasingly common, applications can inadvertently amplify unwanted stereotypes. We show across multiple datasets that the embeddings contain significant gender stereotypes, especially with regard to professions. We created a novel gender analogy task and combined it with crowdsourcing to systematically quantify the gender bias in a given embedding. We developed an efficient algorithm that reduces gender stereotype using just a handful of training examples while preserving the useful geometric properties of the embedding. We evaluated our algorithm on several metrics. While we focus on male/female stereotypes, our framework may be applicable to other types of embedding biases. "
745056117978456064,2016-06-21 00:49:55,https://t.co/3DEduIRnqi,High Confidence Off-Policy Evaluation with Models. (arXiv:1606.06126v1 [cs.AI]) https://t.co/3DEduIRnqi,0,3," Abstract: In many reinforcement learning applications executing a poor policy may be costly or even dangerous. Thus, it is desirable to determine confidence interval lower bounds on the performance of any given policy without executing said policy. Current methods for high confidence off-policy evaluation require a substantial amount of data to achieve a tight lower bound, while existing model-based methods only address the problem in discrete state spaces. We propose two bootstrapping approaches combined with learned MDP transition models in order to efficiently estimate lower confidence bounds on policy performance with limited data in both continuous and discrete state spaces. Since direct use of a model may introduce bias, we derive a theoretical upper bound on model bias when we estimate the model transitions with i.i.d. sampled trajectories. This bound can be used to guide selection between the two methods. Finally, we empirically validate the data-efficiency of our proposed methods across three domains and analyze the settings where one method is preferable to the other. "
745056116502044672,2016-06-21 00:49:55,https://t.co/Uc6WIap7yd,Understanding Innovation to Drive Sustainable Development. (arXiv:1606.06177v1 [cs.CY]) https://t.co/Uc6WIap7yd,1,4," Abstract: Innovation is among the key factors driving a country's economic and social growth. But what are the factors that make a country innovative? How do they differ across different parts of the world and different stages of development? In this work done in collaboration with the World Economic Forum (WEF), we analyze the scores obtained through executive opinion surveys that constitute the WEF's Global Competitiveness Index in conjunction with other country-level metrics and indicators to identify actionable levers of innovation. The findings can help country leaders and organizations shape the policies to drive developmental activities and increase the capacity of innovation. "
745056115088494592,2016-06-21 00:49:54,https://t.co/3HYsEjdlk6,On the prediction loss of the lasso in the partially labeled setting. (arXiv:1606.06179v1 [math.ST]) https://t.co/3HYsEjdlk6,0,2," Abstract: In this paper we revisit the risk bounds of the lasso estimator in the context of transductive and semi-supervised learning. In other terms, the setting under consideration is that of regression with random design under partial labeling. The main goal is to obtain user-friendly bounds on the off-sample prediction risk. To this end, the simple setting of bounded response variable and bounded (high-dimensional) covariates is considered. We propose some new adaptations of the lasso to these settings and establish oracle inequalities both in expectation and in deviation. These results provide non-asymptotic upper bounds on the risk that highlight the interplay between the bias due to the mis-specification of the linear model, the bias due to the approximate sparsity and the variance. They also demonstrate that the presence of a large number of unlabeled features may have significant positive impact in the situations where the restricted eigenvalue of the design matrix vanishes or is very small. "
745056112936787968,2016-06-21 00:49:54,https://t.co/wB8W9ykdn8,Online and Differentially-Private Tensor Decomposition. (arXiv:1606.06237v1 [stat.ML]) https://t.co/wB8W9ykdn8,0,4," Abstract: In this paper, we resolve many of the key algorithmic questions regarding robustness, memory efficiency, and differential privacy of tensor decomposition. We propose simple variants of the tensor power method which enjoy these strong properties. We present the first guarantees for online tensor power method which has a linear memory requirement. Moreover, we present a noise calibrated tensor power method with efficient privacy guarantees. At the heart of all these guarantees lies a careful perturbation analysis derived in this paper which improves up on the existing results significantly. "
745056110579712000,2016-06-21 00:49:53,https://t.co/Ij6k3gLIfw,An Empirical Comparison of Sampling Quality Metrics: A Case Study for Bayesian Nonnegative Matrix Factorization. (… https://t.co/Ij6k3gLIfw,0,2," Abstract: In this work, we empirically explore the question: how can we assess the quality of samples from some target distribution? We assume that the samples are provided by some valid Monte Carlo procedure, so we are guaranteed that the collection of samples will asymptotically approximate the true distribution. Most current evaluation approaches focus on two questions: (1) Has the chain mixed, that is, is it sampling from the distribution? and (2) How independent are the samples (as MCMC procedures produce correlated samples)? Focusing on the case of Bayesian nonnegative matrix factorization, we empirically evaluate standard metrics of sampler quality as well as propose new metrics to capture aspects that these measures fail to expose. The aspect of sampling that is of particular interest to us is the ability (or inability) of sampling methods to move between multiple optima in NMF problems. As a proxy, we propose and study a number of metrics that might quantify the diversity of a set of NMF factorizations obtained by a sampler through quantifying the coverage of the posterior distribution. We compare the performance of a number of standard sampling methods for NMF in terms of these new metrics. "
745056109313069056,2016-06-21 00:49:53,https://t.co/F4btlqbeW8,A Fuzzy Clustering Algorithm for the Mode Seeking Framework. (arXiv:1406.7130v3 [stat.ML] UPDATED) https://t.co/F4btlqbeW8,0,2," Abstract: In this paper, we propose a new fuzzy clustering algorithm based on the mode-seeking framework. Given a dataset in $\mathbb{R}^d$, we define regions of high density that we call cluster cores. We then consider a random walk on a neighborhood graph built on top of our data points which is designed to be attracted by high density regions. The strength of this attraction is controlled by a temperature parameter $\beta > 0$. The membership of a point to a given cluster is then the probability for the random walk to hit the corresponding cluster core before any other. While many properties of random walks (such as hitting times, commute distances, etc\dots) have been shown to enventually encode purely local information when the number of data points grows, we show that the regularization introduced by the use of cluster cores solves this issue. Empirically, we show how the choice of $\beta$ influences the behavior of our algorithm: for small values of $\beta$ the result is close to hard mode-seeking whereas when $\beta$ is close to $1$ the result is similar to the output of a (fuzzy) spectral clustering. Finally, we demonstrate the scalability of our approach by providing the fuzzy clustering of a protein configuration dataset containing a million data points in $30$ dimensions. "
745056107480002560,2016-06-21 00:49:53,https://t.co/AbU1sSZ5Fh,Why Regularized Auto-Encoders learn Sparse Representation?. (arXiv:1505.05561v5 [stat.ML] UPDATED) https://t.co/AbU1sSZ5Fh,1,11," Abstract: While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- \textit{Internal Covariate Shift}-- the current solution has certain drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (especially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size $ 1 $ during training. We address these drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call \textit{Normalization Propagation}. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers. "
745056104158273537,2016-06-21 00:49:52,https://t.co/hF1j6Uih92,A Deep Bag-of-Features Model for Music Auto-Tagging. (arXiv:1508.04999v2 [cs.LG] UPDATED) https://t.co/hF1j6Uih92,0,4," Abstract: Feature learning and deep learning have drawn great attention in recent years as a way of transforming input data into more effective representations using learning algorithms. Such interest has grown in the area of music information retrieval (MIR) as well, particularly in music audio classification tasks such as auto-tagging. In this paper, we present a two-stage learning model to effectively predict multiple labels from music audio. The first stage learns to project local spectral patterns of an audio track onto a high-dimensional sparse space in an unsupervised manner and summarizes the audio track as a bag-of-features. The second stage successively performs the unsupervised learning on the bag-of-features in a layer-by-layer manner to initialize a deep neural network and finally fine-tunes it with the tag labels. Through the experiment, we rigorously examine training choices and tuning parameters, and show that the model achieves high performance on Magnatagatune, a popularly used dataset in music auto-tagging. "
744690341001957377,2016-06-20 00:36:27,https://t.co/T0hEssYwAi,Predicting Ambulance Demand: Challenges and Methods. (arXiv:1606.05363v1 [stat.ML]) https://t.co/T0hEssYwAi,2,8," Abstract: Predicting ambulance demand accurately at a fine resolution in time and space (e.g., every hour and 1 km$^2$) is critical for staff / fleet management and dynamic deployment. There are several challenges: though the dataset is typically large-scale, demand per time period and locality is almost always zero. The demand arises from complex urban geography and exhibits complex spatio-temporal patterns, both of which need to captured and exploited. To address these challenges, we propose three methods based on Gaussian mixture models, kernel density estimation, and kernel warping. These methods provide spatio-temporal predictions for Toronto and Melbourne that are significantly more accurate than the current industry practice. "
744690339978543104,2016-06-20 00:36:27,https://t.co/aE8DJIKvBM,Model-Agnostic Interpretability of Machine Learning. (arXiv:1606.05386v1 [stat.ML]) https://t.co/aE8DJIKvBM,1,10," Abstract: Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges. "
744690338640629761,2016-06-20 00:36:27,https://t.co/64R1qRbpyP,Making Tree Ensembles Interpretable. (arXiv:1606.05390v1 [stat.ML]) https://t.co/64R1qRbpyP,4,8," Abstract: Tree ensembles, such as random forest and boosted trees, are renowned for their high prediction performance, whereas their interpretability is critically limited. In this paper, we propose a post processing method that improves the model interpretability of tree ensembles. After learning a complex tree ensembles in a standard way, we approximate it by a simpler model that is interpretable for human. To obtain the simpler model, we derive the EM algorithm minimizing the KL divergence from the complex ensemble. A synthetic experiment showed that a complicated tree ensemble was approximated reasonably as interpretable. "
744690337617154048,2016-06-20 00:36:26,https://t.co/7B8PMvIt9f,"Complex systems: features, similarity and connectivity. (arXiv:1606.05400v1 [physics.soc-ph]) https://t.co/7B8PMvIt9f",1,6," Abstract: The increasing interest in complex networks research has been a consequence of several intrinsic features of this area, such as the generality of the approach to represent and model virtually any discrete system, and the incorporation of concepts and methods deriving from many areas, from statistical physics to sociology, which are often used in an independent way. Yet, for this same reason, it would be desirable to integrate these various aspects into a more coherent and organic framework, which would imply in several benefits normally allowed by the systematization in science, including the identification of new types of problems and the cross-fertilization between fields. More specifically, the identification of the main areas to which the concepts frequently used in complex networks can be applied paves the way to adopting and applying a larger set of concepts and methods deriving from those respective areas. Among the several areas that have been used in complex networks research, pattern recognition, optimization, linear algebra, and time series analysis seem to play a more basic and recurrent role. In the present manuscript, we propose a systematic way to integrate the concepts from these diverse areas regarding complex networks research. In order to do so, we start by grouping the multidisciplinary concepts into three main groups, namely features, similarity, and network connectivity. Then we show that several of the analysis and modeling approaches to complex networks can be thought as a composition of maps between these three groups, with emphasis on nine main types of mappings, which are presented and illustrated. Such a systematization of principles and approaches also provides an opportunity to review some of the most closely related works in the literature, which is also developed in this article. "
744690336438558720,2016-06-20 00:36:26,https://t.co/lgN3XS8XeG,PSF : Introduction to R Package for Pattern Sequence Based Forecasting Algorithm. (arXiv:1606.05492v1 [stat.ML]) https://t.co/lgN3XS8XeG,2,7," Abstract: This paper discusses about an R package that implements the Pattern Sequence based Forecasting (PSF) algorithm, which was developed for univariate time series forecasting. This algorithm has been successfully applied to many different fields. The PSF algorithm consists of two major parts: clustering and prediction. The clustering part includes selection of the optimum number of clusters. It labels time series data with reference to such clusters. The prediction part includes functions like optimum window size selection for specific patterns and prediction of future values with reference to past pattern sequences. The PSF package consists of various functions to implement the PSF algorithm. It also contains a function which automates all other functions to obtain optimized prediction results. The aim of this package is to promote the PSF algorithm and to ease its implementation with minimum efforts. This paper describes all the functions in the PSF package with their syntax. It also provides a simple example of usage. Finally, the usefulness of this package is discussed by comparing it to auto.arima and ets, well-known time series forecasting functions available on CRAN repository. "
744690335335448576,2016-06-20 00:36:26,https://t.co/AZpVt2391M,Estimation of matrix trace using machine learning. (arXiv:1606.05560v1 [stat.ML]) https://t.co/AZpVt2391M,0,7," Abstract: We present a new trace estimator of the matrix whose explicit form is not given but its matrix multiplication to a vector is available. The form of the estimator is similar to the Hutchison stochastic trace estimator, but instead of the random noise vectors in Hutchison estimator, we use small number of probing vectors determined by machine learning. Evaluation of the quality of estimates and bias correction are discussed. An unbiased estimator is proposed for the calculation of the expectation value of a function of traces. In the numerical experiments with random matrices, it is shown that the precision of trace estimates with $\mathcal{O}(10)$ probing vectors determined by the machine learning is similar to that with $\mathcal{O}(10000)$ random noise vectors. "
744690334228176896,2016-06-20 00:36:26,https://t.co/Y2Jo1btvG5,Learning Interpretable Musical Compositional Rules and Traces. (arXiv:1606.05572v1 [stat.ML]) https://t.co/Y2Jo1btvG5,2,4," Abstract: Throughout music history, theorists have identified and documented interpretable rules that capture the decisions of composers. This paper asks, ""Can a machine behave like a music theorist?"" It presents MUS-ROVER, a self-learning system for automatically discovering rules from symbolic music. MUS-ROVER performs feature learning via $n$-gram models to extract compositional rules --- statistical patterns over the resulting features. We evaluate MUS-ROVER on Bach's (SATB) chorales, demonstrating that it can recover known rules, as well as identify new, characteristic patterns for further study. We discuss how the extracted rules can be used in both machine and human composition. "
744690333167005696,2016-06-20 00:36:25,https://t.co/WHTX9Z1e6t,Early Visual Concept Learning with Unsupervised Deep Learning. (arXiv:1606.05579v1 [stat.ML]) https://t.co/WHTX9Z1e6t,4,17," Abstract: Automated discovery of early visual concepts from raw image data is a major open challenge in AI research. Addressing this problem, we propose an unsupervised approach for learning disentangled representations of the underlying factors of variation. We draw inspiration from neuroscience, and show how this can be achieved in an unsupervised generative model by applying the same learning pressures as have been suggested to act in the ventral visual stream in the brain. By enforcing redundancy reduction, encouraging statistical independence, and exposure to data with transform continuities analogous to those to which human infants are exposed, we obtain a variational autoencoder (VAE) framework capable of learning disentangled factors. Our approach makes few assumptions and works well across a wide variety of datasets. Furthermore, our solution has useful emergent properties, such as zero-shot inference and an intuitive understanding of ""objectness"". "
744690331548024833,2016-06-20 00:36:25,https://t.co/S0yExrjtfS,Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?. (arXiv:1606.0… https://t.co/S0yExrjtfS,2,10," Abstract: We conduct large-scale studies on `human attention' in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Overall, our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans. "
744690330512072705,2016-06-20 00:36:25,https://t.co/aWutxT6jje,Ground Truth Bias in External Cluster Validity Indices. (arXiv:1606.05596v1 [stat.ML]) https://t.co/aWutxT6jje,0,4," Abstract: It has been noticed that some external CVIs exhibit a preferential bias towards a larger or smaller number of clusters which is monotonic (directly or inversely) in the number of clusters in candidate partitions. This type of bias is caused by the functional form of the CVI model. For example, the popular Rand index (RI) exhibits a monotone increasing (NCinc) bias, while the Jaccard Index (JI) index suffers from a monotone decreasing (NCdec) bias. This type of bias has been previously recognized in the literature. In this work, we identify a new type of bias arising from the distribution of the ground truth (reference) partition against which candidate partitions are compared. We call this new type of bias ground truth (GT) bias. This type of bias occurs if a change in the reference partition causes a change in the bias status (e.g., NCinc, NCdec) of a CVI. For example, NCinc bias in the RI can be changed to NCdec bias by skewing the distribution of clusters in the ground truth partition. It is important for users to be aware of this new type of biased behaviour, since it may affect the interpretations of CVI results. The objective of this article is to study the empirical and theoretical implications of GT bias. To the best of our knowledge, this is the first extensive study of such a property for external cluster validity indices. "
744690329350250496,2016-06-20 00:36:24,https://t.co/GGPtXPluEq,Balancing New Against Old Information: The Role of Surprise. (arXiv:1606.05642v1 [stat.ML]) https://t.co/GGPtXPluEq,0,6," Abstract: Surprise is a ubiquitous concept describing a wide range of phenomena from unexpected events to behavioral responses. We propose a measure of surprise, to arrive at a new framework for surprise-driven learning. There are two components to this framework: (i) a confidence-adjusted surprise measure to capture environmental statistics as well as subjective beliefs, (ii) a surprise-minimization learning rule, or SMiLe-rule, which dynamically adjusts the balance between new and old information without making prior assumptions about the temporal statistics in the environment. We apply our framework to a dynamic decision making task and a maze exploration task to demonstrate that it is suitable for learning in complex environments, even if the environment undergoes gradual or sudden changes. Our proposed surprise-modulated belief update algorithm provides a framework to study the behavior of humans and animals encountering surprising events. "
744690328108732416,2016-06-20 00:36:24,https://t.co/lqRd1AP7yU,Approachability in unknown games: Online learning meets multi-objective optimization. (arXiv:1402.2043v2 [stat.ML]… https://t.co/lqRd1AP7yU,0,5," Abstract: In the standard setting of approachability there are two players and a target set. The players play repeatedly a known vector-valued game where the first player wants to have the average vector-valued payoff converge to the target set which the other player tries to exclude it from this set. We revisit this setting in the spirit of online learning and do not assume that the first player knows the game structure: she receives an arbitrary vector-valued reward vector at every round. She wishes to approach the smallest (""best"") possible set given the observed average payoffs in hindsight. This extension of the standard setting has implications even when the original target set is not approachable and when it is not obvious which expansion of it should be approached instead. We show that it is impossible, in general, to approach the best target set in hindsight and propose achievable though ambitious alternative goals. We further propose a concrete strategy to approach these goals. Our method does not require projection onto a target set and amounts to switching between scalar regret minimization algorithms that are performed in episodes. Applications to global cost minimization and to approachability under sample path constraints are considered. "
744690326972018688,2016-06-20 00:36:24,https://t.co/ApJ2CslQPA,Exact Bayesian inference for off-line change-point detection in tree-structured graphical models. (arXiv:1603.0787… https://t.co/ApJ2CslQPA,1,8," Abstract: We consider the problem of change-point detection in multivariate time-series. The multivariate distribution of the observations is supposed to follow a graphical model, whose graph and parameters are affected by abrupt changes throughout time. We demonstrate that it is possible to perform exact Bayesian inference whenever one considers a simple class of undirected graphs called spanning trees as possible structures. We are then able to integrate on the graph and segmentation spaces at the same time by combining classical dynamic programming with algebraic results pertaining to spanning trees. In particular, we show that quantities such as posterior distributions for change-points or posterior edge probabilities over time can efficiently be obtained. We illustrate our results on both synthetic and experimental data arising from biology and neuroscience. "
744690325654904832,2016-06-20 00:36:23,https://t.co/kDgtRvvdSO,Web-scale Topic Models in Spark: An Asynchronous Parameter Server. (arXiv:1605.07422v2 [cs.DC] UPDATED) https://t.co/kDgtRvvdSO,1,9," Abstract: In this paper, we train a Latent Dirichlet Allocation (LDA) topic model on the ClueWeb12 data set, a 27-terabyte Web crawl. We extend Spark, a popular framework for performing large-scale data analysis, with an asynchronous parameter server. Such a parameter server provides a distributed and concurrently accessed parameter space for the model. A Metropolis-Hastings based collapsed Gibbs sampler is implemented using this parameter server achieving an amortized O(1) sampling complexity. We compare our implementation to the default Spark implementations and show that it is significantly faster and more scalable without sacrificing model quality. A topic model with 1,000 topics is trained on the full ClueWeb12 data set, uncovering some of the prevalent themes that appear on the Web. "
744690324241584129,2016-06-20 00:36:23,https://t.co/IIIF5ywllf,Scalable and Optimal Generalized Canonical Correlation Analysis via Alternating Optimization. (arXiv:1605.09459v2 … https://t.co/IIIF5ywllf,0,5," Abstract: Generalized canonical correlation analysis (GCCA) aims at finding latent low-dimensional common structure from multiple views (feature vectors in different domains) of the same entities. Unlike principal component analysis (PCA) that handles a single view, (G)CCA is able to integrate information from different feature spaces. Here we focus on MAX-VAR GCCA, a popular formulation which has recently gained renewed interest in multilingual processing and speech modeling. The classic MAX-VAR GCCA problem can be solved optimally via eigen-decomposition of a matrix that compounds the (whitened) correlation matrices of the views; but this solution has serious scalability issues, and is not directly amenable to incorporating pertinent structural constraints such as non-negativity and sparsity on the canonical components. We posit regularized MAX-VAR GCCA as a non-convex optimization problem and propose an alternating optimization (AO)-based algorithm to handle it. Our algorithm alternates between {\em inexact} solutions of a regularized least squares subproblem and a manifold-constrained non-convex subproblem, thereby achieving substantial memory and computational savings. An important benefit of our design is that it can easily handle structure-promoting regularization. We show that the algorithm globally converges to a critical point at a sublinear rate, and approaches a global optimal solution at a linear rate when no regularization is considered. Judiciously designed simulations and large-scale word embedding tasks are employed to showcase the effectiveness of the proposed algorithm. "
744690323163578368,2016-06-20 00:36:23,https://t.co/ag6ZrNCzEg,Multi-View Treelet Transform. (arXiv:1606.00800v2 [stat.ML] UPDATED) https://t.co/ag6ZrNCzEg,0,4," Abstract: Current multi-view factorization methods make assumptions that are not acceptable for many kinds of data, and in particular, for graphical data with hierarchical structure. At the same time, current hierarchical methods work only in the single-view setting. We generalize the Treelet Transform to the Multi-View Treelet Transform (MVTT) to allow for the capture of hierarchical structure when multiple views are available. Further, we show how this generalization is consistent with the existing theory and how it might be used in denoising empirical networks and in computing the shared response of functional brain data. "
743622172124581888,2016-06-17 01:51:56,https://t.co/j3E7EWIWIF,Combining multiscale features for classification of hyperspectral images: a sequence based kernel approach. (arXiv… https://t.co/j3E7EWIWIF,0,3," Abstract: Nowadays, hyperspectral image classification widely copes with spatial information to improve accuracy. One of the most popular way to integrate such information is to extract hierarchical features from a multiscale segmentation. In the classification context, the extracted features are commonly concatenated into a long vector (also called stacked vector), on which is applied a conventional vector-based machine learning technique (e.g. SVM with Gaussian kernel). In this paper, we rather propose to use a sequence structured kernel: the spectrum kernel. We show that the conventional stacked vector-based kernel is actually a special case of this kernel. Experiments conducted on various publicly available hyperspectral datasets illustrate the improvement of the proposed kernel w.r.t. conventional ones using the same hierarchical spatial features. "
743622169637289984,2016-06-17 01:51:55,https://t.co/emdmBCMncm,Logarithmic Time One-Against-Some. (arXiv:1606.04988v1 [stat.ML]) https://t.co/emdmBCMncm,0,1," Abstract: We create a new online reduction of multiclass classification to binary classification for which training and prediction time scale logarithmically with the number of classes. Compared to previous approaches, we obtain substantially better statistical performance for two reasons: First, we prove a tighter and more complete boosting theorem, and second we translate the results more directly into an algorithm. We show that several simple techniques give rise to an algorithm that can compete with one-against-all in both space and predictive power while offering exponential improvements in speed when the number of classes is large. "
743622167338885125,2016-06-17 01:51:55,https://t.co/8NYphOfBum,A Class of Parallel Doubly Stochastic Algorithms for Large-Scale Learning. (arXiv:1606.04991v1 [cs.LG]) https://t.co/8NYphOfBum,0,1," Abstract: We consider learning problems over training sets in which both, the number of training examples and the dimension of the feature vectors, are large. To solve these problems we propose the random parallel stochastic algorithm (RAPSA). We call the algorithm random parallel because it utilizes multiple parallel processors to operate on a randomly chosen subset of blocks of the feature vector. We call the algorithm stochastic because processors choose training subsets uniformly at random. Algorithms that are parallel in either of these dimensions exist, but RAPSA is the first attempt at a methodology that is parallel in both the selection of blocks and the selection of elements of the training set. In RAPSA, processors utilize the randomly chosen functions to compute the stochastic gradient component associated with a randomly chosen block. The technical contribution of this paper is to show that this minimally coordinated algorithm converges to the optimal classifier when the training objective is convex. Moreover, we present an accelerated version of RAPSA (ARAPSA) that incorporates the objective function curvature information by premultiplying the descent direction by a Hessian approximation matrix. We further extend the results for asynchronous settings and show that if the processors perform their updates without any coordination the algorithms are still convergent to the optimal argument. RAPSA and its extensions are then numerically evaluated on a linear estimation problem and a binary image classification task using the MNIST handwritten digit dataset. "
743622165333938176,2016-06-17 01:51:54,https://t.co/behAMqlarM,Joint Data Compression and MAC Protocol Design for Smartgrids with Renewable Energy. (arXiv:1606.04995v1 [cs.NI]) https://t.co/behAMqlarM,0,1," Abstract: In this paper, we consider the joint design of data compression and 802.15.4-based medium access control (MAC) protocol for smartgrids with renewable energy. We study the setting where a number of nodes, each of which comprises electricity load and/or renewable sources, report periodically their injected powers to a data concentrator. Our design exploits the correlation of the reported data in both time and space to efficiently design the data compression using the compressed sensing (CS) technique and theMAC protocol so that the reported data can be recovered reliably within minimum reporting time. Specifically, we perform the following design tasks: i) we employ the two-dimensional (2D) CS technique to compress the reported data in the distributed manner; ii) we propose to adapt the 802.15.4 MAC protocol frame structure to enable efficient data transmission and reliable data reconstruction; and iii) we develop an analytical model based on which we can obtain efficient MAC parameter configuration to minimize the reporting delay. Finally, numerical results are presented to demonstrate the effectiveness of our proposed framework compared to existing solutions. "
743622162309844992,2016-06-17 01:51:53,https://t.co/qlh6PO69Eb,Improving Power Generation Efficiency using Deep Neural Networks. (arXiv:1606.05018v1 [stat.ML]) https://t.co/qlh6PO69Eb,0,2," Abstract: Recently there has been significant research on power generation, distribution and transmission efficiency especially in the case of renewable resources. The main objective is reduction of energy losses and this requires improvements on data acquisition and analysis. In this paper we address these concerns by using consumers' electrical smart meter readings to estimate network loading and this information can then be used for better capacity planning. We compare Deep Neural Network (DNN) methods with traditional methods for load forecasting. Our results indicate that DNN methods outperform most traditional methods. This comes at the cost of additional computational complexity but this can be addressed with the use of cloud resources. We also illustrate how these results can be used to better support dynamic pricing. "
743622156559462400,2016-06-17 01:51:52,https://t.co/WcwMVuavaT,Learning Optimal Interventions. (arXiv:1606.05027v1 [stat.ML]) https://t.co/WcwMVuavaT,0,4," Abstract: Our goal is to identify beneficial interventions from observational data. We consider interventions that are narrowly focused (impacting few features) and may be tailored to each individual or globally enacted over a population. If the underlying relationship obeys an invariance condition, our approach can identify beneficial interventions directly from observational data, side-stepping causal inference. We provide theoretical guarantees for predicted gains when the relationship is governed by a Gaussian Process, even in settings involving unintentional downstream effects. Empirically, our approach outperforms causal inference techniques (even when our model is misspecified) and is able to discover good interventions in two practical applications: gene perturbation and writing improvement. "
743622154466492416,2016-06-17 01:51:52,https://t.co/ajW421BFQJ,Pruning Random Forests for Prediction on a Budget. (arXiv:1606.05060v1 [stat.ML]) https://t.co/ajW421BFQJ,0,3," Abstract: We propose to prune a random forest (RF) for resource-constrained prediction. We first construct a RF and then prune it to optimize expected feature cost & accuracy. We pose pruning RFs as a novel 0-1 integer program with linear constraints that encourages feature re-use. We establish total unimodularity of the constraint set to prove that the corresponding LP relaxation solves the original integer program. We then exploit connections to combinatorial optimization and develop an efficient primal-dual algorithm, scalable to large datasets. In contrast to our bottom-up approach, which benefits from good RF initialization, conventional methods are top-down acquiring features based on their utility value and is generally intractable, requiring heuristics. Empirically, our pruning algorithm outperforms existing state-of-the-art resource-constrained algorithms. "
743622152876867584,2016-06-17 01:51:51,https://t.co/P2QFQ0qXta,Machine Learning Across Cultures: Modeling the Adoption of Financial Services for the Poor. (arXiv:1606.05105v1 [s… https://t.co/P2QFQ0qXta,0,3," Abstract: Recently, mobile operators in many developing economies have launched ""Mobile Money"" platforms that deliver basic financial services over the mobile phone network. While many believe that these services can improve the lives of the poor, a consistent difficulty has been identifying individuals most likely to benefit from access to the new technology. Here, we combine terabyte-scale data from three different mobile phone operators from Ghana, Pakistan, and Zambia, to better understand the behavioral determinants of mobile money adoption. Our supervised learning models provide insight into the best predictors of adoption in three very distinct cultures. We find that models fit on one population fail to generalize to another, and in general are highly context-dependent. These findings highlight the need for a nuanced approach to understanding the role and potential of financial services for the poor. "
743622150121193472,2016-06-17 01:51:51,https://t.co/eIXWw1vkuH,Machine Learning meets Data-Driven Journalism: Boosting International Understanding and Transparency in News Cover… https://t.co/eIXWw1vkuH,1,2," Abstract: Migration crisis, climate change or tax havens: Global challenges need global solutions. But agreeing on a joint approach is difficult without a common ground for discussion. Public spheres are highly segmented because news are mainly produced and received on a national level. Gain- ing a global view on international debates about important issues is hindered by the enormous quantity of news and by language barriers. Media analysis usually focuses only on qualitative re- search. In this position statement, we argue that it is imperative to pool methods from machine learning, journalism studies and statistics to help bridging the segmented data of the international public sphere, using the Transatlantic Trade and Investment Partnership (TTIP) as a case study. "
743622145817853952,2016-06-17 01:51:50,https://t.co/NeXwUHQAWi,CLEAR: Covariant LEAst-square Re-fitting with applications to image restoration. (arXiv:1606.05158v1 [math.ST]) https://t.co/NeXwUHQAWi,0,3," Abstract: In this paper, we propose a new framework to remove parts of the systematic errors affecting popular restoration algorithms, with a special focus for image processing tasks. Generalizing ideas that emerged for $\ell_1$ regularization, we develop an approach re-fitting the results of standard methods towards the input data. Total variation regularizations and non-local means are special cases of interest. We identify important covariant information that should be preserved by the re-fitting method, and emphasize the importance of preserving the Jacobian (w.r.t. the observed signal) of the original estimator. Then, we provide an approach that has a ""twicing"" flavor and allows re-fitting the restored signal by adding back a local affine transformation of the residual term. We illustrate the benefits of our method on numerical simulations for image restoration tasks. "
743622144064692225,2016-06-17 01:51:49,https://t.co/a5pI60Ds0k,"Assessing and tuning brain decoders: cross-validation, caveats, and guidelines. (arXiv:1606.05201v1 [stat.ML]) https://t.co/a5pI60Ds0k",0,4," Abstract: Decoding, ie prediction from brain images or signals, calls for empirical evaluation of its predictive power. Such evaluation is achieved via cross-validation, a method also used to tune decoders' hyper-parameters. This paper is a review on cross-validation procedures for decoding in neuroimaging. It includes a didactic overview of the relevant theoretical considerations. Practical aspects are highlighted with an extensive empirical study of the common decoders in within-and across-subject predictions, on multiple datasets --anatomical and functional MRI and MEG-- and simulations. Theory and experiments outline that the popular "" leave-one-out "" strategy leads to unstable and biased estimates, and a repeated random splits method should be preferred. Experiments outline the large error bars of cross-validation in neuroimaging settings: typical confidence intervals of 10%. Nested cross-validation can tune decoders' parameters while avoiding circularity bias. However we find that it can be more favorable to use sane defaults, in particular for non-sparse decoders. "
743622142651146240,2016-06-17 01:51:49,https://t.co/ThSGrBhgsw,How many faces can be recognized? Performance extrapolation for multi-class classification. (arXiv:1606.05228v1 [s… https://t.co/ThSGrBhgsw,0,2," Abstract: The difficulty of multi-class classification generally increases with the number of classes. Using data from a subset of the classes, can we predict how well a classifier will scale with an increased number of classes? Under the assumption that the classes are sampled exchangeably, and under the assumption that the classifier is generative (e.g. QDA or Naive Bayes), we show that the expected accuracy when the classifier is trained on $k$ classes is the $k-1$st moment of a \emph{conditional accuracy distribution}, which can be estimated from data. This provides the theoretical foundation for performance extrapolation based on pseudolikelihood, unbiased estimation, and high-dimensional asymptotics. We investigate the robustness of our methods to non-generative classifiers in simulations and one optical character recognition example. "
743622139950006272,2016-06-17 01:51:48,https://t.co/kvKH1YHrS5,Estimating mutual information in high dimensions via classification error. (arXiv:1606.05229v1 [stat.ML]) https://t.co/kvKH1YHrS5,1,6," Abstract: Multivariate pattern analyses approaches in neuroimaging are fundamentally concerned with investigating the quantity and type of information processed by various regions of the human brain; typically, estimates of classification accuracy are used to quantify information. While a extensive and powerful library of methods can be applied to train and assess classifiers, it is not always clear how to use the resulting measures of classification performance to draw scientific conclusions: e.g. for the purpose of evaluating redundancy between brain regions. An additional confound for interpreting classification performance is the dependence of the error rate on the number and choice of distinct classes obtained for the classification task. In contrast, mutual information is a quantity defined independently of the experimental design, and has ideal properties for comparative analyses. Unfortunately, estimating the mutual information based on observations becomes statistically infeasible in high dimensions without some kind of assumption or prior. In this paper, we construct a novel classification-based estimator of mutual information based on high-dimensional asymptotics. We show that in a particular limiting regime, the mutual information is an invertible function of the expected $k$-class Bayes error. While the theory is based on a large-sample, high-dimensional limit, we demonstrate through simulations that our proposed estimator has superior performance to the alternatives in problems of moderate dimensionality. "
743622136191909888,2016-06-17 01:51:47,https://t.co/AMLc7EKYqU,The Mondrian Kernel. (arXiv:1606.05241v1 [stat.ML]) https://t.co/AMLc7EKYqU,0,3," Abstract: We introduce the Mondrian kernel, a fast random feature approximation to the Laplace kernel. It is suitable for both batch and online learning, and admits a fast kernel-width-selection procedure as the random features can be re-used efficiently for all kernel widths. The features are constructed by sampling trees via a Mondrian process [Roy and Teh, 2009], and we highlight the connection to Mondrian forests [Lakshminarayanan et al., 2014], where trees are also sampled via a Mondrian process, but fit independently. This link provides a new insight into the relationship between kernel methods and random forests. "
743622133092331531,2016-06-17 01:51:47,https://t.co/P2VKPiCzRT,The Effect of Heteroscedasticity on Regression Trees. (arXiv:1606.05273v1 [stat.ML]) https://t.co/P2VKPiCzRT,0,2," Abstract: Regression trees are becoming increasingly popular as omnibus predicting tools and as the basis of numerous modern statistical learning ensembles. Part of their popularity is their ability to create a regression prediction without ever specifying a structure for the mean model. However, the method implicitly assumes homogeneous variance across the entire explanatory-variable space. It is unknown how the algorithm behaves when faced with heteroscedastic data. In this study, we assess the performance of the most popular regression-tree algorithm in a single-variable setting under a very simple step-function model for heteroscedasticity. We use simulation to show that the locations of splits, and hence the ability to accurately predict means, are both adversely influenced by the change in variance. We identify the pruning algorithm as the main concern, although the effects on the splitting algorithm may be meaningful in some applications. "
743622130684788736,2016-06-17 01:51:46,https://t.co/L0lhvStXP4,Designing Intelligent Automation based Solutions for Complex Social Problems. (arXiv:1606.05275v1 [stat.ML]) https://t.co/L0lhvStXP4,0,2," Abstract: Deciding effective and timely preventive measures against complex social problems affecting relatively low income geographies is a difficult challenge. There is a strong need to adopt intelligent automation based solutions with low cost imprints to tackle these problems at larger scales. Starting with the hypothesis that analytical modelling and analysis of social phenomena with high accuracy is in general inherently hard, in this paper we propose design framework to enable data-driven machine learning based adaptive solution approach towards enabling more effective preventive measures. We use survey data collected from a socio-economically backward region of India about adolescent girls to illustrate the design approach. "
743622128856104960,2016-06-17 01:51:46,https://t.co/lZ6NgDf8VI,Spectral decomposition method of dialog state tracking via collective matrix factorization. (arXiv:1606.05286v1 [c… https://t.co/lZ6NgDf8VI,0,2," Abstract: The task of dialog management is commonly decomposed into two sequential subtasks: dialog state tracking and dialog policy learning. In an end-to-end dialog system, the aim of dialog state tracking is to accurately estimate the true dialog state from noisy observations produced by the speech recognition and the natural language understanding modules. The state tracking task is primarily meant to support a dialog policy. From a probabilistic perspective, this is achieved by maintaining a posterior distribution over hidden dialog states composed of a set of context dependent variables. Once a dialog policy is learned, it strives to select an optimal dialog act given the estimated dialog state and a defined reward function. This paper introduces a novel method of dialog state tracking based on a bilinear algebric decomposition model that provides an efficient inference schema through collective matrix factorization. We evaluate the proposed approach on the second Dialog State Tracking Challenge (DSTC-2) dataset and we show that the proposed tracker gives encouraging results compared to the state-of-the-art trackers that participated in this standard benchmark. Finally, we show that the prediction schema is computationally efficient in comparison to the previous approaches. "
743622125504827392,2016-06-17 01:51:45,https://t.co/YuiKFVS6wQ,Unsupervised Risk Estimation Using Only Conditional Independence Structure. (arXiv:1606.05313v1 [cs.LG]) https://t.co/YuiKFVS6wQ,0,4," Abstract: We show how to estimate a model's test error from unlabeled data, on distributions very different from the training distribution, while assuming only that certain conditional independencies are preserved between train and test. We do not need to assume that the optimal predictor is the same between train and test, or that the true distribution lies in any parametric family. We can also efficiently differentiate the error estimate to perform unsupervised discriminative learning. Our technical tool is the method of moments, which allows us to exploit conditional independencies in the absence of a fully-specified model. Our framework encompasses a large family of losses including the log and exponential loss, and extends to structured output settings such as hidden Markov models. "
743622122300448769,2016-06-17 01:51:44,https://t.co/8htN2zeFGg,Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models. (arXiv:1606.05320v1 [stat… https://t.co/8htN2zeFGg,4,9," Abstract: As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks (RNNs), state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining an RNN with a hidden Markov model (HMM), a simpler and more transparent model. We explore various combinations of RNNs and HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained first, then a small LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text. "
743622119448322048,2016-06-17 01:51:43,https://t.co/MTZBp9uV9Y,ACDC: $\alpha$-Carving Decision Chain for Risk Stratification. (arXiv:1606.05325v1 [stat.ML]) https://t.co/MTZBp9uV9Y,0,2," Abstract: In many healthcare settings, intuitive decision rules for risk stratification can help effective hospital resource allocation. This paper introduces a novel variant of decision tree algorithms that produces a chain of decisions, not a general tree. Our algorithm, $\alpha$-Carving Decision Chain (ACDC), sequentially carves out ""pure"" subsets of the majority class examples. The resulting chain of decision rules yields a pure subset of the minority class examples. Our approach is particularly effective in exploring large and class-imbalanced health datasets. Moreover, ACDC provides an interactive interpretation in conjunction with visual performance metrics such as Receiver Operating Characteristics curve and Lift chart. "
743622115140767744,2016-06-17 01:51:42,https://t.co/9BEQ20nUkT,On the expressive power of deep neural networks. (arXiv:1606.05336v1 [stat.ML]) https://t.co/9BEQ20nUkT,1,11," Abstract: We study the expressive power of deep neural networks before and after training. Considering neural nets after random initialization, we show that three natural measures of expressivity all display an exponential dependence on the depth of the network. We prove, theoretically and experimentally, that all of these measures are in fact related to a fourth quantity, trajectory length. This quantity grows exponentially in the depth of the network, and is responsible for the depth sensitivity observed. These results translate to consequences for networks during and after training. The connection of all expressivity measures to trajectory length suggests that parameters earlier in the network have greater influence on the expressive power -- in particular, given a layer, its influence on expressivity is determined by the remaining depth of the network after that layer. This is verified with experiments on MNIST and CIFAR-10. We also find that the training process decreases depth sensitivity for real and synthetic data, but at different rates. "
743622112749948928,2016-06-17 01:51:42,https://t.co/uhmxwdmETW,Exponential expressivity in deep neural networks through transient chaos. (arXiv:1606.05340v1 [stat.ML]) https://t.co/uhmxwdmETW,0,6," Abstract: We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in generic, deep neural networks with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted to the analysis of single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions. "
743622110229172224,2016-06-17 01:51:41,https://t.co/2IB6utpPxT,Collaborative Multi-sensor Classification via Sparsity-based Representation. (arXiv:1410.7876v2 [cs.CV] UPDATED) https://t.co/2IB6utpPxT,0,3," Abstract: In this paper, we propose a general collaborative sparse representation framework for multi-sensor classification, which takes into account the correlations as well as complementary information between heterogeneous sensors simultaneously while considering joint sparsity within each sensor's observations. We also robustify our models to deal with the presence of sparse noise and low-rank interference signals. Specifically, we demonstrate that incorporating the noise or interference signal as a low-rank component in our models is essential in a multi-sensor classification problem when multiple co-located sources/sensors simultaneously record the same physical event. We further extend our frameworks to kernelized models which rely on sparsely representing a test sample in terms of all the training samples in a feature space induced by a kernel function. A fast and efficient algorithm based on alternative direction method is proposed where its convergence to an optimal solution is guaranteed. Extensive experiments are conducted on several real multi-sensor data sets and results are compared with the conventional classifiers to verify the effectiveness of the proposed methods. "
743622106278170624,2016-06-17 01:51:40,https://t.co/9r95korbU2,No penalty no tears: Least squares in high-dimensional linear models. (arXiv:1506.02222v5 [stat.ME] UPDATED) https://t.co/9r95korbU2,0,3," Abstract: Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms. "
743622102809513988,2016-06-17 01:51:39,https://t.co/U8TZxPXBTB,The Discrete Dantzig Selector: Estimating Sparse Linear Models via Mixed Integer Linear Optimization. (arXiv:1508.… https://t.co/U8TZxPXBTB,0,2," Abstract: We propose a novel high-dimensional linear regression estimator: the Discrete Dantzig Selector, which minimizes the number of nonzero regression coefficients, subject to a budget on the maximal absolute correlation between the features and residuals. We show that the estimator can be expressed as a solution to a Mixed Integer Linear Optimization (MILO) problem, a computationally tractable framework that delivers provably optimal global solutions. The current state of algorithmics in integer optimization makes our proposal substantially more scalable than the least squares subset selection framework based on integer quadratic optimization, recently proposed in [7] and the continuous nonconvex quadratic optimization framework of [33]. We propose new discrete first-order methods, which, when paired with state-of-the-art MILO solvers, lead to superior upper bounds for the Discrete Dantzig Selector problem for a given computational budget. We demonstrate that the integrated approach, proposed herein, also provides globally optimal solutions in significantly shorter computation times, when compared to off-the-shelf MILO solvers. We demonstrate, both theoretically and empirically, that, in a wide range of regimes, the statistical properties of the Discrete Dantzig Selector are superior to those of popular $\ell_{1}$-based approaches. Our approach gracefully scales to problem instances up to p = 10,000 features with provable optimality, making it, to the best of our knowledge, one of the most scalable exact variable selection approaches in sparse linear modeling at the moment. "
743622098585804804,2016-06-17 01:51:38,https://t.co/h2WzHPKBg2,Interactive algorithms: from pool to stream. (arXiv:1602.01132v3 [stat.ML] UPDATED) https://t.co/h2WzHPKBg2,0,2," Abstract: We consider interactive algorithms in the pool-based setting, and in the stream-based setting. Interactive algorithms observe suggested elements (representing actions or queries), and interactively select some of them and receive responses. Pool-based algorithms can select elements at any order, while stream-based algorithms observe elements in sequence, and can only select elements immediately after observing them. We assume that the suggested elements are generated independently from some source distribution, and ask what is the stream size required for emulating a pool algorithm with a given pool size. We provide algorithms and matching lower bounds for general pool algorithms, and for utility-based pool algorithms. We further show that a maximal gap between the two settings exists also in the special case of active learning for binary classification. "
743622096455073792,2016-06-17 01:51:38,https://t.co/DoVw33RqED,Auxiliary Deep Generative Models. (arXiv:1602.05473v4 [stat.ML] UPDATED) https://t.co/DoVw33RqED,0,6," Abstract: Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST, SVHN and NORB datasets. "
743603836951072768,2016-06-17 00:39:04,https://t.co/Ga8O60qXS9,False Discovery Rate Control and Statistical Quality Assessment of Annotators in Crowdsourced Ranking. (arXiv:1605… https://t.co/Ga8O60qXS9,0,4," Abstract: With the rapid growth of crowdsourcing platforms it has become easy and relatively inexpensive to collect a dataset labeled by multiple annotators in a short time. However due to the lack of control over the quality of the annotators, some abnormal annotators may be affected by position bias which can potentially degrade the quality of the final consensus labels. In this paper we introduce a statistical framework to model and detect annotator's position bias in order to control the false discovery rate (FDR) without a prior knowledge on the amount of biased annotators - the expected fraction of false discoveries among all discoveries being not too high, in order to assure that most of the discoveries are indeed true and replicable. The key technical development relies on some new knockoff filters adapted to our problem and new algorithms based on the Inverse Scale Space dynamics whose discretization is potentially suitable for large scale crowdsourcing data analysis. Our studies are supported by experiments with both simulated examples and real-world data. The proposed framework provides us a useful tool for quantitatively studying annotator's abnormal behavior in crowdsourcing data arising from machine learning, sociology, computer vision, multimedia, etc. "
743603827866214400,2016-06-17 00:39:02,https://t.co/IqztO0W2cB,Deep Structured Energy Based Models for Anomaly Detection. (arXiv:1605.07717v2 [cs.LG] UPDATED) https://t.co/IqztO0W2cB,1,5," Abstract: In this paper, we attack the anomaly detection problem by directly modeling the data distribution with deep architectures. We propose deep structured energy based models (DSEBMs), where the energy function is the output of a deterministic deep neural network with structure. We develop novel model architectures to integrate EBMs with different types of data such as static data, sequential data, and spatial data, and apply appropriate model architectures to adapt to the data structure. Our training algorithm is built upon the recent development of score matching \cite{sm}, which connects an EBM with a regularized autoencoder, eliminating the need for complicated sampling method. Statistically sound decision criterion can be derived for anomaly detection purpose from the perspective of the energy landscape of the data distribution. We investigate two decision criteria for performing anomaly detection: the energy score and the reconstruction error. Extensive empirical studies on benchmark tasks demonstrate that our proposed model consistently matches or outperforms all the competing methods. "
743244777794207745,2016-06-16 00:52:18,https://t.co/zW92rd5ZLv,Masking Strategies for Image Manifolds. (arXiv:1606.04618v1 [stat.ML]) https://t.co/zW92rd5ZLv,0,2," Abstract: We consider the problem of selecting an optimal mask for an image manifold, i.e., choosing a subset of the pixels of the image that preserves the manifold's geometric structure present in the original data. Such masking implements a form of compressive sensing through emerging imaging sensor platforms for which the power expense grows with the number of pixels acquired. Our goal is for the manifold learned from masked images to resemble its full image counterpart as closely as possible. More precisely, we show that one can indeed accurately learn an image manifold without having to consider a large majority of the image pixels. In doing so, we consider two masking methods that preserve the local and global geometric structure of the manifold, respectively. In each case, the process of finding the optimal masking pattern can be cast as a binary integer program, which is computationally expensive but can be approximated by a fast greedy algorithm. Numerical experiments show that the relevant manifold structure is preserved through the data-dependent masking process, even for modest mask sizes. "
743244776246513664,2016-06-16 00:52:18,https://t.co/4d032dUXDE,Differentially Private Stochastic Gradient Descent for in-RDBMS Analytics. (arXiv:1606.04722v1 [cs.LG]) https://t.co/4d032dUXDE,2,2," Abstract: In-RDBMS data analysis has received considerable attention in the past decade and has been widely used in sensitive domains to extract patterns in data using machine learning. For these domains, there has also been growing concern about privacy, and differential privacy has emerged as the gold standard for private data analysis. However, while differentially private machine learning and in-RDBMS data analytics have been studied separately, little previous work has explored private learning in an in-RDBMS system. This work considers a specific algorithm --- stochastic gradient descent (SGD) for differentially private machine learning --- and explores how to integrate it into an RDBMS system. We find that previous solutions on differentially private SGD have characteristics that render them unattractive for an RDBMS implementation. To address this, we propose an algorithm that runs SGD for a constant number of passes and then adds noise to the resulting output. We provide a novel analysis of the privacy properties of this algorithm. We then integrate it, along with two existing versions of differentially private SGD, in the Postgres RDBMS. Experimental results demonstrate that our proposed approach can be easily integrated into an RDBMS, incurs virtually no overhead, and yields substantially better test accuracy than the other private SGD algorithm implementations. "
743244774698852352,2016-06-16 00:52:17,https://t.co/AJlUlMCzPQ,Safe Exploration in Finite Markov Decision Processes with Gaussian Processes. (arXiv:1606.04753v1 [cs.LG]) https://t.co/AJlUlMCzPQ,1,3," Abstract: In classical reinforcement learning, when exploring an environment, agents accept arbitrary short term loss for long term gain. This is infeasible for safety critical applications, such as robotics, where even a single unsafe action may cause system failure. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an, a priori unknown, safety constraint that depends on states and actions. We aim to explore the MDP under this constraint, assuming that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm for this task and prove that it is able to completely explore the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover. "
743244772463222784,2016-06-16 00:52:17,https://t.co/cUCuISjaYn,Network Maximal Correlation. (arXiv:1606.04789v1 [stat.ML]) https://t.co/cUCuISjaYn,0,3," Abstract: We introduce Network Maximal Correlation (NMC) as a multivariate measure of nonlinear association among random variables. NMC is defined via an optimization that infers (non-trivial) transformations of variables by maximizing aggregate inner products between transformed variables. We characterize a solution of the NMC optimization using geometric properties of Hilbert spaces for finite discrete and jointly Gaussian random variables. For finite discrete variables, we propose an algorithm based on alternating conditional expectation to determine NMC. We also show that empirically computed NMC converges to NMC exponentially fast in sample size. For jointly Gaussian variables, we show that under some conditions the NMC optimization is an instance of the Max-Cut problem. We then illustrate an application of NMC and multiple MC in inference of graphical model for bijective, possibly non-monotone, functions of jointly Gaussian variables generalizing the copula setup developed by Liu et al. Finally, we illustrate NMC's utility in a real data application of learning nonlinear dependencies among genes in a cancer dataset. "
743244769971830784,2016-06-16 00:52:16,https://t.co/SVmoSvgkWm,Asaga: Asynchronous Parallel Saga. (arXiv:1606.04809v1 [math.OC]) https://t.co/SVmoSvgkWm,0,1," Abstract: We describe Asaga, an asynchronous parallel version of the incremental gradient algorithm Saga that enjoys fast linear convergence rates. We highlight a subtle but important technical issue present in a large fraction of the recent convergence rate proofs for asynchronous parallel optimization algorithms, and propose a simplification of the recently proposed ""perturbed iterate"" framework that resolves it. We thereby prove that Asaga can obtain a theoretical linear speedup on multi-core systems even without sparsity assumptions. We present results of an implementation on a 40-core architecture illustrating the practical speedup as well as the hardware overhead. "
743244768344412160,2016-06-16 00:52:16,https://t.co/jHrQe0kgBg,Understanding Probabilistic Sparse Gaussian Process Approximations. (arXiv:1606.04820v1 [stat.ML]) https://t.co/jHrQe0kgBg,3,12," Abstract: Good sparse approximations are essential for practical inference in Gaussian Processes as the computational cost of exact methods is prohibitive for large datasets. The Fully Independent Training Conditional (FITC) and the Variational Free Energy (VFE) approximations are two recent popular methods. Despite superficial similarities, these approximations have surprisingly different theoretical properties and behave differently in practice. We thoroughly investigate the two methods for regression both analytically and through illustrative examples, and draw conclusions to guide practical application. "
743244766440202240,2016-06-16 00:52:15,https://t.co/JQObZJOMmz,Optimization Methods for Large-Scale Machine Learning. (arXiv:1606.04838v1 [stat.ML]) https://t.co/JQObZJOMmz,8,23," Abstract: This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations. "
743244763105746945,2016-06-16 00:52:15,https://t.co/usJmBnoMkq,Improving Variational Inference with Inverse Autoregressive Flow. (arXiv:1606.04934v1 [cs.LG]) https://t.co/usJmBnoMkq,12,14," Abstract: We propose a simple and scalable method for improving the flexibility of variational inference through a transformation with autoregressive networks. Autoregressive networks, such as RNNs and MADE, are very powerful models; however, ancestral sampling in such networks is a sequential operation, therefore unappealing for direct use as approximate posteriors in variational inference on parallel hardware such as GPUs. We find that by inverting autoregressive networks we can obtain equally powerful data transformations that can often be computed in parallel. We show that such data transformations, inverse autoregressive flows (IAF), can be used to transform a simple distribution over the latent variables into a much more flexible distribution, while still allowing us to compute the resulting variables' probability density function. The method is simple to implement, can be made arbitrarily flexible, and (in contrast with previous work) is naturally applicable to latent variables that are organized in multidimensional tensors, such as 2D grids or time series. The method is applied to a novel deep architecture of variational auto-encoders. In experiments we demonstrate that autoregressive flow leads to significant performance gains when applied to variational autoencoders for natural images. "
743244760501129217,2016-06-16 00:52:14,https://t.co/UOkOhrkAWp,Achieving Exact Cluster Recovery Threshold via Semidefinite Programming: Extensions. (arXiv:1502.07738v3 [stat.ML]… https://t.co/UOkOhrkAWp,0,1," Abstract: Resolving a conjecture of Abbe, Bandeira and Hall, the authors have recently shown that the semidefinite programming (SDP) relaxation of the maximum likelihood estimator achieves the sharp threshold for exactly recovering the community structure under the binary stochastic block model of two equal-sized clusters. The same was shown for the case of a single cluster and outliers. Extending the proof techniques, in this paper it is shown that SDP relaxations also achieve the sharp recovery threshold in the following cases: (1) Binary stochastic block model with two clusters of sizes proportional to network size but not necessarily equal; (2) Stochastic block model with a fixed number of equal-sized clusters; (3) Binary censored block model with the background graph being Erd\H{o}s-R\'enyi. Furthermore, a sufficient condition is given for an SDP procedure to achieve exact recovery for the general case of a fixed number of clusters plus outliers. These results demonstrate the versatility of SDP relaxation as a simple, general purpose, computationally feasible methodology for community detection. "
743244758026420225,2016-06-16 00:52:13,https://t.co/BVKPsDd3Sr,Discrete Distribution Estimation under Local Privacy. (arXiv:1602.07387v3 [stat.ML] UPDATED) https://t.co/BVKPsDd3Sr,0,3," Abstract: The collection and analysis of user data drives improvements in the app and web ecosystems, but comes with risks to privacy. This paper examines discrete distribution estimation under local privacy, a setting wherein service providers can learn the distribution of a categorical statistic of interest without collecting the underlying data. We present new mechanisms, including hashed K-ary Randomized Response (KRR), that empirically meet or exceed the utility of existing mechanisms at all privacy levels. New theoretical results demonstrate the order-optimality of KRR and the existing RAPPOR mechanism at different privacy regimes. "
743244755342090240,2016-06-16 00:52:13,https://t.co/PgJ8s8swZq,How is a data-driven approach better than random choice in label space division for multi-label classification?. (… https://t.co/PgJ8s8swZq,0,5," Abstract: We propose using five data-driven community detection approaches from social networks to partition the label space for the task of multi-label classification as an alternative to random partitioning into equal subsets as performed by RAkELd: modularity-maximizing fastgreedy and leading eigenvector, infomap, walktrap and label propagation algorithms. We construct a label co-occurence graph (both weighted an unweighted versions) based on training data and perform community detection to partition the label set. We include Binary Relevance and Label Powerset classification methods for comparison. We use gini-index based Decision Trees as the base classifier. We compare educated approaches to label space divisions against random baselines on 12 benchmark data sets over five evaluation measures. We show that in almost all cases seven educated guess approaches are more likely to outperform RAkELd than otherwise in all measures, but Hamming Loss. We show that fastgreedy and walktrap community detection methods on weighted label co-occurence graphs are 85-92% more likely to yield better F1 scores than random partitioning. Infomap on the unweighted label co-occurence graphs is on average 90% of the times better than random paritioning in terms of Subset Accuracy and 89% when it comes to Jaccard similarity. Weighted fastgreedy is better on average than RAkELd when it comes to Hamming Loss. "
742903453886861312,2016-06-15 02:16:00,https://t.co/zIabsQLnm3,Directly Modeling Missing Data in Sequences with RNNs: Improved Classification of Clinical Time Series. (arXiv:160… https://t.co/zIabsQLnm3,2,11," Abstract: We demonstrate a simple strategy to cope with missing data in sequential inputs, addressing the task of multilabel classification of diagnoses given clinical time series. Collected from the pediatric intensive care unit (PICU) at Children's Hospital Los Angeles, our data consists of multivariate time series of observations. The measurements are irregularly spaced, leading to missingness patterns in temporally discretized sequences. While these artifacts are typically handled by imputation, we achieve superior predictive performance by treating the artifacts as features. Unlike linear models, recurrent neural networks can realize this improvement using only simple binary indicators of missingness. For linear models, we show an alternative strategy to capture this signal. Training models on missingness patterns only, we show that for some diseases, what tests are run can be as predictive as the results themselves. "
742903449898123264,2016-06-15 02:15:59,https://t.co/WiWo6mBy61,The Crossover Process: Learnability meets Protection from Inference Attacks. (arXiv:1606.04160v1 [cs.LG]) https://t.co/WiWo6mBy61,0,3," Abstract: It is usual to consider data protection and learnability as conflicting objectives. This is not always the case: we show how to jointly control causal inference --- seen as the attack --- \textit{and} learnability by a noise-free process that mixes training examples, the Crossover Process (cp). One key point is that the cp~is typically able to alter joint distributions without touching on marginals, nor altering the sufficient statistic for the class. In other words, it saves (and sometimes improves) generalization for supervised learning, but can alter the relationship between covariates --- and therefore fool statistical measures of (nonlinear) independence and causal inference into misleading ad-hoc conclusions. Experiments on a dozen readily available domains validate the theory. "
742903446320336896,2016-06-15 02:15:58,https://t.co/fGSILEN174,Modal-set estimation with an application to clustering. (arXiv:1606.04166v1 [stat.ML]) https://t.co/fGSILEN174,0,3," Abstract: We present a first procedure that can estimate -- with statistical consistency guarantees -- any local-maxima of a density, under benign distributional conditions. The procedure estimates all such local maxima, or $\textit{modal-sets}$, of any bounded shape or dimension, including usual point-modes. In practice, modal-sets can arise as dense low-dimensional structures in noisy data, and more generally serve to better model the rich variety of locally-high-density structures in data. The procedure is then shown to be competitive on clustering applications, and moreover is quite stable to a wide range of settings of its tuning parameter. "
742903440741937152,2016-06-15 02:15:57,https://t.co/qUKhcBN4tB,Local Canonical Correlation Analysis for Nonlinear Common Variables Discovery. (arXiv:1606.04268v1 [cs.LG]) https://t.co/qUKhcBN4tB,0,3," Abstract: In this paper, we address the problem of hidden common variables discovery from multimodal data sets of nonlinear high-dimensional observations. We present a metric based on local applications of canonical correlation analysis (CCA) and incorporate it in a kernel-based manifold learning technique.We show that this metric discovers the hidden common variables underlying the multimodal observations by estimating the Euclidean distance between them. Our approach can be viewed both as an extension of CCA to a nonlinear setting as well as an extension of manifold learning to multiple data sets. Experimental results show that our method indeed discovers the common variables underlying high-dimensional nonlinear observations without assuming prior rigid model assumptions. "
742903437621362688,2016-06-15 02:15:56,https://t.co/IHR3TYzAmu,Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis. (arXiv:1606.04316v1 [s… https://t.co/IHR3TYzAmu,0,3," Abstract: The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better - more sound and useful - alternatives for it. "
742903432999272452,2016-06-15 02:15:55,https://t.co/ZyR8GK2Ecr,Calibration of Phone Likelihoods in Automatic Speech Recognition. (arXiv:1606.04317v1 [stat.ML]) https://t.co/ZyR8GK2Ecr,0,3," Abstract: In this paper we study the probabilistic properties of the posteriors in a speech recognition system that uses a deep neural network (DNN) for acoustic modeling. We do this by reducing Kaldi's DNN shared pdf-id posteriors to phone likelihoods, and using test set forced alignments to evaluate these using a calibration sensitive metric. Individual frame posteriors are in principle well-calibrated, because the DNN is trained using cross entropy as the objective function, which is a proper scoring rule. When entire phones are assessed, we observe that it is best to average the log likelihoods over the duration of the phone. Further scaling of the average log likelihoods by the logarithm of the duration slightly improves the calibration, and this improvement is retained when tested on independent test data. "
742903428683292672,2016-06-15 02:15:54,https://t.co/w9mD6I5ecB,LLFR: A Lanczos-Based Latent Factor Recommender for Big Data Scenarios. (arXiv:1606.04335v1 [stat.ML]) https://t.co/w9mD6I5ecB,0,2," Abstract: The purpose if this master's thesis is to study and develop a new algorithmic framework for Collaborative Filtering to produce recommendations in the top-N recommendation problem. Thus, we propose Lanczos Latent Factor Recommender (LLFR); a novel ""big data friendly"" collaborative filtering algorithm for top-N recommendation. Using a computationally efficient Lanczos-based procedure, LLFR builds a low dimensional item similarity model, that can be readily exploited to produce personalized ranking vectors over the item space. A number of experiments on real datasets indicate that LLFR outperforms other state-of-the-art top-N recommendation methods from a computational as well as a qualitative perspective. Our experimental results also show that its relative performance gains, compared to competing methods, increase as the data get sparser, as in the Cold Start Problem. More specifically, this is true both when the sparsity is generalized - as in the New Community Problem, a very common problem faced by real recommender systems in their beginning stages, when there is not sufficient number of ratings for the collaborative filtering algorithms to uncover similarities between items or users - and in the very interesting case where the sparsity is localized in a small fraction of the dataset - as in the New Users Problem, where new users are introduced to the system, they have not rated many items and thus, the CF algorithm can not make reliable personalized recommendations yet. "
742903424887459841,2016-06-15 02:15:53,https://t.co/rZbnGZKIff,Recursive nonlinear-system identification using latent variables. (arXiv:1606.04366v1 [stat.ML]) https://t.co/rZbnGZKIff,0,1," Abstract: In this paper we develop a method for learning nonlinear systems with multiple outputs and inputs. We begin by modelling the errors of a nominal predictor of the system using a latent variable framework. Then using the maximum likelihood principle we derive a criterion for learning the model. The resulting optimization problem is tackled using a majorization-minimization approach. Finally, we develop a convex majorization technique and show that it enables a recursive identification method. The method learns parsimonious predictive models and is tested on both synthetic and real nonlinear systems. "
742903420227604480,2016-06-15 02:15:52,https://t.co/gNKSrDa53n,EvoNet: Evolutionary Synthesis of Deep Neural Networks. (arXiv:1606.04393v1 [cs.CV]) https://t.co/gNKSrDa53n,0,5," Abstract: Taking inspiration from biological evolution, we explore the idea of ""Can deep neural networks evolve naturally over successive generations into highly efficient deep neural networks?"" by introducing the notion of synthesizing new highly efficient, yet powerful deep neural networks over successive generations via an evolutionary process from ancestor deep neural networks. The architectural traits of ancestor deep neural networks are encoded using synaptic probability models, which can be viewed as the `DNA' of these networks. New descendant networks with differing network architectures are synthesized based on these synaptic probability models from the ancestor networks and computational environmental factor models, in a random manner to mimic heredity, natural selection, and random mutation. These offspring networks are then trained into fully functional networks, like one would train a newborn, and have more efficient, more diverse network architectures than their ancestor networks, while achieving powerful modeling capabilities. Experimental results for the task of visual saliency demonstrated that the synthesized `evolved' offspring networks can achieve state-of-the-art performance while having network architectures that are significantly more efficient (with a staggering $\sim$48-fold decrease in synapses by the fourth generation) compared to the original ancestor network. "
742903416251396097,2016-06-15 02:15:51,https://t.co/R44TwGk2x3,The Parallel Knowledge Gradient Method for Batch Bayesian Optimization. (arXiv:1606.04414v1 [stat.ML]) https://t.co/R44TwGk2x3,0,4," Abstract: In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction, this method provides the one-step Bayes optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy. "
742903412476370946,2016-06-15 02:15:50,https://t.co/giYc1HWtIO,A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification. (arXiv:1606.044… https://t.co/giYc1HWtIO,0,4," Abstract: We present a general framework for classification of sparse and irregularly-sampled time series. The properties of such time series can result in substantial uncertainty about the values of the underlying temporal processes, while making the data difficult to deal with using standard classification methods that assume fixed-dimensional feature spaces. To address these challenges, we propose an uncertainty-aware classification framework based on a special computational layer we refer to as the Gaussian process adapter that can connect irregularly sampled time series data to any black-box classifier learnable using gradient descent. We show how to scale up the required computations based on combining the structured kernel interpolation framework and the Lanczos approximation method, and how to discriminatively train the Gaussian process adapter in combination with a number of classifiers end-to-end using backpropagation. "
742903409007812608,2016-06-15 02:15:49,https://t.co/YOyfm6JLjO,Recurrent neural network training with preconditioned stochastic gradient descent. (arXiv:1606.04449v1 [stat.ML]) https://t.co/YOyfm6JLjO,1,11," Abstract: Recurrent neural networks (RNN), especially the ones requiring extremely long term memories, are difficult to training. Hence, they provide an ideal testbed for benchmarking the performance of optimization algorithms. This paper reports test results of a recently proposed preconditioned stochastic gradient descent (PSGD) algorithm on RNN training. We find that PSGD may outperform Hessian-free optimization which achieves the state-of-the-art performance on the target problems, although it is only slightly more complicated than stochastic gradient descent (SGD) and is user friendly, virtually a tuning free algorithm. "
742903402393415680,2016-06-15 02:15:48,https://t.co/3HPQTmccAa,Model-Free Episodic Control. (arXiv:1606.04460v1 [stat.ML]) https://t.co/3HPQTmccAa,3,16," Abstract: State of the art deep reinforcement learning algorithms take many millions of interactions to attain human-level performance. Humans, on the other hand, can very quickly exploit highly rewarding nuances of an environment upon first discovery. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Here we investigate whether a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks. We demonstrate that it not only attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains. "
742881496302800896,2016-06-15 00:48:45,https://t.co/h8kag322VK,Contextual Semibandits via Supervised Learning Oracles. (arXiv:1502.05890v3 [cs.LG] UPDATED) https://t.co/h8kag322VK,0,4," Abstract: We study an online decision making problem where on each round a learner chooses a list of items based on some side information, receives a scalar feedback value for each individual item, and a reward that is linearly related to this feedback. These problems, known as contextual semibandits, arise in crowdsourcing, recommendation, and many other domains. This paper reduces contextual semibandits to supervised learning, allowing us to leverage powerful supervised learning methods in this partial-feedback setting. Our first reduction applies when the mapping from feedback to reward is known and leads to a computationally efficient algorithm with near-optimal regret. We show that this algorithm outperforms state-of-the-art approaches on real-world learning-to-rank datasets, demonstrating the advantage of oracle-based algorithms. Our second reduction applies to the previously unstudied setting when the linear mapping from feedback to reward is unknown. Our regret guarantees are superior to prior techniques that ignore the feedback. "
742881493547110400,2016-06-15 00:48:44,https://t.co/lJMG0LbwVY,De-biasing the Lasso: Optimal Sample Size for Gaussian Designs. (arXiv:1508.02757v3 [math.ST] UPDATED) https://t.co/lJMG0LbwVY,1,6," Abstract: Performing statistical inference in high-dimension is an outstanding challenge. A major source of difficulty is the absence of precise information on the distribution of high-dimensional estimators. Here, we consider linear regression in the high-dimensional regime $p\gg n$. In this context, we would like to perform inference on a high-dimensional parameters vector $\theta^*\in{\mathbb R}^p$. Important progress has been achieved in computing confidence intervals for single coordinates $\theta^*_i$. A key role in these new methods is played by a certain debiased estimator $\hat{\theta}^{\rm d}$ that is constructed from the Lasso. Earlier work establishes that, under suitable assumptions on the design matrix, the coordinates of $\hat{\theta}^{\rm d}$ are asymptotically Gaussian provided $\theta^*$ is $s_0$-sparse with $s_0 = o(\sqrt{n}/\log p )$. The condition $s_0 = o(\sqrt{n}/ \log p )$ is stronger than the one for consistent estimation, namely $s_0 = o(n/ \log p)$. We study Gaussian designs with known or unknown population covariance. When the covariance is known, we prove that the debiased estimator is asymptotically Gaussian under the nearly optimal condition $s_0 = o(n/ (\log p)^2)$. Note that earlier work was limited to $s_0 = o(\sqrt{n}/\log p)$ even for perfectly known covariance. The same conclusion holds if the population covariance is unknown but can be estimated sufficiently well, e.g. under the same sparsity conditions on the inverse covariance as assumed by earlier work. For intermediate regimes, we describe the trade-off between sparsity in the coefficients and in the inverse covariance of the design. We further discuss several applications of our results to high-dimensional inference. In particular, we propose a new estimator that is minimax optimal up to a factor $1+o_n(1)$ for i.i.d. Gaussian designs. "
742881490988589056,2016-06-15 00:48:44,https://t.co/QzioIXBDFa,Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural Networks. (arXiv:1508.04422v3 [stat.ML] UPD… https://t.co/QzioIXBDFa,0,5," Abstract: Several popular graph embedding techniques for representation learning and dimensionality reduction rely on performing computationally expensive eigendecompositions to derive a nonlinear transformation of the input data space. The resulting eigenvectors encode the embedding coordinates for the training samples only, and so the embedding of novel data samples requires further costly computation. In this paper, we present a method for the out-of-sample extension of graph embeddings using deep neural networks (DNN) to parametrically approximate these nonlinear maps. Compared with traditional nonparametric out-of-sample extension methods, we demonstrate that the DNNs can generalize with equal or better fidelity and require orders of magnitude less computation at test time. Moreover, we find that unsupervised pretraining of the DNNs improves optimization for larger network sizes, thus removing sensitivity to model selection. "
742881488987951104,2016-06-15 00:48:43,https://t.co/dGU9UaS11k,Sparse Multinomial Logistic Regression via Approximate Message Passing. (arXiv:1509.04491v2 [cs.IT] UPDATED) https://t.co/dGU9UaS11k,2,6," Abstract: For the problem of multi-class linear classification and feature selection, we propose approximate message passing approaches to sparse multinomial logistic regression (MLR). First, we propose two algorithms based on the Hybrid Generalized Approximate Message Passing (HyGAMP) framework: one finds the maximum a posteriori (MAP) linear classifier and the other finds an approximation of the test-error-rate minimizing linear classifier. Then we design computationally simplified variants of these two algorithms. Next, we detail methods to tune the hyperparameters of their assumed statistical models using Stein's unbiased risk estimate (SURE) and expectation-maximization (EM), respectively. Finally, using both synthetic and real-world datasets, we demonstrate improved error-rate and runtime performance relative to existing state-of-the-art approaches to sparse MLR. "
742881486647504896,2016-06-15 00:48:43,https://t.co/YgRqcamZjY,On the Quality of the Initial Basin in Overspecified Neural Networks. (arXiv:1511.04210v3 [cs.LG] UPDATED) https://t.co/YgRqcamZjY,0,2," Abstract: Deep learning, in the form of artificial neural networks, has achieved remarkable practical success in recent years, for a variety of difficult machine learning applications. However, a theoretical explanation for this remains a major open problem, since training neural networks involves optimizing a highly non-convex objective function, and is known to be computationally hard in the worst case. In this work, we study the \emph{geometric} structure of the associated non-convex objective function, in the context of ReLU networks and starting from a random initialization of the network parameters. We identify some conditions under which it becomes more favorable to optimization, in the sense of (i) High probability of initializing at a point from which there is a monotonically decreasing path to a global minimum; and (ii) High probability of initializing at a basin (suitably defined) with a small minimal objective value. A common theme in our results is that such properties are more likely to hold for larger (""overspecified"") networks, which accords with some recent empirical and theoretical observations. "
742881484340613120,2016-06-15 00:48:42,https://t.co/zKaHNJ4WKi,Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series. (arXiv:1602.07109v5 [stat.ML]… https://t.co/zKaHNJ4WKi,0,16, Abstract: Approximate variational inference has shown to be a powerful tool for modeling unknown complex probability distributions. Recent advances in the field allow us to learn probabilistic models of sequences that actively exploit spatial and temporal structure. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line. 
742881481895358465,2016-06-15 00:48:42,https://t.co/M2BYbq5pV4,Multi-task Recurrent Model for Speech and Speaker Recognition. (arXiv:1603.09643v3 [cs.CL] UPDATED) https://t.co/M2BYbq5pV4,1,3," Abstract: Although highly correlated, speech and speaker recognition have been regarded as two independent tasks and studied by two communities. This is certainly not the way that people behave: we decipher both speech content and speaker traits at the same time. This paper presents a unified model to perform speech and speaker recognition simultaneously and altogether. The model is based on a unified neural network where the output of one task is fed to the input of the other, leading to a multi-task recurrent network. Experiments show that the joint model outperforms the task-specific models on both the two tasks. "
742881479122944000,2016-06-15 00:48:41,https://t.co/QOP46pXfDS,Positive Definite Estimation of Large Covariance Matrix Using Generalized Nonconvex Penalties. (arXiv:1604.04348v2… https://t.co/QOP46pXfDS,0,1," Abstract: This work addresses the issue of large covariance matrix estimation in high-dimensional statistical analysis. Recently, improved iterative algorithms with positive-definite guarantee have been developed. However, these algorithms cannot be directly extended to use a nonconvex penalty for sparsity inducing. Generally, a nonconvex penalty has the capability of ameliorating the bias problem of the popular convex lasso penalty, and thus is more advantageous. In this work, we propose a class of positive-definite covariance estimators using generalized nonconvex penalties. We develop a first-order algorithm based on the alternating direction method framework to solve the nonconvex optimization problem efficiently. The convergence of this algorithm has been proved. Further, the statistical properties of the new estimators have been analyzed for generalized nonconvex penalties. Moreover, extension of this algorithm to covariance estimation from sketched measurements has been considered. The performances of the new estimators have been demonstrated by both a simulation study and a gene clustering example for tumor tissues. Code for the proposed estimators is available at this https URL "
742881476803497984,2016-06-15 00:48:40,https://t.co/XyMlS7IoqR,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation. (arXiv:1606.00776v2 [cs… https://t.co/XyMlS7IoqR,0,8," Abstract: We introduce the multiresolution recurrent neural network, which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes: a sequence of high-level coarse tokens, and a sequence of natural language tokens. There are many ways to estimate or learn the high-level coarse tokens, but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics. Such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences. In contrast to the standard log- likelihood objective w.r.t. natural language tokens (word perplexity), optimizing the joint log-likelihood biases the model towards modeling high-level abstractions. We apply the proposed model to the task of dialogue response generation in two challenging domains: the Ubuntu technical support domain, and Twitter conversations. On Ubuntu, the model outperforms competing approaches by a substantial margin, achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study. On Twitter, the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics. Finally, our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure. "
742522467734917120,2016-06-14 01:02:06,https://t.co/x0EUQt3qMQ,De-identification of Patient Notes with Recurrent Neural Networks. (arXiv:1606.03475v1 [cs.CL]) https://t.co/x0EUQt3qMQ,0,5," Abstract: Objective: Patient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information (PHI) that needs to be removed to de-identify patient notes. Manual de-identification is impractical given the size of EHR databases, the limited number of researchers with access to the non-de-identified notes, and the frequent mistakes of human annotators. A reliable automated de-identification system would consequently be of high value. Materials and Methods: We introduce the first de-identification system based on artificial neural networks (ANNs), which requires no handcrafted features or rules, unlike existing systems. We compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the MIMIC de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset. Results: Our ANN model outperforms the state-of-the-art systems. It yields an F1-score of 97.85 on the i2b2 2014 dataset, with a recall 97.38 and a precision of 97.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with a recall 99.25 and a precision of 99.06. Conclusion: Our findings support the use of ANNs for de-identification of patient notes, as they show better performance than previously published systems while requiring no feature engineering. "
742522462198435840,2016-06-14 01:02:05,https://t.co/CQtHVu12GM,The Mythos of Model Interpretability. (arXiv:1606.03490v1 [cs.LG]) https://t.co/CQtHVu12GM,5,13," Abstract: Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not. "
742522457462956033,2016-06-14 01:02:03,https://t.co/ns0D9rLanI,Incoherent Tensor Norms and Their Applications in Higher Order Tensor Completion. (arXiv:1606.03504v1 [math.ST]) https://t.co/ns0D9rLanI,1,3," Abstract: In this paper, we investigate the sample size requirement for a general class of nuclear norm minimization methods for higher order tensor completion. We introduce a class of tensor norms by allowing for different levels of coherence, which allows us to leverage the incoherence of a tensor. In particular, we show that a $k$th order tensor of rank $r$ and dimension $d\times\cdots\times d$ can be recovered perfectly from as few as $O((r^{(k-1)/2}d^{3/2}+r^{k-1}d)(\log(d))^2)$ uniformly sampled entries through an appropriate incoherent nuclear norm minimization. Our results demonstrate some key differences between completing a matrix and a higher order tensor: They not only point to potential room for improvement over the usual nuclear norm minimization but also highlight the importance of explicitly accounting for incoherence, when dealing with higher order tensors. "
742522452195049472,2016-06-14 01:02:02,https://t.co/66ZdA9J5EV,TRex: A Tomography Reconstruction Proximal Framework for Robust Sparse View X-Ray Applications. (arXiv:1606.03601v… https://t.co/66ZdA9J5EV,0,2," Abstract: We present TRex, a flexible and robust Tomographic Reconstruction framework using proximal algorithms. We provide an overview and perform an experimental comparison between the famous iterative reconstruction methods in terms of reconstruction quality in sparse view situations. We then derive the proximal operators for the four best methods. We show the flexibility of our framework by deriving solvers for two noise models: Gaussian and Poisson; and by plugging in three powerful regularizers. We compare our framework to state of the art methods, and show superior quality on both synthetic and real datasets. "
742522446943813633,2016-06-14 01:02:01,https://t.co/32JEU6ENkH,Drug response prediction by inferring pathway-response associations with Kernelized Bayesian Matrix Factorization.… https://t.co/32JEU6ENkH,1,3," Abstract: A key goal of computational personalized medicine is to systematically utilize genomic and other molecular features of samples to predict drug responses for a previously unseen sample. Such predictions are valuable for developing hypotheses for selecting therapies tailored for individual patients. This is especially valuable in oncology, where molecular and genetic heterogeneity of the cells has a major impact on the response. However, the prediction task is extremely challenging, raising the need for methods that can effectively model and predict drug responses. In this study, we propose a novel formulation of multi-task matrix factorization that allows selective data integration for predicting drug responses. To solve the modeling task, we extend the state-of-the-art kernelized Bayesian matrix factorization (KBMF) method with component-wise multiple kernel learning. In addition, our approach exploits the known pathway information in a novel and biologically meaningful fashion to learn the drug response associations. Our method quantitatively outperforms the state of the art on predicting drug responses in two publicly available cancer data sets as well as on a synthetic data set. In addition, we validated our model predictions with lab experiments using an in-house cancer cell line panel. We finally show the practical applicability of the proposed method by utilizing prior knowledge to infer pathway-drug response associations, opening up the opportunity for elucidating drug action mechanisms. We demonstrate that pathway-response associations can be learned by the proposed model for the well known EGFR and MEK inhibitors. "
742522442506207232,2016-06-14 01:02:00,https://t.co/iTtQ0APVE4,InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. (arXiv:1606.… https://t.co/iTtQ0APVE4,2,6," Abstract: This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods. "
742522437984768000,2016-06-14 01:01:59,https://t.co/bDimdHXUMs,Comparison of Several Sparse Recovery Methods for Low Rank Matrices with Random Samples. (arXiv:1606.03672v1 [cs.L… https://t.co/bDimdHXUMs,0,2," Abstract: In this paper, we will investigate the efficacy of IMAT (Iterative Method of Adaptive Thresholding) in recovering the sparse signal (parameters) for linear models with missing data. Sparse recovery rises in compressed sensing and machine learning problems and has various applications necessitating viable reconstruction methods specifically when we work with big data. This paper will focus on comparing the power of IMAT in reconstruction of the desired sparse signal with LASSO. Additionally, we will assume the model has random missing information. Missing data has been recently of interest in big data and machine learning problems since they appear in many cases including but not limited to medical imaging datasets, hospital datasets, and massive MIMO. The dominance of IMAT over the well-known LASSO will be taken into account in different scenarios. Simulations and numerical results are also provided to verify the arguments. "
742522433291309057,2016-06-14 01:01:58,https://t.co/4DXVixr56n,Efficient KLMS and KRLS Algorithms: A Random Fourier Feature Perspective. (arXiv:1606.03685v1 [cs.LG]) https://t.co/4DXVixr56n,0,3," Abstract: We present a new framework for online Least Squares algorithms for nonlinear modeling in RKH spaces (RKHS). Instead of implicitly mapping the data to a RKHS (e.g., kernel trick), we map the data to a finite dimensional Euclidean space, using random features of the kernel's Fourier transform. The advantage is that, the inner product of the mapped data approximates the kernel function. The resulting ""linear"" algorithm does not require any form of sparsification, since, in contrast to all existing algorithms, the solution's size remains fixed and does not increase with the iteration steps. As a result, the obtained algorithms are computationally significantly more efficient compared to previously derived variants, while, at the same time, they converge at similar speeds and to similar error floors. "
742522428778250240,2016-06-14 01:01:57,https://t.co/bI0k6aP5EB,Specialized Support Vector Machines for open-set recognition. (arXiv:1606.03802v1 [cs.LG]) https://t.co/bI0k6aP5EB,0,2," Abstract: Often, when dealing with real-world recognition problems, we do not need, and often cannot have, knowledge of the entire set of possible classes that might appear during operational testing. Moreover, sometimes some of these classes may be ill-sampled, not sampled at all or undefined. In such cases, we need to think of robust classification methods able to deal with the ""unknown"" and properly reject samples belonging to classes never seen during training. Notwithstanding, almost all existing classifiers to date were mostly developed for the closed-set scenario, i.e., the classification setup in which it is assumed that all test samples belong to one of the classes with which the classifier was trained. In the open-set scenario, however, a test sample can belong to none of the known classes and the classifier must properly reject it by classifying it as unknown. In this work, we extend upon the well-known Support Vector Machines (SVM) classifier and introduce the Specialized Support Vector Machine (SSVM), which is suitable for recognition in open-set setups. The SSVM balances the empirical risk and the risk of the unknown and ensures that the region of the feature space in which a test sample would be classified as known (one of the known classes) is always bounded, ensuring a finite risk of the unknown. The same cannot be guaranteed by the traditional SVM formulation, even when using the Radial Basis Function (RBF) kernel. In this work, we also highlight the properties of the SVM classifier related to the open-set scenario, and provide necessary and sufficient conditions for an RBF SVM to have bounded open-space risk. An extensive set of experiments compares the proposed method with existing solutions in the literature for open-set recognition and the reported results show its effectiveness. "
742522423837351936,2016-06-14 01:01:55,https://t.co/Q9VSVuVMNF,Tuning-Free Heterogeneity Pursuit in Massive Networks. (arXiv:1606.03803v1 [stat.ME]) https://t.co/Q9VSVuVMNF,0,2," Abstract: Heterogeneity is often natural in many contemporary applications involving massive data. While posing new challenges to effective learning, it can play a crucial role in powering meaningful scientific discoveries through the understanding of important differences among subpopulations of interest. In this paper, we exploit multiple networks with Gaussian graphs to encode the connectivity patterns of a large number of features on the subpopulations. To uncover the heterogeneity of these structures across subpopulations, we suggest a new framework of tuning-free heterogeneity pursuit (THP) via large-scale inference, where the number of networks is allowed to diverge. In particular, two new tests, the chi-based test and the linear functional-based test, are introduced and their asymptotic null distributions are established. Under mild regularity conditions, we establish that both tests are optimal in achieving the testable region boundary and the sample size requirement for the latter test is minimal. Both theoretical guarantees and the tuning-free feature stem from efficient multiple-network estimation by our newly suggested approach of heterogeneous group square-root Lasso (HGSL) for high-dimensional multi-response regression with heterogeneous noises. To solve this convex program, we further introduce a tuning-free algorithm that is scalable and enjoys provable convergence to the global optimum. Both computational and theoretical advantages of our procedure are elucidated through simulation and real data examples. "
742522420817494016,2016-06-14 01:01:55,https://t.co/LDhuktkZoI,Efficient Learning with a Family of Nonconvex Regularizers by Redistributing Nonconvexity. (arXiv:1606.03841v1 [ma… https://t.co/LDhuktkZoI,0,2," Abstract: The use of convex regularizers allow for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, for a large class of nonconvex regularizers, we propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex regularizer, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the standard proximal algorithm and Frank-Wolfe algorithm). Moreover, it can be shown that critical points of the transformed problem are also critical points of the original problem. Extensive experiments on a number of nonconvex regularization problems show that the proposed procedure is much faster than the state-of-the-art nonconvex solvers. "
742522417772400641,2016-06-14 01:01:54,https://t.co/iJOzzNm94h,Reweighted Data for Robust Probabilistic Models. (arXiv:1606.03860v1 [stat.ML]) https://t.co/iJOzzNm94h,0,3," Abstract: Probabilistic models analyze data by relying on a set of assumptions. When a model performs poorly, we challenge its assumptions. This approach has led to myriad hand-crafted robust models; they offer protection against small deviations from their assumptions. We propose a simple way to systematically mitigate mismatch of a large class of probabilistic models. The idea is to raise the likelihood of each observation to a weight. Inferring these weights allows a model to identify observations that match its assumptions; down-weighting others enables robust inference and improved predictive accuracy. We study four different forms of model mismatch, ranging from missing latent groups to structure misspecification. A Poisson factorization analysis of the Movielens dataset shows the benefits of reweighting in a real data scenario. "
742522414463127552,2016-06-14 01:01:53,https://t.co/eM0oC1SAJS,Prediction Performance After Learning in Gaussian Process Regression. (arXiv:1606.03865v1 [stat.ML]) https://t.co/eM0oC1SAJS,0,2," Abstract: This paper considers the quantification of the prediction performance in Gaussian process regression. The standard approach is to base the prediction error bars on the theoretical predictive variance, which is a lower bound on the mean square-error. This approach, however, does not take into account that the statistical model is learned from the data. We show that this omission leads to a systematic underestimation of the prediction errors. Starting from a generalization of the Cram\'er-Rao bound, we derive a more accurate measure of uncertainty for prediction of Gaussian processes and illustrate it using synthetic and real data examples. "
742522410541404160,2016-06-14 01:01:52,https://t.co/5UFuFvSkzi,Inferring Sparsity: Compressed Sensing using Generalized Restricted Boltzmann Machines. (arXiv:1606.03956v1 [cs.IT… https://t.co/5UFuFvSkzi,1,3," Abstract: In this work, we consider compressed sensing reconstruction from $M$ measurements of $K$-sparse structured signals which do not possess a writable correlation model. Assuming that a generative statistical model, such as a Boltzmann machine, can be trained in an unsupervised manner on example signals, we demonstrate how this signal model can be used within a Bayesian framework of signal reconstruction. By deriving a message-passing inference for general distribution restricted Boltzmann machines, we are able to integrate these inferred signal models into approximate message passing for compressed sensing reconstruction. Finally, we show for the MNIST dataset that this approach can be very effective, even for $M < K$. "
742522406628163588,2016-06-14 01:01:51,https://t.co/8VGSCutDo3,Bounding and Minimizing Counterfactual Error. (arXiv:1606.03976v1 [stat.ML]) https://t.co/8VGSCutDo3,0,2," Abstract: There is intense interest in applying machine learning to problems of causal inference in healthcare, economics, education, and other fields. In particular, individual-level causal inference has applications such as precision medicine and personalized advertising. We give a new theoretical analysis and family of algorithms for estimating individual treatment effect (ITE) from observational data. The algorithm itself learns a ""balanced"" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distance. Experiments on real and simulated data show the new algorithms match or outperform state-of-the-art methods. "
742522403239153664,2016-06-14 01:01:50,https://t.co/zbno8xGRh2,"Dialog state tracking, a machine reading approach using a memory-enhanced neural network. (arXiv:1606.04052v1 [cs.… https://t.co/zbno8xGRh2",0,2," Abstract: In an end-to-end dialog system, the aim of dialog state tracking is to accurately estimate a compact representation of the current dialog status from a sequence of noisy observations produced by the speech recognition and the natural language understanding modules. This paper introduces a novel method of dialog state tracking based on the general paradigm of machine reading and proposes to solve it using an End-to-End Memory Network, MemN2N, a memory-enhanced neural network architecture. We evaluate the proposed approach on the second Dialog State Tracking Challenge (DSTC-2) dataset. The corpus has been converted for the occasion in order to frame the hidden state variable inference as a question-answering task based on a sequence of utterances extracted from a dialog. We show that the proposed tracker gives encouraging results. Then, we propose to extend the DSTC-2 dataset with specific reasoning capabilities requirement like counting, list maintenance, yes-no question answering and indefinite knowledge management. Finally, we present encouraging results using our proposed MemN2N based tracking model. "
742522398734471168,2016-06-14 01:01:49,https://t.co/gNTAl43667,Matching Networks for One Shot Learning. (arXiv:1606.04080v1 [cs.LG]) https://t.co/gNTAl43667,2,8," Abstract: Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank. "
742522395014103041,2016-06-14 01:01:49,https://t.co/jRU4b5s7HB,Variational Inference with Normalizing Flows. (arXiv:1505.05770v5 [stat.ML] UPDATED) https://t.co/jRU4b5s7HB,0,2," Abstract: The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference. "
742522389972541440,2016-06-14 01:01:47,https://t.co/CsJ0SPfoO8,Distributional Smoothing with Virtual Adversarial Training. (arXiv:1507.00677v9 [stat.ML] UPDATED) https://t.co/CsJ0SPfoO8,0,2," Abstract: We propose local distributional smoothness (LDS), a new notion of smoothness for statistical model that can be used as a regularization term to promote the smoothness of the model distribution. We named the LDS based regularization as virtual adversarial training (VAT). The LDS of a model at an input datapoint is defined as the KL-divergence based robustness of the model distribution against local perturbation around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semi-supervised learning. The computational cost for VAT is relatively low. For neural network, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. When we applied our technique to supervised and semi-supervised learning for the MNIST dataset, it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We also applied our method to SVHN and NORB, and confirmed our method's superior performance over the current state of the art semi-supervised method applied to these datasets. "
742522385983737858,2016-06-14 01:01:46,https://t.co/BqTPCgalwb,Faster Stochastic Variational Inference using Proximal-Gradient Methods with General Divergence Functions. (arXiv:… https://t.co/BqTPCgalwb,0,1," Abstract: Several recent works have explored stochastic gradient methods for variational inference that exploit the geometry of the variational-parameter space. However, the theoretical properties of these methods are not well-understood and these methods typically only apply to conditionally-conjugate models. We present a new stochastic method for variational inference which exploits the geometry of the variational-parameter space and also yields simple closed-form updates even for non-conjugate models. We also give a convergence-rate analysis of our method and many other previous methods which exploit the geometry of the space. Our analysis generalizes existing convergence results for stochastic mirror-descent on non-convex objectives by using a more general class of divergence functions. Beyond giving a theoretical justification for a variety of recent methods, our experiments show that new algorithms derived in this framework lead to state of the art results on a variety of problems. Further, due to its generality, we expect that our theoretical analysis could also apply to other applications. "
742522381714006020,2016-06-14 01:01:45,https://t.co/5Zi38ArxH7,Kalman-based Stochastic Gradient Method with Stop Condition and Insensitivity to Conditioning. (arXiv:1512.01139v2… https://t.co/5Zi38ArxH7,0,2," Abstract: Modern proximal and stochastic gradient descent (SGD) methods are believed to efficiently minimize large composite objective functions, but such methods have two algorithmic challenges: (1) a lack of fast or justified stop conditions, and (2) sensitivity to the objective function's conditioning. In response to the first challenge, modern proximal and SGD methods guarantee convergence only after multiple epochs, but such a guarantee renders proximal and SGD methods infeasible when the number of component functions is very large or infinite. In response to the second challenge, second order SGD methods have been developed, but they are marred by the complexity of their analysis. In this work, we address these challenges on the limited, but important, linear regression problem by introducing and analyzing a second order proximal/SGD method based on Kalman Filtering (kSGD). Through our analysis, we show kSGD is asymptotically optimal, develop a fast algorithm for very large, infinite or streaming data sources with a justified stop condition, prove that kSGD is insensitive to the problem's conditioning, and develop a unique approach for analyzing the complex second order dynamics. Our theoretical results are supported by numerical experiments on three regression problems (linear, nonparametric wavelet, and logistic) using three large publicly available datasets. Moreover, our analysis and experiments lay a foundation for embedding kSGD in multiple epoch algorithms, extending kSGD to other problem classes, and developing parallel and low memory kSGD implementations. "
742522378425643013,2016-06-14 01:01:45,https://t.co/2SBDD4gEmg,Discovering Neuronal Cell Types and Their Gene Expression Profiles Using a Spatial Point Process Mixture Model. (a… https://t.co/2SBDD4gEmg,0,1," Abstract: Cataloging the neuronal cell types that comprise circuitry of individual brain regions is a major goal of modern neuroscience and the BRAIN initiative. Single-cell RNA sequencing can now be used to measure the gene expression profiles of individual neurons and to categorize neurons based on their gene expression profiles. While the single-cell techniques are extremely powerful and hold great promise, they are currently still labor intensive, have a high cost per cell, and, most importantly, do not provide information on spatial distribution of cell types in specific regions of the brain. We propose a complementary approach that uses computational methods to infer the cell types and their gene expression profiles through analysis of brain-wide single-cell resolution in situ hybridization (ISH) imagery contained in the Allen Brain Atlas (ABA). We measure the spatial distribution of neurons labeled in the ISH image for each gene and model it as a spatial point process mixture, whose mixture weights are given by the cell types which express that gene. By fitting a point process mixture model jointly to the ISH images, we infer both the spatial point process distribution for each cell type and their gene expression profile. We validate our predictions of cell type-specific gene expression profiles using single cell RNA sequencing data, recently published for the mouse somatosensory cortex. Jointly with the gene expression profiles, cell features such as cell size, orientation, intensity and local density level are inferred per cell type. "
742522374550085632,2016-06-14 01:01:44,https://t.co/at1lZTk78f,Learning Granger Causality for Hawkes Processes. (arXiv:1602.04511v2 [cs.LG] UPDATED) https://t.co/at1lZTk78f,0,2," Abstract: Learning Granger causality for general point processes is a very challenging task. In this paper, we propose an effective method, learning Granger causality, for a special but significant type of point processes --- Hawkes process. We reveal the relationship between Hawkes process's impact function and its Granger causality graph. Specifically, our model represents impact functions using a series of basis functions and recovers the Granger causality graph via group sparsity of the impact functions' coefficients. We propose an effective learning algorithm combining a maximum likelihood estimator (MLE) with a sparse-group-lasso (SGL) regularizer. Additionally, the flexibility of our model allows to incorporate the clustering structure event types into learning framework. We analyze our learning algorithm and propose an adaptive procedure to select basis functions. Experiments on both synthetic and real-world data show that our method can learn the Granger causality graph and the triggering patterns of the Hawkes processes simultaneously. "
742522369835687937,2016-06-14 01:01:43,https://t.co/P4H0TA6rY2,Pairwise Choice Markov Chains. (arXiv:1603.02740v2 [stat.ML] UPDATED) https://t.co/P4H0TA6rY2,0,1," Abstract: As datasets capturing human choices grow in richness and scale---particularly in online domains---there is an increasing need for choice models that escape traditional choice-theoretic axioms such as regularity, stochastic transitivity, and Luce's choice axiom. In this work we introduce the Pairwise Choice Markov Chain (PCMC) model of discrete choice, an inferentially tractable model that does not assume any of the above axioms while still satisfying the foundational axiom of uniform expansion, a considerably weaker assumption than Luce's choice axiom. We show that the PCMC model significantly outperforms the Multinomial Logit (MNL) model in prediction tasks on both synthetic and empirical datasets known to exhibit violations of Luce's axiom. Our analysis also synthesizes several recent observations connecting the Multinomial Logit model and Markov chains; the PCMC model retains the Multinomial Logit model as a special case. "
742522363749793792,2016-06-14 01:01:41,https://t.co/IIpxIc2Ryc,Online Optimization Methods for the Quantification Problem. (arXiv:1605.04135v3 [stat.ML] UPDATED) https://t.co/IIpxIc2Ryc,0,2," Abstract: The estimation of class prevalence, i.e., the fraction of a population that belongs to a certain class, is a very useful tool in data analytics and learning, and finds applications in many domains such as sentiment analysis, epidemiology, etc. For example, in sentiment analysis, the objective is often not to estimate whether a specific text conveys a positive or a negative sentiment, but rather estimate the overall distribution of positive and negative sentiments during an event window. A popular way of performing the above task, often dubbed quantification, is to use supervised learning to train a prevalence estimator from labeled data. Contemporary literature cites several performance measures used to measure the success of such prevalence estimators. In this paper we propose the first online stochastic algorithms for directly optimizing these quantification-specific performance measures. We also provide algorithms that optimize hybrid performance measures that seek to balance quantification and classification performance. Our algorithms present a significant advancement in the theory of multivariate optimization and we show, by a rigorous theoretical analysis, that they exhibit optimal convergence. We also report extensive experiments on benchmark and real data sets which demonstrate that our methods significantly outperform existing optimization techniques used for these performance measures. "
742522359010238464,2016-06-14 01:01:40,https://t.co/6jxar8oNMJ,Fast $\epsilon$-free Inference of Simulation Models with Bayesian Conditional Density Estimation. (arXiv:1605.0637… https://t.co/6jxar8oNMJ,0,5," Abstract: Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an $\epsilon$-ball around the observed data, which is only correct in the limit $\epsilon\!\rightarrow\!0$. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as $\epsilon\!\rightarrow\!0$, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior. "
742522353993809920,2016-06-14 01:01:39,https://t.co/oXGpYWwLZa,The bag-of-frames approach: a not so sufficient model for urban soundscapes. (arXiv:1412.4052v2 [cs.SD] CROSS LIST… https://t.co/oXGpYWwLZa,0,4," Abstract: The ""bag-of-frames"" approach (BOF), which encodes audio signals as the long-term statistical distribution of short-term spectral features, is commonly regarded as an effective and sufficient way to represent environmental sound recordings (soundscapes) since its introduction in an influential 2007 article. The present paper describes a concep-tual replication of this seminal article using several new soundscape datasets, with results strongly questioning the adequacy of the BOF approach for the task. We show that the good accuracy originally re-ported with BOF likely result from a particularly thankful dataset with low within-class variability, and that for more realistic datasets, BOF in fact does not perform significantly better than a mere one-point av-erage of the signal's features. Soundscape modeling, therefore, may not be the closed case it was once thought to be. Progress, we ar-gue, could lie in reconsidering the problem of considering individual acoustical events within each soundscape. "
742154995768889344,2016-06-13 00:41:54,https://t.co/njihgAkoM2,Mutual Exclusivity Loss for Semi-Supervised Deep Learning. (arXiv:1606.03141v1 [cs.CV]) https://t.co/njihgAkoM2,0,5, Abstract: In this paper we consider the problem of semi-supervised learning with deep Convolutional Neural Networks (ConvNets). Semi-supervised learning is motivated on the observation that unlabeled data is cheap and can be used to improve the accuracy of classifiers. In this paper we propose an unsupervised regularization term that explicitly forces the classifier's prediction for multiple classes to be mutually-exclusive and effectively guides the decision boundary to lie on the low density space between the manifolds corresponding to different classes of data. Our proposed approach is general and can be used with any backpropagation-based learning method. We show through different experiments that our method can improve the object recognition performance of ConvNets using unlabeled data. 
742154994502258688,2016-06-13 00:41:53,https://t.co/qQvDSr6cdn,Causal Bandits: Learning Good Interventions via Causal Inference. (arXiv:1606.03203v1 [stat.ML]) https://t.co/qQvDSr6cdn,1,6, Abstract: We study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment. Our formalism combines multi-arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information. 
742154993390784512,2016-06-13 00:41:53,https://t.co/YWu1Kj3VUo,Network Lasso Optimization For Smart City Ride Share Prediction. (arXiv:1606.03276v1 [cs.CY]) https://t.co/YWu1Kj3VUo,2,6," Abstract: Ride sharing has important implications in terms of environmental, social and individual goals by reducing carbon footprints, fostering social interactions and economizing commuter costs. The ride sharing systems that are commonly available lack adaptive and scalable techniques that can simultaneously learn from the large scale data and predict in real-time dynamic fashion. In this paper, we study such a problem towards a smart city initiative, where a generic ride sharing system is conceived capable of making predictions about ride share opportunities based on the historically recorded data while satisfying real-time ride requests. Underpinning the system is an application of a powerful machine learning convex optimization framework called Network Lasso that uses the Alternate Direction Method of Multipliers (ADMM) optimization for learning and dynamic prediction. We propose an application of a robust and scalable unified optimization framework within the ride sharing case-study. The application of Network Lasso framework is capable of jointly optimizing and clustering different rides based on their spatial and model similarity. The prediction from the framework clusters new ride requests, making accurate price prediction based on the clusters, detecting hidden correlations in the data and allowing fast convergence due to the network topology. We provide an empirical evaluation of the application of ADMM network Lasso on real trip record and simulated data, proving their effectiveness since the mean squared error of the algorithm's prediction is minimized on the test rides. "
742154992077918208,2016-06-13 00:41:53,https://t.co/L6JROzPSD8,Conditional Generation and Snapshot Learning in Neural Dialogue Systems. (arXiv:1606.03352v1 [cs.CL]) https://t.co/L6JROzPSD8,0,5," Abstract: Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used. "
742154990916096000,2016-06-13 00:41:53,https://t.co/zDlFOUwzeU,Extended Gauss-Newton and Gauss-Newton-ADMM Algorithms for Low-Rank Matrix Optimization. (arXiv:1606.03358v1 [math… https://t.co/zDlFOUwzeU,0,2," Abstract: We develop a generic Gauss-Newton (GN) framework for solving a class of nonconvex optimization problems involving low-rank matrix variables. As opposed to standard Gauss-Newton method, our framework allows one to handle general smooth convex cost function via its surrogate. The main complexity-per-iteration consists of the inverse of two rank-size matrices and at most six small matrix multiplications to compute a closed form Gauss-Newton direction, and a backtracking linesearch. We show, under mild conditions, that the proposed algorithm globally and locally converges to a stationary point of the original nonconvex problem. We also show empirically that the Gauss-Newton algorithm achieves much higher accurate solutions compared to the well studied alternating direction method (ADM). Then, we specify our Gauss-Newton framework to handle the symmetric case and prove its convergence, where ADM is not applicable without lifting variables. Next, we incorporate our Gauss-Newton scheme into the alternating direction method of multipliers (ADMM) to design a GN-ADMM algorithm for solving the low-rank optimization problem. We prove that, under mild conditions and a proper choice of the penalty parameter, our GN-ADMM globally converges to a stationary point of the original problem. Finally, we apply our algorithms to solve several problems in practice such as low-rank approximation, matrix completion, robust low-rank matrix recovery, and matrix recovery in quantum tomography. The numerical experiments provide encouraging results to motivate the use of nonconvex optimization. "
742154988458278912,2016-06-13 00:41:52,https://t.co/gejSE68TAX,Scan Order in Gibbs Sampling: Models in Which it Matters and Bounds on How Much. (arXiv:1606.03432v1 [cs.LG]) https://t.co/gejSE68TAX,0,3," Abstract: Gibbs sampling is a Markov Chain Monte Carlo sampling technique that iteratively samples variables from their conditional distributions. There are two common scan orders for the variables: random scan and systematic scan. Due to the benefits of locality in hardware, systematic scan is commonly used, even though most statistical guarantees are only for random scan. While it has been conjectured that the mixing times of random scan and systematic scan do not differ by more than a logarithmic factor, we show by counterexample that this is not the case, and we prove that that the mixing times do not differ by more than a polynomial factor under mild conditions. To prove these relative bounds, we introduce a method of augmenting the state space to study systematic scan using conductance. "
742154983005691904,2016-06-13 00:41:51,https://t.co/SM3pYKj8vP,Deep Directed Generative Models with Energy-Based Probability Estimation. (arXiv:1606.03439v1 [cs.LG]) https://t.co/SM3pYKj8vP,3,11," Abstract: Training energy-based probabilistic models is confronted with apparently intractable sums, whose Monte Carlo estimation requires sampling from the estimated probability distribution in the inner loop of training. This can be approximately achieved by Markov chain Monte Carlo methods, but may still face a formidable obstacle that is the difficulty of mixing between modes with sharp concentrations of probability. Whereas an MCMC process is usually derived from a given energy function based on mathematical considerations and requires an arbitrarily long time to obtain good and varied samples, we propose to train a deep directed generative model (not a Markov chain) so that its sampling distribution approximately matches the energy function that is being trained. Inspired by generative adversarial networks, the proposed framework involves training of two models that represent dual views of the estimated probability distribution: the energy function (mapping an input configuration to a scalar energy value) and the generator (mapping a noise vector to a generated configuration), both represented by deep neural networks. "
742154981755752448,2016-06-13 00:41:50,https://t.co/1htJ7QQLDj,"Fast, Robust and Non-convex Subspace Recovery. (arXiv:1406.6145v2 [cs.LG] UPDATED) https://t.co/1htJ7QQLDj",0,4," Abstract: This work presents a fast and non-convex algorithm for robust subspace recovery. The data sets considered include inliers drawn around a low-dimensional subspace of a higher dimensional ambient space, and a possibly large portion of outliers that do not lie nearby this subspace. The proposed algorithm, which we refer to as Fast Median Subspace (FMS), is designed to robustly determine the underlying subspace of such data sets, while having lower computational complexity than existing methods. We prove convergence of the FMS iterates to a stationary point. Further, under a special model of data, FMS converges to a point which is near to the global minimum with overwhelming probability. Under this model, we show that the iteration complexity is globally bounded and locally $r$-linear. The latter theorem holds for any fixed fraction of outliers (less than 1) and any fixed positive distance between the limit point and the global minimum. Numerical experiments on synthetic and real data demonstrate its competitive speed and accuracy. "
742154980203847684,2016-06-13 00:41:50,https://t.co/FAQTRRs0Kz,Interpretable Classification Models for Recidivism Prediction. (arXiv:1503.07810v5 [stat.ML] UPDATED) https://t.co/FAQTRRs0Kz,0,2," Abstract: We investigate a long-debated question, which is how to create predictive models of recidivism that are sufficiently accurate, transparent, and interpretable to use for decision-making. This question is complicated as these models are used to support different decisions, from sentencing, to determining release on probation, to allocating preventative social services. Each use case might have an objective other than classification accuracy, such as a desired true positive rate (TPR) or false positive rate (FPR). Each (TPR, FPR) pair is a point on the receiver operator characteristic (ROC) curve. We use popular machine learning methods to create models along the full ROC curve on a wide range of recidivism prediction problems. We show that many methods (SVM, Ridge Regression) produce equally accurate models along the full ROC curve. However, methods that designed for interpretability (CART, C5.0) cannot be tuned to produce models that are accurate and/or interpretable. To handle this shortcoming, we use a new method known as SLIM (Supersparse Linear Integer Models) to produce accurate, transparent, and interpretable models along the full ROC curve. These models can be used for decision-making for many different use cases, since they are just as accurate as the most powerful black-box machine learning models, but completely transparent, and highly interpretable. "
742154979033686016,2016-06-13 00:41:50,https://t.co/s6lE4MHj13,Reducing Overfitting in Deep Networks by Decorrelating Representations. (arXiv:1511.06068v4 [cs.LG] UPDATED) https://t.co/s6lE4MHj13,5,15," Abstract: One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization. Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout. "
742154977720868864,2016-06-13 00:41:49,https://t.co/mMNP1SBSuP,Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing. (arXiv:1512.00442v2 [cs.DS] UPDATED) https://t.co/mMNP1SBSuP,1,3," Abstract: Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning, which is the underlying strategy used by most existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality of the space and sub-linear in the intrinsic dimensionality and the size of the dataset and takes space constant in dimensionality of the space and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis, automatically adapts to variations in data density, supports dynamic updates to the dataset and is easy-to-implement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing (LSH) in terms of approximation quality, speed and space efficiency. "
742154976147951617,2016-06-13 00:41:49,https://t.co/9rW21AerQT,Square Root Graphical Models: Multivariate Generalizations of Univariate Exponential Families that Permit Positive… https://t.co/9rW21AerQT,1,3," Abstract: We develop Square Root Graphical Models (SQR), a novel class of parametric graphical models that provides multivariate generalizations of univariate exponential family distributions. Previous multivariate graphical models [Yang et al. 2015] did not allow positive dependencies for the exponential and Poisson generalizations. However, in many real-world datasets, variables clearly have positive dependencies. For example, the airport delay time in New York---modeled as an exponential distribution---is positively related to the delay time in Boston. With this motivation, we give an example of our model class derived from the univariate exponential distribution that allows for almost arbitrary positive and negative dependencies with only a mild condition on the parameter matrix---a condition akin to the positive definiteness of the Gaussian covariance matrix. Our Poisson generalization allows for both positive and negative dependencies without any constraints on the parameter values. We also develop parameter estimation methods using node-wise regressions with $\ell_1$ regularization and likelihood approximation methods using sampling. Finally, we demonstrate our exponential generalization on a synthetic dataset and a real-world dataset of airport delay times. "
742154974625464322,2016-06-13 00:41:49,https://t.co/v1FHoMnYdP,Predict or classify: The deceptive role of time-locking in brain signal classification. (arXiv:1605.08228v2 [q-bio… https://t.co/v1FHoMnYdP,2,3," Abstract: Several experimental studies claim to be able to predict the outcome of simple decisions from brain signals measured before subjects are aware of their decision. Often, these studies use multivariate pattern recognition methods with the underlying assumption that the ability to classify the brain signal is equivalent to predict the decision itself. Here we show instead that it is possible to correctly classify a signal even if it does not contain any predictive information about the decision. We first define a simple stochastic model that mimics the random decision process between two equivalent alternatives, and generate a large number of independent trials that contain no choice-predictive information. The trials are first time-locked to the time point of the final event and then classified using standard machine-learning techniques. The resulting classification accuracy is above chance level long before the time point of time-locking. We then analyze the same trials using information theory. We demonstrate that the high classification accuracy is a consequence of time-locking and that its time behavior is simply related to the large relaxation time of the process. We conclude that when time-locking is a crucial step in the analysis of neural activity patterns, both the emergence and the timing of the classification accuracy are affected by structural properties of the network that generates the signal. "
741068951577333760,2016-06-10 00:46:21,https://t.co/hn66qhxbSY,Variational Information Maximization for Feature Selection. (arXiv:1606.02827v1 [stat.ML]) https://t.co/hn66qhxbSY,1,12," Abstract: Feature selection is one of the most fundamental problems in machine learning. An extensive body of work on information-theoretic feature selection exists which is based on maximizing mutual information between subsets of features and class labels. Practical methods are forced to rely on approximations due to the difficulty of estimating mutual information. We demonstrate that approximations made by existing methods are based on unrealistic assumptions. We formulate a more flexible and general class of assumptions based on variational distributions and use them to tractably generate lower bounds for mutual information. These bounds define a novel information-theoretic framework for feature selection, which we prove to be optimal under tree graphical models with proper choice of variational distributions. Our experiments demonstrate that the proposed method strongly outperforms existing information-theoretic feature selection approaches. "
741068947710181376,2016-06-10 00:46:20,https://t.co/JRNbbkhgL4,Sketching for Large-Scale Learning of Mixture Models. (arXiv:1606.02838v1 [cs.LG]) https://t.co/JRNbbkhgL4,0,3," Abstract: Learning parameters from voluminous data can be prohibitive in terms of memory and computational requirements. We propose a ""compressive learning"" framework where we estimate model parameters from a sketch of the training data. This sketch is a collection of generalized moments of the underlying probability distribution of the data. It can be computed in a single pass on the training set, and is easily computable on streams or distributed datasets. The proposed framework shares similarities with compressive sensing, which aims at drastically reducing the dimension of high-dimensional signals while preserving the ability to reconstruct them. To perform the estimation task, we derive an iterative algorithm analogous to sparse reconstruction algorithms in the context of linear inverse problems. We exemplify our framework with the compressive estimation of a Gaussian Mixture Model (GMM), providing heuristics on the choice of the sketching procedure and theoretical guarantees of reconstruction. We experimentally show on synthetic data that the proposed algorithm yields results comparable to the classical Expectation-Maximization (EM) technique while requiring significantly less memory and fewer computations when the number of database elements is large. We further demonstrate the potential of the approach on real large-scale data (over 10 8 training samples) for the task of model-based speaker verification. Finally, we draw some connections between the proposed framework and approximate Hilbert space embedding of probability distributions using random features. We show that the proposed sketching operator can be seen as an innovative method to design translation-invariant kernels adapted to the analysis of GMMs. We also use this theoretical framework to derive information preservation guarantees, in the spirit of infinite-dimensional compressive sensing. "
741068942324551680,2016-06-10 00:46:18,https://t.co/D2SAsbFQPF,Sequence-to-Sequence Learning as Beam-Search Optimization. (arXiv:1606.02960v1 [cs.CL]) https://t.co/D2SAsbFQPF,1,9," Abstract: Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation. "
741068937891303424,2016-06-10 00:46:17,https://t.co/W8Fj3GbEAW,Generative Topic Embedding: a Continuous Representation of Documents (Extended Version with Proofs). (arXiv:1606.0… https://t.co/W8Fj3GbEAW,0,6," Abstract: Word embedding maps words into a low-dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is influenced by both its local context and its topic. A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classification tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document. "
741068932967215104,2016-06-10 00:46:16,https://t.co/PY8KOZLFzl,Neural computation from first principles: Using the maximum entropy method to obtain an optimal bits-per-joule neu… https://t.co/PY8KOZLFzl,1,6," Abstract: Optimization results are one method for understanding neural computation from Nature's perspective and for defining the physical limits on neuron-like engineering. Earlier work looks at individual properties or performance criteria and occasionally a combination of two, such as energy and information. Here we make use of Jaynes' maximum entropy method and combine a larger set of constraints, possibly dimensionally distinct, each expressible as an expectation. The method identifies a likelihood-function and a sufficient statistic arising from each such optimization. This likelihood is a first-hitting time distribution in the exponential class. Particular constraint sets are identified that, from an optimal inference perspective, justify earlier neurocomputational models. Interactions between constraints, mediated through the inferred likelihood, restrict constraint-set parameterizations, e.g., the energy-budget limits estimation performance which, in turn, matches an axonal communication constraint. Such linkages are, for biologists, experimental predictions of the method. In addition to the related likelihood, at least one type of constraint set implies marginal distributions, and in this case, a Shannon bits/joule statement arises. "
741068929292996608,2016-06-10 00:46:15,https://t.co/1jlyKp5yQT,Adaptive Normalized Risk-Averting Training For Deep Neural Networks. (arXiv:1506.02690v3 [cs.LG] UPDATED) https://t.co/1jlyKp5yQT,0,4," Abstract: This paper proposes a set of new error criteria and learning approaches, Adaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convex optimization problem in training deep neural networks (DNNs). Theoretically, we demonstrate its effectiveness on global and local convexity lower-bounded by the standard $L_p$-norm error. By analyzing the gradient on the convexity index $\lambda$, we explain the reason why to learn $\lambda$ adaptively using gradient descent works. In practice, we show how this method improves training of deep neural networks to solve visual recognition tasks on the MNIST and CIFAR-10 datasets. Without using pretraining or other tricks, we obtain results comparable or superior to those reported in recent literature on the same tasks using standard ConvNets + MSE/cross entropy. Performance on deep/shallow multilayer perceptrons and Denoised Auto-encoders is also explored. ANRAT can be combined with other quasi-Newton training methods, innovative network variants, regularization techniques and other specific tricks in DNNs. Other than unsupervised pretraining, it provides a new perspective to address the non-convex optimization problem in DNNs. "
741068924771520512,2016-06-10 00:46:14,https://t.co/3VafoH2Thu,The Information Sieve. (arXiv:1507.02284v3 [stat.ML] UPDATED) https://t.co/3VafoH2Thu,0,4," Abstract: We introduce a new framework for unsupervised learning of representations based on a novel hierarchical decomposition of information. Intuitively, data is passed through a series of progressively fine-grained sieves. Each layer of the sieve recovers a single latent factor that is maximally informative about multivariate dependence in the data. The data is transformed after each pass so that the remaining unexplained information trickles down to the next layer. Ultimately, we are left with a set of latent factors explaining all the dependence in the original data and remainder information consisting of independent noise. We present a practical implementation of this framework for discrete variables and apply it to a variety of fundamental tasks in unsupervised learning including independent component analysis, lossy and lossless compression, and predicting missing values in data. "
741068920216539136,2016-06-10 00:46:13,https://t.co/bJLAUK1uJT,Context-Aware Bandits. (arXiv:1510.03164v3 [cs.LG] UPDATED) https://t.co/bJLAUK1uJT,0,5," Abstract: We propose an efficient Context-Aware clustering of Bandits (CAB) algorithm, which can capture collaborative effects. CAB can be easily deployed in a real-world recommendation system, where multi-armed bandits have been shown to perform well in particular with respect to the cold-start problem. CAB utilizes a context-aware clustering augmented by exploration-exploitation strategies. CAB dynamically clusters the users based on the content universe under consideration. We give a theoretical analysis in the standard stochastic multi-armed bandits setting. We show the efficiency of our approach on production and real-world datasets, demonstrate the scalability, and, more importantly, the significant increased prediction performance against several state-of-the-art methods. "
741068916387151872,2016-06-10 00:46:12,https://t.co/ezDXEEcv6f,Hyperparameter optimization with approximate gradient. (arXiv:1602.02355v4 [stat.ML] UPDATED) https://t.co/ezDXEEcv6f,1,9," Abstract: Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of L2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods. "
741068912251555840,2016-06-10 00:46:11,https://t.co/URHhK9guUt,Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. (arXiv:1603.05953v3 [math.OC] UPDATED) https://t.co/URHhK9guUt,1,10," Abstract: We introduce $\mathtt{Katyusha}$, the first direct, primal-only stochastic gradient method that has a provably accelerated convergence rate in convex optimization. In contrast, previous methods are based on dual coordinate descent which are more restrictive, or based on outer-inner loops which make them ""blind"" to the underlying stochastic nature of the optimization process. $\mathtt{Katyusha}$ is the first algorithm that incorporates acceleration directly into stochastic gradient updates. Unlike previous results, $\mathtt{Katyusha}$ obtains an optimal convergence rate. It also supports proximal updates, non-Euclidean norm smoothness, non-uniform sampling, and mini-batch sampling. When applied to interesting classes of convex objectives, including smooth objectives (e.g., Lasso, Logistic Regression), strongly-convex objectives (e.g., SVM), and non-smooth objectives (e.g., L1SVM), $\mathtt{Katyusha}$ improves the best known convergence rates. The main ingredient behind our result is $\textit{Katyusha momentum}$, a novel ""negative momentum on top of momentum"" that can be incorporated into a variance-reduction based algorithm and speed it up. As a result, since variance reduction has been successfully applied to a fast growing list of practical problems, our paper suggests that in each of such cases, one had better hurry up and give Katyusha a hug. "
741068908174708736,2016-06-10 00:46:10,https://t.co/5RmzfF0U5v,On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis. (arXiv:1603.07294v2 [cs.LG] UPDATED) https://t.co/5RmzfF0U5v,1,7," Abstract: Bayesian inference has great promise for the privacy-preserving analysis of sensitive data, as posterior sampling automatically preserves differential privacy, an algorithmic notion of data privacy, under certain conditions (Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample (OPS) approach elegantly provides privacy ""for free,"" it is data inefficient in the sense of asymptotic relative efficiency (ARE). We show that a simple alternative based on the Laplace mechanism, the workhorse of differential privacy, is as asymptotically efficient as non-private posterior inference, under general assumptions. This technique also has practical advantages including efficient use of the privacy budget for MCMC. We demonstrate the practicality of our approach on a time-series analysis of sensitive military records from the Afghanistan and Iraq wars disclosed by the Wikileaks organization. "
741068901086339073,2016-06-10 00:46:09,https://t.co/qlTGFKj1hc,Stopping criteria for boosting automatic experimental design using real-time fMRI with Bayesian optimization. (arX… https://t.co/qlTGFKj1hc,2,8," Abstract: Bayesian optimization has been proposed as a practical and efficient tool through which to tune parameters in many difficult settings. Recently, such techniques have been combined with real-time fMRI to propose a novel framework which turns on its head the conventional functional neuroimaging approach. This closed-loop method automatically designs the optimal experiment to evoke a desired target brain pattern. One of the challenges associated with extending such methods to real-time brain imaging is the need for adequate stopping criteria, an aspect of Bayesian optimization which has received limited attention. In light of high scanning costs and limited attentional capacities of subjects an accurate and reliable stopping criteria is essential. In order to address this issue we propose and empirically study the performance of two stopping criteria. "
740707199899586560,2016-06-09 00:48:52,https://t.co/HQEgamSw4k,Sifting Common Information from Many Variables. (arXiv:1606.02307v1 [stat.ML]) https://t.co/HQEgamSw4k,0,5," Abstract: Measuring the relationship between any two variables is a rich and active area of research at the core of the scientific enterprise. In contrast, characterizing the common information among a group of observed variables has remained a speculative undertaking producing no practical methods for high-dimensional data. A promising solution would be a multivariate generalization of the famous Wyner common information, but this approach relies on solving an apparently intractable optimization problem. We formulate an incremental version of this problem called the information sieve that not only admits a simple fixed-point solution, but also empirically exhibits an exponential rate of convergence. We use this scalable method to demonstrate that common information is a useful concept for machine learning. The sieve outperforms standard methods on dimensionality reduction tasks, solves a blind source separation problem involving Gaussian sources that cannot be solved with ICA, and accurately recovers structure in brain imaging data. "
740707198226071552,2016-06-09 00:48:52,https://t.co/AlJl5ghORx,Better Conditional Density Estimation for Neural Networks. (arXiv:1606.02321v1 [stat.ML]) https://t.co/AlJl5ghORx,0,6," Abstract: The vast majority of the neural network literature focuses on predicting point values for a given set of response variables, conditioned on a feature vector. In many cases we need to model the full joint conditional distribution over the response variables rather than simply making point predictions. In this paper, we present two novel approaches to such conditional density estimation (CDE): Multiscale Nets (MSNs) and CDE Trend Filtering. Multiscale nets transform the CDE regression task into a hierarchical classification task by decomposing the density into a series of half-spaces and learning boolean probabilities of each split. CDE Trend Filtering applies a k-th order graph trend filtering penalty to the unnormalized logits of a multinomial classifier network, with each edge in the graph corresponding to a neighboring point on a discretized version of the density. We compare both methods against plain multinomial classifier networks and mixture density networks (MDNs) on a simulated dataset and three real-world datasets. The results suggest the two methods are complementary: MSNs work well in a high-data-per-feature regime and CDE-TF is well suited for few-samples-per-feature scenarios where overfitting is a primary concern. "
740707192223961088,2016-06-09 00:48:50,https://t.co/ZjdwlpQNQw,Resting state brain networks from EEG: Hidden Markov states vs. classical microstates. (arXiv:1606.02344v1 [q-bio.… https://t.co/ZjdwlpQNQw,0,5," Abstract: Functional brain networks exhibit dynamics on the sub-second temporal scale and are often assumed to embody the physiological substrate of cognitive processes. Here we analyse the temporal and spatial dynamics of these states, as measured by EEG, with a hidden Markov model and compare this approach to classical EEG microstate analysis. We find dominating state lifetimes of 100--150\,ms for both approaches. The state topographies show obvious similarities. However, they also feature distinct spatial and especially temporal properties. These differences may carry physiological meaningful information originating from patterns in the data that the HMM is able to integrate while the microstate analysis is not. This hypothesis is supported by a consistently high pairwise correlation of the temporal evolution of EEG microstates which is not observed for the HMM states and which seems unlikely to be a good description of the underlying physiology. However, further investigation is required to determine the robustness and the functional and clinical relevance of EEG HMM states in comparison to EEG microstates. "
740707190680485888,2016-06-09 00:48:50,https://t.co/SYvEmbPCjB,Locally-Optimized Inter-Subject Alignment of Functional Cortical Regions. (arXiv:1606.02349v1 [q-bio.NC]) https://t.co/SYvEmbPCjB,0,2," Abstract: Inter-subject registration of cortical areas is necessary in functional imaging (fMRI) studies for making inferences about equivalent brain function across a population. However, many high-level visual brain areas are defined as peaks of functional contrasts whose cortical position is highly variable. As such, most alignment methods fail to accurately map functional regions of interest (ROIs) across participants. To address this problem, we propose a locally optimized registration method that directly predicts the location of a seed ROI on a separate target cortical sheet by maximizing the functional correlation between their time courses, while simultaneously allowing for non-smooth local deformations in region topology. Our method outperforms the two most commonly used alternatives (anatomical landmark-based AFNI alignment and cortical convexity-based FreeSurfer alignment) in overlap between predicted region and functionally-defined LOC. Furthermore, the maps obtained using our method are more consistent across subjects than both baseline measures. Critically, our method represents an important step forward towards predicting brain regions without explicit localizer scans and deciphering the poorly understood relationship between the location of functional regions, their anatomical extent, and the consistency of computations those regions perform across people. "
740707189183111168,2016-06-09 00:48:50,https://t.co/3xYY7fgs5i,Active Long Term Memory Networks. (arXiv:1606.02355v1 [cs.LG]) https://t.co/3xYY7fgs5i,0,5," Abstract: Continual Learning in artificial neural networks suffers from interference and forgetting when different tasks are learned sequentially. This paper introduces the Active Long Term Memory Networks (A-LTM), a model of sequential multi-task deep learning that is able to maintain previously learned association between sensory input and behavioral output while acquiring knew knowledge. A-LTM exploits the non-convex nature of deep neural networks and actively maintains knowledge of previously learned, inactive tasks using a distillation loss. Distortions of the learned input-output map are penalized but hidden layers are free to transverse towards new local optima that are more favorable for the multi-task objective. We re-frame the McClelland's seminal Hippocampal theory with respect to Catastrophic Inference (CI) behavior exhibited by modern deep architectures trained with back-propagation and inhomogeneous sampling of latent factors across epochs. We present empirical results of non-trivial CI during continual learning in Deep Linear Networks trained on the same task, in Convolutional Neural Networks when the task shifts from predicting semantic to graphical factors and during domain adaptation from simple to complex environments. We present results of the A-LTM model's ability to maintain viewpoint recognition learned in the highly controlled iLab-20M dataset with 10 object categories and 88 camera viewpoints, while adapting to the unstructured domain of Imagenet with 1,000 object categories. "
740707187639607296,2016-06-09 00:48:49,https://t.co/YXxZ4KbkhR,Structure Learning in Graphical Modeling. (arXiv:1606.02359v1 [stat.ME]) https://t.co/YXxZ4KbkhR,0,7," Abstract: A graphical model is a statistical model that is associated to a graph whose nodes correspond to variables of interest. The edges of the graph reflect allowed conditional dependencies among the variables. Graphical models admit computationally convenient factorization properties and have long been a valuable tool for tractable modeling of multivariate distributions. More recently, applications such as reconstructing gene regulatory networks from gene expression data have driven major advances in structure learning, that is, estimating the graph underlying a model. We review some of these advances and discuss methods such as the graphical lasso and neighborhood selection for undirected graphical models (or Markov random fields), and the PC algorithm and score-based search methods for directed graphical models (or Bayesian networks). We further review extensions that account for effects of latent variables and heterogeneous data sources. "
740707185873850368,2016-06-09 00:48:49,https://t.co/l3o9798ggG,Deep Successor Reinforcement Learning. (arXiv:1606.02396v1 [stat.ML]) https://t.co/l3o9798ggG,0,6," Abstract: Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine. "
740707179666231296,2016-06-09 00:48:47,https://t.co/0H1gQODa3V,On estimating a mixture on graphons. (arXiv:1606.02401v1 [stat.ML]) https://t.co/0H1gQODa3V,0,3," Abstract: Community detection, which focuses on clustering nodes or detecting communities in (mostly) a single network, is a problem of considerable practical interest and has received a great deal of attention in the research community. While being able to cluster within a network is important, there are emerging needs to be able to cluster multiple networks. This is largely motivated by the routine collection of network data that are generated from potentially different populations, such as brain networks of subjects from different disease groups, genders, or biological networks generated under different experimental conditions, etc. We propose a simple and general framework for clustering multiple networks based on a mixture model on graphons. Our clustering method employs graphon estimation as a first step and performs spectral clustering on the matrix of distances between estimated graphons. This is illustrated through both simulated and real data sets, and theoretical justification of the algorithm is given in terms of consistency. "
740707178059829248,2016-06-09 00:48:47,https://t.co/6rvmyvPdvR,Clustering with Same-Cluster Queries. (arXiv:1606.02404v1 [cs.LG]) https://t.co/6rvmyvPdvR,0,3," Abstract: We propose a framework for Semi-Supervised Active Clustering framework (SSAC), where the learner is allowed to interact with a domain expert, asking whether two given instances belong to the same cluster or not. We study the query and computational complexity of clustering in this framework. We consider a setting where the expert conforms to a center-based clustering with a notion of margin. We show that there is a trade off between computational complexity and query complexity; We prove that for the case of $k$-means clustering (i.e., when the expert conforms to a solution of $k$-means), having access to relatively few such queries allows efficient solutions to otherwise NP hard problems. In particular, we provide a probabilistic polynomial-time (BPP) algorithm for clustering in this setting that asks $O\big(k^2\log k + k\log n)$ same-cluster queries and runs with time complexity $O\big(kn\log n)$ (where $k$ is the number of clusters and $n$ is the number of instances). The algorithm succeeds with high probability for data satisfying margin conditions under which, without queries, we show that the problem is NP hard. We also prove a lower bound on the number of queries needed to have a computationally efficient clustering algorithm in this setting. "
740707176642121728,2016-06-09 00:48:47,https://t.co/STbbsUHUkR,Gossip Dual Averaging for Decentralized Optimization of Pairwise Functions. (arXiv:1606.02421v1 [stat.ML]) https://t.co/STbbsUHUkR,0,3," Abstract: In decentralized networks (of sensors, connected objects, etc.), there is an important need for efficient algorithms to optimize a global cost function, for instance to learn a global model from the local data collected by each computing unit. In this paper, we address the problem of decentralized minimization of pairwise functions of the data points, where these points are distributed over the nodes of a graph defining the communication topology of the network. This general problem finds applications in ranking, distance metric learning and graph inference, among others. We propose new gossip algorithms based on dual averaging which aims at solving such problems both in synchronous and asynchronous settings. The proposed framework is flexible enough to deal with constrained and regularized variants of the optimization problem. Our theoretical analysis reveals that the proposed algorithms preserve the convergence rate of centralized dual averaging up to an additive bias term. We present numerical simulations on Area Under the ROC Curve (AUC) maximization and metric learning problems which illustrate the practical interest of our approach. "
740707170552053760,2016-06-09 00:48:45,https://t.co/fnSX5ZqeMY,A Locally Adaptive Normal Distribution. (arXiv:1606.02518v1 [stat.ML]) https://t.co/fnSX5ZqeMY,1,2," Abstract: The multivariate normal density is a monotonic function of the distance to the mean, and its ellipsoidal shape is due to the underlying Euclidean metric. We suggest to replace this metric with a locally adaptive, smoothly changing (Riemannian) metric that favors regions of high local density. The resulting locally adaptive normal distribution (LAND) is a generalization of the normal distribution to the ""manifold"" setting, where data is assumed to lie near a potentially low-dimensional manifold embedded in $\mathbb{R}^D$. The LAND is parametric, depending only on a mean and a covariance, and is the maximum entropy distribution under the given metric. The underlying metric is, however, non-parametric. We develop a maximum likelihood algorithm to infer the distribution parameters that relies on a combination of gradient descent and Monte Carlo integration. We further extend the LAND to mixture models, and provide the corresponding EM algorithm. We demonstrate the efficiency of the LAND to fit non-trivial probability distributions over both synthetic data, and EEG measurements of human sleep. "
740707168756858881,2016-06-09 00:48:45,https://t.co/uFsa4flLGi]),A moment-matching Ferguson and Klass algorithm. (arXiv:1606.02566v1 [https://t.co/uFsa4flLGi]) https://t.co/9oeWbpNwC6,0,2,INDEXERROR
740707166802284544,2016-06-09 00:48:44,https://t.co/o1APDx9YKy,Safe and Efficient Off-Policy Reinforcement Learning. (arXiv:1606.02647v1 [cs.LG]) https://t.co/o1APDx9YKy,0,3," Abstract: In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace($\lambda$), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of ""off-policyness""; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to $Q^*$ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q($\lambda$), which was an open problem since 1989. We illustrate the benefits of Retrace($\lambda$) on a standard suite of Atari 2600 games. "
740707165267202048,2016-06-09 00:48:44,https://t.co/P88hbKpCBd,Learning Power Spectrum Maps from Quantized Power Measurements. (arXiv:1606.02679v1 [cs.IT]) https://t.co/P88hbKpCBd,0,3," Abstract: Using power measurements collected by a network of low-cost sensors, power spectral density (PSD) maps are constructed to capture the distribution of RF power across space and frequency. Linearly compressed and quantized power measurements enable wideband sensing at affordable implementation complexity using a small number of bits. Strengths of data- and model-driven approaches are combined to develop estimators capable of incorporating multiple forms of spectral and propagation prior information while fitting the rapid variations of shadow fading across space. To this end, novel nonparametric and semiparametric formulations are investigated. It is shown that the desired PSD maps can be obtained using support vector machine-type solvers. In addition to batch approaches, an online algorithm attuned to real-time operation is developed. Numerical tests assess the performance of the novel algorithms. "
740707163782447104,2016-06-09 00:48:44,https://t.co/kfvr4uawD4,Efficient Smoothed Concomitant Lasso Estimation for High Dimensional Regression. (arXiv:1606.02702v1 [stat.ML]) https://t.co/kfvr4uawD4,0,2," Abstract: In high dimensional settings, sparse structures are crucial for efficiency, both in term of memory, computation and performance. It is customary to consider $\ell_1$ penalty to enforce sparsity in such scenarios. Sparsity enforcing methods, the Lasso being a canonical example, are popular candidates to address high dimension. For efficiency, they rely on tuning a parameter trading data fitting versus sparsity. For the Lasso theory to hold this tuning parameter should be proportional to the noise level, yet the latter is often unknown in practice. A possible remedy is to jointly optimize over the regression parameter as well as over the noise level. This has been considered under several names in the literature: Scaled-Lasso, Square-root Lasso, Concomitant Lasso estimation for instance, and could be of interest for confidence sets or uncertainty quantification. In this work, after illustrating numerical difficulties for the Smoothed Concomitant Lasso formulation, we propose a modification we coined Smoothed Concomitant Lasso, aimed at increasing numerical stability. We propose an efficient and accurate solver leading to a computational cost no more expansive than the one for the Lasso. We leverage on standard ingredients behind the success of fast Lasso solvers: a coordinate descent algorithm, combined with safe screening rules to achieve speed efficiency, by eliminating early irrelevant features. "
740707162406653952,2016-06-09 00:48:43,https://t.co/Q3u5Qx9OLb,Phase Transitions for High Dimensional Clustering and Related Problems. (arXiv:1502.06952v4 [math.ST] UPDATED) https://t.co/Q3u5Qx9OLb,0,3," Abstract: Consider a two-class clustering problem where we observe $X_i = \ell_i \mu + Z_i$, $Z_i \stackrel{iid}{\sim} N(0, I_p)$, $1 \leq i \leq n$. The feature vector $\mu\in R^p$ is unknown but is presumably sparse. The class labels $\ell_i\in\{-1, 1\}$ are also unknown and the main interest is to estimate them. We are interested in the statistical limits. In the two-dimensional phase space calibrating the rarity and strengths of useful features, we find the precise demarcation for the Region of Impossibility and Region of Possibility. In the former, useful features are too rare/weak for successful clustering. In the latter, useful features are strong enough to allow successful clustering. The results are extended to the case of colored noise using Le Cam's idea on comparison of experiments. We also extend the study on statistical limits for clustering to that for signal recovery and that for hypothesis testing. We compare the statistical limits for three problems and expose some interesting insight. We propose classical PCA and Important Features PCA (IF-PCA) for clustering. For a threshold $t > 0$, IF-PCA clusters by applying classical PCA to all columns of $X$ with an $L^2$-norm larger than $t$. We also propose two aggregation methods. For any parameter in the Region of Possibility, some of these methods yield successful clustering. We find an interesting phase transition for IF-PCA. Our results require delicate analysis, especially on post-selection Random Matrix Theory and on lower bound arguments. "
740707160770875392,2016-06-09 00:48:43,https://t.co/i31xugGNu8,Hierarchical learning of grids of microtopics. (arXiv:1503.03701v4 [stat.ML] UPDATED) https://t.co/i31xugGNu8,0,2," Abstract: The counting grid is a grid of microtopics, sparse word/feature distributions. The generative model associated with the grid does not use these microtopics individually. Rather, it groups them in overlapping rectangular windows and uses these grouped microtopics as either mixture or admixture components. This paper builds upon the basic counting grid model and it shows that hierarchical reasoning helps avoid bad local minima, produces better classification accuracy and, most interestingly, allows for extraction of large numbers of coherent microtopics even from small datasets. We evaluate this in terms of consistency, diversity and clarity of the indexed content, as well as in a user study on word intrusion tasks. We demonstrate that these models work well as a technique for embedding raw images and discuss interesting parallels between hierarchical CG models and other deep architectures. "
740707159260966914,2016-06-09 00:48:43,https://t.co/9Js6xBGXbD,Revealed Preference at Scale: Learning Personalized Preferences from Assortment Choices. (arXiv:1509.05113v2 [stat… https://t.co/9Js6xBGXbD,0,2," Abstract: We consider the problem of learning the preferences of a heterogeneous population by observing choices from an assortment of products, ads, or other offerings. Our observation model takes a form common in assortment planning applications: each arriving customer is offered an assortment consisting of a subset of all possible offerings; we observe only the assortment and the customer's single choice. In this paper we propose a mixture choice model with a natural underlying low-dimensional structure, and show how to estimate its parameters. In our model, the preferences of each customer or segment follow a separate parametric choice model, but the underlying structure of these parameters over all the models has low dimension. We show that a nuclear-norm regularized maximum likelihood estimator can learn the preferences of all customers using a number of observations much smaller than the number of item-customer combinations. This result shows the potential for structural assumptions to speed up learning and improve revenues in assortment planning and customization. We provide a specialized factored gradient descent algorithm and study the success of the approach empirically. "
740707157084082176,2016-06-09 00:48:42,https://t.co/QkypknvAVs,Feature-Level Domain Adaptation. (arXiv:1512.04829v2 [stat.ML] UPDATED) https://t.co/QkypknvAVs,0,4," Abstract: Domain adaptation is the supervised learning setting in which the training and test data are sampled from different distributions: training data is sampled from a source domain, whilst test data is sampled from a target domain. This paper proposes and studies an approach, called feature-level domain adaptation (FLDA), that models the dependence between the two domains by means of a feature-level transfer model that is trained to describe the transfer from source to target domain. Subsequently, we train a domain-adapted classifier by minimizing the expected loss under the resulting transfer model. For linear classifiers and a large family of loss functions and transfer models, this expected loss can be computed or approximated analytically, and minimized efficiently. Our empirical evaluation of FLDA focuses on problems comprising binary and count data in which the transfer can be naturally modeled via a dropout distribution, which allows the classifier to adapt to differences in the marginal probability of features in the source and the target domain. Our experiments on several real-world problems show that FLDA performs on par with state-of-the-art domain-adaptation techniques. "
740707155192520704,2016-06-09 00:48:42,https://t.co/Sn7GhthFWV,A Latent-Variable Lattice Model. (arXiv:1512.07587v7 [cs.LG] UPDATED) https://t.co/Sn7GhthFWV,0,2," Abstract: Markov random field (MRF) learning is intractable, and its approximation algorithms are computationally expensive. We target a small subset of MRF that is used frequently in computer vision. We characterize this subset with three concepts: Lattice, Homogeneity, and Inertia; and design a non-markov model as an alternative. Our goal is robust learning from small datasets. Our learning algorithm uses vector quantization and, at time complexity O(U log U) for a dataset of U pixels, is much faster than that of general-purpose MRF. "
740707149135941636,2016-06-09 00:48:40,https://t.co/xbcM1UvKA7,Nonparametric Modeling of Dynamic Functional Connectivity in fMRI Data. (arXiv:1601.00496v2 [stat.AP] UPDATED) https://t.co/xbcM1UvKA7,0,3," Abstract: Dynamic functional connectivity (FC) has in recent years become a topic of interest in the neuroimaging community. Several models and methods exist for both functional magnetic resonance imaging (fMRI) and electroencephalography (EEG), and the results point towards the conclusion that FC exhibits dynamic changes. The existing approaches modeling dynamic connectivity have primarily been based on time-windowing the data and k-means clustering. We propose a non-parametric generative model for dynamic FC in fMRI that does not rely on specifying window lengths and number of dynamic states. Rooted in Bayesian statistical modeling we use the predictive likelihood to investigate if the model can discriminate between a motor task and rest both within and across subjects. We further investigate what drives dynamic states using the model on the entire data collated across subjects and task/rest. We find that the number of states extracted are driven by subject variability and preprocessing differences while the individual states are almost purely defined by either task or rest. This questions how we in general interpret dynamic FC and points to the need for more research on what drives dynamic FC. "
740707146854195200,2016-06-09 00:48:40,https://t.co/Eq53Jwskl6,An analytic comparison of regularization methods for Gaussian Processes. (arXiv:1602.00853v2 [math.OC] UPDATED) https://t.co/Eq53Jwskl6,1,4," Abstract: Gaussian Processes (GPs) are often used to predict the output of a parameterized deterministic experiment. They have many applications in the field of Computer Experiments, in particular to perform sensitivity analysis, adaptive design of experiments and global optimization. Nearly all of the applications of GPs to Computer Experiments require the inversion of a covariance matrix. Because this matrix is often ill-conditioned, regularization techniques are required. Today, there is still a need to better regularize GPs.The two most classical regularization methods to avoid degeneracy of the covariance matrix are i) pseudoinverse (PI) and ii) adding a small positive constant to the main diagonal, i.e., the case of noisy observations. Herein, we will refer to the second regularization technique with a slight abuse of language as nugget. This paper provides algebraic calculations which allow comparing PI and nugget regularizations. It is proven that pseudoinverse regularization averages the output values and makes the variance null at redundant points. On the opposite, nugget regularization lacks interpolationproperties but preserves a non-zero variance at every point. However, these two regularization techniques become similar as the nugget value decreases. A distribution-wise GP is introduced which interpolates Gaussian distributions instead of data points and mitigates the drawbacks of pseudoinverse and nugget regularized GPs. Finally, data-model discrepancy is discussed and serves as a guide for choosing a regularization technique. "
740707144819937280,2016-06-09 00:48:39,https://t.co/TwlmAokjGF,Online optimization and regret guarantees for non-additive long-term constraints. (arXiv:1602.05394v2 [stat.ML] UP… https://t.co/TwlmAokjGF,0,2," Abstract: We consider online optimization in the 1-lookahead setting, where the objective does not decompose additively over the rounds of the online game. The resulting formulation enables us to deal with non-stationary and/or long-term constraints , which arise, for example, in online display advertising problems. We propose an on-line primal-dual algorithm for which we obtain dynamic cumulative regret guarantees. They depend on the convexity and the smoothness of the non-additive penalty, as well as terms capturing the smoothness with which the residuals of the non-stationary and long-term constraints vary over the rounds. We conduct experiments on synthetic data to illustrate the benefits of the non-additive penalty and show vanishing regret convergence on live traffic data collected by a display advertising platform in production. "
740707129892429824,2016-06-09 00:48:36,https://t.co/tHrNxzNSVo,Clustering with a Reject Option: Interactive Clustering as Bayesian Prior Elicitation. (arXiv:1602.06886v2 [stat.M… https://t.co/tHrNxzNSVo,0,3," Abstract: A good clustering can help a data analyst to explore and understand a data set, but what constitutes a good clustering may depend on domain-specific and application-specific criteria. These criteria can be difficult to formalize, even when it is easy for an analyst to know a good clustering when she sees one. We present a new approach to interactive clustering for data exploration, called \ciif, based on a particularly simple feedback mechanism, in which an analyst can choose to reject individual clusters and request new ones. The new clusters should be different from previously rejected clusters while still fitting the data well. We formalize this interaction in a novel Bayesian prior elicitation framework. In each iteration, the prior is adapted to account for all the previous feedback, and a new clustering is then produced from the posterior distribution. To achieve the computational efficiency necessary for an interactive setting, we propose an incremental optimization method over data minibatches using Lagrangian relaxation. Experiments demonstrate that \ciif can produce accurate and diverse clusterings. "
740707128457977856,2016-06-09 00:48:35,https://t.co/P4iPIKd3Kn,Resource Constrained Structured Prediction. (arXiv:1602.08761v2 [stat.ML] UPDATED) https://t.co/P4iPIKd3Kn,0,3," Abstract: We study the problem of structured prediction under test-time budget constraints. We propose a novel approach applicable to a wide range of structured prediction problems in computer vision and natural language processing. Our approach seeks to adaptively generate computationally costly features during test-time in order to reduce the computational cost of prediction while maintaining prediction performance. We show that training the adaptive feature generation system can be reduced to a series of structured learning problems, resulting in efficient training using existing structured learning algorithms. This framework provides theoretical justification for several existing heuristic approaches found in literature. We evaluate our proposed adaptive system on two structured prediction tasks, optical character recognition (OCR) and dependency parsing and show strong performance in reduction of the feature costs without degrading accuracy. "
740707127015149568,2016-06-09 00:48:35,https://t.co/ivVRIURiwQ,Classifying Options for Deep Reinforcement Learning. (arXiv:1604.08153v2 [cs.LG] UPDATED) https://t.co/ivVRIURiwQ,0,3," Abstract: In this paper we combine one method for hierarchical reinforcement learning - the options framework - with deep Q-networks (DQNs) through the use of different ""option heads"" on the policy network, and a supervisory network for choosing between the different options. We utilise our setup to investigate the effects of architectural constraints in subtasks with positive and negative transfer, across a range of network capacities. We empirically show that our augmented DQN has lower sample complexity when simultaneously learning subtasks with negative transfer, without degrading performance when learning subtasks with positive transfer. "
740707107188707328,2016-06-09 00:48:30,https://t.co/m0ggsQHcJh,Learning Representations for Counterfactual Inference. (arXiv:1605.03661v2 [stat.ML] UPDATED) https://t.co/m0ggsQHcJh,1,4," Abstract: Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, ""Would this patient have lower blood sugar had she received a different medication?"". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art. "
740345135062470656,2016-06-08 00:50:09,https://t.co/orWyztkziS,Expectile and Quantile Matrix Factorization for Extreme Data Analysis. (arXiv:1606.01984v1 [stat.ML]) https://t.co/orWyztkziS,0,1," Abstract: Matrix factorization is a popular approach to matrix estimation based on partial observations. Existing matrix factorization methods are mostly based on least squares and aim to yield a low-rank matrix to interpret conditional sample means. However, in many real applications with extreme data, least squares cannot explain their central tendency or tail distributions, incurring undesired estimates. In this paper, we formulate expectile and quantile matrix factorization problems by introducing expectile or quantile regression into the matrix factorization framework. We propose efficient algorithms based on alternating minimization and iterative reweighted least squares (IRLS) to effectively solve the new formulations. We prove that both algorithms converge to the global optima and exactly recover the true low rank matrices when noise is zero. For synthetic data with skewed noise and a real-world dataset containing web service latencies, our schemes can achieve lower recovery errors and better recommendation performance than the traditional least squares matrix factorization. "
740345128565497856,2016-06-08 00:50:08,https://t.co/jlKjxnkfcZ,Application of the Signature Method to Pattern Recognition in the CEQUEL Clinical Trial. (arXiv:1606.02074v1 [stat… https://t.co/jlKjxnkfcZ,0,1," Abstract: The classification procedure of streaming data usually requires various ad hoc methods or particular heuristic models. We explore a novel non-parametric and systematic approach to analysis of heterogeneous sequential data. We demonstrate an application of this method to classification of the delays in responding to the prompts, from subjects with bipolar disorder collected during a clinical trial, using both synthetic and real examples. We show how this method can provide a natural and systematic way to extract characteristic features from sequential data. "
740345121103831041,2016-06-08 00:50:06,https://t.co/szuqdDbmEV,Regret Bounds for Non-decomposable Metrics with Missing Labels. (arXiv:1606.02077v1 [cs.LG]) https://t.co/szuqdDbmEV,0,1," Abstract: We consider the problem of recommending relevant labels (items) for a given data point (user). In particular, we are interested in the practically important setting where the evaluation is with respect to non-decomposable (over labels) performance metrics like the $F_1$ measure, and the training data has missing labels. To this end, we propose a generic framework that given a performance metric $\Psi$, can devise a regularized objective function and a threshold such that all the values in the predicted score vector above and only above the threshold are selected to be positive. We show that the regret or generalization error in the given metric $\Psi$ is bounded ultimately by estimation error of certain underlying parameters. In particular, we derive regret bounds under three popular settings: a) collaborative filtering, b) multilabel classification, and c) PU (positive-unlabeled) learning. For each of the above problems, we can obtain precise non-asymptotic regret bound which is small even when a large fraction of labels is missing. Our empirical results on synthetic and benchmark datasets demonstrate that by explicitly modeling for missing labels and optimizing the desired performance metric, our algorithm indeed achieves significantly better performance (like $F_1$ score) when compared to methods that do not model missing label information carefully. "
740345113956716545,2016-06-08 00:50:04,https://t.co/hqEwNUPw5Y,Efficient differentially private learning improves drug sensitivity prediction. (arXiv:1606.02109v1 [stat.ML]) https://t.co/hqEwNUPw5Y,0,5," Abstract: Users of a personalised recommendation system face a dilemma: recommendations can be improved by learning from data, but only if the other users are willing to share their private information. Good personalised predictions are vitally important in precision medicine, but genomic information on which the predictions are based is also particularly sensitive, as it directly identifies the patients and hence cannot easily be anonymised. Differential privacy has emerged as a potentially promising solution: privacy is considered sufficient if presence of individual patients cannot be distinguished. However, differentially private learning with current methods does not improve predictions with feasible data sizes and dimensionalities. Here we show that useful predictors can be learned under powerful differential privacy guarantees, and even from moderately-sized data sets, by demonstrating significant improvements with a new robust private regression method in the accuracy of private drug sensitivity prediction. The method combines two key properties not present even in recent proposals, which can be generalised to other predictors: we prove it is asymptotically consistently and efficiently private, and demonstrate that it performs well on finite data. Good finite data performance is achieved by limiting the sharing of private information by decreasing the dimensionality and by projecting outliers to fit tighter bounds, therefore needing to add less noise for equal privacy. As already the simple-to-implement method shows promise on the challenging genomic data, we anticipate rapid progress towards practical applications in many fields, such as mobile sensing and social media, in addition to the badly needed precision medicine solutions. "
740345107174547457,2016-06-08 00:50:03,https://t.co/573cCMgkLJ,Towards a Neural Statistician. (arXiv:1606.02185v1 [stat.ML]) https://t.co/573cCMgkLJ,1,9," Abstract: An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. "
740345101004726272,2016-06-08 00:50:01,https://t.co/ZSKAjpbikH,A Minimax Approach to Supervised Learning. (arXiv:1606.02206v1 [stat.ML]) https://t.co/ZSKAjpbikH,0,3," Abstract: Given a task of predicting $Y$ from $X$, a loss function $L$, and a set of probability distributions $\Gamma$ on $(X,Y)$, what is the optimal decision rule minimizing the worst-case expected loss over $\Gamma$? In this paper, we address this question by introducing a generalization of the maximum entropy principle. Applying this principle to sets of distributions with marginal on $X$ constrained to be the empirical marginal, we provide a minimax interpretation of the maximum likelihood problem over generalized linear models, which connects the minimax problem for each loss function to a generalized linear model. While in some cases such as quadratic and logarithmic loss functions we revisit well-known linear and logistic regression models, our approach reveals novel models for other loss functions. In particular, for the 0-1 loss we derive a classification approach which we call the minimax SVM. The minimax SVM minimizes the worst-case expected 0-1 loss over the proposed $\Gamma$ by solving a tractable optimization problem. Moreover, applying the minimax approach to the Brier loss function we derive a new classification model called the minimax Brier. The maximum likelihood problem for this model uses the Huber penalty function. We perform several numerical experiments to show the power of the minimax SVM and the minimax Brier. "
740345094474223616,2016-06-08 00:50:00,https://t.co/MicYh9o2hR,Reducing the error of Monte Carlo Algorithms by Learning Control Variates. (arXiv:1606.02261v1 [stat.ML]) https://t.co/MicYh9o2hR,0,3," Abstract: Monte Carlo (MC) sampling algorithms are an extremely widely-used technique to estimate expectations of functions f(x), especially in high dimensions. Control variates are a very powerful technique to reduce the error of such estimates, but in their conventional form rely on having an accurate approximation of f, a priori. Stacked Monte Carlo (StackMC) is a recently introduced technique designed to overcome this limitation by fitting a control variate to the data samples themselves. Done naively, forming a control variate to the data would result in overfitting, typically worsening the MC algorithm's performance. StackMC uses in-sample / out-sample techniques to remove this overfitting. Crucially, it is a post-processing technique, requiring no additional samples, and can be applied to data generated by any MC estimator. Our preliminary experiments demonstrated that StackMC improved the estimates of expectations when it was used to post-process samples produces by a ""simple sampling"" MC estimator. Here we substantially extend this earlier work. We provide an in-depth analysis of the StackMC algorithm, which we use to construct an improved version of the original algorithm, with lower estimation error. We then perform experiments of StackMC on several additional kinds of MC estimators, demonstrating improved performance when the samples are generated via importance sampling, Latin-hypercube sampling and quasi-Monte Carlo sampling. We also show how to extend StackMC to combine multiple fitting functions, and how to apply it to discrete input spaces x. "
740345089713659906,2016-06-08 00:49:58,https://t.co/EpUbbhqSbM,Measuring the reliability of MCMC inference with bidirectional Monte Carlo. (arXiv:1606.02275v1 [cs.LG]) https://t.co/EpUbbhqSbM,0,7," Abstract: Markov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic inference, but it is notoriously hard to measure the quality of approximate posterior samples. This challenge is particularly salient in black box inference methods, which can hide details and obscure inference failures. In this work, we extend the recently introduced bidirectional Monte Carlo technique to evaluate MCMC-based posterior inference algorithms. By running annealed importance sampling (AIS) chains both from prior to posterior and vice versa on simulated data, we upper bound in expectation the symmetrized KL divergence between the true posterior distribution and the distribution of approximate samples. We present Bounding Divergences with REverse Annealing (BREAD), a protocol for validating the relevance of simulated data experiments to real datasets, and integrate it into two probabilistic programming languages: WebPPL and Stan. As an example of how BREAD can be used to guide the design of inference algorithms, we apply it to study the effectiveness of different model representations in both WebPPL and Stan. "
740345082121998336,2016-06-08 00:49:57,https://t.co/Ntx570SJPi,A Novel Approach for Phase Identification in Smart Grids Using Graph Theory and Principal Component Analysis. (arX… https://t.co/Ntx570SJPi,0,2," Abstract: Consumers with low demand, like households, are generally supplied single-phase power by connecting their service mains to one of the phases of a distribution transformer. The distribution companies face the problem of keeping a record of consumer connectivity to a phase due to uninformed changes that happen. The exact phase connectivity information is important for the efficient operation and control of distribution system. We propose a new data driven approach to the problem based on Principal Component Analysis (PCA) and its Graph Theoretic interpretations, using energy measurements in equally timed short intervals, generated from smart meters. We propose an algorithm for inferring phase connectivity from noisy measurements. The algorithm is demonstrated using simulated data for phase connectivities in distribution networks. "
740345076598120448,2016-06-08 00:49:55,https://t.co/dM7AWjJGGP,Classification of weak multi-view signals by sharing factors in a mixture of Bayesian group factor analyzers. (arX… https://t.co/dM7AWjJGGP,0,1," Abstract: We propose a novel classification model for weak signal data, building upon a recent model for Bayesian multi-view learning, Group Factor Analysis (GFA). Instead of assuming all data to come from a single GFA model, we allow latent clusters, each having a different GFA model and producing a different class distribution. We show that sharing information across the clusters, by sharing factors, increases the classification accuracy considerably; the shared factors essentially form a flexible noise model that explains away the part of data not related to classification. Motivation for the setting comes from single-trial functional brain imaging data, having a very low signal-to-noise ratio and a natural multi-view setting, with the different sensors, measurement modalities (EEG, MEG, fMRI) and possible auxiliary information as views. We demonstrate our model on a MEG dataset. "
740345070407258113,2016-06-08 00:49:54,https://t.co/b2JdeK3B1h,A Sharp Oracle Inequality for Bayesian Non-Negative Matrix Factorization. (arXiv:1601.01345v2 [stat.ML] UPDATED) https://t.co/b2JdeK3B1h,0,2, Abstract: The aim of this paper is to provide some theoretical understanding of Bayesian non-negative matrix factorization methods. We derive an oracle inequality for a quasi-Bayesian estimator. This result holds for a very general class of prior distributions and shows how the prior affects the rate of convergence. We illustrate our theoretical results with a short numerical study along with a discussion on existing implementations. 
740345063318933504,2016-06-08 00:49:52,https://t.co/Yfu54S60um,"Adaptive Skills, Adaptive Partitions (ASAP). (arXiv:1602.03351v2 [cs.LG] UPDATED) https://t.co/Yfu54S60um",1,1," Abstract: We introduce the Adaptive Skills, Adaptive Partitions (ASAP) framework that (1) learns skills (i.e., temporally extended actions or options) as well as (2) where to apply them. We believe that both (1) and (2) are necessary for a truly general skill learning framework, which is a key building block needed to scale up to lifelong learning agents. The ASAP framework can also solve related new tasks simply by adapting where it applies its existing learned skills. We prove that ASAP converges to a local optimum under natural conditions. Finally, our experimental results, which include a RoboCup domain, demonstrate the ability of ASAP to learn where to reuse skills as well as solve multiple tasks with considerably less experience than solving each task from scratch. "
740345057195241472,2016-06-08 00:49:51,https://t.co/xPReIzke7e,Distributed Clustering of Linear Bandits in Peer to Peer Networks. (arXiv:1604.07706v3 [cs.LG] UPDATED) https://t.co/xPReIzke7e,0,2," Abstract: We provide two distributed confidence ball algorithms for solving linear bandit problems in peer to peer networks with limited communication capabilities. For the first, we assume that all the peers are solving the same linear bandit problem, and prove that our algorithm achieves the optimal asymptotic regret rate of any centralised algorithm that can instantly communicate information between the peers. For the second, we assume that there are clusters of peers solving the same bandit problem within each cluster, and we prove that our algorithm discovers these clusters, while achieving the optimal asymptotic regret rate within each one. Through experiments on several real-world datasets, we demonstrate the performance of proposed algorithms compared to the state-of-the-art. "
740345050589241344,2016-06-08 00:49:49,https://t.co/SgiZ2vfxkH,Fast Randomized Semi-Supervised Clustering. (arXiv:1605.06422v2 [cs.LG] CROSS LISTED) https://t.co/SgiZ2vfxkH,0,3," Abstract: We consider the problem of clustering partially labeled data from a minimal number of randomly chosen pairwise comparisons between the items. We introduce an efficient local algorithm based on a power iteration of the non-backtracking operator and study its performance on a simple model. For the case of two clusters, we give bounds on the classification error and show that a small error can be achieved from $O(n)$ randomly chosen measurements, where $n$ is the number of items in the dataset. Our algorithm is therefore efficient both in terms of time and space complexities. We also investigate numerically the performance of the algorithm on synthetic and real world data. "
739986569294163969,2016-06-07 01:05:21,https://t.co/i4YqinAihD,Scalable Algorithms for Tractable Schatten Quasi-Norm Minimization. (arXiv:1606.01245v1 [cs.NA]) https://t.co/i4YqinAihD,0,4," Abstract: The Schatten-p quasi-norm $(0<p<1)$ is usually used to replace the standard nuclear norm in order to approximate the rank function more accurately. However, existing Schatten-p quasi-norm minimization algorithms involve singular value decomposition (SVD) or eigenvalue decomposition (EVD) in each iteration, and thus may become very slow and impractical for large-scale problems. In this paper, we first define two tractable Schatten quasi-norms, i.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, and then prove that they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively, which lead to the design of very efficient algorithms that only need to update two much smaller factor matrices. We also design two efficient proximal alternating linearized minimization algorithms for solving representative matrix completion problems. Finally, we provide the global convergence and performance guarantees for our algorithms, which have better convergence properties than existing algorithms. Experimental results on synthetic and real-world data show that our algorithms are more accurate than the state-of-the-art methods, and are orders of magnitude faster. "
739986568530829313,2016-06-07 01:05:20,https://t.co/HYgJ4lZkzi,Statistical Pattern Recognition for Driving Styles Based on Bayesian Probability and Kernel Density Estimation. (a… https://t.co/HYgJ4lZkzi,0,5," Abstract: Driving styles have a great influence on vehicle fuel economy, active safety, and drivability. To recognize driving styles of path-tracking behaviors for different divers, a statistical pattern-recognition method is developed to deal with the uncertainty of driving styles or characteristics based on probability density estimation. First, to describe driver path-tracking styles, vehicle speed and throttle opening are selected as the discriminative parameters, and a conditional kernel density function of vehicle speed and throttle opening is built, respectively, to describe the uncertainty and probability of two representative driving styles, e.g., aggressive and normal. Meanwhile, a posterior probability of each element in feature vector is obtained using full Bayesian theory. Second, a Euclidean distance method is involved to decide to which class the driver should be subject instead of calculating the complex covariance between every two elements of feature vectors. By comparing the Euclidean distance between every elements in feature vector, driving styles are classified into seven levels ranging from low normal to high aggressive. Subsequently, to show benefits of the proposed pattern-recognition method, a cross-validated method is used, compared with a fuzzy logic-based pattern-recognition method. The experiment results show that the proposed statistical pattern-recognition method for driving styles based on kernel density estimation is more efficient and stable than the fuzzy logic-based method. "
739986567582867456,2016-06-07 01:05:20,https://t.co/N3zKHWoyaq,Provable non-convex projected gradient descent for a class of constrained matrix optimization problems. (arXiv:160… https://t.co/N3zKHWoyaq,0,4," Abstract: We study the projected gradient descent method on low-rank matrix problems with a strongly convex objective. We use the Burer-Monteiro factorization approach to implicitly enforce low-rankness; such factorization introduces non-convexity in the objective. We focus on constraint sets that include both positive semi-definite (PSD) constraints and specific matrix norm-constraints. Such criteria appear in quantum state tomography and phase retrieval applications. We show that non-convex projected gradient descent favors local linear convergence in the factored space. We build our theory on a novel descent lemma, that non-trivially extends recent results on the unconstrained problem. The resulting algorithm is Projected Factored Gradient Descent, abbreviated as ProjFGD, and shows superior performance compared to state of the art on quantum state tomography and sparse phase retrieval applications. "
739986566869880832,2016-06-07 01:05:20,https://t.co/Am7WsaTe8F,TripleSpin - a generic compact paradigm for fast machine learning computations. (arXiv:1605.09046v2 [cs.LG] UPDATE… https://t.co/Am7WsaTe8F,1,6," Abstract: We present a generic compact computational framework relying on structured random matrices that can be applied to speed up several machine learning algorithms with almost no loss of accuracy. The applications include new fast LSH-based algorithms, efficient kernel computations via random feature maps, convex optimization algorithms, quantization techniques and many more. Certain models of the presented paradigm are even more compressible since they apply only bit matrices. This makes them suitable for deploying on mobile devices. All our findings come with strong theoretical guarantees. In particular, as a byproduct of the presented techniques and by using relatively new Berry-Esseen-type CLT for random vectors, we give the first theoretical guarantees for one of the most efficient existing LSH algorithms based on the $\textbf{HD}_{3}\textbf{HD}_{2}\textbf{HD}_{1}$ structured matrix (""Practical and Optimal LSH for Angular Distance""). These guarantees as well as theoretical results for other aforementioned applications follow from the same general theoretical principle that we present in the paper. Our structured family contains as special cases all previously considered structured schemes, including the recently introduced $P$-model. Experimental evaluation confirms the accuracy and efficiency of TripleSpin matrices. "
739986565406064641,2016-06-07 01:05:20,https://t.co/ogJkmZIPPD,Beyond LDA: A Unified Framework for Learning Latent Normalized Infinitely Divisible Topic Models through Spectral … https://t.co/ogJkmZIPPD,0,3," Abstract: In this paper, we propose guaranteed spectral methods for learning a broad range of topic models, which generalize the popular Latent Dirichlet Allocation (LDA). We overcome the limitation of LDA to incorporate arbitrary topic correlations, by assuming that the hidden topic proportions are drawn from a flexible class of Normalized Infinitely Divisible (NID) distributions. NID distributions are generated through the process of normalizing a family of independent Infinitely Divisible (ID) random variables. The Dirichlet distribution is a special case obtained by normalizing a set of Gamma random variables. We prove that this flexible topic model class can be learned via spectral methods using only moments up to the third order, with (low order) polynomial sample and computational complexity. The proof is based on a key new technique derived here that allows us to diagonalize the moments of the NID distribution through an efficient procedure that requires evaluating only univariate integrals, despite the fact that we are handling high dimensional multivariate moments. In order to assess the performance of our proposed Latent NID topic model, we use two real datasets of articles collected from New York Times and Pubmed. Our experiments yield improved perplexity on both datasets compared with the baseline. "
739982726741262336,2016-06-07 00:50:04,https://t.co/EzkIXcmFGL,Statistical Inference for Algorithmic Leveraging. (arXiv:1606.01473v1 [stat.AP]) https://t.co/EzkIXcmFGL,0,2," Abstract: The age of big data has produced data sets that are computationally expensive to analyze. To deal with such large-scale data sets, the method of algorithmic leveraging proposes that we sample according to some special distribution, rescale the data, and then perform analysis on the smaller sample. Ma, Mahoney, and Yu (2015) provides a framework to determine the statistical properties of algorithmic leveraging in the context of estimating the regression coefficients in a linear model with a fixed number of predictors. In this paper, we discuss how to perform statistical inference on regression coefficients estimated using algorithmic leveraging. In particular, we show how to construct confidence intervals for each estimated coefficient and present an efficient algorithm for doing so when the error variance is known. Through simulations, we confirm that our procedure controls the type I errors of significance tests for the regression coefficients and show that it has good power for those tests. "
739982725612986368,2016-06-07 00:50:04,https://t.co/DgjaUiAcFr,Relaxation of the EM Algorithm via Quantum Annealing. (arXiv:1606.01484v1 [stat.ML]) https://t.co/DgjaUiAcFr,1,8," Abstract: The EM algorithm is a novel numerical method to obtain maximum likelihood estimates and is often used for practical calculations. However, many of maximum likelihood estimation problems are nonconvex, and it is known that the EM algorithm fails to give the optimal estimate by being trapped by local optima. In order to deal with this difficulty, we propose a deterministic quantum annealing EM algorithm by introducing the mathematical mechanism of quantum fluctuations into the conventional EM algorithm because quantum fluctuations induce the tunnel effect and are expected to relax the difficulty of nonconvex optimization problems in the maximum likelihood estimation problems. We show a theorem that guarantees its convergence and give numerical experiments to verify its efficiency. "
739982724744765440,2016-06-07 00:50:04,https://t.co/HX97QkDvH6,Bounds for Vector-Valued Function Estimation. (arXiv:1606.01487v1 [stat.ML]) https://t.co/HX97QkDvH6,0,5," Abstract: We present a framework to derive risk bounds for vector-valued learning with a broad class of feature maps and loss functions. Multi-task learning and one-vs-all multi-category learning are treated as examples. We discuss in detail vector-valued functions with one hidden layer, and demonstrate that the conditions under which shared representations are beneficial for multi- task learning are equally applicable to multi-category learning. "
739982723868151808,2016-06-07 00:50:04,https://t.co/4IBAjMXbH2,Semi-Supervised Learning with Generative Adversarial Networks. (arXiv:1606.01583v1 [stat.ML]) https://t.co/4IBAjMXbH2,1,13," Abstract: We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels. We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes. At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G. We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN. "
739982723016753153,2016-06-07 00:50:04,https://t.co/797yd2E4s3,Integrated perception with recurrent multi-task neural networks. (arXiv:1606.01735v1 [stat.ML]) https://t.co/797yd2E4s3,0,3," Abstract: Modern discriminative predictors have been shown to match natural intelligences in specific perceptual tasks in image classification, object and part detection, boundary extraction, etc. However, a major advantage that natural intelligences still have is that they work well for ""all"" perceptual problems together, solving them efficiently and coherently in an ""integrated manner"". In order to capture some of these advantages in machine perception, we ask two questions: whether deep neural networks can learn universal image representations, useful not only for a single task but for all of them, and how the solutions to the different tasks can be integrated in this framework. We answer by proposing a new architecture, which we call ""MultiNet"", in which not only deep image features are shared between tasks, but where tasks can interact in a recurrent manner by encoding the results of their analysis in a common shared representation of the data. In this manner, we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then, more significantly, by integrating their solutions in the common representation. "
739982721880076289,2016-06-07 00:50:03,https://t.co/USni4U3fJg,Low-rank optimization with convex constraints. (arXiv:1606.01793v1 [math.OC]) https://t.co/USni4U3fJg,0,6," Abstract: The problem of low-rank approximation with convex constraints, which often appears in data analysis, image compression and model order reduction, is considered. Given a data matrix, the objective is to find an approximation of desired lower rank that fulfills the convex constraints and minimizes the distance to the data matrix in the Frobenius norm. The problem of matrix completion can be seen as a special case of this. Today, one of the most widely used techniques is to approximate this non-convex problem using convex nuclear norm regularization. In many situations, this technique does not give solutions with desirable properties. We instead propose to use the largest convex minorizer (under-approximation) of the Frobenius norm and the rank constraint as a convex proxy. This optimal convex proxy can be combined with other convex constraints to form an optimal convex minorizer of the original non-convex problem. With this approach, we get easily verifiable conditions under which the solutions to the convex relaxation and the original non-convex problem coincide. Several numerical examples are provided for which that is the case. We also see that our proposed convex relaxation consistently performs better than the nuclear norm heuristic, especially in the matrix completion case. The expressibility and computational tractability is of great importance for a convex relaxation. We provide a closed-form expression for the proposed convex approximation, and show how to represent it as a semi-definite program. We also show how to compute the proximal operator of the convex approximation. This allows us to use scalable first-order methods to solve convex approximation problems of large size. "
739982720672104448,2016-06-07 00:50:03,https://t.co/Saih39wRgF,Finite Sample Analysis of Approximate Message Passing. (arXiv:1606.01800v1 [cs.IT]) https://t.co/Saih39wRgF,2,4," Abstract: This paper analyzes the performance of Approximate Message Passing (AMP) in the regime where the problem dimension is large but finite. We consider the setting of high-dimensional regression, where the goal is to estimate a high-dimensional vector $\beta_0$ from a noisy measurement $y=A \beta_0 + w$. AMP is a low-complexity, scalable algorithm for this problem. Under suitable assumptions on the measurement matrix $A$, AMP has the attractive feature that its performance can be accurately characterized in the asymptotic large system limit by a simple scalar iteration called state evolution. Previous proofs of the validity of state evolution have all been asymptotic convergence results. In this paper, we derive a concentration result for AMP with i.i.d. Gaussian measurement matrices with finite dimension $n \times N$. The result shows that the probability of deviation from the state evolution prediction falls exponentially in $n$. Our result provides theoretical support for empirical findings that have demonstrated excellent agreement of AMP performance with state evolution predictions for moderately large dimensions. "
739982719719985153,2016-06-07 00:50:03,https://t.co/vkHzVO1QQg,ROCS-Derived Features for Virtual Screening. (arXiv:1606.01822v1 [stat.ML]) https://t.co/vkHzVO1QQg,1,2," Abstract: Rapid overlay of chemical structures (ROCS) is a standard tool for the calculation of 3D shape and chemical (""color"") similarity. ROCS uses unweighted sums to combine many aspects of similarity, yielding parameter-free models for virtual screening. In this report, we decompose the ROCS color force field into ""color components"" and ""color atom overlaps"", novel color similarity features that can be weighted in a system-specific manner by machine learning algorithms. In cross-validation experiments, these additional features significantly improve virtual screening performance (ROC AUC scores) relative to standard ROCS. "
739982718809833472,2016-06-07 00:50:03,https://t.co/hrH6F7K3fp,Bayesian Poisson Tucker Decomposition for Learning the Structure of International Relations. (arXiv:1606.01855v1 [… https://t.co/hrH6F7K3fp,0,3," Abstract: We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling country--country interaction event data. These data consist of interaction events of the form ""country $i$ took action $a$ toward country $j$ at time $t$."" BPTD discovers overlapping country--community memberships, including the number of latent communities. In addition, it discovers directed community--community interaction networks that are specific to ""topics"" of action types and temporal ""regimes."" We show that BPTD yields an efficient MCMC inference algorithm and achieves better predictive performance than related models. We also demonstrate that it discovers interpretable latent structure that agrees with our knowledge of international relations. "
739982717706723328,2016-06-07 00:50:02,https://t.co/fB3EGK8slr,Recurrent Neural Networks for Multivariate Time Series with Missing Values. (arXiv:1606.01865v1 [cs.LG]) https://t.co/fB3EGK8slr,0,9," Abstract: Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis. "
739982716624613376,2016-06-07 00:50:02,https://t.co/ox1nyRXTY0,On Robustness of Kernel Clustering. (arXiv:1606.01869v1 [stat.ML]) https://t.co/ox1nyRXTY0,0,4," Abstract: Clustering is one of the most important unsupervised problems in machine learning and statistics. Among many existing algorithms, kernel \km has drawn much research attention due to its ability to find non-linear cluster boundaries and its inherent simplicity. There are two main approaches for kernel k-means: SVD of the kernel matrix and convex relaxations. Despite the attention kernel clustering has received both from theoretical and applied quarters, not much is known about robustness of the methods. In this paper we first introduce a semidefinite programming relaxation for the kernel clustering problem, then prove that under a suitable model specification, both the K-SVD and SDP approaches are consistent in the limit, albeit SDP is strongly consistent, i.e. achieves exact recovery, whereas K-SVD is weakly consistent, i.e. the fraction of misclassified nodes vanish. "
739982715538149376,2016-06-07 00:50:02,https://t.co/MCnDawGCfy,Learning to Optimize. (arXiv:1606.01885v1 [cs.LG]) https://t.co/MCnDawGCfy,2,11," Abstract: Algorithm design is a laborious process and often requires many iterations of ideation and validation. In this paper, we explore automating algorithm design and present a method to learn an optimization algorithm, which we believe to be the first method that can automatically discover a better algorithm. We approach this problem from a reinforcement learning perspective and represent any particular optimization algorithm as a policy. We learn an optimization algorithm using guided policy search and demonstrate that the resulting algorithm outperforms existing hand-engineered algorithms in terms of convergence speed and/or the final objective value. "
739982714141573120,2016-06-07 00:50:01,https://t.co/IbhRPv28x7,Learning Non-Parametric Basis Independent Models from Point Queries via Low-Rank Methods. (arXiv:1310.1826v2 [stat… https://t.co/IbhRPv28x7,0,2," Abstract: We consider the problem of learning multi-ridge functions of the form f(x) = g(Ax) from point evaluations of f. We assume that the function f is defined on an l_2-ball in R^d, g is twice continuously differentiable almost everywhere, and A \in R^{k \times d} is a rank k matrix, where k << d. We propose a randomized, polynomial-complexity sampling scheme for estimating such functions. Our theoretical developments leverage recent techniques from low rank matrix recovery, which enables us to derive a polynomial time estimator of the function f along with uniform approximation guarantees. We prove that our scheme can also be applied for learning functions of the form: f(x) = \sum_{i=1}^{k} g_i(a_i^T x), provided f satisfies certain smoothness conditions in a neighborhood around the origin. We also characterize the noise robustness of the scheme. Finally, we present numerical examples to illustrate the theoretical bounds in action. "
739982713088843776,2016-06-07 00:50:01,https://t.co/0I2B7TQWN6,PAC-Bayes Analysis of Multi-view Learning. (arXiv:1406.5614v2 [cs.LG] UPDATED) https://t.co/0I2B7TQWN6,0,2," Abstract: This paper presents eight PAC-Bayes bounds to analyze the generalization performance of multi-view classifiers. These bounds adopt data dependent Gaussian priors which emphasize classifiers with high view agreements. The center of the prior for the first two bounds is the origin, while the center of the prior for the third and fourth bounds is given by a data dependent vector. An important technique to obtain these bounds is two derived logarithmic determinant inequalities whose difference lies in whether the dimensionality of data is involved. The centers of the fifth and sixth bounds are calculated on a separate subset of the training set. The last two bounds use unlabeled data to represent view agreements and are thus applicable to semi-supervised multi-view learning. We evaluate all the presented multi-view PAC-Bayes bounds on benchmark data and compare them with previous single-view PAC-Bayes bounds. The usefulness and performance of the multi-view bounds are discussed. "
739982710949711873,2016-06-07 00:50:01,https://t.co/cumZnBfBru,Multilayer bootstrap networks. (arXiv:1408.0848v7 [cs.LG] UPDATED) https://t.co/cumZnBfBru,0,4," Abstract: Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear network from bottom up for unsupervised nonlinear dimensionality reduction. Each layer of the network is a group of k-centers clusterings. Each clustering uses randomly sampled data points with randomly selected features as its centers, and learns a one-of-k encoding by one-nearest-neighbor optimization. Thanks to the binarized encoding, the similarity of two data points is measured by the number of the nearest centers they share in common, which is an adaptive similarity metric in the discrete space that needs no model assumption and parameter tuning. Thanks to the network structure, larger and larger local variations of data are gradually reduced from bottom up. The information loss caused by the binarized encoding is proportional to the correlation of the clusterings, both of which are reduced by the randomization steps. "
739982709523501056,2016-06-07 00:50:00,https://t.co/8Uo1IxKxFN,Trend Filtering on Graphs. (arXiv:1410.7690v5 [stat.ML] UPDATED) https://t.co/8Uo1IxKxFN,1,3," Abstract: We introduce a family of adaptive estimators on graphs, based on penalizing the $\ell_1$ norm of discrete graph differences. This generalizes the idea of trend filtering [Kim et al. (2009), Tibshirani (2014)], used for univariate nonparametric regression, to graphs. Analogous to the univariate case, graph trend filtering exhibits a level of local adaptivity unmatched by the usual $\ell_2$-based graph smoothers. It is also defined by a convex minimization problem that is readily solved (e.g., by fast ADMM or Newton algorithms). We demonstrate the merits of graph trend filtering through examples and theory. "
739982708391186432,2016-06-07 00:50:00,https://t.co/J8mfS14tnE,Towards stability and optimality in stochastic gradient descent. (arXiv:1505.02417v3 [stat.ME] UPDATED) https://t.co/J8mfS14tnE,0,5," Abstract: Iterative procedures for parameter estimation based on stochastic gradient descent allow the estimation to scale to massive data sets. However, in both theory and practice, they suffer from numerical instability. Moreover, they are statistically inefficient as estimators of the true parameter value. To address these two issues, we propose a new iterative procedure termed averaged implicit SGD (AI-SGD). For statistical efficiency, AI-SGD employs averaging of the iterates, which achieves the optimal Cram\'{e}r-Rao bound under strong convexity, i.e., it is an optimal unbiased estimator of the true parameter value. For numerical stability, AI-SGD employs an implicit update at each iteration, which is related to proximal operators in optimization. In practice, AI-SGD achieves competitive performance with other state-of-the-art procedures. Furthermore, it is more stable than averaging procedures that do not employ proximal updates, and is simple to implement as it requires fewer tunable hyperparameters than procedures that do employ proximal updates. "
739982707225137152,2016-06-07 00:50:00,https://t.co/SXkFzHrqmM,Structured Prediction: From Gaussian Perturbations to Linear-Time Principled Algorithms. (arXiv:1508.00945v4 [stat… https://t.co/SXkFzHrqmM,0,3," Abstract: Margin-based structured prediction commonly uses a maximum loss over all possible structured outputs \cite{Altun03,Collins04b,Taskar03}. In natural language processing, recent work \cite{Zhang14,Zhang15} has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution. This method is linear-time in the number of random structured outputs and trivially parallelizable. We study this family of loss functions in the PAC-Bayes framework under Gaussian perturbations \cite{McAllester07}. Under some technical conditions and up to statistical accuracy, we show that this family of loss functions produces a tighter upper bound of the Gibbs decoder distortion than commonly used methods. Thus, using the maximum loss over random structured outputs is a principled way of learning the parameter of structured prediction models. Besides explaining the experimental success of \cite{Zhang14,Zhang15}, our theoretical results show that more general techniques are possible. "
739982706134618112,2016-06-07 00:50:00,https://t.co/9QTjLbYrQr,Neural Variational Inference for Text Processing. (arXiv:1511.06038v4 [cs.CL] UPDATED) https://t.co/9QTjLbYrQr,0,5," Abstract: Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks. "
739982705220280320,2016-06-07 00:49:59,https://t.co/NAPBt135zT,Learning Discriminative Features via Label Consistent Neural Network. (arXiv:1602.01168v2 [cs.CV] UPDATED) https://t.co/NAPBt135zT,0,2," Abstract: Deep Convolutional Neural Networks (CNN) enforces supervised information only at the output layer, and hidden layers are trained by back propagating the prediction error from the output layer without explicit supervision. We propose a supervised feature learning approach, Label Consistent Neural Network, which enforces direct supervision in late hidden layers. We associate each neuron in a hidden layer with a particular class label and encourage it to be activated for input signals from the same class. More specifically, we introduce a label consistency regularization called ""discriminative representation error"" loss for late hidden layers and combine it with classification error loss to build our overall objective function. This label consistency constraint alleviates the common problem of gradient vanishing and tends to faster convergence; it also makes the features derived from late hidden layers discriminative enough for classification even using a simple $k$-NN classifier, since input signals from the same class will have very similar representations. Experimental results demonstrate that our approach achieves state-of-the-art performances on several public benchmarks for action and object category recognition. "
739982704096186368,2016-06-07 00:49:59,https://t.co/e5uObYLUTw,Characterizing Diseases from Unstructured Text: A Vocabulary Driven Word2vec Approach. (arXiv:1603.00106v2 [cs.LG]… https://t.co/e5uObYLUTw,0,1," Abstract: Traditional disease surveillance can be augmented with a wide variety of real-time sources such as, news and social media. However, these sources are in general unstructured and, construction of surveillance tools such as taxonomical correlations and trace mapping involves considerable human supervision. In this paper, we motivate a disease vocabulary driven word2vec model (Dis2Vec) to model diseases and constituent attributes as word embeddings from the HealthMap news corpus. We use these word embeddings to automatically create disease taxonomies and evaluate our model against corresponding human annotated taxonomies. We compare our model accuracies against several state-of-the art word2vec methods. Our results demonstrate that Dis2Vec outperforms traditional distributed vector representations in its ability to faithfully capture taxonomical attributes across different class of diseases such as endemic, emerging and rare. "
739982702863060992,2016-06-07 00:49:59,https://t.co/KxKCdgYmrA,Estimating Treatment Effects using Multiple Surrogates: The Role of the Surrogate Score and the Surrogate Index. (… https://t.co/KxKCdgYmrA,0,1," Abstract: Estimating the long-term effects of treatments is of interest in many fields. A common challenge in estimating such treatment effects is that long-term outcomes are unobserved in the time frame needed to make policy decisions. One approach to overcome this missing data problem is to analyze treatments effects on an intermediate outcome, often called a statistical surrogate, if it satisfies the condition that treatment and outcome are independent conditional on the statistical surrogate. The validity of the surrogacy condition is often controversial. Here we exploit that fact that in modern datasets, researchers often observe a large number, possibly hundreds or thousands, of intermediate outcomes, thought to lie on or close to the causal chain between the treatment and the long-term outcome of interest. Even if none of the individual proxies satisfies the statistical surrogacy criterion by itself, using multiple proxies can be useful in causal inference. We focus primarily on a setting with two samples, an experimental sample containing data about the treatment indicator and the surrogates and an observational sample containing information about the surrogates and the primary outcome. We state assumptions under which the average treatment effect be identified and estimated with a high-dimensional vector of proxies that collectively satisfy the surrogacy assumption, and derive the bias from violations of the surrogacy assumption, and show that even if the primary outcome is also observed in the experimental sample, there is still information to be gained from using surrogates. "
739982701843877888,2016-06-07 00:49:58,https://t.co/ML21KpJDPY,Learning Convolutional Neural Networks for Graphs. (arXiv:1605.05273v3 [cs.LG] UPDATED) https://t.co/ML21KpJDPY,2,7," Abstract: Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient. "
739982699734179840,2016-06-07 00:49:58,https://t.co/Ga8O60qXS9,False Discovery Rate Control and Statistical Quality Assessment of Annotators in Crowdsourced Ranking. (arXiv:1605… https://t.co/Ga8O60qXS9,0,2," Abstract: With the rapid growth of crowdsourcing platforms it has become easy and relatively inexpensive to collect a dataset labeled by multiple annotators in a short time. However due to the lack of control over the quality of the annotators, some abnormal annotators may be affected by position bias which can potentially degrade the quality of the final consensus labels. In this paper we introduce a statistical framework to model and detect annotator's position bias in order to control the false discovery rate (FDR) without a prior knowledge on the amount of biased annotators - the expected fraction of false discoveries among all discoveries being not too high, in order to assure that most of the discoveries are indeed true and replicable. The key technical development relies on some new knockoff filters adapted to our problem and new algorithms based on the Inverse Scale Space dynamics whose discretization is potentially suitable for large scale crowdsourcing data analysis. Our studies are supported by experiments with both simulated examples and real-world data. The proposed framework provides us a useful tool for quantitatively studying annotator's abnormal behavior in crowdsourcing data arising from machine learning, sociology, computer vision, multimedia, etc. "
739616985013714944,2016-06-06 00:36:45,https://t.co/icFoSb56FR,Sequential Principal Curves Analysis. (arXiv:1606.00856v1 [stat.ML]) https://t.co/icFoSb56FR,0,6," Abstract: This work includes all the technical details of the Sequential Principal Curves Analysis (SPCA) in a single document. SPCA is an unsupervised nonlinear and invertible feature extraction technique. The identified curvilinear features can be interpreted as a set of nonlinear sensors: the response of each sensor is the projection onto the corresponding feature. Moreover, it can be easily tuned for different optimization criteria; e.g. infomax, error minimization, decorrelation; by choosing the right way to measure distances along each curvilinear feature. Even though proposed in [Laparra et al. Neural Comp. 12] and shown to work in multiple modalities in [Laparra and Malo Frontiers Hum. Neuro. 15], the SPCA framework has its original roots in the nonlinear ICA algorithm in [Malo and Gutierrez Network 06]. Later on, the SPCA philosophy for nonlinear generalization of PCA originated substantially faster alternatives at the cost of introducing different constraints in the model. Namely, the Principal Polynomial Analysis (PPA) [Laparra et al. IJNS 14], and the Dimensionality Reduction via Regression (DRR) [Laparra et al. IEEE TGRS 15]. This report illustrates the reasons why we developed such family and is the appropriate technical companion for the missing details in [Laparra et al., NeCo 12, Laparra and Malo, Front.Hum.Neuro. 15]. See also the data, code and examples in the dedicated sites this http URL and this http URL effects.html "
739616983772200961,2016-06-06 00:36:45,https://t.co/XncMGl9ws4,Multi-Organ Cancer Classification and Survival Analysis. (arXiv:1606.00897v1 [q-bio.QM]) https://t.co/XncMGl9ws4,0,2," Abstract: Accurate and robust cell nuclei classification is the cornerstone for a wider range of tasks in digital and Computational Pathology. However, most machine learning systems require extensive labeling from expert pathologists for each individual problem at hand, with no or limited abilities for knowledge transfer between datasets and organ sites. In this paper we implement and evaluate a variety of deep neural network models and model ensembles for nuclei classification in renal cell cancer (RCC) and prostate cancer (PCa). We propose a convolutional neural network system based on residual learning which significantly improves over the state-of-the-art in cell nuclei classification. However, the main thrust of our work is to demonstrate for the first time the models ability to transfer the learned concepts from one organ type to another, with robust performance. Finally, we show that the combination of tissue types during training increases not only classification accuracy but also overall survival analysis. The best model, trained on combined data of RCC and PCa, exhibits optimal performance on PCa classification and better survival group stratification than an expert pathologist ($p=0.006$). All code, image data and expert labels are made publicly available to serve as benchmark for the community for future research into computational pathology. "
739616982459420672,2016-06-06 00:36:44,https://t.co/HiLQSF4aYS,Nonlinear Statistical Learning with Truncated Gaussian Graphical Models. (arXiv:1606.00906v1 [stat.ML]) https://t.co/HiLQSF4aYS,2,4," Abstract: We introduce the truncated Gaussian graphical model (TGGM) as a novel framework for designing statistical models for nonlinear learning. A TGGM is a Gaussian graphical model (GGM) with a subset of variables truncated to be nonnegative. The truncated variables are assumed latent and integrated out to induce a marginal model. We show that the variables in the marginal model are non-Gaussian distributed and their expected relations are nonlinear. We use expectation-maximization to break the inference of the nonlinear model into a sequence of TGGM inference problems, each of which is efficiently solved by using the properties and numerical methods of multivariate Gaussian distributions. We use the TGGM to design models for nonlinear regression and classification, with the performances of these models demonstrated on extensive benchmark datasets and compared to state-of-the-art competing results. "
739616981419229184,2016-06-06 00:36:44,https://t.co/JzKEKRHymR,Convolutional Imputation of Matrix Network. (arXiv:1606.00925v1 [cs.LG]) https://t.co/JzKEKRHymR,1,5," Abstract: A matrix network is a family of matrices, where the relationship between them is modeled as a weighted graph. Each node represents a matrix, and the weight on each edge represents the similarity between the two matrices. Suppose that we observe a few entries of each matrix with noise, and the fraction of entries we observe varies from matrix to matrix. Even worse, a subset of matrices in this family may be completely unobserved. How can we recover the entire matrix network from noisy and incomplete observations? One motivating example is the cold start problem, where we need to do inference on new users or items that come with no information. To recover this network of matrices, we propose a structural assumption that the matrix network can be approximated by generalized convolution of low rank matrices living on the same network. We propose an iterative imputation algorithm to complete the matrix network. This algorithm is efficient for large scale applications and is guaranteed to accurately recover all matrices, as long as there are enough observations accumulated over the network. "
739616980395827200,2016-06-06 00:36:44,https://t.co/QR2EX4DEkD,Deep Survival: A Deep Cox Proportional Hazards Network. (arXiv:1606.00931v1 [stat.ML]) https://t.co/QR2EX4DEkD,0,5," Abstract: Previous research has shown that neural networks can model survival data in situations in which some patients' death times are unknown, e.g. right-censored. However, neural networks have rarely been shown to outperform their linear counterparts such as the Cox proportional hazards model. In this paper, we run simulated experiments and use real survival data to build upon the risk-regression architecture proposed by Faraggi and Simon. We demonstrate that our model, DeepSurv, not only works as well as other survival models but actually outperforms in predictive ability on survival data with linear and nonlinear risk functions. We then show that the neural network can also serve as a recommender system by including a categorical variable representing a treatment group. This can be used to provide personalized treatment recommendations based on an individual's calculated risk. We provide an open source Python module that implements these methods in order to advance research on deep learning and survival analysis. "
739616979208830976,2016-06-06 00:36:43,https://t.co/4l3vt2oCsi,Synthesizing Dynamic Textures and Sounds by Spatial-Temporal Generative ConvNet. (arXiv:1606.00972v1 [stat.ML]) https://t.co/4l3vt2oCsi,0,3," Abstract: Dynamic textures are spatial-temporal processes that exhibit statistical stationarity or stochastic repetitiveness in the temporal dimension. In this paper, we study the problem of modeling and synthesizing dynamic textures using a generative version of the convolution neural network (ConvNet or CNN) that consists of multiple layers of spatial-temporal filters to capture the spatial-temporal patterns in the dynamic textures. We show that such spatial-temporal generative ConvNet can synthesize realistic dynamic textures. We also apply the temporal generative ConvNet to the one-dimensional sound data, and show that the model can synthesize realistic natural and man-made sounds. The videos and sounds can be found at this http URL "
739616977925312512,2016-06-06 00:36:43,https://t.co/Ia94Xno07i,A Graph-Based Semi-Supervised k Nearest-Neighbor Method for Nonlinear Manifold Distributed Data Classification. (a… https://t.co/Ia94Xno07i,2,6," Abstract: $k$ Nearest Neighbors ($k$NN) is one of the most widely used supervised learning algorithms to classify Gaussian distributed data, but it does not achieve good results when it is applied to nonlinear manifold distributed data, especially when a very limited amount of labeled samples are available. In this paper, we propose a new graph-based $k$NN algorithm which can effectively handle both Gaussian distributed data and nonlinear manifold distributed data. To achieve this goal, we first propose a constrained Tired Random Walk (TRW) by constructing an $R$-level nearest-neighbor strengthened tree over the graph, and then compute a TRW matrix for similarity measurement purposes. After this, the nearest neighbors are identified according to the TRW matrix and the class label of a query point is determined by the sum of all the TRW weights of its nearest neighbors. To deal with online situations, we also propose a new algorithm to handle sequential samples based a local neighborhood reconstruction. Comparison experiments are conducted on both synthetic data sets and real-world data sets to demonstrate the validity of the proposed new $k$NN algorithm and its improvements to other version of $k$NN algorithms. Given the widespread appearance of manifold structures in real-world problems and the popularity of the traditional $k$NN algorithm, the proposed manifold version $k$NN shows promising potential for classifying manifold-distributed data. "
739616976419618816,2016-06-06 00:36:43,https://t.co/5lnsdRXUHM,Gaussian Processes for Music Audio Modelling and Content Analysis. (arXiv:1606.01039v1 [stat.ML]) https://t.co/5lnsdRXUHM,0,7," Abstract: Real music signals are highly variable, yet they have strong statistical structure. Prior information about the underlying physical mechanisms by which sounds are generated and rules by which complex sound structure is constructed (notes, chords, a complete musical score), can be naturally unified using Bayesian modelling techniques. Typically algorithms for Automatic Music Transcription independently carry out individual tasks such as multiple-F0 detection and beat tracking. The challenge remains to perform joint estimation of all parameters. We present a Bayesian approach for modelling music audio, and content analysis. The proposed methodology based on Gaussian processes seeks joint estimation of multiple music concepts by incorporating into the kernel prior information about non-stationary behaviour, dynamics, and rich spectral content present in the modelled music signal. We illustrate the benefits of this approach via two tasks: pitch estimation, and inferring missing segments in a polyphonic audio recording. "
739616975412985856,2016-06-06 00:36:43,https://t.co/mLmyIUyZj2,Property-driven State-Space Coarsening for Continuous Time Markov Chains. (arXiv:1606.01111v1 [cs.SY]) https://t.co/mLmyIUyZj2,0,3," Abstract: Dynamical systems with large state-spaces are often expensive to thoroughly explore experimentally. Coarse-graining methods aim to define simpler systems which are more amenable to analysis and exploration; most current methods, however, focus on a priori state aggregation based on similarities in transition rates, which is not necessarily reflected in similar behaviours at the level of trajectories. We propose a way to coarsen the state-space of a system which optimally preserves the satisfaction of a set of logical specifications about the system's trajectories. Our approach is based on Gaussian Process emulation and Multi-Dimensional Scaling, a dimensionality reduction technique which optimally preserves distances in non-Euclidean spaces. We show how to obtain low-dimensional visualisations of the system's state-space from the perspective of properties' satisfaction, and how to define macro-states which behave coherently with respect to the specifications. Our approach is illustrated on a non-trivial running example, showing promising performance and high computational efficiency. "
739616974179819520,2016-06-06 00:36:42,https://t.co/0IZO6DskNI,Difference of Convex Functions Programming Applied to Control with Expert Data. (arXiv:1606.01128v1 [math.OC]) https://t.co/0IZO6DskNI,1,3," Abstract: This paper reports applications of Difference of Convex functions (DC) programming to Learning from Demonstrations (LfD) and Reinforcement Learning (RL) with expert data. This is made possible because the norm of the Optimal Bellman Residual (OBR), which is at the heart of many RL and LfD algorithms, is DC. Improvement in performance is demonstrated on two specific algorithms, namely Reward-regularized Classification for Apprenticeship Learning (RCAL) and Reinforcement Learning with Expert Demonstrations (RLED), through experiments on generic Markov Decision Processes (MDP), called Garnets. "
739616973018038272,2016-06-06 00:36:42,https://t.co/0mZoao7KHO,On Valid Optimal Assignment Kernels and Applications to Graph Classification. (arXiv:1606.01141v1 [cs.LG]) https://t.co/0mZoao7KHO,0,3," Abstract: The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel. "
739616971797504001,2016-06-06 00:36:42,https://t.co/GUkJUOgRbc,Robust Ensemble Clustering Using Probability Trajectories. (arXiv:1606.01160v1 [stat.ML]) https://t.co/GUkJUOgRbc,0,3," Abstract: Although many successful ensemble clustering approaches have been developed in recent years, there are still two limitations to most of the existing approaches. First, they mostly overlook the issue of uncertain links, which may mislead the overall consensus process. Second, they generally lack the ability to incorporate global information to refine the local links. To address these two limitations, in this paper, we propose a novel ensemble clustering approach based on sparse graph representation and probability trajectory analysis. In particular, we present the elite neighbor selection strategy to identify the uncertain links by locally adaptive thresholds and build a sparse graph with a small number of probably reliable links. We argue that a small number of probably reliable links can lead to significantly better consensus results than using all graph links regardless of their reliability. The random walk process driven by a new transition probability matrix is utilized to explore the global information in the graph. We derive a novel and dense similarity measure from the sparse graph by analyzing the probability trajectories of the random walkers, based on which two consensus functions are further proposed. Experimental results on multiple real-world datasets demonstrate the effectiveness and efficiency of our approach. "
739616970518220800,2016-06-06 00:36:41,https://t.co/LdUJdtC3vn,Dense Associative Memory for Pattern Recognition. (arXiv:1606.01164v1 [cs.NE]) https://t.co/LdUJdtC3vn,0,4," Abstract: A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set. "
739616969348026368,2016-06-06 00:36:41,https://t.co/0Odh2X2EI1,A Sharp Bound on the Computation-Accuracy Tradeoff for Majority Voting Ensembles. (arXiv:1303.0727v2 [math.PR] UPD… https://t.co/0Odh2X2EI1,0,4," Abstract: When random forests are used for binary classification, an ensemble of $t=1,2,\dots$ randomized classifiers is generated, and the predictions of the classifiers are aggregated by majority vote. Due to the randomness in the algorithm, there is a natural tradeoff between statistical performance and computational cost. On one hand, as $t$ increases, the (random) prediction error of the ensemble tends to decrease and stabilize. On the other hand, larger ensembles require greater computational cost for training and making new predictions. The present work offers a new approach for quantifying this tradeoff: Given a fixed training set $\mathcal{D}$, let the random variables $\text{Err}_{t,0}$ and $\text{Err}_{t,1}$ denote the class-wise prediction error rates of a randomly generated ensemble of size $t$. As $t\to\infty$, we provide a general bound on the ""algorithmic variance"", $\text{var}(\text{Err}_{t,l}|\mathcal{D})\leq \frac{f_l(1/2)^2}{4t}+o(\frac{1}{t})$, where $l\in\{0,1\}$, and $f_l$ is a density function that arises from the ensemble method. Conceptually, this result is somewhat surprising, because $\text{var}(\text{Err}_{t,l}|\mathcal{D})$ describes how $\text{Err}_{t,l}$ varies over repeated runs of the algorithm, and yet, the formula leads to a method for bounding $\text{var}(\text{Err}_{t,l}|\mathcal{D})$ with a single ensemble. The bound is also sharp in the sense that it is attained by an explicit family of randomized classifiers. With regard to the task of estimating $f_l(1/2)$, the presence of the ensemble leads to a unique twist on the classical setup of non-parametric density estimation --- wherein the effects of sample size and computational cost are intertwined. In particular, we propose an estimator for $f_l(1/2)$, and derive an upper bound on its MSE that matches ""standard optimal non-parametric rates"" when $t$ is sufficiently large. "
739616967980634113,2016-06-06 00:36:41,https://t.co/rQa99T9ifZ,Combining Multiple Clusterings via Crowd Agreement Estimation and Multi-Granularity Link Analysis. (arXiv:1405.129… https://t.co/rQa99T9ifZ,1,3," Abstract: The clustering ensemble technique aims to combine multiple clusterings into a probably better and more robust clustering and has been receiving an increasing attention in recent years. There are mainly two aspects of limitations in the existing clustering ensemble approaches. Firstly, many approaches lack the ability to weight the base clusterings without access to the original data and can be affected significantly by the low-quality, or even ill clusterings. Secondly, they generally focus on the instance level or cluster level in the ensemble system and fail to integrate multi-granularity cues into a unified model. To address these two limitations, this paper proposes to solve the clustering ensemble problem via crowd agreement estimation and multi-granularity link analysis. We present the normalized crowd agreement index (NCAI) to evaluate the quality of base clusterings in an unsupervised manner and thus weight the base clusterings in accordance with their clustering validity. To explore the relationship between clusters, the source aware connected triple (SACT) similarity is introduced with regard to their common neighbors and the source reliability. Based on NCAI and multi-granularity information collected among base clusterings, clusters, and data instances, we further propose two novel consensus functions, termed weighted evidence accumulation clustering (WEAC) and graph partitioning with multi-granularity link analysis (GP-MGLA) respectively. The experiments are conducted on eight real-world datasets. The experimental results demonstrate the effectiveness and robustness of the proposed methods. "
739616966252564480,2016-06-06 00:36:40,https://t.co/ZNMzc7Rhc4,Density Evolution in the Degree-correlated Stochastic Block Model. (arXiv:1509.03281v2 [stat.ML] UPDATED) https://t.co/ZNMzc7Rhc4,0,5," Abstract: There is a recent surge of interest in identifying the sharp recovery thresholds for cluster recovery under the stochastic block model. In this paper, we address the more refined question of how many vertices that will be misclassified on average. We consider the binary form of the stochastic block model, where $n$ vertices are partitioned into two clusters with edge probability $a/n$ within the first cluster, $c/n$ within the second cluster, and $b/n$ across clusters. Suppose that as $n \to \infty$, $a= b+ \mu \sqrt{ b} $, $c=b+ \nu \sqrt{ b} $ for two fixed constants $\mu, \nu$, and $b \to \infty$ with $b=n^{o(1)}$. When the cluster sizes are balanced and $\mu \neq \nu$, we show that the minimum fraction of misclassified vertices on average is given by $Q(\sqrt{v^*})$, where $Q(x)$ is the Q-function for standard normal, $v^*$ is the unique fixed point of $v= \frac{(\mu-\nu)^2}{16} + \frac{ (\mu+\nu)^2 }{16} \mathbb{E}[ \tanh(v+ \sqrt{v} Z)],$ and $Z$ is standard normal. Moreover, the minimum misclassified fraction on average is attained by a local algorithm, namely belief propagation, in time linear in the number of edges. Our proof techniques are based on connecting the cluster recovery problem to tree reconstruction problems, and analyzing the density evolution of belief propagation on trees with Gaussian approximations. "
739616964910407680,2016-06-06 00:36:40,https://t.co/DoVw33RqED,Auxiliary Deep Generative Models. (arXiv:1602.05473v3 [stat.ML] UPDATED) https://t.co/DoVw33RqED,0,4," Abstract: Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST, SVHN and NORB datasets. "
739616963618607104,2016-06-06 00:36:40,https://t.co/aVMkMuP6s1,Semidefinite Programs for Exact Recovery of a Hidden Community. (arXiv:1602.06410v2 [stat.ML] UPDATED) https://t.co/aVMkMuP6s1,0,3," Abstract: We study a semidefinite programming (SDP) relaxation of the maximum likelihood estimation for exactly recovering a hidden community of cardinality $K$ from an $n \times n$ symmetric data matrix $A$, where for distinct indices $i,j$, $A_{ij} \sim P$ if $i, j$ are both in the community and $A_{ij} \sim Q$ otherwise, for two known probability distributions $P$ and $Q$. We identify a sufficient condition and a necessary condition for the success of SDP for the general model. For both the Bernoulli case ($P={{\rm Bern}}(p)$ and $Q={{\rm Bern}}(q)$ with $p>q$) and the Gaussian case ($P=\mathcal{N}(\mu,1)$ and $Q=\mathcal{N}(0,1)$ with $\mu>0$), which correspond to the problem of planted dense subgraph recovery and submatrix localization respectively, the general results lead to the following findings: (1) If $K=\omega( n /\log n)$, SDP attains the information-theoretic recovery limits with sharp constants; (2) If $K=\Theta(n/\log n)$, SDP is order-wise optimal, but strictly suboptimal by a constant factor; (3) If $K=o(n/\log n)$ and $K \to \infty$, SDP is order-wise suboptimal. The same critical scaling for $K$ is found to hold, up to constant factors, for the performance of SDP on the stochastic block model of $n$ vertices partitioned into multiple communities of equal size $K$. A key ingredient in the proof of the necessary condition is a construction of a primal feasible solution based on random perturbation of the true cluster matrix. "
739616962158952448,2016-06-06 00:36:39,https://t.co/x8EgL4as5T,Group Equivariant Convolutional Networks. (arXiv:1602.07576v3 [cs.LG] UPDATED) https://t.co/x8EgL4as5T,1,4," Abstract: We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST. "
739616960816746497,2016-06-06 00:36:39,https://t.co/nMW1JHnZZF,Thompson Sampling is Asymptotically Optimal in General Environments. (arXiv:1602.07905v2 [cs.LG] UPDATED) https://t.co/nMW1JHnZZF,0,8," Abstract: We discuss a variant of Thompson sampling for nonparametric reinforcement learning in a countable classes of general stochastic environments. These environments can be non-Markov, non-ergodic, and partially observable. We show that Thompson sampling learns the environment class in the sense that (1) asymptotically its value converges to the optimal value in mean and (2) given a recoverability assumption regret is sublinear. "
739616959633973248,2016-06-06 00:36:39,https://t.co/x8mQazVgeI,Beyond CCA: Moment Matching for Multi-View Models. (arXiv:1602.09013v2 [stat.ML] UPDATED) https://t.co/x8mQazVgeI,0,6," Abstract: We introduce three novel semi-parametric extensions of probabilistic canonical correlation analysis with identifiability guarantees. We consider moment matching techniques for estimation in these models. For that, by drawing explicit links between the new models and a discrete version of independent component analysis (DICA), we first extend the DICA cumulant tensors to the new discrete version of CCA. By further using a close connection with independent component analysis, we introduce generalized covariance matrices, which can replace the cumulant tensors in the moment matching framework, and, therefore, improve sample complexity and simplify derivations and algorithms significantly. As the tensor power method or orthogonal joint diagonalization are not applicable in the new setting, we use non-orthogonal joint diagonalization techniques for matching the cumulants. We demonstrate performance of the proposed models and estimation techniques on experiments with both synthetic and real datasets. "
739616958551863296,2016-06-06 00:36:38,https://t.co/id6KVR5zCz,Training Input-Output Recurrent Neural Networks through Spectral Methods. (arXiv:1603.00954v3 [cs.LG] UPDATED) https://t.co/id6KVR5zCz,1,6," Abstract: We consider the problem of training input-output recurrent neural networks (RNN) for sequence labeling tasks. We propose a novel spectral approach for learning the network parameters. It is based on decomposition of the cross-moment tensor between the output and a non-linear transformation of the input, based on score functions. We guarantee consistent learning with polynomial sample and computational complexity under transparent conditions such as non-degeneracy of model parameters, polynomial activations for the neurons, and a Markovian evolution of the input sequence. We also extend our results to Bidirectional RNN which uses both previous and future information to output the label at each time point, and is employed in many NLP tasks such as POS tagging. "
739616957452935168,2016-06-06 00:36:38,https://t.co/FiCvTcAjhw,Degrees of Freedom in Deep Neural Networks. (arXiv:1603.09260v2 [cs.LG] UPDATED) https://t.co/FiCvTcAjhw,1,10," Abstract: In this paper, we explore degrees of freedom in deep sigmoidal neural networks. We show that the degrees of freedom in these models is related to the expected optimism, which is the expected difference between test error and training error. We provide an efficient Monte-Carlo method to estimate the degrees of freedom for multi-class classification methods. We show degrees of freedom are lower than the parameter count in a simple XOR network. We extend these results to neural nets trained on synthetic and real data, and investigate impact of network's architecture and different regularization choices. The degrees of freedom in deep networks are dramatically smaller than the number of parameters, in some real datasets several orders of magnitude. Further, we observe that for fixed number of parameters, deeper networks have less degrees of freedom exhibiting a regularization-by-depth. "
739616956005912577,2016-06-06 00:36:38,https://t.co/NnnVP5l8Ig,A constrained L1 minimization approach for estimating multiple Sparse Gaussian or Nonparanormal Graphical Models. … https://t.co/NnnVP5l8Ig,0,5," Abstract: Identifying context-specific entity networks from aggregated data is an important task, arising often in bioinformatics and neuroimaging. Computationally, this task can be formulated as jointly estimating multiple different, but related, sparse Undirected Graphical Models (UGM) from aggregated samples across several contexts. Previous joint-UGM studies have mostly focused on sparse Gaussian Graphical Models (sGGMs) and can't identify context-specific edge patterns directly. We, therefore, propose a novel approach, SIMULE (detecting Shared and Individual parts of MULtiple graphs Explicitly) to learn multi-UGM via a constrained L1 minimization. SIMULE automatically infers both specific edge patterns that are unique to each context and shared interactions preserved among all the contexts. Through the L1 constrained formulation, this problem is cast as multiple independent subtasks of linear programming that can be solved efficiently in parallel. In addition to Gaussian data, SIMULE can also handle multivariate Nonparanormal data that greatly relaxes the normality assumption that many real-world applications do not follow. We provide a novel theoretical proof showing that SIMULE achieves a consistent result at the rate O(log(Kp)/n_{tot}). On multiple synthetic datasets and two biomedical datasets, SIMULE shows significant improvement over state-of-the-art multi-sGGM and single-UGM baselines. "
738530716133281792,2016-06-03 00:40:18,https://t.co/5hnQJrNZwt,Graph-Guided Banding of the Covariance Matrix. (arXiv:1606.00451v1 [stat.ME]) https://t.co/5hnQJrNZwt,0,3," Abstract: Regularization has become a primary tool for developing reliable estimators of the covariance matrix in high-dimensional settings. To curb the curse of dimensionality, numerous methods assume that the population covariance (or inverse covariance) matrix is sparse, while making no particular structural assumptions on the desired pattern of sparsity. A highly-related, yet complementary, literature studies the specific setting in which the measured variables have a known ordering, in which case a banded population matrix is often assumed. While the banded approach is conceptually and computationally easier than asking for ""patternless sparsity,"" it is only applicable in very specific situations (such as when data are measured over time or one-dimensional space). This work proposes a generalization of the notion of bandedness that greatly expands the range of problems in which banded estimators apply. We develop convex regularizers occupying the broad middle ground between the former approach of ""patternless sparsity"" and the latter reliance on having a known ordering. Our framework defines bandedness with respect to a known graph on the measured variables. Such a graph is available in diverse situations, and we provide a theoretical, computational, and applied treatment of two new estimators. "
738530713599922176,2016-06-03 00:40:18,https://t.co/raAomcIoKY,Forecasting wind power - Modeling periodic and non-linear effects under conditional heteroscedasticity. (arXiv:160… https://t.co/raAomcIoKY,0,2," Abstract: In this article we present an approach that enables joint wind speed and wind power forecasts for a wind park. We combine a multivariate seasonal time varying threshold autoregressive moving average (TVARMA) model with a power threshold generalized autoregressive conditional heteroscedastic (power-TGARCH) model. The modeling framework incorporates diurnal and annual periodicity modeling by periodic B-splines, conditional heteroscedasticity and a complex autoregressive structure with non-linear impacts. In contrast to usually time-consuming estimation approaches as likelihood estimation, we apply a high-dimensional shrinkage technique. We utilize an iteratively re-weighted least absolute shrinkage and selection operator (lasso) technique. It allows for conditional heteroscedasticity, provides fast computing times and guarantees a parsimonious and regularized specification, even though the parameter space may be vast. We are able to show that our approach provides accurate forecasts of wind power at a turbine-specific level for forecasting horizons of up to 48 h (short- to medium-term forecasts). "
738530709388812288,2016-06-03 00:40:17,https://t.co/BHO2v0VwQ2,Variance-Reduced Proximal Stochastic Gradient Descent for Non-convex Composite optimization. (arXiv:1606.00602v1 [… https://t.co/BHO2v0VwQ2,1,9," Abstract: Here we study non-convex composite optimization: first, a finite-sum of smooth but non-convex functions, and second, a general function that admits a simple proximal mapping. Most research on stochastic methods for composite optimization assumes convexity or strong convexity of each function. In this paper, we extend this problem into the non-convex setting using variance reduction techniques, such as prox-SVRG and prox-SAGA. We prove that, with a constant step size, both prox-SVRG and prox-SAGA are suitable for non-convex composite optimization, and help the problem converge to a stationary point within $O(1/\epsilon)$ iterations. That is similar to the convergence rate seen with the state-of-the-art RSAG method and faster than stochastic gradient descent. Our analysis is also extended into the min-batch setting, which linearly accelerates the convergence. To the best of our knowledge, this is the first analysis of convergence rate of variance-reduced proximal stochastic gradient for non-convex composite optimization. "
738530706205331457,2016-06-03 00:40:16,https://t.co/yXdZAz4Ta6,Forecasting Framework for Open Access Time Series in Energy. (arXiv:1606.00656v1 [stat.AP]) https://t.co/yXdZAz4Ta6,0,3," Abstract: In this paper we propose a framework for automated forecasting of energy-related time series using open access data from European Network of Transmission System Operators for Electricity (ENTSO-E). The framework provides forecasts for various European countries using publicly available historical data only. Our solution was benchmarked using the actual load data and the country provided estimates (where available). We conclude that the proposed system can produce timely forecasts with comparable prediction accuracy in a number of cases. We also investigate the probabilistic case of forecasting - that is, providing a probability distribution rather than a simple point forecast - and incorporate it into a web based API that provides quick and easy access to reliable forecasts. "
738530704326307840,2016-06-03 00:40:15,https://t.co/veg0asBYNs,Unified Scalable Equivalent Formulations for Schatten Quasi-Norms. (arXiv:1606.00668v1 [cs.IT]) https://t.co/veg0asBYNs,0,2," Abstract: The Schatten quasi-norm can be used to bridge the gap between the nuclear norm and rank function, and is the tighter approximation to matrix rank. However, most existing Schatten quasi-norm minimization (SQNM) algorithms, as well as for nuclear norm minimization, are too slow or even impractical for large-scale problems, due to the SVD or EVD of the whole matrix in each iteration. In this paper, we rigorously prove that for any p, p1, p2>0 satisfying 1/p=1/p1+1/p2, the Schatten-p quasi-norm of any matrix is equivalent to minimizing the product of the Schatten-p1 norm (or quasi-norm) and Schatten-p2 norm (or quasi-norm) of its two factor matrices. Then we present and prove the equivalence relationship between the product formula of the Schatten quasi-norm and its weighted sum formula for the two cases of p1 and p2: p1=p2 and p1\neq p2. In particular, when p>1/2, there is an equivalence between the Schatten-p quasi-norm of any matrix and the Schatten-2p norms of its two factor matrices, where the widely used equivalent formulation of the nuclear norm can be viewed as a special case. That is, various SQNM problems with p>1/2 can be transformed into the one only involving smooth, convex norms of two factor matrices, which can lead to simpler and more efficient algorithms than conventional methods. We further extend the theoretical results of two factor matrices to the cases of three and more factor matrices, from which we can see that for any 0<p<1, the Schatten-p quasi-norm of any matrix is the minimization of the mean of the Schatten-(p3+1)p norms of all factor matrices, where p3 denotes the largest integer not exceeding 1/p. In other words, for any 0<p<1, the SQNM problem can be transformed into an optimization problem only involving the smooth, convex norms of multiple factor matrices. "
738530702354964482,2016-06-03 00:40:15,https://t.co/LRRvtBer1G,Adversarially Learned Inference. (arXiv:1606.00704v1 [stat.ML]) https://t.co/LRRvtBer1G,2,7," Abstract: We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network that is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with other recent approaches on the semi-supervised SVHN task. "
738530699490283520,2016-06-03 00:40:14,https://t.co/tTWyL2Hr4l,f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization. (arXiv:1606.00709v1 [stat.ML… https://t.co/tTWyL2Hr4l,5,17," Abstract: Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models. "
738530695124025344,2016-06-03 00:40:13,https://t.co/trZxPOTG0e,Differentially Private Gaussian Processes. (arXiv:1606.00720v1 [stat.ML]) https://t.co/trZxPOTG0e,7,12," Abstract: A major challenge for machine learning is increasing the availability of data while respecting the privacy of individuals. Differential privacy is a framework which allows algorithms to have provable privacy guarantees. Gaussian processes are a widely used approach for dealing with uncertainty in functions. This paper explores differentially private mechanisms for Gaussian processes. We compare binning and adding noise before regression with adding noise post-regression. For the former we develop a new kernel for use with binned data. For the latter we show that using inducing inputs allows us to reduce the scale of the added perturbation. We find that, for the datasets used, adding noise to a binned dataset has superior accuracy. Together these methods provide a starter toolkit for combining differential privacy and Gaussian processes. "
738530693064577026,2016-06-03 00:40:13,https://t.co/XyMlS7IoqR,Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation. (arXiv:1606.00776v1 [cs… https://t.co/XyMlS7IoqR,1,6," Abstract: We introduce the multiresolution recurrent neural network, which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes: a sequence of high-level coarse tokens, and a sequence of natural language tokens. There are many ways to estimate or learn the high-level coarse tokens, but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics. Such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences. In contrast to the standard log- likelihood objective w.r.t. natural language tokens (word perplexity), optimizing the joint log-likelihood biases the model towards modeling high-level abstractions. We apply the proposed model to the task of dialogue response generation in two challenging domains: the Ubuntu technical support domain, and Twitter conversations. On Ubuntu, the model outperforms competing approaches by a substantial margin, achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study. On Twitter, the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics. Finally, our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure. "
738530691277852672,2016-06-03 00:40:12,https://t.co/etTjacB1u9,Prior Swapping for Data-Independent Inference. (arXiv:1606.00787v1 [stat.ML]) https://t.co/etTjacB1u9,0,3," Abstract: While Bayesian methods are praised for their ability to incorporate useful prior knowledge, in practice, priors that allow for computationally convenient or tractable inference are more commonly used. In this paper, we investigate the following question: for a given model, is it possible to use any convenient prior to infer a false posterior, and afterwards, given some true prior of interest, quickly transform this result into the true posterior? We present a procedure to carry out this task: given an inferred false posterior and true prior, our algorithm generates samples from the true posterior. This transformation procedure, which we call ""prior swapping"" works for arbitrary priors. Notably, its cost is independent of data size. It therefore allows us, in some cases, to apply significantly less-costly inference procedures to more-sophisticated models than previously possible. It also lets us quickly perform any additional inferences, such as with updated priors or for many different hyperparameter settings, without touching the data. We prove that our method can generate asymptotically exact samples, and demonstrate it empirically on a number of models and priors. "
738530689134518272,2016-06-03 00:40:12,https://t.co/ag6ZrNCzEg,Multi-View Treelet Transform. (arXiv:1606.00800v1 [stat.ML]) https://t.co/ag6ZrNCzEg,0,3," Abstract: Current multi-view factorization methods make assumptions that are not acceptable for many kinds of data, and in particular, for graphical data with hierarchical structure. At the same time, current hierarchical methods work only in the single-view setting. We generalize the Treelet Transform to the Multi-View Treelet Transform (MVTT) to allow for the capture of hierarchical structure when multiple views are available. Further, we show how this generalization is consistent with the existing theory and how it might be used in denoising empirical networks and in computing the shared response of functional brain data. "
738530687783948288,2016-06-03 00:40:11,https://t.co/ZripqiEcZc,Generalized Root Models: Beyond Pairwise Graphical Models for Univariate Exponential Families. (arXiv:1606.00813v1… https://t.co/ZripqiEcZc,0,2," Abstract: We present a novel k-way high-dimensional graphical model called the Generalized Root Model (GRM) that explicitly models dependencies between variable sets of size k > 2---where k = 2 is the standard pairwise graphical model. This model is based on taking the k-th root of the original sufficient statistics of any univariate exponential family with positive sufficient statistics, including the Poisson and exponential distributions. As in the recent work with square root graphical (SQR) models [Inouye et al. 2016]---which was restricted to pairwise dependencies---we give the conditions of the parameters that are needed for normalization using the radial conditionals similar to the pairwise case [Inouye et al. 2016]. In particular, we show that the Poisson GRM has no restrictions on the parameters and the exponential GRM only has a restriction akin to negative definiteness. We develop a simple but general learning algorithm based on L1-regularized node-wise regressions. We also present a general way of numerically approximating the log partition function and associated derivatives of the GRM univariate node conditionals---in contrast to [Inouye et al. 2016], which only provided algorithm for estimating the exponential SQR. To illustrate GRM, we model word counts with a Poisson GRM and show the associated k-sized variable sets. We finish by discussing methods for reducing the parameter space in various situations. "
738530686081093632,2016-06-03 00:40:11,https://t.co/ejx7D2NFgK,High Dimensional Multivariate Regression and Precision Matrix Estimation via Nonconvex Optimization. (arXiv:1606.0… https://t.co/ejx7D2NFgK,1,4," Abstract: We propose a nonconvex estimator for joint multivariate regression and precision matrix estimation in the high dimensional regime, under sparsity constraints. A gradient descent algorithm with hard thresholding is developed to solve the nonconvex estimator, and it attains a linear rate of convergence to the true regression coefficients and precision matrix simultaneously, up to the statistical error. Compared with existing methods along this line of research, which have little theoretical guarantee, the proposed algorithm not only is computationally much more efficient with provable convergence guarantee, but also attains the optimal finite sample statistical rate up to a logarithmic factor. Thorough experiments on both synthetic and real datasets back up our theory. "
738530684545929216,2016-06-03 00:40:11,https://t.co/wIPWM4XkBK,On the robustness of learning in games with stochastically perturbed payoff observations. (arXiv:1412.6565v2 [math… https://t.co/wIPWM4XkBK,0,2," Abstract: Motivated by the scarcity of accurate payoff feedback in practical applications of game theory, we examine a class of learning dynamics where players adjust their choices based on past payoff observations that are subject to noise and random disturbances. First, in the single-player case (corresponding to an agent trying to adapt to an arbitrarily changing environment), we show that the stochastic dynamics under study lead to no regret almost surely, irrespective of the noise level in the player's observations. In the multi-player case, we find that dominated strategies become extinct and we show that strict Nash equilibria are stochastically stable and attracting; conversely, if a state is stable or attracting with positive probability, then it is a Nash equilibrium. Finally, we provide an averaging principle for 2-player games, and we show that in zero-sum games with an interior equilibrium, time averages converge to Nash equilibrium for any noise level. "
738530682931183616,2016-06-03 00:40:10,https://t.co/6KAdzWlRVj,Fast Saddle-Point Algorithm for Generalized Dantzig Selector and FDR Control with the Ordered l1-Norm. (arXiv:1511… https://t.co/6KAdzWlRVj,0,2," Abstract: In this paper we propose a primal-dual proximal extragradient algorithm to solve the generalized Dantzig selector (GDS) estimation problem, based on a new convex-concave saddle-point (SP) reformulation. Our new formulation makes it possible to adopt recent developments in saddle-point optimization, to achieve the optimal $O(1/k)$ rate of convergence. Compared to the optimal non-SP algorithms, ours do not require specification of sensitive parameters that affect algorithm performance or solution quality. We also provide a new analysis showing a possibility of local acceleration to achieve the rate of $O(1/k^2)$ in special cases even without strong convexity or strong smoothness. As an application, we propose a GDS equipped with the ordered $\ell_1$-norm, showing its false discovery rate control properties in variable selection. Algorithm performance is compared between ours and other alternatives, including the linearized ADMM, Nesterov's smoothing, Nemirovski's mirror-prox, and the accelerated hybrid proximal extragradient techniques. "
738530679332442113,2016-06-03 00:40:09,https://t.co/T3yX0Dg0uc,"Conditional Dependence via Shannon Capacity: Axioms, Estimators and Applications. (arXiv:1602.03476v3 [cs.IT] UPDA… https://t.co/T3yX0Dg0uc",0,3," Abstract: We conduct an axiomatic study of the problem of estimating the strength of a known causal relationship between a pair of variables. We propose that an estimate of causal strength should be based on the conditional distribution of the effect given the cause (and not on the driving distribution of the cause), and study dependence measures on conditional distributions. Shannon capacity, appropriately regularized, emerges as a natural measure under these axioms. We examine the problem of calculating Shannon capacity from the observed samples and propose a novel fixed-$k$ nearest neighbor estimator, and demonstrate its consistency. Finally, we demonstrate an application to single-cell flow-cytometry, where the proposed estimators significantly reduce sample complexity. "
738530674534187008,2016-06-03 00:40:08,https://t.co/HUxIHA1S4V,Bayesian Learning of Kernel Embeddings. (arXiv:1603.02160v2 [stat.ML] UPDATED) https://t.co/HUxIHA1S4V,2,14," Abstract: Kernel methods are one of the mainstays of machine learning, but the problem of kernel learning remains challenging, with only a few heuristics and very little theory. This is of particular importance in methods based on estimation of kernel mean embeddings of probability measures. For characteristic kernels, which include most commonly used ones, the kernel mean embedding uniquely determines its probability measure, so it can be used to design a powerful statistical testing framework, which includes nonparametric two-sample and independence tests. In practice, however, the performance of these tests can be very sensitive to the choice of kernel and its lengthscale parameters. To address this central issue, we propose a new probabilistic model for kernel mean embeddings, the Bayesian Kernel Embedding model, combining a Gaussian process prior over the Reproducing Kernel Hilbert Space containing the mean embedding with a conjugate likelihood function, thus yielding a closed form posterior over the mean embedding. The posterior mean of our model is closely related to recently proposed shrinkage estimators for kernel mean embeddings, while the posterior uncertainty is a new, interesting feature with various possible applications. Critically for the purposes of kernel learning, our model gives a simple, closed form marginal pseudolikelihood of the observed data given the kernel hyperparameters. This marginal pseudolikelihood can either be optimized to inform the hyperparameter choice or fully Bayesian inference can be used. "
738168451215298560,2016-06-02 00:40:47,https://t.co/5eI6FGxRtm,Quantifying the probable approximation error of probabilistic inference programs. (arXiv:1606.00068v1 [cs.AI]) https://t.co/5eI6FGxRtm,2,9," Abstract: This paper introduces a new technique for quantifying the approximation error of a broad class of probabilistic inference programs, including ones based on both variational and Monte Carlo approaches. The key idea is to derive a subjective bound on the symmetrized KL divergence between the distribution achieved by an approximate inference program and its true target distribution. The bound's validity (and subjectivity) rests on the accuracy of two auxiliary probabilistic programs: (i) a ""reference"" inference program that defines a gold standard of accuracy and (ii) a ""meta-inference"" program that answers the question ""what internal random choices did the original approximate inference program probably make given that it produced a particular result?"" The paper includes empirical results on inference problems drawn from linear regression, Dirichlet process mixture modeling, HMMs, and Bayesian networks. The experiments show that the technique is robust to the quality of the reference inference program and that it can detect implementation bugs that are not apparent from predictive performance. "
738168449206099968,2016-06-02 00:40:47,https://t.co/zfiBVuJwQv,Identifying Outliers using Influence Function of Multiple Kernel Canonical Correlation Analysis. (arXiv:1606.00113… https://t.co/zfiBVuJwQv,0,3," Abstract: Imaging genetic research has essentially focused on discovering unique and co-association effects, but typically ignoring to identify outliers or atypical objects in genetic as well as non-genetics variables. Identifying significant outliers is an essential and challenging issue for imaging genetics and multiple sources data analysis. Therefore, we need to examine for transcription errors of identified outliers. First, we address the influence function (IF) of kernel mean element, kernel covariance operator, kernel cross-covariance operator, kernel canonical correlation analysis (kernel CCA) and multiple kernel CCA. Second, we propose an IF of multiple kernel CCA, which can be applied for more than two datasets. Third, we propose a visualization method to detect influential observations of multiple sources of data based on the IF of kernel CCA and multiple kernel CCA. Finally, the proposed methods are capable of analyzing outliers of subjects usually found in biomedical applications, in which the number of dimension is large. To examine the outliers, we use the stem-and-leaf display. Experiments on both synthesized and imaging genetics data (e.g., SNP, fMRI, and DNA methylation) demonstrate that the proposed visualization can be applied effectively. "
738168445364113408,2016-06-02 00:40:46,https://t.co/dg6tZ4t78r,Gene-Gene association for Imaging Genetics Data using Robust Kernel Canonical Correlation Analysis. (arXiv:1606.00… https://t.co/dg6tZ4t78r,0,2," Abstract: In genome-wide interaction studies, to detect gene-gene interactions, most methods are divided into two folds: single nucleotide polymorphisms (SNP) based and gene-based methods. Basically, the methods based on the gene are more effective than the methods based on a single SNP. Recent years, while the kernel canonical correlation analysis (Classical kernel CCA) based U statistic (KCCU) has proposed to detect the nonlinear relationship between genes. To estimate the variance in KCCU, they have used resampling based methods which are highly computationally intensive. In addition, classical kernel CCA is not robust to contaminated data. We, therefore, first discuss robust kernel mean element, the robust kernel covariance, and cross-covariance operators. Second, we propose a method based on influence function to estimate the variance of the KCCU. Third, we propose a nonparametric robust KCCU method based on robust kernel CCA, which is designed for contaminated data and less sensitive to noise than classical kernel CCA. Finally, we investigate the proposed methods to synthesized data and imaging genetic data set. Based on gene ontology and pathway analysis, the synthesized and genetics analysis demonstrate that the proposed robust method shows the superior performance of the state-of-the-art methods. "
738168443434721280,2016-06-02 00:40:46,https://t.co/3x5Z4Tt96u,Latent Contextual Bandits. (arXiv:1606.00119v1 [cs.LG]) https://t.co/3x5Z4Tt96u,1,3," Abstract: Motivated by online recommendation and advertising systems, we consider a causal model for stochastic contextual bandits with a latent low-dimensional confounder. In our model, there are $L$ observed contexts and $K$ arms of the bandit. The observed context influences the reward obtained through a latent confounder variable with cardinality $m$ ($m \ll L,K$). The arm choice and the latent confounder causally determines the reward while the observed context is correlated with the confounder. Under this model, the $L \times K$ mean reward matrix $\mathbf{U}$ (for each context in $[L]$ and each arm in $[K]$) factorizes into non-negative factors $\mathbf{A}$ ($L \times m$) and $\mathbf{W}$ ($m \times K$). This insight enables us to propose an $\epsilon$-greedy NMF-Bandit algorithm that designs a sequence of interventions (selecting specific arms), that achieves a balance between learning this low-dimensional structure and selecting the best arm to minimize regret. Our algorithm achieves a regret of $\mathcal{O}\left(L\mathrm{poly}(m, \log K) \log T \right)$ at time $T$, as compared to $\mathcal{O}(LK\log T)$ for conventional contextual bandits, assuming a constant gap between the best arm and the rest for each context. These guarantees are obtained under mild sufficiency conditions on the factors that are weaker versions of the well-known Statistical RIP condition. We further propose a class of generative models that satisfy our sufficient conditions, and derive a lower bound of $\mathcal{O}\left(Km\log T\right)$. These are the first regret guarantees for online matrix completion with bandit feedback, when the rank is greater than one. We further compare the performance of our algorithm with the state of the art, on synthetic and real world data-sets. "
738168441408917505,2016-06-02 00:40:45,https://t.co/a4Z9HSncdh,Efficiently Bounding Optimal Solutions after Small Data Modification in Large-Scale Empirical Risk Minimization. (… https://t.co/a4Z9HSncdh,0,3," Abstract: We study large-scale classification problems in changing environments where a small part of the dataset is modified, and the effect of the data modification must be quickly incorporated into the classifier. When the entire dataset is large, even if the amount of the data modification is fairly small, the computational cost of re-training the classifier would be prohibitively large. In this paper, we propose a novel method for efficiently incorporating such a data modification effect into the classifier without actually re-training it. The proposed method provides bounds on the unknown optimal classifier with the cost only proportional to the size of the data modification. We demonstrate through numerical experiments that the proposed method provides sufficiently tight bounds with negligible computational costs, especially when a small part of the dataset is modified in a large-scale classification problem. "
738168439928455168,2016-06-02 00:40:45,https://t.co/UWex9EbWUz,Model selection consistency from the perspective of generalization ability and VC theory with an application to La… https://t.co/UWex9EbWUz,0,1," Abstract: Model selection is difficult to analyse yet theoretically and empirically important, especially for high-dimensional data analysis. Recently the least absolute shrinkage and selection operator (Lasso) has been applied in the statistical and econometric literature. Consis- tency of Lasso has been established under various conditions, some of which are difficult to verify in practice. In this paper, we study model selection from the perspective of generalization ability, under the framework of structural risk minimization (SRM) and Vapnik-Chervonenkis (VC) theory. The approach emphasizes the balance between the in-sample and out-of-sample fit, which can be achieved by using cross-validation to select a penalty on model complexity. We show that an exact relationship exists between the generalization ability of a model and model selection consistency. By implementing SRM and the VC inequality, we show that Lasso is L2-consistent for model selection under assumptions similar to those imposed on OLS. Furthermore, we derive a probabilistic bound for the distance between the penalized extremum estimator and the extremum estimator without penalty, which is dominated by overfitting. We also propose a new measurement of overfitting, GR2, based on generalization ability, that converges to zero if model selection is consistent. Using simulations, we demonstrate that the proposed CV-Lasso algorithm performs well in terms of model selection and overfitting control. "
738168437948616704,2016-06-02 00:40:44,https://t.co/D43jeKMXRY,"Crowdsourcing: Low complexity, Minimax Optimal Algorithms. (arXiv:1606.00226v1 [stat.ML]) https://t.co/D43jeKMXRY",2,1," Abstract: We consider the problem of accurately estimating the reliability of workers based on noisy labels they provide, which is a fundamental question in crowdsourcing. We propose a novel lower bound on the minimax estimation error which applies to any estimation procedure. We further propose Triangular Estimation (TE), an algorithm for estimating the reliability of workers. TE has low complexity, may be implemented in a streaming setting when labels are provided by workers in real time, and does not rely on an iterative procedure. We further prove that TE is minimax optimal and matches our lower bound. We conclude by assessing the performance of TE and other state-of-the-art algorithms on both synthetic and real-world data sets. "
738168436283478016,2016-06-02 00:40:44,https://t.co/NvwwRBgtLL,Clustering with phylogenetic tools in astrophysics. (arXiv:1606.00235v1 [astro-ph.IM]) https://t.co/NvwwRBgtLL,0,2," Abstract: Phylogenetic approaches are finding more and more applications outside the field of biology. Astrophysics is no exception since an overwhelming amount of multivariate data has appeared in the last twenty years or so. In particular, the diversification of galaxies throughout the evolution of the Universe quite naturally invokes phylogenetic approaches. We have demonstrated that Maximum Parsimony brings useful astrophysical results, and we now proceed toward the analyses of large datasets for galaxies. In this talk I present how we solve the major difficulties for this goal: the choice of the parameters, their discretization, and the analysis of a high number of objects with an unsupervised NP-hard classification technique like cladistics. 1. Introduction How do the galaxy form, and when? How did the galaxy evolve and transform themselves to create the diversity we observe? What are the progenitors to present-day galaxies? To answer these big questions, observations throughout the Universe and the physical modelisation are obvious tools. But between these, there is a key process, without which it would be impossible to extract some digestible information from the complexity of these systems. This is classification. One century ago, galaxies were discovered by Hubble. From images obtained in the visible range of wavelengths, he synthetised his observations through the usual process: classification. With only one parameter (the shape) that is qualitative and determined with the eye, he found four categories: ellipticals, spirals, barred spirals and irregulars. This is the famous Hubble classification. He later hypothetized relationships between these classes, building the Hubble Tuning Fork. The Hubble classification has been refined, notably by de Vaucouleurs, and is still used as the only global classification of galaxies. Even though the physical relationships proposed by Hubble are not retained any more, the Hubble Tuning Fork is nearly always used to represent the classification of the galaxy diversity under its new name the Hubble sequence (e.g. Delgado-Serrano, 2012). Its success is impressive and can be understood by its simplicity, even its beauty, and by the many correlations found between the morphology of galaxies and their other properties. And one must admit that there is no alternative up to now, even though both the Hubble classification and diagram have been recognised to be unsatisfactory. Among the most obvious flaws of this classification, one must mention its monovariate, qualitative, subjective and old-fashioned nature, as well as the difficulty to characterise the morphology of distant galaxies. The first two most significant multivariate studies were by Watanabe et al. (1985) and Whitmore (1984). Since the year 2005, the number of studies attempting to go beyond the Hubble classification has increased largely. Why, despite of this, the Hubble classification and its sequence are still alive and no alternative have yet emerged (Sandage, 2005)? My feeling is that the results of the multivariate analyses are not easily integrated into a one-century old practice of modeling the observations. In addition, extragalactic objects like galaxies, stellar clusters or stars do evolve. Astronomy now provides data on very distant objects, raising the question of the relationships between those and our present day nearby galaxies. Clearly, this is a phylogenetic problem. Astrocladistics 1 aims at exploring the use of phylogenetic tools in astrophysics (Fraix-Burnet et al., 2006a,b). We have proved that Maximum Parsimony (or cladistics) can be applied in astrophysics and provides a new exploration tool of the data (Fraix-Burnet et al., 2009, 2012, Cardone \& Fraix-Burnet, 2013). As far as the classification of galaxies is concerned, a larger number of objects must now be analysed. In this paper, I "
738168434333126657,2016-06-02 00:40:43,https://t.co/tuIBOH2wq4,Finding Singular Features. (arXiv:1606.00265v1 [stat.ME]) https://t.co/tuIBOH2wq4,0,2," Abstract: We present a method for finding high density, low-dimensional structures in noisy point clouds. These structures are sets with zero Lebesgue measure with respect to the $D$-dimensional ambient space and belong to a $d<D$ dimensional space. We call them ""singular features."" Hunting for singular features corresponds to finding unexpected or unknown structures hidden in point clouds belonging to $\R^D$. Our method outputs well defined sets of dimensions $d<D$. Unlike spectral clustering, the method works well in the presence of noise. We show how to find singular features by first finding ridges in the estimated density, followed by a filtering step based on the eigenvalues of the Hessian of the density. "
738168432571490305,2016-06-02 00:40:43,https://t.co/6p0DXlcwEU,Discovering Phase Transitions with Unsupervised Learning. (arXiv:1606.00318v1 [cond-mat.stat-mech]) https://t.co/6p0DXlcwEU,0,5," Abstract: Unsupervised learning is a discipline of machine learning which aims at discovering patterns in big data sets or classifying the data into several categories without being trained explicitly. We show that unsupervised learning techniques can be readily used to identify phases and phases transitions of many body systems. Starting with raw spin configurations of a prototypical Ising model, we use principal component analysis to extract relevant low dimensional representations the original data and use clustering analysis to identify distinct phases in the feature space. This approach successfully finds out physical concepts such as order parameter and structure factor to be indicators of the phase transition. We discuss future prospects of discovering more complex phases and phase transitions using unsupervised learning techniques. "
738168430616973313,2016-06-02 00:40:43,https://t.co/XPQL0rJe4F,Stream Clipper: Scalable Submodular Maximization on Stream. (arXiv:1606.00389v1 [stat.ML]) https://t.co/XPQL0rJe4F,0,2," Abstract: Applying submodular maximization in the streaming setting is nontrivial because the commonly used greedy algorithm exceeds the fixed memory and computational limits typically needed during stream processing. We introduce a new algorithm, called stream clipper, that uses two thresholds to select elements either into a solution set $S$ or an extra buffer $B$. The output is achieved by a greedy algorithm that starts from $S$ and then, if needed, greedily adds elements from $B$. Swapping elements out of $S$ may also be triggered lazily for further improvements, and elements may also be removed from $B$ (and corresponding thresholds adjusted) in order to keep memory use bounded by a constant. Although the worst-case approximation factor does not outperform the previous worst-case of $1/2$, stream clipper can perform better than $1/2$ depending on the order of the elements in the stream. We develop the idea of an ""order complexity"" to characterize orders on which an approximation factor of $1-\alpha$ can be achieved. In news and video summarization tasks, stream clipper significantly outperforms other streaming methods. It shows similar performance to the greedy algorithm but with less computation and memory costs. "
738168428050026498,2016-06-02 00:40:42,https://t.co/ViWkfoPCfA,Short Communication on QUIST: A Quick Clustering Algorithm. (arXiv:1606.00398v1 [cs.LG]) https://t.co/ViWkfoPCfA,0,1," Abstract: In this short communication we introduce the quick clustering algorithm (QUIST), an efficient hierarchical clustering algorithm based on sorting. QUIST is a poly-logarithmic divisive clustering algorithm that does not assume the number of clusters, and/or the cluster size to be known ahead of time. It is also insensitive to the original ordering of the input. "
738168426556882944,2016-06-02 00:40:42,https://t.co/0zAO1Djnpc,Scaling Submodular Maximization via Pruned Submodularity Graphs. (arXiv:1606.00399v1 [cs.LG]) https://t.co/0zAO1Djnpc,1,3," Abstract: We propose a new random pruning method (called ""submodular sparsification (SS)"") to reduce the cost of submodular maximization. The pruning is applied via a ""submodularity graph"" over the $n$ ground elements, where each directed edge is associated with a pairwise dependency defined by the submodular function. In each step, SS prunes a $1-1/\sqrt{c}$ (for $c>1$) fraction of the nodes using weights on edges computed based on only a small number ($O(\log n)$) of randomly sampled nodes. The algorithm requires $\log_{\sqrt{c}}n$ steps with a small and highly parallelizable per-step computation. An accuracy-speed tradeoff parameter $c$, set as $c = 8$, leads to a fast shrink rate $\sqrt{2}/4$ and small iteration complexity $\log_{2\sqrt{2}}n$. Analysis shows that w.h.p., the greedy algorithm on the pruned set of size $O(\log^2 n)$ can achieve a guarantee similar to that of processing the original dataset. In news and video summarization tasks, SS is able to substantially reduce both computational costs and memory usage, while maintaining (or even slightly exceeding) the quality of the original (and much more costly) greedy algorithm. "
738168424753307648,2016-06-02 00:40:41,https://t.co/LTjCabFaDN,Temporal Topic Modeling to Assess Associations between News Trends and Infectious Disease Outbreaks. (arXiv:1606.0… https://t.co/LTjCabFaDN,0,3," Abstract: In retrospective assessments, internet news reports have been shown to capture early reports of unknown infectious disease transmission prior to official laboratory confirmation. In general, media interest and reporting peaks and wanes during the course of an outbreak. In this study, we quantify the extent to which media interest during infectious disease outbreaks is indicative of trends of reported incidence. We introduce an approach that uses supervised temporal topic models to transform large corpora of news articles into temporal topic trends. The key advantages of this approach include, applicability to a wide range of diseases, and ability to capture disease dynamics - including seasonality, abrupt peaks and troughs. We evaluated the method using data from multiple infectious disease outbreaks reported in the United States of America (U.S.), China and India. We noted that temporal topic trends extracted from disease-related news reports successfully captured the dynamics of multiple outbreaks such as whooping cough in U.S. (2012), dengue outbreaks in India (2013) and China (2014). Our observations also suggest that efficient modeling of temporal topic trends using time-series regression techniques can estimate disease case counts with increased precision before official reports by health organizations. "
738168422526160897,2016-06-02 00:40:41,https://t.co/y1wZEZQE9D,The local convexity of solving systems of quadratic equations. (arXiv:1506.07868v5 [math.NA] UPDATED) https://t.co/y1wZEZQE9D,0,1," Abstract: This paper considers the recovery of a rank $r$ positive semidefinite matrix $X X^T\in\mathbb{R}^{n\times n}$ from $m$ scalar measurements of the form $y_i := a_i^T X X^T a_i$ (i.e., quadratic measurements of $X$). Such problems arise in a variety of applications, including covariance sketching of high-dimensional data streams, quadratic regression, quantum state tomography, among others. A natural approach to this problem is to minimize the loss function $f(U) = \sum_i (y_i - a_i^TUU^Ta_i)^2$ which has an entire manifold of solutions given by $\{XO\}_{O\in\mathcal{O}_r}$ where $\mathcal{O}_r$ is the orthogonal group of $r\times r$ orthogonal matrices; this is {\it non-convex} in the $n\times r$ matrix $U$, but methods like gradient descent are simple and easy to implement (as compared to semidefinite relaxation approaches). In this paper we show that once we have $m \geq C nr \log^2(n)$ samples from isotropic gaussian $a_i$, with high probability {\em (a)} this function admits a dimension-independent region of {\em local strong convexity} on lines perpendicular to the solution manifold, and {\em (b)} with an additional polynomial factor of $r$ samples, a simple spectral initialization will land within the region of convexity with high probability. Together, this implies that gradient descent with initialization (but no re-sampling) will converge linearly to the correct $X$, up to an orthogonal transformation. We believe that this general technique (local convexity reachable by spectral initialization) should prove applicable to a broader class of nonconvex optimization problems. "
738168420600971265,2016-06-02 00:40:40,https://t.co/Fv040U2Pku,Black-box $\alpha$-divergence Minimization. (arXiv:1511.03243v3 [stat.ML] UPDATED) https://t.co/Fv040U2Pku,0,3," Abstract: Black-box alpha (BB-$\alpha$) is a new approximate inference method based on the minimization of $\alpha$-divergences. BB-$\alpha$ scales to large datasets because it can be implemented using stochastic gradient descent. BB-$\alpha$ can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients. These gradients can be easily obtained using automatic differentiation. By changing the divergence parameter $\alpha$, the method is able to interpolate between variational Bayes (VB) ($\alpha \rightarrow 0$) and an algorithm similar to expectation propagation (EP) ($\alpha = 1$). Experiments on probit regression and neural network regression and classification problems show that BB-$\alpha$ with non-standard settings of $\alpha$, such as $\alpha = 0.5$, usually produces better predictions than with $\alpha \rightarrow 0$ (VB) or $\alpha = 1$ (EP). "
738168418734510081,2016-06-02 00:40:40,https://t.co/Iw2zsaXI5i,SCOPE: Scalable Composite Optimization for Learning on Spark. (arXiv:1602.00133v3 [stat.ML] UPDATED) https://t.co/Iw2zsaXI5i,0,2," Abstract: Many machine learning models, such as logistic regression~(LR) and support vector machine~(SVM), can be formulated as composite optimization problems. Recently, many distributed stochastic optimization~(DSO) methods have been proposed to solve the large-scale composite optimization problems, which have shown better performance than traditional batch methods. However, most of these DSO methods are not scalable enough. In this paper, we propose a novel DSO method, called \underline{s}calable \underline{c}omposite \underline{op}timization for l\underline{e}arning~({SCOPE}), and implement it on the fault-tolerant distributed platform \mbox{Spark}. SCOPE is both computation-efficient and communication-efficient. Theoretical analysis shows that SCOPE is convergent with linear convergence rate when the objective function is convex. Furthermore, empirical results on real datasets show that SCOPE can outperform other state-of-the-art distributed learning methods on Spark, including both batch learning methods and DSO methods. "
738168416511500289,2016-06-02 00:40:39,https://t.co/Yht1FYse3r,DCM Bandits: Learning to Rank with Multiple Clicks. (arXiv:1602.03146v2 [cs.LG] UPDATED) https://t.co/Yht1FYse3r,0,3," Abstract: A search engine recommends to the user a list of web pages. The user examines this list, from the first page to the last, and clicks on all attractive pages until the user is satisfied. This behavior of the user can be described by the dependent click model (DCM). We propose DCM bandits, an online learning variant of the DCM where the goal is to maximize the probability of recommending satisfactory items, such as web pages. The main challenge of our learning problem is that we do not observe which attractive item is satisfactory. We propose a computationally-efficient learning algorithm for solving our problem, dcmKL-UCB; derive gap-dependent upper bounds on its regret under reasonable assumptions; and also prove a matching lower bound up to logarithmic factors. We evaluate our algorithm on synthetic and real-world problems, and show that it performs well even when our model is misspecified. This work presents the first practical and regret-optimal online algorithm for learning to rank with multiple clicks in a cascade-like click model. "
738168413726466048,2016-06-02 00:40:38,https://t.co/mduZZs6cnu,Optimal Best Arm Identification with Fixed Confidence. (arXiv:1602.04589v2 [math.ST] UPDATED) https://t.co/mduZZs6cnu,0,2," Abstract: We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the `Track-and-Stop' strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis. "
738168410278748161,2016-06-02 00:40:38,https://t.co/a7gC7ojrLU,Variational inference for Monte Carlo objectives. (arXiv:1602.06725v2 [cs.LG] UPDATED) https://t.co/a7gC7ojrLU,0,3," Abstract: Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2016) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multi-sample objectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator proposed for the single-sample variational objective, and is competitive with the currently used biased estimators. "
738168407917367298,2016-06-02 00:40:37,https://t.co/h6SMJ5ffxL,Cost-sensitive Label Embedding for Multi-label Classification. (arXiv:1603.09048v3 [cs.LG] UPDATED) https://t.co/h6SMJ5ffxL,0,3," Abstract: Label embedding (LE) is an important family of multi-label classification algorithms that digest the label information jointly for better performance. Different real-world applications evaluate performance by different cost functions of interest. Current LE algorithms often aim to optimize one specific cost function, but they can suffer from bad performance with respect to other cost functions. In this paper, we resolve the performance issue by proposing a novel cost-sensitive LE algorithm that takes the cost function of interest into account. The proposed algorithm, cost-sensitive label embedding with multidimensional scaling (CLEMS), approximates the cost information with the distances of the embedded vectors using the classic multidimensional scaling approach for manifold learning. CLEMS is able to deal with both symmetric and asymmetric cost functions, and effectively makes cost-sensitive decisions by nearest-neighbor decoding within the embedded vectors. Theoretical results justify that CLEMS achieves the cost-sensitivity and extensive experimental results demonstrate that CLEMS is significantly better than a wide spectrum of existing LE algorithms and state-of-the-art cost-sensitive algorithms across different cost functions. "
738168404893306883,2016-06-02 00:40:36,https://t.co/OrFGN8lxyx,Sub-Gaussian estimators of the mean of a random matrix with heavy-tailed entries. (arXiv:1605.07129v2 [math.ST] UP… https://t.co/OrFGN8lxyx,0,3," Abstract: Estimation of the covariance matrix has attracted a lot of attention of the statistical research community over the years, partially due to important applications such as Principal Component Analysis. However, frequently used empirical covariance estimator (and its modifications) is very sensitive to outliers in the data. As P. J. Huber wrote in 1964, ""...This raises a question which could have been asked already by Gauss, but which was, as far as I know, only raised a few years ago (notably by Tukey): what happens if the true distribution deviates slightly from the assumed normal one? As is now well known, the sample mean then may have a catastrophically bad performance..."" Motivated by this question, we develop a new estimator of the (element-wise) mean of a random matrix, which includes covariance estimation problem as a special case. Assuming that the entries of a matrix possess only finite second moment, this new estimator admits sub-Gaussian or sub-exponential concentration around the unknown mean in the operator norm. We will explain the key ideas behind our construction, as well as applications to covariance estimation and matrix completion problems. "
737805149066956800,2016-06-01 00:37:09,https://t.co/IIIF5ywllf,Scalable and Optimal Generalized Canonical Correlation Analysis via Alternating Optimization. (arXiv:1605.09459v1 … https://t.co/IIIF5ywllf,2,1," Abstract: Generalized canonical correlation analysis (GCCA) aims at finding latent low-dimensional common structure from multiple views (feature vectors in different domains) of the same entities. Unlike principal component analysis (PCA) that handles a single view, (G)CCA is able to integrate information from different feature spaces. Here we focus on MAX-VAR GCCA, a popular formulation which has recently gained renewed interest in multilingual processing and speech modeling. The classic MAX-VAR GCCA problem can be solved optimally via eigen-decomposition of a matrix that compounds the (whitened) correlation matrices of the views; but this solution has serious scalability issues, and is not directly amenable to incorporating pertinent structural constraints such as non-negativity and sparsity on the canonical components. We posit regularized MAX-VAR GCCA as a non-convex optimization problem and propose an alternating optimization (AO)-based algorithm to handle it. Our algorithm alternates between {\em inexact} solutions of a regularized least squares subproblem and a manifold-constrained non-convex subproblem, thereby achieving substantial memory and computational savings. An important benefit of our design is that it can easily handle structure-promoting regularization. We show that the algorithm globally converges to a critical point at a sublinear rate, and approaches a global optimal solution at a linear rate when no regularization is considered. Judiciously designed simulations and large-scale word embedding tasks are employed to showcase the effectiveness of the proposed algorithm. "
737805146663583744,2016-06-01 00:37:09,https://t.co/l35U1npF21,Bayesian optimization under mixed constraints with a slack-variable augmented Lagrangian. (arXiv:1605.09466v1 [sta… https://t.co/l35U1npF21,0,2," Abstract: An augmented Lagrangian (AL) can convert a constrained optimization problem into a sequence of simpler (e.g., unconstrained) problems, which are then usually solved with local solvers. Recently, surrogate-based Bayesian optimization (BO) sub-solvers have been successfully deployed in the AL framework for a more global search in the presence of inequality constraints; however, a drawback was that expected improvement (EI) evaluations relied on Monte Carlo. Here we introduce an alternative slack variable AL, and show that in this formulation the EI may be evaluated with library routines. The slack variables furthermore facilitate equality as well as inequality constraints, and mixtures thereof. We show how our new slack ""ALBO"" compares favorably to the original. Its superiority over conventional alternatives is reinforced on several mixed constraint examples. "
737805143928930304,2016-06-01 00:37:08,https://t.co/pO7ok7zQ3s,A Neural Autoregressive Approach to Collaborative Filtering. (arXiv:1605.09477v1 [cs.IR]) https://t.co/pO7ok7zQ3s,1,4," Abstract: This paper proposes CF-NADE, a neural autoregressive architecture for collaborative filtering (CF) tasks, which is inspired by the Restricted Boltzmann Machine (RBM) based CF model and the Neural Autoregressive Distribution Estimator (NADE). We first describe the basic CF-NADE model for CF tasks. Then we propose to improve the model by sharing parameters between different ratings. A factored version of CF-NADE is also proposed for better scalability. Furthermore, we take the ordinal nature of the preferences into consideration and propose an ordinal cost to optimize CF-NADE, which shows superior performance. Finally, CF-NADE can be extended to a deep model, with only moderately increased computational complexity. Experimental results show that CF-NADE with a single hidden layer beats all previous state-of-the-art methods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more hidden layers can further improve the performance. "
737805137054445568,2016-06-01 00:37:07,https://t.co/t9qEj0M07l,Extreme Stochastic Variational Inference: Distributed and Asynchronous. (arXiv:1605.09499v1 [stat.ML]) https://t.co/t9qEj0M07l,1,8," Abstract: We propose extreme stochastic variational inference (ESVI), which allows multiple processors to simultaneously and asynchronously perform variational inference updates. Moreover, by using a classic owner computes paradigm, our algorithm can be made lock-free. ESVI exhibits data and model parallelism, that is, each processor only needs access to a subset of the data and a subset of the parameters. In our experiments we show that our new algorithm outperforms a straightforward strategy for parallelizing variational inference, which requires bulk synchronization after every iteration. "
737805134705659904,2016-06-01 00:37:06,https://t.co/6251KrR3MM,Kernel Mean Embedding of Distributions: A Review and Beyonds. (arXiv:1605.09522v1 [stat.ML]) https://t.co/6251KrR3MM,5,11," Abstract: A Hilbert space embedding of distributions---in short, kernel mean embedding---has recently emerged as a powerful machinery for probabilistic modeling, statistical inference, machine learning, and causal discovery. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can be extended to probability measures. It gave rise to a great deal of research and novel applications of positive definite kernels. The goal of this survey is to give a comprehensive review of existing works and recent advances in this research area, and to discuss some of the most challenging issues and open problems that could potentially lead to new research directions. The survey begins with a brief introduction to the RKHS and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and review of its applications. The embedding of distributions enables us to apply RKHS methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, group anomaly detection, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way using the new representation of distributions in RKHS. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions. "
737805132574916608,2016-06-01 00:37:06,https://t.co/1nTFieT5dI,Controlling Exploration Improves Training for Deep Neural Networks. (arXiv:1605.09593v1 [cs.LG]) https://t.co/1nTFieT5dI,0,8," Abstract: Stochastic optimization methods are widely used for training of deep neural networks. However, it is still a challenging research problem to achieve effective training by using stochastic optimization methods. This is due to the difficulties in finding good parameters on a loss function that have many saddle points. In this paper, we propose a stochastic optimization method called STDProp for effective training of deep neural networks. Its key idea is to effectively explore parameters on a complex surface of a loss function. We additionally develop momentum version of STDProp. While our approaches are easy to implement with high memory efficiency, it is more effective than other practical stochastic optimization methods for deep neural networks. "
737805130536456192,2016-06-01 00:37:05,https://t.co/Asj1wawoyZ,Horizontally Scalable Submodular Maximization. (arXiv:1605.09619v1 [stat.ML]) https://t.co/Asj1wawoyZ,0,2," Abstract: A variety of large-scale machine learning problems can be cast as instances of constrained submodular maximization. Existing approaches for distributed submodular maximization have a critical drawback: The capacity - number of instances that can fit in memory - must grow with the data set size. In practice, while one can provision many machines, the capacity of each machine is limited by physical constraints. We propose a truly scalable approach for distributed submodular maximization under fixed capacity. The proposed framework applies to a broad class of algorithms and constraints and provides theoretical guarantees on the approximation factor for any available capacity. We empirically evaluate the proposed algorithm on a variety of data sets and demonstrate that it achieves performance competitive with the centralized greedy solution. "
737805127843741696,2016-06-01 00:37:04,https://t.co/fUk1Xm2Dxp,Average-case Hardness of RIP Certification. (arXiv:1605.09646v1 [cs.LG]) https://t.co/fUk1Xm2Dxp,0,3," Abstract: The restricted isometry property (RIP) for design matrices gives guarantees for optimal recovery in sparse linear models. It is of high interest in compressed sensing and statistical learning. This property is particularly important for computationally efficient recovery methods. As a consequence, even though it is in general NP-hard to check that RIP holds, there have been substantial efforts to find tractable proxies for it. These would allow the construction of RIP matrices and the polynomial-time verification of RIP given an arbitrary matrix. We consider the framework of average-case certifiers, that never wrongly declare that a matrix is RIP, while being often correct for random instances. While there are such functions which are tractable in a suboptimal parameter regime, we show that this is a computationally hard task in any better regime. Our results are based on a new, weaker assumption on the problem of detecting dense subgraphs. "
737805125637574657,2016-06-01 00:37:04,https://t.co/JJH5WuDmyB,An Iterative Smoothing Algorithm for Regression with Structured Sparsity. (arXiv:1605.09658v1 [stat.ML]) https://t.co/JJH5WuDmyB,0,3," Abstract: In the context high-dimensionnal predictive models, we consider the problem of optimizing the sum of a smooth convex loss and a non-smooth convex penalty, whose proximal operator is known, and a non-smooth convex structured penalties such as total variation, or overlapping group lasso. We propose to smooth the structured penalty, since it allows a generic framework in which a large range of non-smooth convex structured penalties can be minimized without computing their proximal operators that are either not known or expensive to compute. The problem can be minimized with an accelerated proximal gradient method to benefit of (non-smoothed) sparsity-inducing penalties. We propose an expression of the duality gap to control the convergence of the global non-smooth problem. This expression is applicable to a large range of structured penalties. However, smoothing methods have many limitations that the proposed solver aims to overcome. Therefore, we propose a continuation algorithm, called CONESTA, that dynamically generates a decreasing sequence of smoothing parameters in order to maintain the optimal convergence speed towards any globally desired precision. At each continuation step, the aforementioned duality gap provides the current error and thus the next smaller prescribed precision. Given this precision, we propose a expression to calculate the optimal smoothing parameter, that minimizes the number of iterations to reach such precision. We demonstrate that CONESTA achieves an improved convergence rate compared to classical (without continuation) proximal gradient smoothing. Moreover, experiments conducted on both simulated and high-dimensional neuroimaging (MRI) data, exhibit that CONESTA significantly outperforms the excessive gap method, ADMM, classical proximal gradient smoothing and inexact FISTA in terms of convergence speed and/or precision of the solution. "
737805123519447040,2016-06-01 00:37:03,https://t.co/DpFe0TUwBa,Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks. (arXiv:1605.09674v1 [cs.… https://t.co/DpFe0TUwBa,2,6," Abstract: Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards. "
737805120893820928,2016-06-01 00:37:03,https://t.co/9T5ZYtsuHT,CYCLADES: Conflict-free Asynchronous Machine Learning. (arXiv:1605.09721v1 [stat.ML]) https://t.co/9T5ZYtsuHT,0,4," Abstract: We present CYCLADES, a general framework for parallelizing stochastic optimization algorithms in a shared memory setting. CYCLADES is asynchronous during shared model updates, and requires no memory locking mechanisms, similar to HOGWILD!-type algorithms. Unlike HOGWILD!, CYCLADES introduces no conflicts during the parallel execution, and offers a black-box analysis for provable speedups across a large family of algorithms. Due to its inherent conflict-free nature and cache locality, our multi-core implementation of CYCLADES consistently outperforms HOGWILD!-type algorithms on sufficiently sparse datasets, leading to up to 40% speedup gains compared to the HOGWILD! implementation of SGD, and up to 5x gains over asynchronous implementations of variance reduction algorithms. "
737805118771462148,2016-06-01 00:37:02,https://t.co/rvipAnhxAw,Information Theoretically Aided Reinforcement Learning for Embodied Agents. (arXiv:1605.09735v1 [cs.AI]) https://t.co/rvipAnhxAw,0,2," Abstract: Reinforcement learning for embodied agents is a challenging problem. The accumulated reward to be optimized is often a very rugged function, and gradient methods are impaired by many local optimizers. We demonstrate, in an experimental setting, that incorporating an intrinsic reward can smoothen the optimization landscape while preserving the global optimizers of interest. We show that policy gradient optimization for locomotion in a complex morphology is significantly improved when supplementing the extrinsic reward by an intrinsic reward defined in terms of the mutual information of time consecutive sensor readings. "
737805116548517888,2016-06-01 00:37:02,https://t.co/6tuwNHnWqP,"Asynchrony begets Momentum, with an Application to Deep Learning. (arXiv:1605.09774v1 [stat.ML]) https://t.co/6tuwNHnWqP",0,5," Abstract: Asynchronous methods are widely used in deep learning, but have limited theoretical justification when applied to non-convex problems. We show that running stochastic gradient descent (SGD) in an asynchronous manner can be viewed as adding a momentum-like term to the SGD iteration. Our result does not assume convexity of the objective function, so it is applicable to deep learning systems. We observe that a standard queuing model of asynchrony results in a form of momentum that is commonly used by deep learning practitioners. This forges a link between queuing theory and asynchrony in deep learning systems, which could be useful for systems builders. For convolutional neural networks, we experimentally validate that the degree of asynchrony directly correlates with the momentum, confirming our main result. An important implication is that tuning the momentum parameter is important when considering different levels of asynchrony. We assert that properly tuned momentum reduces the number of steps required for convergence. Finally, our theory suggests new ways of counteracting the adverse effects of asynchrony: a simple mechanism like using negative algorithmic momentum can improve performance under high asynchrony. Since asynchronous methods have better hardware efficiency, this result may shed light on when asynchronous execution is more efficient for deep learning systems. "
737805114539429888,2016-06-01 00:37:01,https://t.co/BImJXOQkkM,Adversarial Feature Learning. (arXiv:1605.09782v1 [cs.LG]) https://t.co/BImJXOQkkM,1,15," Abstract: The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to ""linearize semantics"" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning. "
737805111729262593,2016-06-01 00:37:01,https://t.co/30CwbzZ3X7,New insights and perspectives on the natural gradient method. (arXiv:1412.1193v7 [cs.LG] UPDATED) https://t.co/30CwbzZ3X7,0,13," Abstract: Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of approximate 2nd-order optimization method, where the Fisher information matrix used to compute the natural gradient direction can be viewed as an approximation of the Hessian. This perspective turns out to have significant implications for how to design a practical and robust version of the method. Among our various other contributions is a thorough analysis of the convergence speed of natural gradient descent and more general stochastic methods, a critical examination of the oft-used ""empirical"" approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by the method, which we show still holds for certain other choices of the curvature matrix, but notably not the Hessian. "
737805108801642496,2016-06-01 00:37:00,https://t.co/H8W785HKTi,Collaborative Filtering Bandits. (arXiv:1502.03473v7 [cs.LG] UPDATED) https://t.co/H8W785HKTi,0,4," Abstract: Classical collaborative filtering, and content-based filtering methods try to learn a static recommendation model given training data. These approaches are far from ideal in highly dynamic recommendation domains such as news recommendation and computational advertisement, where the set of items and users is very fluid. In this work, we investigate an adaptive clustering technique for content recommendation based on exploration-exploitation strategies in contextual multi-armed bandit settings. Our algorithm takes into account the collaborative effects that arise due to the interaction of the users with the items, by dynamically grouping users based on the items under consideration and, at the same time, grouping items based on the similarity of the clusterings induced over the users. The resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods. We provide an empirical analysis on medium-size real-world datasets, showing scalability and increased prediction performance (as measured by click-through rate) over state-of-the-art methods for clustering bandits. We also provide a regret analysis within a standard linear stochastic noise setting. "
737805106209525760,2016-06-01 00:36:59,https://t.co/FxJ6JSgKFl,Coordinate Descent Methods for Symmetric Nonnegative Matrix Factorization. (arXiv:1509.01404v2 [cs.NA] UPDATED) https://t.co/FxJ6JSgKFl,0,5," Abstract: Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrix factorization (symNMF) is the problem of finding a nonnegative matrix $H$, usually with much fewer columns than $A$, such that $A \approx HH^T$. SymNMF can be used for data analysis and in particular for various clustering tasks. In this paper, we propose simple and very efficient coordinate descent schemes to solve this problem, and that can handle large and sparse input matrices. The effectiveness of our methods is illustrated on synthetic and real-world data sets, and we show that they perform favorably compared to recent state-of-the-art methods. "
737805103609090049,2016-06-01 00:36:59,https://t.co/3ojUhzyHZx,Hierarchical Variational Models. (arXiv:1511.02386v2 [stat.ML] UPDATED) https://t.co/3ojUhzyHZx,1,4," Abstract: Black box variational inference allows researchers to easily prototype and evaluate an array of models. Recent advances allow such algorithms to scale to high dimensions. However, a central question remains: How to specify an expressive variational distribution that maintains efficient computation? To address this, we develop hierarchical variational models (HVMs). HVMs augment a variational approximation with a prior on its parameters, which allows it to capture complex structure for both discrete and continuous latent variables. The algorithm we develop is black box, can be used for any HVM, and has the same computational efficiency as the original approximation. We study HVMs on a variety of deep discrete latent variable models. HVMs generalize other expressive variational distributions and maintains higher fidelity to the posterior. "
737805101646151680,2016-06-01 00:36:58,https://t.co/GjVql1bZwq,Mixture Proportion Estimation via Kernel Embedding of Distributions. (arXiv:1603.02501v2 [cs.LG] UPDATED) https://t.co/GjVql1bZwq,0,3," Abstract: Mixture proportion estimation (MPE) is the problem of estimating the weight of a component distribution in a mixture, given samples from the mixture and component. This problem constitutes a key part in many ""weakly supervised learning"" problems like learning with positive and unlabelled samples, learning with label noise, anomaly detection and crowdsourcing. While there have been several methods proposed to solve this problem, to the best of our knowledge no efficient algorithm with a proven convergence rate towards the true proportion exists for this problem. We fill this gap by constructing a provably correct algorithm for MPE, and derive convergence rates under certain assumptions on the distribution. Our method is based on embedding distributions onto an RKHS, and implementing it only requires solving a simple convex quadratic programming problem a few times. We run our algorithm on several standard classification datasets, and demonstrate that it performs comparably to or better than other algorithms on most datasets. "
737805099070820352,2016-06-01 00:36:58,https://t.co/PMeluM7bmO,Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation. (arXiv:1604.0… https://t.co/PMeluM7bmO,1,13," Abstract: Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'. "
737805092762624000,2016-06-01 00:36:56,https://t.co/miQsYfbkJL,Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs. (arXiv:1605.09346v1 [cs.LG] CROSS LISTED) https://t.co/miQsYfbkJL,0,4," Abstract: In this paper, we propose several improvements on the block-coordinate Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to optimize the structured support vector machine (SSVM) objective in the context of structured prediction, though it has wider applications. The key intuition behind our improvements is that the estimates of block gaps maintained by BCFW reveal the block suboptimality that can be used as an adaptive criterion. First, we sample objects at each iteration of BCFW in an adaptive non-uniform way via gapbased sampling. Second, we incorporate pairwise and away-step variants of Frank-Wolfe into the block-coordinate setting. Third, we cache oracle calls with a cache-hit criterion based on the block gaps. Fourth, we provide the first method to compute an approximate regularization path for SSVM. Finally, we provide an exhaustive empirical evaluation of all our methods on four structured prediction datasets. "
737447251069505536,2016-05-31 00:55:00,https://t.co/Eupogf2BmW,Asymptotic Analysis of Objectives based on Fisher Information in Active Learning. (arXiv:1605.08798v1 [stat.ML]) https://t.co/Eupogf2BmW,0,9," Abstract: Obtaining labels can be costly and time-consuming. Active learning allows a learning algorithm to intelligently query samples to be labeled for efficient learning. Fisher information ratio (FIR) has been used as an objective for selecting queries in active learning. However, little is known about the theory behind the use of FIR for active learning. There is a gap between the underlying theory and the motivation of its usage in practice. In this paper, we attempt to fill this gap and provide a rigorous framework for analyzing existing FIR-based active learning methods. In particular, we show that FIR can be asymptotically viewed as an upper bound of the expected variance of the log-likelihood ratio. Additionally, our analysis suggests a unifying framework that not only enables us to make theoretical comparisons among the existing querying methods based on FIR, but also allows us to give insight into the development of new active learning approaches based on this objective. "
737447249232461824,2016-05-31 00:54:59,https://t.co/CEupEKak98,Muffled Semi-Supervised Learning. (arXiv:1605.08833v1 [cs.LG]) https://t.co/CEupEKak98,0,1," Abstract: We explore a novel approach to semi-supervised learning. This approach is contrary to the common approach in that the unlabeled examples serve to ""muffle,"" rather than enhance, the guidance provided by the labeled examples. We provide several variants of the basic algorithm and show experimentally that they can achieve significantly higher AUC than boosted trees, random forests and logistic regression when unlabeled examples are available. "
737447246862639104,2016-05-31 00:54:59,https://t.co/jkNsY98b8j,Optimal Learning for Multi-pass Stochastic Gradient Methods. (arXiv:1605.08882v1 [cs.LG]) https://t.co/jkNsY98b8j,0,4," Abstract: We analyze the learning properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. In particular, we consider the square loss and show that for a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on a unifying approach, encompassing both batch and stochastic gradient methods as special cases. "
737447244186652672,2016-05-31 00:54:58,https://t.co/aDdyk2S8ai,Interaction Pursuit with Feature Screening and Selection. (arXiv:1605.08933v1 [stat.ME]) https://t.co/aDdyk2S8ai,0,1," Abstract: Understanding how features interact with each other is of paramount importance in many scientific discoveries and contemporary applications. Yet interaction identification becomes challenging even for a moderate number of covariates. In this paper, we suggest an efficient and flexible procedure, called the interaction pursuit (IP), for interaction identification in ultra-high dimensions. The suggested method first reduces the number of interactions and main effects to a moderate scale by a new feature screening approach, and then selects important interactions and main effects in the reduced feature space using regularization methods. Compared to existing approaches, our method screens interactions separately from main effects and thus can be more effective in interaction screening. Under a fairly general framework, we establish that for both interactions and main effects, the method enjoys the sure screening property in screening and oracle inequalities in selection. Our method and theoretical results are supported by several simulation and real data examples. "
737447241942732800,2016-05-31 00:54:58,https://t.co/F9zfX637lZ,A simple and provable algorithm for sparse diagonal CCA. (arXiv:1605.08961v1 [stat.ML]) https://t.co/F9zfX637lZ,0,3," Abstract: Given two sets of variables, derived from a common set of samples, sparse Canonical Correlation Analysis (CCA) seeks linear combinations of a small number of variables in each set, such that the induced canonical variables are maximally correlated. Sparse CCA is NP-hard. We propose a novel combinatorial algorithm for sparse diagonal CCA, i.e., sparse CCA under the additional assumption that variables within each set are standardized and uncorrelated. Our algorithm operates on a low rank approximation of the input data and its computational complexity scales linearly with the number of input variables. It is simple to implement, and parallelizable. In contrast to most existing approaches, our algorithm administers precise control on the sparsity of the extracted canonical vectors, and comes with theoretical data-dependent global approximation guarantees, that hinge on the spectrum of the input data. Finally, it can be straightforwardly adapted to other constrained variants of CCA enforcing structure beyond sparsity. We empirically evaluate the proposed scheme and apply it on a real neuroimaging dataset to investigate associations between brain activity and behavior measurements. "
737447239824609280,2016-05-31 00:54:57,https://t.co/Ne69Sr1XXL,Tight (Lower) Bounds for the Fixed Budget Best Arm Identification Bandit Problem. (arXiv:1605.09004v1 [stat.ML]) https://t.co/Ne69Sr1XXL,0,3," Abstract: We consider the problem of \textit{best arm identification} with a \textit{fixed budget $T$}, in the $K$-armed stochastic bandit setting, with arms distribution defined on $[0,1]$. We prove that any bandit strategy, for at least one bandit problem characterized by a complexity $H$, will misidentify the best arm with probability lower bounded by $$\exp\Big(-\frac{T}{\log(K)H}\Big),$$ where $H$ is the sum for all sub-optimal arms of the inverse of the squared gaps. Our result disproves formally the general belief - coming from results in the fixed confidence setting - that there must exist an algorithm for this problem whose probability of error is upper bounded by $\exp(-T/H)$. This also proves that some existing strategies based on the Successive Rejection of the arms are optimal - closing therefore the current gap between upper and lower bounds for the fixed budget best arm identification problem. "
737447236414640128,2016-05-31 00:54:56,https://t.co/RfY0DOUkyO,Singular ridge regression with homoscedastic residuals: generalization error with estimated parameters. (arXiv:160… https://t.co/RfY0DOUkyO,0,0," Abstract: This paper characterizes the conditional distribution properties of the finite sample ridge regression estimator and uses that result to evaluate total regression and generalization errors that incorporate the inaccuracies committed at the time of parameter estimation. The paper provides explicit formulas for those errors. Unlike other classical references in this setup, our results take place in a fully singular setup that does not assume the existence of a solution for the non-regularized regression problem. In exchange, we invoke a conditional homoscedasticity hypothesis on the regularized regression residuals that is crucial in our developments. "
737447233793183744,2016-05-31 00:54:56,https://t.co/gBBKrymm6c,MCMC assisted by Belief Propagaion. (arXiv:1605.09042v1 [stat.ML]) https://t.co/gBBKrymm6c,1,4," Abstract: Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most popular algorithms for computational inference in Graphical Models (GM). In principle, MCMC is an exact probabilistic method which, however, often suffers from exponentially slow mixing. In contrast, BP is a deterministic method, which is typically fast, empirically very successful, however in general lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC algorithms correcting the approximation error of BP, i.e., we provide a way to compensate for BP errors via a consecutive BP-aware MCMC. Our framework is based on the Loop Calculus (LC) approach which allows to express the BP error as a sum of weighted generalized loops. Although the full series is computationally intractable, it is known that a truncated series, summing up all 2-regular loops, is computable in polynomial-time for planar pair-wise binary GMs and it also provides a highly accurate approximation empirically. Motivated by this, we first propose a polynomial-time approximation MCMC scheme for the truncated series of general (non-planar) pair-wise binary models. Our main idea here is to use the Worm algorithm, known to provide fast mixing in other (related) problems, and then design an appropriate rejection scheme to sample 2-regular loops. Furthermore, we also design an efficient rejection-free MCMC scheme for approximating the full series. The main novelty underlying our design is in utilizing the concept of cycle basis, which provides an efficient decomposition of the generalized loops. In essence, the proposed MCMC schemes run on transformed GM built upon the non-trivial BP solution, and our experiments show that this synthesis of BP and MCMC outperforms both direct MCMC and bare BP schemes. "
737447231217905664,2016-05-31 00:54:55,https://t.co/Am7WsaTe8F,TripleSpin - a generic compact paradigm for fast machine learning computations. (arXiv:1605.09046v1 [cs.LG]) https://t.co/Am7WsaTe8F,1,1," Abstract: We present a generic compact computational framework relying on structured random matrices that can be applied to speed up several machine learning algorithms with almost no loss of accuracy. The applications include new fast LSH-based algorithms, efficient kernel computations via random feature maps, convex optimization algorithms, quantization techniques and many more. Certain models of the presented paradigm are even more compressible since they apply only bit matrices. This makes them suitable for deploying on mobile devices. All our findings come with strong theoretical guarantees. In particular, as a byproduct of the presented techniques and by using relatively new Berry-Esseen-type CLT for random vectors, we give the first theoretical guarantees for one of the most efficient existing LSH algorithms based on the $\textbf{HD}_{3}\textbf{HD}_{2}\textbf{HD}_{1}$ structured matrix (""Practical and Optimal LSH for Angular Distance""). These guarantees as well as theoretical results for other aforementioned applications follow from the same general theoretical principle that we present in the paper. Our structured family contains as special cases all previously considered structured schemes, including the recently introduced $P$-model. Experimental evaluation confirms the accuracy and efficiency of TripleSpin matrices. "
737447228344807424,2016-05-31 00:54:55,https://t.co/ghkX2s1Yyn,Recycling Randomness with Structure for Sublinear time Kernel Expansions. (arXiv:1605.09049v1 [cs.LG]) https://t.co/ghkX2s1Yyn,0,0," Abstract: We propose a scheme for recycling Gaussian random vectors into structured matrices to approximate various kernel functions in sublinear time via random embeddings. Our framework includes the Fastfood construction as a special case, but also extends to Circulant, Toeplitz and Hankel matrices, and the broader family of structured matrices that are characterized by the concept of low-displacement rank. We introduce notions of coherence and graph-theoretic structural constants that control the approximation quality, and prove unbiasedness and low-variance properties of random feature maps that arise within our framework. For the case of low-displacement matrices, we show how the degree of structure and randomness can be controlled to reduce statistical variance at the cost of increased computation and storage requirements. Empirical results strongly support our theory and justify the use of a broader family of structured matrices for scaling up kernel methods using random features. "
737447225622712320,2016-05-31 00:54:54,https://t.co/8dSTCfWY2i,A budget-constrained inverse classification framework for smooth classifiers. (arXiv:1605.09068v1 [cs.LG]) https://t.co/8dSTCfWY2i,0,2," Abstract: Inverse classification is the process of manipulating an instance such that it is more likely to conform to a specific class. Past methods that address such a problem have shortcomings. Greedy methods make changes that are overly radical, often relying on data that is strictly discrete. Other methods rely on certain data points, the presence of which cannot be guaranteed. In this paper we propose a general framework and method that overcomes these and other limitations. The formulation of our method uses any differentiable classification function. We demonstrate the method by using Gaussian kernel SVMs. We constrain the inverse classification to occur on features that can actually be changed, each of which incurs an individual cost. We further subject such changes to fall within a certain level of cumulative change (budget). Our framework can also accommodate the estimation of features whose values change as a consequence of actions taken (indirectly changeable features). Furthermore, we propose two methods for specifying feature-value ranges that result in different algorithmic behavior. We apply our method, and a proposed sensitivity analysis-based benchmark method, to two freely available datasets: Student Performance, from the UCI Machine Learning Repository and a real-world cardiovascular disease dataset. The results obtained demonstrate the validity and benefits of our framework and method. "
737447223915646980,2016-05-31 00:54:53,https://t.co/ogJkmZIPPD,Beyond LDA: A Unified Framework for Learning Latent Normalized Infinitely Divisible Topic Models through Spectral … https://t.co/ogJkmZIPPD,1,6," Abstract: In this paper, we propose guaranteed spectral methods for learning a broad range of topic models, which generalize the popular Latent Dirichlet Allocation (LDA). We overcome the limitation of LDA to incorporate arbitrary topic correlations, by assuming that the hidden topic proportions are drawn from a flexible class of Normalized Infinitely Divisible (NID) distributions. NID distributions are generated through the process of normalizing a family of independent Infinitely Divisible (ID) random variables. The Dirichlet distribution is a special case obtained by normalizing a set of Gamma random variables. We prove that this flexible topic model class can be learned via spectral methods using only moments up to the third order, with (low order) polynomial sample and computational complexity. The proof is based on a key new technique derived here that allows us to diagonalize the moments of the NID distribution through an efficient procedure that requires evaluating only univariate integrals, despite the fact that we are handling high dimensional multivariate moments. In order to assess the performance of our proposed Latent NID topic model, we use two real datasets of articles collected from New York Times and Pubmed. Our experiments yield improved perplexity on both datasets compared with the baseline. "
737447221172572161,2016-05-31 00:54:53,https://t.co/e9jqFANgDi,Understanding Convolutional Neural Networks. (arXiv:1605.09081v1 [stat.ML]) https://t.co/e9jqFANgDi,4,14," Abstract: Convoulutional Neural Networks (CNNs) exhibit extraordinary performance on a variety of machine learning tasks. However, their mathematical properties and behavior are quite poorly understood. There is some work, in the form of a framework, for analyzing the operations that they perform. The goal of this project is to present key results from this theory, and provide intuition for why CNNs work. "
737447218723049472,2016-05-31 00:54:52,https://t.co/niLCX2VwBB,Stochastic Function Norm Regularization of Deep Networks. (arXiv:1605.09085v1 [cs.LG]) https://t.co/niLCX2VwBB,0,7," Abstract: Deep neural networks have had an enormous impact on image analysis. State-of-the-art training methods, based on weight decay and DropOut, result in impressive performance when a very large training set is available. However, they tend to have large problems overfitting to small data sets. Indeed, the available regularization methods deal with the complexity of the network function only indirectly. In this paper, we study the feasibility of directly using the $L_2$ function norm for regularization. Two methods to integrate this new regularization in the stochastic backpropagation are proposed. Moreover, the convergence of these new algorithms is studied. We finally show that they outperform the state-of-the-art methods in the low sample regime on benchmark datasets (MNIST and CIFAR10). The obtained results demonstrate very clear improvement, especially in the context of small sample regimes with data laying in a low dimensional manifold. Source code of the method can be found at \url{this https URL}. "
737447216927911936,2016-05-31 00:54:52,https://t.co/ePPfLMN0Cj,"ParMAC: distributed optimisation of nested functions, with application to learning binary autoencoders. (arXiv:160… https://t.co/ePPfLMN0Cj",0,1," Abstract: Many powerful machine learning models are based on the composition of multiple processing layers, such as deep nets, which gives rise to nonconvex objective functions. A general, recent approach to optimise such ""nested"" functions is the method of auxiliary coordinates (MAC). MAC introduces an auxiliary coordinate for each data point in order to decouple the nested model into independent submodels. This decomposes the optimisation into steps that alternate between training single layers and updating the coordinates. It has the advantage that it reuses existing single-layer algorithms, introduces parallelism, and does not need to use chain-rule gradients, so it works with nondifferentiable layers. With large-scale problems, or when distributing the computation is necessary for faster training, the dataset may not fit in a single machine. It is then essential to limit the amount of communication between machines so it does not obliterate the benefit of parallelism. We describe a general way to achieve this, ParMAC. ParMAC works on a cluster of processing machines with a circular topology and alternates two steps until convergence: one step trains the submodels in parallel using stochastic updates, and the other trains the coordinates in parallel. Only submodel parameters, no data or coordinates, are ever communicated between machines. ParMAC exhibits high parallelism, low communication overhead, and facilitates data shuffling, load balancing, fault tolerance and streaming data processing. We study the convergence of ParMAC and propose a theoretical model of its runtime and parallel speedup. We develop ParMAC to learn binary autoencoders for fast, approximate image retrieval. We implement it in MPI in a distributed system and demonstrate nearly perfect speedups in a 128-processor cluster with a training set of 100 million high-dimensional points. "
737447214662946816,2016-05-31 00:54:51,https://t.co/OYpPbMSWCs,Hyperspectral Image Classification with Support Vector Machines on Kernel Distribution Embeddings. (arXiv:1605.091… https://t.co/OYpPbMSWCs,0,3," Abstract: We propose a novel approach for pixel classification in hyperspectral images, leveraging on both the spatial and spectral information in the data. The introduced method relies on a recently proposed framework for learning on distributions -- by representing them with mean elements in reproducing kernel Hilbert spaces (RKHS) and formulating a classification algorithm therein. In particular, we associate each pixel to an empirical distribution of its neighbouring pixels, a judicious representation of which in an RKHS, in conjunction with the spectral information contained in the pixel itself, give a new explicit set of features that can be fed into a suite of standard classification techniques -- we opt for a well-established framework of support vector machines (SVM). Furthermore, the computational complexity is reduced via random Fourier features formalism. We study the consistency and the convergence rates of the proposed method and the experiments demonstrate strong performance on hyperspectral data with gains in comparison to the state-of-the-art results. "
737447212830105600,2016-05-31 00:54:51,https://t.co/DbIo9EX9TV,Forest Floor Visualizations of Random Forests. (arXiv:1605.09196v1 [stat.ML]) https://t.co/DbIo9EX9TV,0,3," Abstract: We propose a novel methodology, forest floor, to visualize and interpret random forest (RF) models. RF is a popular and useful tool for non-linear multi-variate classification and regression, which yields a good trade-off between robustness (low variance) and adaptiveness (low bias). Direct interpretation of a RF model is difficult, as the explicit ensemble model of hundreds of deep trees is complex. Nonetheless, it is possible to visualize a RF model fit by its mapping from feature space to prediction space. Hereby the user is first presented with the overall geometrical shape of the model structure, and when needed one can zoom in on local details. Dimensional reduction by projection is used to visualize high dimensional shapes. The traditional method to visualize RF model structure, partial dependence plots, achieve this by averaging multiple parallel projections. We suggest to first use feature contributions, a method to decompose trees by splitting features, and then subsequently perform projections. The advantages of forest floor over partial dependence plots is that interactions are not masked by averaging. As a consequence, it is possible to locate interactions, which are not visualized in a given projection. Furthermore, we introduce: a goodness-of-visualization measure, use of colour gradients to identify interactions and an out-of-bag cross validated variant of feature contributions. "
737447209873117184,2016-05-31 00:54:50,https://t.co/dKNe0GclLD,Tradeoffs between Convergence Speed and Reconstruction Accuracy in Inverse Problems. (arXiv:1605.09232v1 [cs.NA]) https://t.co/dKNe0GclLD,1,4," Abstract: Solving inverse problems with iterative algorithms such as stochastic gradient descent is a popular technique, especially for large data. In applications, due to time constraints, the number of iterations one may apply is usually limited, consequently limiting the accuracy achievable by certain methods. Given a reconstruction error one is willing to tolerate, an important question is whether it is possible to modify the original iterations to obtain a faster convergence to a minimizer with the allowed error. Relying on recent recovery techniques developed for settings in which the desired signal belongs to some low-dimensional set, we show that using a coarse estimate of this set leads to faster convergence to an error related to the accuracy of the set approximation. Our theory ties to recent advances in sparse recovery, compressed sensing and deep learning. In particular, it provides an explanation for the successful approximation of the ISTA solution by neural networks with layers representing iterations. "
737447207637516288,2016-05-31 00:54:50,https://t.co/G57qOeyHmx,Unsupervised Discovery of El Nino Using Causal Feature Learning on Microlevel Climate Data. (arXiv:1605.09370v1 [s… https://t.co/G57qOeyHmx,0,2," Abstract: We show that the climate phenomena of El Nino and La Nina arise naturally as states of macro-variables when our recent causal feature learning framework (Chalupka 2015, Chalupka 2016) is applied to micro-level measures of zonal wind (ZW) and sea surface temperatures (SST) taken over the equatorial band of the Pacific Ocean. The method identifies these unusual climate states on the basis of the relation between ZW and SST patterns without any input about past occurrences of El Nino or La Nina. The simpler alternatives of (i) clustering the SST fields while disregarding their relationship with ZW patterns, or (ii) clustering the joint ZW-SST patterns, do not discover El Nino. We discuss the degree to which our method supports a causal interpretation and use a low-dimensional toy example to explain its success over other clustering approaches. Finally, we propose a new robust and scalable alternative to our original algorithm (Chalupka 2016), which circumvents the need for high-dimensional density learning. "
737447205720739842,2016-05-31 00:54:49,https://t.co/uEkNYMED7R,Minimax Optimal Convergence Rates for Estimating Ground Truth from Crowdsourced Labels. (arXiv:1310.5764v6 [stat.M… https://t.co/uEkNYMED7R,0,2," Abstract: Crowdsourcing has become a primary means for label collection in many real-world machine learning applications. A classical method for inferring the true labels from the noisy labels provided by crowdsourcing workers is Dawid-Skene estimator. In this paper, we prove convergence rates of a projected EM algorithm for the Dawid-Skene estimator. The revealed exponent in the rate of convergence is shown to be optimal via a lower bound argument. Our work resolves the long standing issue of whether Dawid-Skene estimator has sound theoretical guarantees besides its good performance observed in practice. In addition, a comparative study with majority voting illustrates both advantages and pitfalls of the Dawid-Skene estimator. "
737447198015803392,2016-05-31 00:54:47,https://t.co/jNFNw1k9pV,Variational Tempering. (arXiv:1411.1810v4 [stat.ML] UPDATED) https://t.co/jNFNw1k9pV,1,5," Abstract: Variational inference (VI) combined with data subsampling enables approximate posterior inference over large data sets, but suffers from poor local optima. We first formulate a deterministic annealing approach for the generic class of conditionally conjugate exponential family models. This approach uses a decreasing temperature parameter which deterministically deforms the objective during the course of the optimization. A well-known drawback to this annealing approach is the choice of the cooling schedule. We therefore introduce variational tempering, a variational algorithm that introduces a temperature latent variable to the model. In contrast to related work in the Markov chain Monte Carlo literature, this algorithm results in adaptive annealing schedules. Lastly, we develop local variational tempering, which assigns a latent temperature to each data point; this allows for dynamic annealing that varies across data. Compared to the traditional VI, all proposed approaches find improved predictive likelihoods on held-out data. "
737447195805372417,2016-05-31 00:54:47,https://t.co/s0nsZtS7eo,A New Approach to Building the Interindustry Input--Output Table. (arXiv:1504.01362v7 [stat.ML] UPDATED) https://t.co/s0nsZtS7eo,0,1," Abstract: We present a new approach to estimating the interdependence of industries in an economy by applying data science solutions. By exploiting interfirm buyer--seller network data, we show that the problem of estimating the interdependence of industries is similar to the problem of uncovering the latent block structure in network science literature. To estimate the underlying structure with greater accuracy, we propose an extension of the sparse block model that incorporates node textual information and an unbounded number of industries and interactions among them. The latter task is accomplished by extending the well-known Chinese restaurant process to two dimensions. Inference is based on collapsed Gibbs sampling, and the model is evaluated on both synthetic and real-world datasets. We show that the proposed model improves in predictive accuracy and successfully provides a satisfactory solution to the motivated problem. We also discuss issues that affect the future performance of this approach. "
737447193196560384,2016-05-31 00:54:46,https://t.co/njUoVclOkF,Stagewise Learning for Sparse Clustering of Discretely-Valued Data. (arXiv:1506.02975v2 [stat.ML] UPDATED) https://t.co/njUoVclOkF,0,3," Abstract: The performance of EM in learning mixtures of product distributions often depends on the initialization. This can be problematic in crowdsourcing and other applications, e.g. when a small number of 'experts' are diluted by a large number of noisy, unreliable participants. We develop a new EM algorithm that is driven by these experts. In a manner that differs from other approaches, we start from a single mixture class. The algorithm then develops the set of 'experts' in a stagewise fashion based on a mutual information criterion. At each stage EM operates on this subset of the players, effectively regularizing the E rather than the M step. Experiments show that stagewise EM outperforms other initialization techniques for crowdsourcing and neurosciences applications, and can guide a full EM to results comparable to those obtained knowing the exact distribution. "
737447190403108864,2016-05-31 00:54:45,https://t.co/TmY91BRmnG,Robust Gaussian Filtering using a Pseudo Measurement. (arXiv:1509.04072v3 [stat.ML] UPDATED) https://t.co/TmY91BRmnG,0,1," Abstract: Many sensors, such as range, sonar, radar, GPS and visual devices, produce measurements which are contaminated by outliers. This problem can be addressed by using fat-tailed sensor models, which account for the possibility of outliers. Unfortunately, all estimation algorithms belonging to the family of Gaussian filters (such as the widely-used extended Kalman filter and unscented Kalman filter) are inherently incompatible with such fat-tailed sensor models. The contribution of this paper is to show that any Gaussian filter can be made compatible with fat-tailed sensor models by applying one simple change: Instead of filtering with the physical measurement, we propose to filter with a pseudo measurement obtained by applying a feature function to the physical measurement. We derive such a feature function which is optimal under some conditions. Simulation results show that the proposed method can effectively handle measurement outliers and allows for robust filtering in both linear and nonlinear systems. "
737447188175958016,2016-05-31 00:54:45,https://t.co/xq0NF1SCZb,Spatial Semantic Scan: Jointly Detecting Subtle Events and their Spatial Footprint. (arXiv:1511.00352v3 [cs.LG] UP… https://t.co/xq0NF1SCZb,0,1," Abstract: Many methods have been proposed for detecting emerging events in text streams using topic modeling. However, these methods have shortcomings that make them unsuitable for rapid detection of locally emerging events on massive text streams. We describe Spatially Compact Semantic Scan (SCSS) that has been developed specifically to overcome the shortcomings of current methods in detecting new spatially compact events in text streams. SCSS employs alternating optimization between using semantic scan to estimate contrastive foreground topics in documents, and discovering spatial neighborhoods with high occurrence of documents containing the foreground topics. We evaluate our method on Emergency Department chief complaints dataset (ED dataset) to verify the effectiveness of our method in detecting real-world disease outbreaks from free-text ED chief complaint data. "
737447186229809152,2016-05-31 00:54:44,https://t.co/R7L4Y869YY,Regret Analysis of the Finite-Horizon Gittins Index Strategy for Multi-Armed Bandits. (arXiv:1511.06014v3 [cs.LG] … https://t.co/R7L4Y869YY,0,3, Abstract: I analyse the frequentist regret of the famous Gittins index strategy for multi-armed bandits with Gaussian noise and a finite horizon. Remarkably it turns out that this approach leads to finite-time regret guarantees comparable to those available for the popular UCB algorithm. Along the way I derive finite-time bounds on the Gittins index that are asymptotically exact and may be of independent interest. I also discuss some computational issues and present experimental results suggesting that a particular version of the Gittins index strategy is a modest improvement on existing algorithms with finite-time regret guarantees such as UCB and Thompson sampling. 
737447183390265345,2016-05-31 00:54:44,https://t.co/tK7nE8xgDd,Gauss quadrature for matrix inverse forms with applications. (arXiv:1512.01904v2 [stat.ML] UPDATED) https://t.co/tK7nE8xgDd,0,1," Abstract: We present a framework for accelerating a spectrum of machine learning algorithms that require computation of bilinear inverse forms $u^\top A^{-1}u$, where $A$ is a positive definite matrix and $u$ a given vector. Our framework is built on Gauss-type quadrature and easily scales to large, sparse matrices. Further, it allows retrospective computation of lower and upper bounds on $u^\top A^{-1}u$, which in turn accelerates several algorithms. We prove that these bounds tighten iteratively and converge at a linear (geometric) rate. To our knowledge, ours is the first work to demonstrate these key properties of Gauss-type quadrature, which is a classical and deeply studied topic. We illustrate empirical consequences of our results by using quadrature to accelerate machine learning tasks involving determinantal point processes and submodular optimization, and observe tremendous speedups in several instances. "
737447180781400064,2016-05-31 00:54:43,https://t.co/GE2JONerdB,Value Iteration Networks. (arXiv:1602.02867v2 [cs.AI] UPDATED) https://t.co/GE2JONerdB,0,2," Abstract: We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains. "
737447178042544129,2016-05-31 00:54:43,https://t.co/UZ6pPlB5ZV,Fast Mini-Batch K-Means by Nesting. (arXiv:1602.02934v3 [stat.ML] UPDATED) https://t.co/UZ6pPlB5ZV,0,4," Abstract: A new algorithm is proposed which accelerates the mini-batch k-means algorithm of Sculley (2010) by using the distance bounding approach of Elkan (2003). We argue that, when incorporating distance bounds into a mini-batch algorithm, already used data should preferentially be reused. To this end we propose using nested mini-batches, whereby data in a mini-batch at iteration t is automatically reused at iteration t+1. Using nested mini-batches presents two difficulties. The first is that unbalanced use of data can bias estimates, which we resolve by ensuring that each data sample contributes exactly once to centroids. The second is in choosing mini-batch sizes, which we address by balancing premature fine-tuning of centroids with redundancy induced slow-down. Experiments show that the resulting nmbatch algorithm is very effective, often arriving within 1% of the empirical minimum 100 times earlier than the standard mini-batch algorithm. "
737447166675984384,2016-05-31 00:54:40,https://t.co/mIoSkB052Y,A Theory of Generative ConvNet. (arXiv:1602.03264v2 [stat.ML] UPDATED) https://t.co/mIoSkB052Y,0,13," Abstract: We show that a generative random field model, which we call generative ConvNet, can be derived from the commonly used discriminative ConvNet, by assuming a ConvNet for multi-category classification and assuming one of the categories is a base category generated by a reference distribution. If we further assume that the non-linearity in the ConvNet is Rectified Linear Unit (ReLU) and the reference distribution is Gaussian white noise, then we obtain a generative ConvNet model that is unique among energy-based models: The model is piecewise Gaussian, and the means of the Gaussian pieces are defined by an auto-encoder, where the filters in the bottom-up encoding become the basis functions in the top-down decoding, and the binary activation variables detected by the filters in the bottom-up convolution process become the coefficients of the basis functions in the top-down deconvolution process. The Langevin dynamics for sampling the generative ConvNet is driven by the reconstruction error of this auto-encoder. The contrastive divergence learning of the generative ConvNet reconstructs the training images by the auto-encoder. The maximum likelihood learning algorithm can synthesize realistic natural image patterns. "
737079902667706368,2016-05-30 00:35:17,https://t.co/Cvc0G0QC0d,Linear dynamical neural population models through nonlinear embeddings. (arXiv:1605.08454v1 [q-bio.NC]) https://t.co/Cvc0G0QC0d,2,4," Abstract: A body of recent work in modeling neural activity focuses on recovering low-dimensional latent features that capture the statistical structure of large-scale neural populations. Most such approaches have focused on linear generative models, where inference is computationally tractable. Here, we propose fLDS, a general class of nonlinear generative models that permits the firing rate of each neuron to vary as an arbitrary smooth function of a latent, linear dynamical state. This extra flexibility allows the model to capture a richer set of neural variability than a purely linear model, but retains an easily visualizable low-dimensional latent space. To fit this class of non-conjugate models we propose a variational inference scheme, along with a novel approximate posterior capable of capturing rich temporal correlations across time. We show that our techniques permit inference in a wide class of generative models.We also show in application to two neural datasets that, compared to state-of-the-art neural population models, fLDS captures a much larger proportion of neural variability with a small number of latent dimensions, providing superior predictive performance and interpretability. "
737079900985790464,2016-05-30 00:35:17,https://t.co/uXjFQFr5Ys,Suppressing Background Radiation Using Poisson Principal Component Analysis. (arXiv:1605.08455v1 [cs.LG]) https://t.co/uXjFQFr5Ys,0,2," Abstract: Performance of nuclear threat detection systems based on gamma-ray spectrometry often strongly depends on the ability to identify the part of measured signal that can be attributed to background radiation. We have successfully applied a method based on Principal Component Analysis (PCA) to obtain a compact null-space model of background spectra using PCA projection residuals to derive a source detection score. We have shown the method's utility in a threat detection system using mobile spectrometers in urban scenes (Tandon et al 2012). While it is commonly assumed that measured photon counts follow a Poisson process, standard PCA makes a Gaussian assumption about the data distribution, which may be a poor approximation when photon counts are low. This paper studies whether and in what conditions PCA with a Poisson-based loss function (Poisson PCA) can outperform standard Gaussian PCA in modeling background radiation to enable more sensitive and specific nuclear threat detection. "
737079898712444928,2016-05-30 00:35:16,https://t.co/oDGGaAeKMA,Provable Algorithms for Inference in Topic Models. (arXiv:1605.08491v1 [cs.LG]) https://t.co/oDGGaAeKMA,2,6," Abstract: Recently, there has been considerable progress on designing algorithms with provable guarantees -- typically using linear algebraic methods -- for parameter learning in latent variable models. But designing provable algorithms for inference has proven to be more challenging. Here we take a first step towards provable inference in topic models. We leverage a property of topic models that enables us to construct simple linear estimators for the unknown topic proportions that have small variance, and consequently can work with short documents. Our estimators also correspond to finding an estimate around which the posterior is well-concentrated. We show lower bounds that for shorter documents it can be information theoretically impossible to find the hidden topics. Finally, we give empirical results that demonstrate that our algorithm works on realistic topic models. It yields good solutions on synthetic data and runs in time comparable to a {\em single} iteration of Gibbs sampling. "
737079896917311489,2016-05-30 00:35:16,https://t.co/AU0eqbT2Tn,Local Region Sparse Learning for Image-on-Scalar Regression. (arXiv:1605.08501v1 [stat.ML]) https://t.co/AU0eqbT2Tn,0,1," Abstract: Identification of regions of interest (ROI) associated with certain disease has a great impact on public health. Imposing sparsity of pixel values and extracting active regions simultaneously greatly complicate the image analysis. We address these challenges by introducing a novel region-selection penalty in the framework of image-on-scalar regression. Our penalty combines the Smoothly Clipped Absolute Deviation (SCAD) regularization, enforcing sparsity, and the SCAD of total variation (TV) regularization, enforcing spatial contiguity, into one group, which segments contiguous spatial regions against zero-valued background. Efficient algorithm is based on the alternative direction method of multipliers (ADMM) which decomposes the non-convex problem into two iterative optimization problems with explicit solutions. Another virtue of the proposed method is that a divide and conquer learning algorithm is developed, thereby allowing scaling to large images. Several examples are presented and the experimental results are compared with other state-of-the-art approaches. "
737079895025651713,2016-05-30 00:35:15,https://t.co/uFsa4flLGi]),Merging MCMC Subposteriors through Gaussian-Process Approximations. (arXiv:1605.08576v1 [https://t.co/uFsa4flLGi]) https://t.co/K6dbIFKzN8,1,2,INDEXERROR
737079893326958592,2016-05-30 00:35:15,https://t.co/hYYEALJ69l,Variational Bayesian Inference for Hidden Markov Models With Multivariate Gaussian Output Distributions. (arXiv:16… https://t.co/hYYEALJ69l,2,6," Abstract: Hidden Markov Models (HMM) have been used for several years in many time series analysis or pattern recognitions tasks. HMM are often trained by means of the Baum-Welch algorithm which can be seen as a special variant of an expectation maximization (EM) algorithm. Second-order training techniques such as Variational Bayesian Inference (VI) for probabilistic models regard the parameters of the probabilistic models as random variables and define distributions over these distribution parameters, hence the name of this technique. VI can also bee regarded as a special case of an EM algorithm. In this article, we bring both together and train HMM with multivariate Gaussian output distributions with VI. The article defines the new training technique for HMM. An evaluation based on some case studies and a comparison to related approaches is part of our ongoing work. "
737079891536007168,2016-05-30 00:35:15,https://t.co/hju2oD7BLd,PAC-Bayesian Theory Meets Bayesian Inference. (arXiv:1605.08636v1 [stat.ML]) https://t.co/hju2oD7BLd,1,10," Abstract: We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks "
737079889644425216,2016-05-30 00:35:14,https://t.co/NdZkcz2Xw7,An optimal algorithm for the Thresholding Bandit Problem. (arXiv:1605.08671v1 [stat.ML]) https://t.co/NdZkcz2Xw7,0,7," Abstract: We study a specific \textit{combinatorial pure exploration stochastic bandit problem} where the learner aims at finding the set of arms whose means are above a given threshold, up to a given precision, and \textit{for a fixed time horizon}. We propose a parameter-free algorithm based on an original heuristic, and prove that it is optimal for this problem by deriving matching upper and lower bounds. To the best of our knowledge, this is the first non-trivial pure exploration setting with \textit{fixed budget} for which optimal strategies are constructed. "
737079887979241473,2016-05-30 00:35:14,https://t.co/MV3bGSf1IG,On the Sensitivity of the Lasso to the Number of Predictor Variables. (arXiv:1403.4544v3 [stat.ML] UPDATED) https://t.co/MV3bGSf1IG,0,2," Abstract: The Lasso is a computationally efficient regression regularization procedure that can produce sparse estimators when the number of predictors (p) is large. Oracle inequalities provide probability loss bounds for the Lasso estimator at a deterministic choice of the regularization parameter. These bounds tend to zero if p is appropriately controlled, and are thus commonly cited as theoretical justification for the Lasso and its ability to handle high-dimensional settings. Unfortunately, in practice the regularization parameter is not selected to be a deterministic quantity, but is instead chosen using a random, data-dependent procedure. To address this shortcoming of previous theoretical work, we study the loss of the Lasso estimator when tuned optimally for prediction. Assuming orthonormal predictors and a sparse true model, we prove that the probability that the best possible predictive performance of the Lasso deteriorates as p increases is positive and can be arbitrarily close to one given a sufficiently high signal to noise ratio and sufficiently large p. We further demonstrate empirically that the amount of deterioration in performance can be far worse than the oracle inequalities suggest and provide a real data example where deterioration is observed. "
737079885827559425,2016-05-30 00:35:13,https://t.co/GuljSB0Ji9,Particle Metropolis-adjusted Langevin algorithms. (arXiv:1412.7299v3 [stat.ME] UPDATED) https://t.co/GuljSB0Ji9,1,2," Abstract: This paper proposes a new sampling scheme based on Langevin dynamics that is applicable within pseudo-marginal and particle Markov chain Monte Carlo algorithms. We investigate this algorithm's theoretical properties under standard asymptotics, which correspond to an increasing dimension of the parameters, $n$. Our results show that the behaviour of the algorithm depends crucially on how accurately one can estimate the gradient of the log target density. If the error in the estimate of the gradient is not sufficiently controlled as dimension increases, then asymptotically there will be no advantage over the simpler random-walk algorithm. However, if the error is sufficiently well-behaved, then the optimal scaling of this algorithm will be $O(n^{-1/6})$ compared to $O(n^{-1/2})$ for the random walk. Our theory also gives guidelines on how to tune the number of Monte Carlo samples in the likelihood estimate and the proposal step-size. "
737079884481187840,2016-05-30 00:35:13,https://t.co/6aNeGDjQ3c,Structure Learning of Partitioned Markov Networks. (arXiv:1504.00624v5 [stat.ML] UPDATED) https://t.co/6aNeGDjQ3c,0,2," Abstract: We learn the structure of a Markov Network between two groups of random variables from joint observations. Since modelling and learning the full MN structure may be hard, learning the links between two groups directly may be a preferable option. We introduce a novel concept called the \emph{partitioned ratio} whose factorization directly associates with the Markovian properties of random variables across two groups. A simple one-shot convex optimization procedure is proposed for learning the \emph{sparse} factorizations of the partitioned ratio and it is theoretically guaranteed to recover the correct inter-group structure under mild conditions. The performance of the proposed method is experimentally compared with the state of the art MN structure learning methods using ROC curves. Real applications on analyzing bipartisanship in US congress and pairwise DNA/time-series alignments are also reported. "
737079882442801152,2016-05-30 00:35:12,https://t.co/4YnphHj7Bm,Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives. (arXiv:1506.01972v3 [cs.LG] UPDATED) https://t.co/4YnphHj7Bm,1,4," Abstract: Many classical algorithms are found until several years later to outlive the confines in which they were conceived, and continue to be relevant in unforeseen settings. In this paper, we show that SVRG is one such method: being originally designed for strongly convex objectives, it is also very robust in non-strongly convex or sum-of-non-convex settings. More precisely, we provide new analysis to improve the state-of-the-art running times in both settings by either applying SVRG or its novel variant. Since non-strongly convex objectives include important examples such as Lasso or logistic regression, and sum-of-non-convex objectives include famous examples such as stochastic PCA and is even believed to be related to training deep neural nets, our results also imply better performances in these applications. "
737079880840536064,2016-05-30 00:35:12,https://t.co/aKsSrk4chV,ARock: an Algorithmic Framework for Asynchronous Parallel Coordinate Updates. (arXiv:1506.02396v5 [math.OC] UPDATE… https://t.co/aKsSrk4chV,0,2," Abstract: Finding a fixed point to a nonexpansive operator, i.e., $x^*=Tx^*$, abstracts many problems in numerical linear algebra, optimization, and other areas of scientific computing. To solve fixed-point problems, we propose ARock, an algorithmic framework in which multiple agents (machines, processors, or cores) update $x$ in an asynchronous parallel fashion. Asynchrony is crucial to parallel computing since it reduces synchronization wait, relaxes communication bottleneck, and thus speeds up computing significantly. At each step of ARock, an agent updates a randomly selected coordinate $x_i$ based on possibly out-of-date information on $x$. The agents share $x$ through either global memory or communication. If writing $x_i$ is atomic, the agents can read and write $x$ without memory locks. Theoretically, we show that if the nonexpansive operator $T$ has a fixed point, then with probability one, ARock generates a sequence that converges to a fixed points of $T$. Our conditions on $T$ and step sizes are weaker than comparable work. Linear convergence is also obtained. We propose special cases of ARock for linear systems, convex optimization, machine learning, as well as distributed and decentralized consensus problems. Numerical experiments of solving sparse logistic regression problems are presented. "
737079879141871616,2016-05-30 00:35:12,https://t.co/QEWmjKvxfS,Mondrian Forests for Large-Scale Regression when Uncertainty Matters. (arXiv:1506.03805v4 [stat.ML] UPDATED) https://t.co/QEWmjKvxfS,1,2," Abstract: Many real-world regression problems demand a measure of the uncertainty associated with each prediction. Standard decision forests deliver efficient state-of-the-art predictive performance, but high-quality uncertainty estimates are lacking. Gaussian processes (GPs) deliver uncertainty estimates, but scaling GPs to large-scale data sets comes at the cost of approximating the uncertainty estimates. We extend Mondrian forests, first proposed by Lakshminarayanan et al. (2014) for classification problems, to the large-scale non-parametric regression setting. Using a novel hierarchical Gaussian prior that dovetails with the Mondrian forest framework, we obtain principled uncertainty estimates, while still retaining the computational advantages of decision forests. Through a combination of illustrative examples, real-world large-scale datasets, and Bayesian optimization benchmarks, we demonstrate that Mondrian forests outperform approximate GPs on large-scale regression tasks and deliver better-calibrated uncertainty assessments than decision-forest-based methods. "
737079876746915841,2016-05-30 00:35:11,https://t.co/QzioIXBDFa,Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural Networks. (arXiv:1508.04422v2 [stat.ML] UPD… https://t.co/QzioIXBDFa,0,5," Abstract: Several popular graph embedding techniques for representation learning and dimensionality reduction rely on performing computationally expensive eigendecompositions to derive a nonlinear transformation of the input data space. The resulting eigenvectors encode the embedding coordinates for the training samples only, and so the embedding of novel data samples requires further costly computation. In this paper, we present a method for the out-of-sample extension of graph embeddings using deep neural networks (DNN) to parametrically approximate these nonlinear maps. Compared with traditional nonparametric out-of-sample extension methods, we demonstrate that the DNNs can generalize with equal or better fidelity and require orders of magnitude less computation at test time. Moreover, we find that unsupervised pretraining of the DNNs improves optimization for larger network sizes, thus removing sensitivity to model selection. "
737079874188398592,2016-05-30 00:35:10,https://t.co/V9St6Ox3xy,On the Expressive Power of Deep Learning: A Tensor Analysis. (arXiv:1509.05009v3 [cs.NE] UPDATED) https://t.co/V9St6Ox3xy,0,16," Abstract: It has long been conjectured that hypotheses spaces suitable for data that is compositional in nature, such as text or images, may be more efficiently represented with deep hierarchical networks than with shallow ones. Despite the vast empirical evidence supporting this belief, theoretical justifications to date are limited. In particular, they do not account for the locality, sharing and pooling constructs of convolutional networks, the most successful deep learning architecture to date. In this work we derive a deep network architecture based on arithmetic circuits that inherently employs locality, sharing and pooling. An equivalence between the networks and hierarchical tensor factorizations is established. We show that a shallow network corresponds to CP (rank-1) decomposition, whereas a deep network corresponds to Hierarchical Tucker decomposition. Using tools from measure theory and matrix algebra, we prove that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be realized (or even approximated) by a shallow network. Since log-space computation transforms our networks into SimNets, the result applies directly to a deep learning architecture demonstrating promising empirical performance. The construction and theory developed in this paper shed new light on various practices and ideas employed by the deep learning community. "
737079871772459008,2016-05-30 00:35:10,https://t.co/dctxgPEJYa,Evasion and Hardening of Tree Ensemble Classifiers. (arXiv:1509.07892v2 [cs.LG] UPDATED) https://t.co/dctxgPEJYa,0,1," Abstract: Classifier evasion consists in finding for a given instance $x$ the nearest instance $x'$ such that the classifier predictions of $x$ and $x'$ are different. We present two novel algorithms for systematically computing evasions for tree ensembles such as boosted trees and random forests. Our first algorithm uses a Mixed Integer Linear Program solver and finds the optimal evading instance under an expressive set of constraints. Our second algorithm trades off optimality for speed by using symbolic prediction, a novel algorithm for fast finite differences on tree ensembles. On a digit recognition task, we demonstrate that both gradient boosted trees and random forests are extremely susceptible to evasions. Finally, we harden a boosted tree model without loss of predictive accuracy by augmenting the training set of each boosting round with evading instances, a technique we call adversarial boosting. "
737079868446396416,2016-05-30 00:35:09,https://t.co/vmhgPKUK4Z,Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling. (arXiv:1512.09103v3 [math.OC] UPDATED) https://t.co/vmhgPKUK4Z,1,5," Abstract: Accelerated coordinate descent is widely used in optimization due to its cheap per-iteration cost and scalability to large-scale problems. Up to a primal-dual transformation, it is also the same as accelerated stochastic gradient descent that is one of the central methods used in machine learning. In this paper, we improve the best known running time of accelerated coordinate descent by a factor up to $\sqrt{n}$. Our improvement is based on a clean, novel non-uniform sampling that selects each coordinate with a probability proportional to the square root of its smoothness parameter. Our proof technique also deviates from the classical estimation sequence technique used in prior work. Our speed-up applies to important problems such as empirical risk minimization and solving linear systems, both in theory and in practice. "
737079865711726592,2016-05-30 00:35:08,https://t.co/pGVaserLwP,Spectrum Estimation from Samples. (arXiv:1602.00061v2 [cs.LG] UPDATED) https://t.co/pGVaserLwP,0,2," Abstract: We consider the problem of approximating the set of eigenvalues of the covariance matrix of a multivariate distribution (equivalently, the problem of approximating the ""population spectrum""), given access to samples drawn from the distribution. The eigenvalues of the covariance of a distribution contain basic information about the distribution, including the presence or lack of structure in the distribution, the effective dimensionality of the distribution, and the applicability of higher-level machine learning and multivariate statistical tools. We consider this fundamental recovery problem in the regime where the number of samples is comparable, or even sublinear in the dimensionality of the distribution in question. First, we propose a theoretically optimal and computationally efficient algorithm for recovering the moments of the eigenvalues of the population covariance matrix. We then leverage this accurate moment recovery, via a Wasserstein distance argument, to show that the vector of eigenvalues can be accurately recovered. We provide finite--sample bounds on the expected error of the recovered eigenvalues, which imply that our estimator is asymptotically consistent as the dimensionality of the distribution and sample size tend towards infinity, even in the sublinear sample regime where the ratio of the sample size to the dimensionality tends to zero. In addition to our theoretical results, we show that our approach performs well in practice for a broad range of distributions and sample sizes. "
737079864025550848,2016-05-30 00:35:08,https://t.co/ezDXEEcv6f,Hyperparameter optimization with approximate gradient. (arXiv:1602.02355v3 [stat.ML] UPDATED) https://t.co/ezDXEEcv6f,2,15," Abstract: Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of L2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods. "
737079862347890688,2016-05-30 00:35:08,https://t.co/EmbAK8SMxZ,Learning may need only a few bits of synaptic precision. (arXiv:1602.04129v2 [cond-mat.dis-nn] UPDATED) https://t.co/EmbAK8SMxZ,0,5," Abstract: Learning in neural networks poses peculiar challenges when using discretized rather then continuous synaptic states. The choice of discrete synapses is motivated by biological reasoning and experiments, and possibly by hardware implementation considerations as well. In this paper we extend a previous large deviations analysis which unveiled the existence of peculiar dense regions in the space of synaptic states which accounts for the possibility of learning efficiently in networks with binary synapses. We extend the analysis to synapses with multiple states and generally more plausible biological features. The results clearly indicate that the overall qualitative picture is unchanged with respect to the binary case, and very robust to variation of the details of the model. We also provide quantitative results which suggest that the advantages of increasing the synaptic precision (i.e.~the number of internal synaptic states) rapidly vanish after the first few bits, and therefore that, for practical applications, only few bits may be needed for near-optimal performance, consistently with recent biological findings. Finally, we demonstrate how the theoretical analysis can be exploited to design efficient algorithmic search strategies. "
737079860233940992,2016-05-30 00:35:07,https://t.co/ONzmk7AThx,Structured Sparse Regression via Greedy Hard-Thresholding. (arXiv:1602.06042v2 [stat.ML] UPDATED) https://t.co/ONzmk7AThx,0,3," Abstract: Several learning applications require solving high-dimensional regression problems where the relevant features belong to a small number of (overlapping) groups. For very large datasets and under standard sparsity constraints, hard thresholding methods have proven to be extremely efficient, but such methods require NP hard projections when dealing with overlapping groups. In this paper, we show that such NP-hard projections can not only be avoided by appealing to submodular optimization, but such methods come with strong theoretical guarantees even in the presence of poorly conditioned data (i.e. say when two features have correlation $\geq 0.99$), which existing analyses cannot handle. These methods exhibit an interesting computation-accuracy trade-off and can be extended to significantly harder problems such as sparse overlapping groups. Experiments on both real and synthetic data validate our claims and demonstrate that the proposed methods are orders of magnitude faster than other greedy and convex relaxation techniques for learning with group-structured sparsity. "
737079858283577345,2016-05-30 00:35:07,https://t.co/6q8nAfiLPS,Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?. (arXiv:1603.05691v2 [stat.ML] UPDATED) https://t.co/6q8nAfiLPS,2,13," Abstract: Yes, they do. This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained. Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution. Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher. "
737079855423033344,2016-05-30 00:35:06,https://t.co/i1aAWS4L5e,Combinatorial Topic Models using Small-Variance Asymptotics. (arXiv:1604.02027v2 [cs.LG] UPDATED) https://t.co/i1aAWS4L5e,0,1," Abstract: Topic models have emerged as fundamental tools in unsupervised machine learning. Most modern topic modeling algorithms take a probabilistic view and derive inference algorithms based on Latent Dirichlet Allocation (LDA) or its variants. In contrast, we study topic modeling as a combinatorial optimization problem, and propose a new objective function derived from LDA by passing to the small-variance limit. We minimize the derived objective by using ideas from combinatorial optimization, which results in a new, fast, and high-quality topic modeling algorithm. In particular, we show that our results are competitive with popular LDA-based topic modeling approaches, and also discuss the (dis)similarities between our approach and its probabilistic counterparts. "
737079853254639616,2016-05-30 00:35:05,https://t.co/ljvzBOiwID,Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis. (arXi… https://t.co/ljvzBOiwID,0,2," Abstract: This paper considers the problem of canonical-correlation analysis (CCA) (Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a pair of symmetric matrices. These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009). We provide simple iterative algorithms, with improved runtimes, for solving these problems that are globally linearly convergent with moderate dependencies on the condition numbers and eigenvalue gaps of the matrices involved. We obtain our results by reducing CCA to the top-$k$ generalized eigenvector problem. We solve this problem through a general framework that simply requires black box access to an approximate linear system solver. Instantiating this framework with accelerated gradient descent we obtain a running time of $O(\frac{z k \sqrt{\kappa}}{\rho} \log(1/\epsilon) \log \left(k\kappa/\rho\right))$ where $z$ is the total number of nonzero entries, $\kappa$ is the condition number and $\rho$ is the relative eigenvalue gap of the appropriate matrices. Our algorithm is linear in the input size and the number of components $k$ up to a $\log(k)$ factor. This is essential for handling large-scale matrices that appear in practice. To the best of our knowledge this is the first such algorithm with global linear convergence. We hope that our results prompt further research and ultimately improve the practical running time for performing these important data analysis procedures on large data sets. "
737079850675097600,2016-05-30 00:35:05,https://t.co/9pu6VjjTVF,The Z-loss: a shift and scale invariant classification loss belonging to the Spherical Family. (arXiv:1604.08859v2… https://t.co/9pu6VjjTVF,0,2," Abstract: Despite being the standard loss function to train multi-class neural networks, the log-softmax has two potential limitations. First, it involves computations that scale linearly with the number of output classes, which can restrict the size of problems we are able to tackle with current hardware. Second, it remains unclear how close it matches the task loss such as the top-k error rate or other non-differentiable evaluation metrics which we aim to optimize ultimately. In this paper, we introduce an alternative classification loss function, the Z-loss, which is designed to address these two issues. Unlike the log-softmax, it has the desirable property of belonging to the spherical loss family (Vincent et al., 2015), a class of loss functions for which training can be performed very efficiently with a complexity independent of the number of output classes. We show experimentally that it significantly outperforms the other spherical loss functions previously investigated. Furthermore, we show on a word language modeling task that it also outperforms the log-softmax with respect to certain ranking scores, such as top-k scores, suggesting that the Z-loss has the flexibility to better match the task loss. These qualities thus makes the Z-loss an appealing candidate to train very efficiently large output networks such as word-language models or other extreme classification problems. On the One Billion Word (Chelba et al., 2014) dataset, we are able to train a model with the Z-loss 40 times faster than the log-softmax and more than 4 times faster than the hierarchical softmax. "
737079848355672064,2016-05-30 00:35:04,https://t.co/uOcqmRDmyJ,EEF: Exponentially Embedded Families with Class-Specific Features for Classification. (arXiv:1605.03631v2 [stat.ML… https://t.co/uOcqmRDmyJ,0,1," Abstract: In this letter, we present a novel exponentially embedded families (EEF) based classification method, in which the probability density function (PDF) on raw data is estimated from the PDF on features. With the PDF construction, we show that class-specific features can be used in the proposed classification method, instead of a common feature subset for all classes as used in conventional approaches. We apply the proposed EEF classifier for text categorization as a case study and derive an optimal Bayesian classification rule with class-specific feature selection based on the Information Gain (IG) score. The promising performance on real-life data sets demonstrates the effectiveness of the proposed approach and indicates its wide potential applications. "
737079845994258432,2016-05-30 00:35:04,https://t.co/l6E39yogy9,Actively Learning Hemimetrics with Applications to Eliciting User Preferences. (arXiv:1605.07144v2 [stat.ML] UPDAT… https://t.co/l6E39yogy9,0,3," Abstract: Motivated by an application of eliciting users' preferences, we investigate the problem of learning hemimetrics, i.e., pairwise distances among a set of $n$ items that satisfy triangle inequalities and non-negativity constraints. In our application, the (asymmetric) distances quantify private costs a user incurs when substituting one item by another. We aim to learn these distances (costs) by asking the users whether they are willing to switch from one item to another for a given incentive offer. Without exploiting structural constraints of the hemimetric polytope, learning the distances between each pair of items requires $\Theta(n^2)$ queries. We propose an active learning algorithm that substantially reduces this sample complexity by exploiting the structural constraints on the version space of hemimetrics. Our proposed algorithm achieves provably-optimal sample complexity for various instances of the task. For example, when the items are embedded into $K$ tight clusters, the sample complexity of our algorithm reduces to $O(n K)$. Extensive experiments on a restaurant recommendation data set support the conclusions of our theoretical analysis. "
735995760358109185,2016-05-27 00:47:18,https://t.co/hFGILDkNUB,A PAC RL Algorithm for Episodic POMDPs. (arXiv:1605.08062v1 [cs.LG]) https://t.co/hFGILDkNUB,1,2," Abstract: Many interesting real world domains involve reinforcement learning (RL) in partially observable environments. Efficient learning in such domains is important, but existing sample complexity bounds for partially observable RL are at least exponential in the episode length. We give, to our knowledge, the first partially observable RL algorithm with a polynomial bound on the number of episodes on which the algorithm may not achieve near-optimal performance. Our algorithm is suitable for an important class of episodic POMDPs. Our approach builds on recent advances in method of moments for latent variable model estimation. "
735995759162753024,2016-05-27 00:47:17,https://t.co/uWW17JWewo,FLAG: Fast Linearly-Coupled Adaptive Gradient Method. (arXiv:1605.08108v1 [math.OC]) https://t.co/uWW17JWewo,0,3," Abstract: The celebrated Nesterov's accelerated gradient method offers great speed-ups compared to the classical gradient descend method as it attains the optimal first-order oracle complexity for smooth convex optimization. On the other hand, the popular AdaGrad algorithm competes with mirror descent under the best regularizer by adaptively scaling the gradient. Recently, it has been shown that the accelerated gradient descent can be viewed as a linear combination of gradient descent and mirror descent steps. Here, we draw upon these ideas and present a fast linearly-coupled adaptive gradient method (FLAG) as an accelerated version of AdaGrad, and show that our algorithm can indeed offer the best of both worlds. Like Nesterov's accelerated algorithm and its proximal variant, FISTA, our method has a convergence rate of $1/T^2$ after $T$ iterations. Like AdaGrad our method adaptively chooses a regularizer, in a way that performs almost as well as the best choice of regularizer in hindsight. "
735995758168670208,2016-05-27 00:47:17,https://t.co/SNzeEncUhU,Adiabatic Persistent Contrastive Divergence Learning. (arXiv:1605.08174v1 [cs.LG]) https://t.co/SNzeEncUhU,1,6," Abstract: This paper studies the problem of parameter learning in probabilistic graphical models having latent variables, where the standard approach is the expectation maximization algorithm alternating expectation (E) and maximization (M) steps. However, both E and M steps are computationally intractable for high dimensional data, while the substitution of one step to a faster surrogate for combating against intractability can often cause failure in convergence. We propose a new learning algorithm which is computationally efficient and provably ensures convergence to a correct optimum. Its key idea is to run only a few cycles of Markov Chains (MC) in both E and M steps. Such an idea of running incomplete MC has been well studied only for M step in the literature, called Contrastive Divergence (CD) learning. While such known CD-based schemes find approximated gradients of the log-likelihood via the mean-field approach in E step, our proposed algorithm does exact ones via MC algorithms in both steps due to the multi-time-scale stochastic approximation theory. Despite its theoretical guarantee in convergence, the proposed scheme might suffer from the slow mixing of MC in E step. To tackle it, we also propose a hybrid approach applying both mean-field and MC approximation in E step, where the hybrid approach outperforms the bare mean-field CD scheme in our experiments on real-world datasets. "
735995757174640640,2016-05-27 00:47:17,https://t.co/QvYl76trsV,Discovering Causal Signals in Images. (arXiv:1605.08179v1 [stat.ML]) https://t.co/QvYl76trsV,0,2," Abstract: The purpose of this paper is to point out and assay observable causal signals within collections of static images. We achieve this goal in two steps. First, we take a learning approach to observational causal inference, and build a classifier that achieves state-of-the-art performance on finding the causal direction between pairs of random variables, when given samples from their joint distribution. Second, we use our causal direction finder to effectively distinguish between features of objects and features of their contexts in collections of static images. Our experiments demonstrate the existence of (1) a relation between the direction of causality and the difference between objects and their contexts, and (2) observable causal signals in collections of static images. "
735995756147056640,2016-05-27 00:47:17,https://t.co/vK1RAuoOSc,Towards optimal nonlinearities for sparse recovery using higher-order statistics. (arXiv:1605.08201v1 [cs.IT]) https://t.co/vK1RAuoOSc,0,7," Abstract: We consider machine learning techniques to develop low-latency approximate solutions to a class of inverse problems. More precisely, we use a probabilistic approach for the problem of recovering sparse stochastic signals that are members of the $\ell_p$-balls. In this context, we analyze the Bayesian mean-square-error (MSE) for two types of estimators: (i) a linear estimator and (ii) a structured estimator composed of a linear operator followed by a Cartesian product of univariate nonlinear mappings. By construction, the complexity of the proposed nonlinear estimator is comparable to that of its linear counterpart since the nonlinear mapping can be implemented efficiently in hardware by means of look-up tables (LUTs). The proposed structure lends itself to neural networks and iterative shrinkage/thresholding-type algorithms restricted to a single iterate (e.g. due to imposed hardware or latency constraints). By resorting to an alternating minimization technique, we obtain a sequence of optimized linear operators and nonlinear mappings that converge in the MSE objective. The result is attractive for real-time applications where general iterative and convex optimization methods are infeasible. "
735995755031302144,2016-05-27 00:47:16,https://t.co/v1FHoMnYdP,Predict or classify: The deceptive role of time-locking in brain signal classification. (arXiv:1605.08228v1 [q-bio… https://t.co/v1FHoMnYdP,1,1," Abstract: Several experimental studies claim to be able to predict the outcome of simple decisions from brain signals measured before subjects are aware of their decision. Often, these studies use multivariate pattern recognition methods with the underlying assumption that the ability to classify the brain signal is equivalent to predict the decision itself. Here we show instead that it is possible to correctly classify a signal even if it does not contain any predictive information about the decision. We first define a simple stochastic model that mimics the random decision process between two equivalent alternatives, and generate a large number of independent trials that contain no choice-predictive information. The trials are first time-locked to the time point of the final event and then classified using standard machine-learning techniques. The resulting classification accuracy is above chance level long before the time point of time-locking. We then analyze the same trials using information theory. We demonstrate that the high classification accuracy is a consequence of time-locking and that its time behavior is simply related to the large relaxation time of the process. We conclude that when time-locking is a crucial step in the analysis of neural activity patterns, both the emergence and the timing of the classification accuracy are affected by structural properties of the network that generates the signal. "
735995754049884160,2016-05-27 00:47:16,https://t.co/kqEJcGvDZb,Stochastic Variance Reduced Riemannian Eigensolver. (arXiv:1605.08233v1 [cs.LG]) https://t.co/kqEJcGvDZb,1,2," Abstract: We study the stochastic Riemannian gradient algorithm for matrix eigen-decomposition. The state-of-the-art stochastic Riemannian algorithm requires the learning rate to decay to zero and thus suffers from slow convergence and sub-optimal solutions. In this paper, we address this issue by deploying the variance reduction (VR) technique of stochastic gradient descent (SGD). The technique was originally developed to solve convex problems in the Euclidean space. We generalize it to Riemannian manifolds and realize it to solve the non-convex eigen-decomposition problem. We are the first to propose and analyze the generalization of SVRG to Riemannian manifolds. Specifically, we propose the general variance reduction form, SVRRG, in the framework of the stochastic Riemannian gradient optimization. It's then specialized to the problem with eigensolvers and induces the SVRRG-EIGS algorithm. We provide a novel and elegant theoretical analysis on this algorithm. The theory shows that a fixed learning rate can be used in the Riemannian setting with an exponential global convergence rate guaranteed. The theoretical results make a significant improvement over existing studies, with the effectiveness empirically verified. "
735995753106178048,2016-05-27 00:47:16,https://t.co/3Tw8FASh9Q,Margin Preservation of Deep Neural Networks. (arXiv:1605.08254v1 [stat.ML]) https://t.co/3Tw8FASh9Q,2,5," Abstract: The generalization error of deep neural networks via their classification margin is studied in this work. Our approach is based on the Jacobian matrix of a deep neural network and can be applied to networks with arbitrary non-linearities and pooling layers, and to networks with different architectures such as feed forward networks and residual networks. Our analysis leads to the conclusion that a bounded spectral norm of the network's Jacobian matrix in the neighbourhood of the training samples is crucial for a deep neural network of arbitrary depth and width to generalize well. This is a significant improvement over the current bounds in the literature, which imply that the generalization error grows with either the width or the depth of the network. Moreover, it shows that the the recently proposed batch normalization and weight normalization re-parametrizations enjoy good generalization properties, and leads to a novel network regularizer based on the network's Jacobian matrix. The analysis is supported with experimental results on the MNIST and CIFAR-10 datasets. "
735995752179195904,2016-05-27 00:47:16,https://t.co/rC3lw7X6RG,Low-rank tensor completion: a Riemannian manifold preconditioning approach. (arXiv:1605.08257v1 [cs.LG]) https://t.co/rC3lw7X6RG,0,3," Abstract: We propose a novel Riemannian manifold preconditioning approach for the tensor completion problem with rank constraint. A novel Riemannian metric or inner product is proposed that exploits the least-squares structure of the cost function and takes into account the structured symmetry that exists in Tucker decomposition. The specific metric allows to use the versatile framework of Riemannian optimization on quotient manifolds to develop preconditioned nonlinear conjugate gradient and stochastic gradient descent algorithms for batch and online setups, respectively. Concrete matrix representations of various optimization-related ingredients are listed. Numerical comparisons suggest that our proposed algorithms robustly outperform state-of-the-art algorithms across different synthetic and real-world datasets. "
735995751080296448,2016-05-27 00:47:15,https://t.co/escHuonSIU,Discrete Deep Feature Extraction: A Theory and New Architectures. (arXiv:1605.08283v1 [cs.LG]) https://t.co/escHuonSIU,1,9," Abstract: First steps towards a mathematical theory of deep convolutional neural networks for feature extraction were made---for the continuous-time case---in Mallat, 2012, and Wiatowski and B\""olcskei, 2015. This paper considers the discrete case, introduces new convolutional neural network architectures, and proposes a mathematical framework for their analysis. Specifically, we establish deformation and translation sensitivity results of local and global nature, and we investigate how certain structural properties of the input signal are reflected in the corresponding feature vectors. Our theory applies to general filters and general Lipschitz-continuous non-linearities and pooling operators. Experiments on handwritten digit classification and facial landmark detection---including feature importance evaluation---complement the theoretical findings. "
735995749905879044,2016-05-27 00:47:15,https://t.co/zHBDOJqNyt,Solving Random Systems of Quadratic Equations via Truncated Generalized Gradient Flow. (arXiv:1605.08285v1 [stat.M… https://t.co/zHBDOJqNyt,0,1," Abstract: This paper puts forth a new algorithm, termed \emph{truncated amplitude flow} (TAF), to recover an unknown $n$-dimensional real-/complex-valued vector $\bm{x}$ from $m$ quadratic equations of the form $y_i=|\langle\bm{a}_i,\bm{x}\rangle|^2$. This problem is known to be \emph{NP-hard} in general. We prove that as soon as the number of equations $m$ is on the order of the number of unknowns $n$, TAF recovers the solution exactly (up to a global unimodular constant) with high probability and complexity growing linearly with the time required to read the data. Our method adopts the \emph{amplitude-based} cost function and proceeds in two stages: In stage one, we introduce an \emph{orthogonality-promoting} initialization that is obtained with a few simple power iterations. Stage two refines the initial estimate by successive updates of scalable \emph{truncated generalized gradient iterations}. The former is in sharp contrast to existing spectral initializations, while the latter handles the rather challenging nonconvex and nonsmooth amplitude-based cost function. In particular for real-valued vectors, our gradient truncation rule provably eliminates the erroneously estimated signs with high probability to markedly improve upon its untruncated version. Numerical tests demonstrate that our initialization method returns more accurate and robust estimates relative to its spectral counterparts. Furthermore, even under the same initialization, our amplitude-based refinement outperforms Wirtinger-based alternatives, corroborating the superior performance of TAF over state-of-the-art algorithms. "
735995748622307328,2016-05-27 00:47:15,https://t.co/zAIYPZK6wb,High-Dimensional Trimmed Estimators: A General Framework for Robust Structured Estimation. (arXiv:1605.08299v1 [st… https://t.co/zAIYPZK6wb,0,1," Abstract: We consider the problem of robustifying high-dimensional structured estimation. Robust techniques are key in real-world applications, as these often involve outliers and data corruption. We focus on trimmed versions of structurally regularized M-estimators in the high-dimensional setting, including the popular Least Trimmed Squared estimator, as well as analogous estimators for generalized linear models, graphical models, using possibly non-convex loss functions. We present a general analysis of their statistical convergence rates and consistency, and show how to extend any algorithms for M-estimators to fit trimmed variants. We then take a closer look at the $\ell_1$-regularized Least Trimmed Squared estimator as a special case. Our results show that this estimator can tolerate a larger fraction of corrupted observations than state-of-the-art alternatives. The competitive performance of high-dimensional trimmed estimators is illustrated numerically using both simulated and real-world genomics data. "
735995747494203392,2016-05-27 00:47:14,https://t.co/O1zMCeRMGU,Predictive Coarse-Graining. (arXiv:1605.08301v1 [stat.ML]) https://t.co/O1zMCeRMGU,1,3," Abstract: We propose a data-driven, coarse-graining formulation in the context of equilibrium statistical mechanics. In contrast to existing techniques which are based on a fine-to-coarse map, we adopt the opposite strategy by prescribing a probabilistic coarse-to-fine map. This corresponds to a directed probabilistic model where the coarse variables play the role of latent generators of the fine scale (all-atom) data. From an information-theoretic perspective, the framework proposed provides an improvement upon the relative entropy method and is capable of quantifying the uncertainty due to the information loss that unavoidably takes place during the CG process. Furthermore, it can be readily extended to a fully Bayesian model where various sources of uncertainties are reflected in the posterior of the model parameters. The latter can be used to produce not only point estimates of fine-scale reconstructions or macroscopic observables, but more importantly, predictive posterior distributions on these quantities. Predictive posterior distributions reflect the confidence of the model as a function of the amount of data and the level of coarse-graining. The issues of model complexity and model selection are seamlessly addressed by employing a hierarchical prior that favors the discovery of sparse solutions, revealing the most prominent features in the coarse-grained model. A flexible and parallelizable Monte Carlo - Expectation-Maximization (MC-EM) scheme is proposed for carrying out inference and learning tasks. A comparative assessment of the proposed methodology is presented for a lattice spin system and the SPC/E water model. "
735995746336575488,2016-05-27 00:47:14,https://t.co/IZWraOymST,Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks. (arXiv:1605.08346v1 [cs.IT]) https://t.co/IZWraOymST,0,4," Abstract: Recurrent neural networks (RNNs) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time-varying data processing tasks. To understand the success and limitations of RNNs, it is critical that we advance our analysis of their fundamental memory properties. We focus on echo state networks (ESNs), which are RNNs with simple memoryless nodes and random connectivity. In most existing analyses, the short-term memory (STM) capacity results conclude that the ESN network size must scale linearly with the input size for unstructured inputs. The main contribution of this paper is to provide general results characterizing the STM capacity for linear ESNs with multidimensional input streams when the inputs have common low-dimensional structure: sparsity in a basis or significant statistical dependence between inputs. In both cases, we show that the number of nodes in the network must scale linearly with the information rate and poly-logarithmically with the ambient input dimension. The analysis relies on advanced applications of random matrix theory and results in explicit non-asymptotic bounds on the recovery error. Taken together, this analysis provides a significant step forward in our understanding of the STM properties in RNNs. "
735995745124397056,2016-05-27 00:47:14,https://t.co/2nnQa5GrZK,No bad local minima: Data independent training error guarantees for multilayer neural networks. (arXiv:1605.08361v… https://t.co/2nnQa5GrZK,3,8," Abstract: We use smoothed analysis techniques to provide guarantees on the training loss of Multilayer Neural Networks (MNNs) at differentiable local minima. Specifically, we examine MNNs with piecewise linear activation functions, quadratic loss and a single output, under mild over-parametrization. We prove that for a MNN with one hidden layer, the training error is zero at every differentiable local minimum, for almost every dataset and dropout-like noise realization. We then extend these results to the case of more than one hidden layer. Our theoretical guarantees assume essentially nothing on the training data, and are verified numerically. These results suggest why the highly non-convex loss of such MNNs can be easily optimized using local updates (e.g., stochastic gradient descent), as observed empirically. "
735995744017108992,2016-05-27 00:47:14,https://t.co/rsDBv31khx,Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent. (arXiv:1605.08370v1 [cs.LG… https://t.co/rsDBv31khx,0,4," Abstract: Matrix completion, where we wish to recover a low rank matrix by observing a few entries from it, is a widely studied problem in both theory and practice with wide applications. Most of the provable algorithms so far on this problem have been restricted to the offline setting where they provide an estimate of the unknown matrix using all observations simultaneously. However, in many applications, the online version, where we observe one entry at a time and dynamically update our estimate, is more appealing. While existing algorithms are efficient for the offline setting, they could be highly inefficient for the online setting. In this paper, we propose the first provable, efficient online algorithm for matrix completion. Our algorithm starts from an initial estimate of the matrix and then performs non-convex stochastic gradient descent (SGD). After every observation, it performs a fast update involving only one row of two tall matrices, giving near linear total runtime. Our algorithm can be naturally used in the offline setting as well, where it gives competitive sample complexity and runtime to state of the art algorithms. Our proofs introduce a general framework to show that SGD updates tend to stay away from saddle surfaces and could be of broader interests for other non-convex problems to prove tight rates. "
735995743023030272,2016-05-27 00:47:13,https://t.co/0VGo7jvs3G,Kronecker Determinantal Point Processes. (arXiv:1605.08374v1 [cs.LG]) https://t.co/0VGo7jvs3G,0,2," Abstract: Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set of $N$ items. They have recently gained prominence in several applications that rely on ""diverse"" subsets. However, their applicability to large problems is still limited due to the $\mathcal O(N^3)$ complexity of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling. But contrary to what one may expect, leveraging the Kronecker product structure for speeding up DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a KronDPP. "
735995742012198914,2016-05-27 00:47:13,https://t.co/ycwccYmP6Q,Generalization Properties and Implicit Regularization for Multiple Passes SGM. (arXiv:1605.08375v1 [cs.LG]) https://t.co/ycwccYmP6Q,0,1," Abstract: We study the generalization properties of stochastic gradient methods for learning with convex loss functions and linearly parameterized functions. We show that, in the absence of penalizations or constraints, the stability and approximation properties of the algorithm can be controlled by tuning either the step-size or the number of passes over the data. In this view, these parameters can be seen to control a form of implicit regularization. Numerical results complement the theoretical findings. "
735995741089476608,2016-05-27 00:47:13,https://t.co/Bdy8rcPSla,"Total Variation Classes Beyond 1d: Minimax Rates, and the Limitations of Linear Smoothers. (arXiv:1605.08400v1 [ma… https://t.co/Bdy8rcPSla",0,2," Abstract: We consider the problem of estimating a function defined over $n$ locations on a $d$-dimensional grid (having all side lengths equal to $n^{1/d}$). When the function is constrained to have discrete total variation bounded by $C_n$, we derive the minimax optimal (squared) $\ell_2$ estimation error rate, parametrized by $n$ and $C_n$. Total variation denoising, also known as the fused lasso, is seen to be rate optimal. Several simpler estimators exist, such as Laplacian smoothing and Laplacian eigenmaps. A natural question is: can these simpler estimators perform just as well? We prove that these estimators, and more broadly all estimators given by linear transformations of the input data, are suboptimal over the class of functions with bounded variation. This extends fundamental findings of Donoho and Johnstone [1998] on 1-dimensional total variation spaces to higher dimensions. The implication is that the computationally simpler methods cannot be used for such sophisticated denoising tasks, without sacrificing statistical accuracy. We also derive minimax rates for discrete Sobolev spaces over $d$-dimensional grids, which are, in some sense, smaller than the total variation function spaces. Indeed, these are small enough spaces that linear estimators can be optimal---and a few well-known ones are, such as Laplacian smoothing and Laplacian eigenmaps, as we show. Lastly, we investigate the problem of adaptivity of the total variation denoiser to these smaller Sobolev function spaces. "
735995739776679936,2016-05-27 00:47:13,https://t.co/YyKWgaokFr,Subspace Learning with Partial Information. (arXiv:1402.4844v2 [cs.LG] UPDATED) https://t.co/YyKWgaokFr,0,4," Abstract: The goal of subspace learning is to find a $k$-dimensional subspace of $\mathbb{R}^d$, such that the expected squared distance between instance vectors and the subspace is as small as possible. In this paper we study subspace learning in a partial information setting, in which the learner can only observe $r \le d$ attributes from each instance vector. We propose several efficient algorithms for this task, and analyze their sample complexity "
735995738623234050,2016-05-27 00:47:12,https://t.co/zDUBrQF1aY,Projected Nesterov's Proximal-Gradient Algorithm for Sparse Signal Reconstruction with a Convex Constraint. (arXiv… https://t.co/zDUBrQF1aY,0,3," Abstract: We develop a projected Nesterov's proximal-gradient (PNPG) approach for sparse signal reconstruction that combines adaptive step size with Nesterov's momentum acceleration. The objective function that we wish to minimize is the sum of a convex differentiable data-fidelity (negative log-likelihood (NLL)) term and a convex regularization term. We apply sparse signal regularization where the signal belongs to a closed convex set within the closure of the domain of the NLL; the convex-set constraint facilitates flexible NLL domains and accurate signal recovery. Signal sparsity is imposed using the $\ell_1$-norm penalty on the signal's linear transform coefficients or gradient map, respectively. The PNPG approach employs projected Nesterov's acceleration step with restart and an inner iteration to compute the proximal mapping. We propose an adaptive step-size selection scheme to obtain a good local majorizing function of the NLL and reduce the time spent backtracking. Thanks to step-size adaptation, PNPG does not require Lipschitz continuity of the gradient of the NLL. We present an integrated derivation of the momentum acceleration and its $\mathcal{O}(k^{-2})$ convergence-rate and iterate convergence proofs, which account for adaptive step-size selection, inexactness of the iterative proximal mapping, and the convex-set constraint. The tuning of PNPG is largely application-independent. Tomographic and compressed-sensing reconstruction experiments with Poisson generalized linear and Gaussian linear measurement models demonstrate the performance of the proposed approach. "
735995737385885696,2016-05-27 00:47:12,https://t.co/eYdpFGnTjp,Online Low-Rank Subspace Clustering by Basis Dictionary Pursuit. (arXiv:1503.08356v3 [stat.ML] UPDATED) https://t.co/eYdpFGnTjp,0,5," Abstract: Low-Rank Representation~(LRR) has been a significant method for segmenting data that are generated from a union of subspaces. It is also known that solving LRR is challenging in terms of time complexity and memory footprint, in that the size of the nuclear norm regularized matrix is $n$-by-$n$ (where $n$ is the number of samples). In this paper, we thereby develop a novel online implementation of LRR that reduces the memory cost from $O(n^2)$ to $O(pd)$, with $p$ being the ambient dimension and $d$ being some estimated rank~($d < p \ll n$). We also establish the theoretical guarantee that the sequence of solutions produced by our algorithm converges to a stationary point of the expected loss function asymptotically. Extensive experiments on synthetic and realistic datasets further substantiate that our algorithm is fast, robust and memory efficient. "
735995736278638592,2016-05-27 00:47:12,https://t.co/tQKPgnweoy,Domain-Adversarial Training of Neural Networks. (arXiv:1505.07818v4 [stat.ML] UPDATED) https://t.co/tQKPgnweoy,0,3," Abstract: We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application. "
735995735330697216,2016-05-27 00:47:12,https://t.co/tWSkE6aheh,Doubly Robust Off-policy Value Evaluation for Reinforcement Learning. (arXiv:1511.03722v3 [cs.LG] UPDATED) https://t.co/tWSkE6aheh,0,1," Abstract: We study the problem of off-policy value evaluation in reinforcement learning (RL), where one aims to estimate the value of a new policy based on data collected by a different policy. This problem is often a critical step when applying RL in real-world problems. Despite its importance, existing general methods either have uncontrolled bias or suffer high variance. In this work, we extend the doubly robust estimator for bandits to sequential decision-making problems, which gets the best of both worlds: it is guaranteed to be unbiased and can have a much lower variance than the popular importance sampling estimators. We demonstrate the estimator's accuracy in several benchmark problems, and illustrate its use as a subroutine in safe policy improvement. We also provide theoretical results on the hardness of the problem, and show that our estimator can match the lower bound in certain scenarios. "
735995734026293248,2016-05-27 00:47:11,https://t.co/Gb6kO8l9ZN,Predicting online user behaviour using deep learning algorithms. (arXiv:1511.06247v3 [cs.LG] UPDATED) https://t.co/Gb6kO8l9ZN,0,2, Abstract: We propose a robust classifier to predict buying intentions based on user behaviour within a large e-commerce website. In this work we compare traditional machine learning techniques with the most advanced deep learning approaches. We show that both Deep Belief Networks and Stacked Denoising auto-Encoders achieved a substantial improvement by extracting features from high dimensional data during the pre-train phase. They prove also to be more convenient to deal with severe class imbalance. 
735995731539005441,2016-05-27 00:47:11,https://t.co/Dn1TPkbdcR,Optimal whitening and decorrelation. (arXiv:1512.00809v2 [stat.ME] UPDATED) https://t.co/Dn1TPkbdcR,0,3," Abstract: Whitening, or sphering, is a common preprocessing step in statistical analysis to transform random variables to orthogonality. However, due to rotational freedom there are infinitely many possible whitening procedures. Consequently, there is a diverse range of sphering methods in use, for example based on principal component analysis (PCA), Cholesky matrix decomposition and zero-phase component analysis (ZCA), among others. Here we provide an overview of the underlying theory and discuss five natural whitening procedures. Subsequently, we demonstrate that investigating the cross-covariance and the cross-correlation matrix between sphered and original variables allows to break the rotational invariance and to identify optimal whitening transformations. As a result we recommend two particular approaches: ZCA-cor whitening to produce sphered variables that are maximally similar to the original variables, and PCA-cor whitening to obtain sphered variables that maximally compress the original variables. "
735995730465292289,2016-05-27 00:47:10,https://t.co/H60pJ4Ylhi,Incremental Method for Spectral Clustering of Increasing Orders. (arXiv:1512.07349v3 [cs.SI] UPDATED) https://t.co/H60pJ4Ylhi,0,2," Abstract: The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs) of a graph Laplacian matrix have been widely used for spectral clustering and community detection. However, in real-life applications the number of clusters or communities (say, $K$) is generally unknown a-priori. Consequently, the majority of the existing methods either choose $K$ heuristically or they repeat the clustering method with different choices of $K$ and accept the best clustering result. The first option, more often, yields suboptimal result, while the second option is computationally expensive. In this work, we propose an incremental method for constructing the eigenspectrum of the graph Laplacian matrix. This method leverages the eigenstructure of graph Laplacian matrix to obtain the $K$-th eigenpairs of the Laplacian matrix given a collection of all the $K-1$ smallest eigenpairs. Our proposed method adapts the Laplacian matrix such that the batch eigenvalue decomposition problem transforms into an efficient sequential leading eigenpair computation problem. As a practical application, we consider user-guided spectral clustering. Specifically, we demonstrate that users can utilize the proposed incremental method for effective eigenpair computation and determining the desired number of clusters based on multiple clustering metrics. "
735995729236398080,2016-05-27 00:47:10,https://t.co/whQ3fABhUv,Information-theoretic limits of learning the structure of Bayesian networks. (arXiv:1601.07460v2 [cs.LG] UPDATED) https://t.co/whQ3fABhUv,1,7," Abstract: In this paper, we study the information-theoretic limits of learning the structure of Bayesian networks (BNs), on discrete as well as continuous random variables, from a finite amount of data. We show that the minimum number of samples required by any procedure to recover the correct structure grows as $\Omega(m)$ and $\Omega(k \log m + (k^2/m))$ for non-sparse and sparse BNs respectively, where $m$ is the number of variables and $k$ is the maximum number of parents per node. We provide a simple recipe, based on an extension of the Fano's inequality, to obtain information-theoretic limits of structure recovery for any exponential family BN. We instantiate our result for specific conditional distributions in the exponential family to characterize the fundamental limits of learning various commonly used BNs, such as Conditional Probability Table (CPT) based networks, Gaussian BNs, Noisy-OR networks, and Logistic regression (LR) networks. For each of the aforementioned networks, we identify important parameters of the conditional distributions that affect the complexity of learning such models. We also show that an existing procedure called SparsityBoost, for learning binary CPT networks is not information-theoretically optimal in the number of variables. En route to obtaining our main results, we obtain tight bounds on the number of sparse and non-sparse essential-DAGs. Finally, as a byproduct, we recover the information-theoretic limits of sparse variable selection for logistic regression. "
735995728082931712,2016-05-27 00:47:10,https://t.co/rynITojbu8,Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings. (arXiv:1602.02373v2 [stat.ML]… https://t.co/rynITojbu8,0,3," Abstract: One-hot CNN (convolutional neural network) has been shown to be effective for text categorization (Johnson & Zhang, 2015). We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of `text region embedding + pooling'. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings. The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data. The results indicate that on this task, embeddings of text regions, which can convey complex concepts, are more useful than embeddings of single words in isolation. We report performances exceeding the previous best results on four benchmark datasets. "
735995726778503168,2016-05-27 00:47:10,https://t.co/DoVw33RqED,Auxiliary Deep Generative Models. (arXiv:1602.05473v2 [stat.ML] UPDATED) https://t.co/DoVw33RqED,0,4," Abstract: Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST, SVHN and NORB datasets. "
735630421510238208,2016-05-26 00:35:34,https://t.co/AuFOGRdJvu,Communication-efficient distributed statistical learning. (arXiv:1605.07689v1 [stat.ML]) https://t.co/AuFOGRdJvu,1,4," Abstract: We present a Communication-efficient Surrogate Likelihood (CSL) framework for solving distributed statistical inference problems. CSL provides a communication-efficient surrogate to the global likelihood that can be used for low-dimensional estimation, high-dimensional regularized estimation and Bayesian inference. For low-dimensional estimation, CSL provably improves upon naive averaging schemes and facilitates the construction of confidence intervals. For high-dimensional regularized estimation, CSL leads to a minimax-optimal estimator with controlled communication cost. For Bayesian inference, CSL can be used to form a communication-efficient quasi-posterior distribution that converges to the true posterior. This quasi-posterior procedure significantly improves the computational efficiency of MCMC algorithms even in a non-distributed setting. We present both theoretical analysis and experiments to explore the properties of the CSL approximation. "
735630420671401984,2016-05-26 00:35:34,https://t.co/oO75145C4N,Exact Exponent in Optimal Rates for Crowdsourcing. (arXiv:1605.07696v1 [stat.ML]) https://t.co/oO75145C4N,0,1," Abstract: In many machine learning applications, crowdsourcing has become the primary means for label collection. In this paper, we study the optimal error rate for aggregating labels provided by a set of non-expert workers. Under the classic Dawid-Skene model, we establish matching upper and lower bounds with an exact exponent $mI(\pi)$ in which $m$ is the number of workers and $I(\pi)$ the average Chernoff information that characterizes the workers' collective ability. Such an exact characterization of the error exponent allows us to state a precise sample size requirement $m>\frac{1}{I(\pi)}\log\frac{1}{\epsilon}$ in order to achieve an $\epsilon$ misclassification error. In addition, our results imply the optimality of various EM algorithms for crowdsourcing initialized by consistent estimators. "
735630419757010944,2016-05-26 00:35:34,https://t.co/IqztO0W2cB,Deep Structured Energy Based Models for Anomaly Detection. (arXiv:1605.07717v1 [cs.LG]) https://t.co/IqztO0W2cB,0,2," Abstract: In this paper, we attack the anomaly detection problem by directly modeling the data distribution with deep architectures. We propose deep structured energy based models (DSEBMs), where the energy function is the output of a deterministic deep neural network with structure. We develop novel model architectures to integrate EBMs with different types of data such as static data, sequential data, and spatial data, and apply appropriate model architectures to adapt to the data structure. Our training algorithm is built upon the recent development of score matching \cite{sm}, which connects an EBM with a regularized autoencoder, eliminating the need for complicated sampling method. Statistically sound decision criterion can be derived for anomaly detection purpose from the perspective of the energy landscape of the data distribution. We investigate two decision criteria for performing anomaly detection: the energy score and the reconstruction error. Extensive empirical studies on benchmark tasks demonstrate that our proposed model consistently matches or outperforms all the competing methods. "
735630418855251968,2016-05-26 00:35:33,https://t.co/DdytqXbHbW,Reshaped Wirtinger Flow for Solving Quadratic Systems of Equations. (arXiv:1605.07719v1 [stat.ML]) https://t.co/DdytqXbHbW,0,1," Abstract: We study the phase retrieval problem, which solves quadratic system of equations, i.e., recovers a vector $\boldsymbol{x}\in \mathbb{R}^n$ from its magnitude measurements $y_i=|\langle \boldsymbol{a}_i, \boldsymbol{x}\rangle|, i=1,..., m$. We develop a gradient-like algorithm (referred to as RWF representing reshaped Wirtinger flow) by minimizing a nonconvex nonsmooth loss function. In comparison with existing nonconvex Wirtinger flow (WF) algorithm \cite{candes2015phase}, although the loss function becomes nonsmooth, it involves only the second power of variable and hence reduces the complexity. We show that for random Gaussian measurements, RWF enjoys geometric convergence to a global optimal point as long as the number $m$ of measurements is on the order of $n$, the dimension of the unknown $\boldsymbol{x}$. This improves the sample complexity of WF, and achieves the same sample complexity as truncated Wirtinger flow (TWF) \cite{chen2015solving}, but without truncation in gradient loop. Furthermore, RWF costs less computationally than WF, and runs faster numerically than both WF and TWF. We further develop the incremental (stochastic) reshaped Wirtinger flow (IRWF) and show that IRWF converges linearly to the true signal. We further establish performance guarantee of an existing Kaczmarz method for the phase retrieval problem based on its connection to IRWF. We also empirically demonstrate that IRWF outperforms existing ITWF algorithm (stochastic version of TWF) as well as other batch algorithms. "
735630418003779584,2016-05-26 00:35:33,https://t.co/ev7EVvpJ6y,"Data Programming: Creating Large Training Sets, Quickly. (arXiv:1605.07723v1 [stat.ML]) https://t.co/ev7EVvpJ6y",2,5," Abstract: Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users provide a set of labeling functions, which are programs that heuristically label large subsets of data points, albeit noisily. By viewing these labeling functions as implicitly describing a generative model for this noise, we show that we can recover the parameters of this model to ""denoise"" the training set. Then, we show how to modify a discriminative loss function to make it noise-aware. We demonstrate our method over a range of discriminative models including logistic regression and LSTMs. We establish theoretically that we can recover the parameters of these generative models in a handful of settings. Experimentally, on the 2014 TAC-KBP relation extraction challenge, we show that data programming would have obtained a winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a supervised LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way to create machine learning models for non-experts. "
735630417085227008,2016-05-26 00:35:33,https://t.co/kwNcxLRNTd,Virtual Adversarial Training for Semi-Supervised Text Classification. (arXiv:1605.07725v1 [stat.ML]) https://t.co/kwNcxLRNTd,1,3," Abstract: Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting. "
735630416103759872,2016-05-26 00:35:33,https://t.co/qmhja1rG5O,NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and Stochastic Optimization. (arXiv:1605.07747v1 [… https://t.co/qmhja1rG5O,0,1," Abstract: We study a stochastic and distributed algorithm for nonconvex problems whose objective consists of a sum of $N$ nonconvex $L_i/N$-smooth functions, plus a nonsmooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT) algorithm splits the problem into $N$ subproblems, and utilizes an augmented Lagrangian based primal-dual scheme to solve it in a distributed and stochastic manner. With a special non-uniform sampling, a version of NESTT achieves $\epsilon$-stationary solution using $\mathcal{O}((\sum_{i=1}^N\sqrt{L_i/N})^2/\epsilon)$ gradient evaluations, which can be up to $\mathcal{O}(N)$ times better than the (proximal) gradient descent methods. It also achieves Q-linear convergence rate for nonconvex $\ell_1$ penalized quadratic problems with polyhedral constraints. Further, we reveal a fundamental connection between primal-dual based methods and a few primal only methods such as IAG/SAG/SAGA. "
735630415130730497,2016-05-26 00:35:32,https://t.co/QABuk2a9Ma,Fast Algorithms for Robust PCA via Gradient Descent. (arXiv:1605.07784v1 [cs.IT]) https://t.co/QABuk2a9Ma,1,7," Abstract: We consider the problem of Robust PCA in the fully and partially observed settings. Without corruptions, this is the well-known matrix completion problem. From a statistical standpoint this problem has been recently well-studied, and conditions on when recovery is possible (how many observations do we need, how many corruptions can we tolerate) via polynomial-time algorithms is by now understood. This paper presents and analyzes a non-convex optimization approach that greatly reduces the computational complexity of the above problems, compared to the best available algorithms. In particular, in the fully observed case, with $r$ denoting rank and $d$ dimension, we reduce the complexity from $\mathcal{O}(r^2d^2\log(1/\varepsilon))$ to $\mathcal{O}(rd^2\log(1/\varepsilon))$ -- a big savings when the rank is big. For the partially observed case, we show the complexity of our algorithm is no more than $\mathcal{O}(r^4d \log d \log(1/\varepsilon))$. Not only is this the best-known run-time for a provable algorithm under partial observation, but in the setting where $r$ is small compared to $d$, it also allows for near-linear-in-$d$ run-time that can be exploited in the fully-observed case as well, by simply running our algorithm on a subset of the observations. "
735630414296064000,2016-05-26 00:35:32,https://t.co/JnvUtHqD9h,Asymptotically exact conditional inference in deep generative models and differentiable simulators. (arXiv:1605.07… https://t.co/JnvUtHqD9h,0,2, Abstract: Many generative models can be expressed as a differentiable function of random inputs drawn from some simple probability density. This framework includes both deep generative architectures such as Variational Autoencoders and a large class of 'likelihood-free' simulator models. We present a method for performing efficient MCMC inference in such models when conditioning on observations of the model output. For some models this offers an asymptotically exact inference method where Approximate Bayesian Computation might otherwise be employed. We use the intuition that inference corresponds to integrating a density across the manifold corresponding to the set of inputs consistent with the observed outputs. This motivates the use of a constrained variant of Hamiltonian Monte Carlo which leverages the smooth geometry of the manifold to coherently move between inputs exactly consistent with observations. We validate the method by performing inference tasks in a diverse set of models. 
735630413327175685,2016-05-26 00:35:32,https://t.co/Mq3xDtjLKn,Simultaneous Sparse Dictionary Learning and Pruning. (arXiv:1605.07870v1 [stat.ML]) https://t.co/Mq3xDtjLKn,0,2," Abstract: Dictionary learning is a cutting-edge area in imaging processing, that has recently led to state-of-the-art results in many signal processing tasks. The idea is to conduct a linear decomposition of a signal using a few atoms of a learned and usually over-completed dictionary instead of a pre-defined basis. Determining a proper size of the to-be-learned dictionary is crucial for both precision and efficiency of the process, while most of the existing dictionary learning algorithms choose the size quite arbitrarily. In this paper, a novel regularization method called the Grouped Smoothly Clipped Absolute Deviation (GSCAD) is employed for learning the dictionary. The proposed method can simultaneously learn a sparse dictionary and select the appropriate dictionary size. Efficient algorithm is designed based on the alternative direction method of multipliers (ADMM) which decomposes the joint non-convex problem with the non-convex penalty into two convex optimization problems. Several examples are presented for image denoising and the experimental results are compared with other state-of-the-art approaches. "
735630412484104193,2016-05-26 00:35:32,https://t.co/6Yfi60nof2,How priors of initial hyperparameters affect Gaussian process regression models. (arXiv:1605.07906v1 [stat.ML]) https://t.co/6Yfi60nof2,0,10," Abstract: Gaussian Process Regression (GPR) is a kernel-based nonparametric method and has been proved to be effective and powerful. Its performance, however, relies on appropriate selection of kernel and the involving hyperparameters. The hyperparameters for a specified kernel are often estimated from the data via the maximum marginal likelihood. Unfortunately, the marginal likelihood functions are not usually convex with respect to the hyperparameters, therefore the optimization may not converge to global maxima. A common approach to tackle this issue is to use multiple starting points randomly selected from a specific prior distribution. Therefore, the choice of prior distribution may play a vital rule in the usefulness of this approach. In this paper, we study the sensitivity of prior distributions to the hyperparameter estimation and the performance of GPR. We consider different types of priors, including vague and data-dominated, for the initial values of hyperparameters for some commonly used kernels and investigate the influence of the priors on the performance of GPR models. The results show that the sensitivity of the hyperparameter estimation depends on the choice of kernels, but the priors have little influence on the performance of the GPR models in terms of predictability. "
735630411573907460,2016-05-26 00:35:32,https://t.co/vIq5o9hga1,A First Order Free Lunch for SQRT-Lasso. (arXiv:1605.07950v1 [cs.LG]) https://t.co/vIq5o9hga1,1,5," Abstract: Many statistical machine learning techniques sacrifice convenient computational structures to gain estimation robustness and modeling flexibility. In this paper, we study this fundamental tradeoff through a SQRT-Lasso problem for sparse linear regression and sparse precision matrix estimation in high dimensions. We explain how novel optimization techniques help address these computational challenges. Particularly, we propose a pathwise iterative smoothing shrinkage thresholding algorithm for solving the SQRT-Lasso optimization problem. We further provide a novel model-based perspective for analyzing the smoothing optimization framework, which allows us to establish a nearly linear convergence (R-linear convergence) guarantee for our proposed algorithm. This implies that solving the SQRT-Lasso optimization is almost as easy as solving the Lasso optimization. Moreover, we show that our proposed algorithm can also be applied to sparse precision matrix estimation, and enjoys good computational properties. Numerical experiments are provided to support our theory. "
735630410592464896,2016-05-26 00:35:31,https://t.co/AwPns2XYbm,Efficient Distributed Learning with Sparsity. (arXiv:1605.07991v1 [stat.ML]) https://t.co/AwPns2XYbm,0,3," Abstract: We propose a novel, efficient approach for distributed sparse learning in high-dimensions, where observations are randomly partitioned across machines. Computationally, at each round our method only requires the master machine to solve a shifted ell_1 regularized M-estimation problem, and other workers to compute the gradient. In respect of communication, the proposed approach provably matches the estimation error bound of centralized methods within constant rounds of communications (ignoring logarithmic factors). We conduct extensive experiments on both simulated and real world datasets, and demonstrate encouraging performances on high-dimensional regression and classification tasks. "
735630408952451072,2016-05-26 00:35:31,https://t.co/sHN8QcG17q,Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. (arXiv:1506.02142v5 [stat.ML… https://t.co/sHN8QcG17q,6,16," Abstract: Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning. "
735630408021356544,2016-05-26 00:35:31,https://t.co/fVh8OT2udG,Dropout as a Bayesian Approximation: Appendix. (arXiv:1506.02157v5 [stat.ML] UPDATED) https://t.co/fVh8OT2udG,0,6," Abstract: We show that a neural network with arbitrary depth and non-linearities, with dropout applied before every weight layer, is mathematically equivalent to an approximation to a well known Bayesian model. This interpretation might offer an explanation to some of dropout's key properties, such as its robustness to over-fitting. Our interpretation allows us to reason about uncertainty in deep learning, and allows the introduction of the Bayesian machinery into existing deep learning frameworks in a principled way. This document is an appendix for the main paper ""Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"" by Gal and Ghahramani, 2015. "
735630407027298304,2016-05-26 00:35:31,https://t.co/anFP8tnLhF,Bidirectional Helmholtz Machines. (arXiv:1506.03877v5 [cs.LG] UPDATED) https://t.co/anFP8tnLhF,0,5," Abstract: Efficient unsupervised training and inference in deep generative models remains a challenging problem. One basic approach, called Helmholtz machine, involves training a top-down directed generative model together with a bottom-up auxiliary model used for approximate inference. Recent results indicate that better generative models can be obtained with better approximate inference procedures. Instead of improving the inference procedure, we here propose a new model which guarantees that the top-down and bottom-up distributions can efficiently invert each other. We achieve this by interpreting both the top-down and the bottom-up directed models as approximate inference distributions and by defining the model distribution to be the geometric mean of these two. We present a lower-bound for the likelihood of this model and we show that optimizing this bound regularizes the model so that the Bhattacharyya distance between the bottom-up and top-down approximate distributions is minimized. This approach results in state of the art generative models which prefer significantly deeper architectures while it allows for orders of magnitude more efficient approximate inference. "
735630406066802688,2016-05-26 00:35:30,https://t.co/57HhdfTrsc,A Novel Approach for Stable Selection of Informative Redundant Features from High Dimensional fMRI Data. (arXiv:15… https://t.co/57HhdfTrsc,0,2," Abstract: Feature selection is among the most important components because it not only helps enhance the classification accuracy, but also or even more important provides potential biomarker discovery. However, traditional multivariate methods is likely to obtain unstable and unreliable results in case of an extremely high dimensional feature space and very limited training samples, where the features are often correlated or redundant. In order to improve the stability, generalization and interpretations of the discovered potential biomarker and enhance the robustness of the resultant classifier, the redundant but informative features need to be also selected. Therefore we introduced a novel feature selection method which combines a recent implementation of the stability selection approach and the elastic net approach. The advantage in terms of better control of false discoveries and missed discoveries of our approach, and the resulted better interpretability of the obtained potential biomarker is verified in both synthetic and real fMRI experiments. In addition, we are among the first to demonstrate the robustness of feature selection benefiting from the incorporation of stability selection and also among the first to demonstrate the possible unrobustness of the classical univariate two-sample t-test method. Specifically, we show the robustness of our feature selection results in existence of noisy (wrong) training labels, as well as the robustness of the resulted classifier based on our feature selection results in the existence of data variation, demonstrated by a multi-center attention-deficit/hyperactivity disorder (ADHD) fMRI data. "
735630405148233729,2016-05-26 00:35:30,https://t.co/YNTIgnkiM2,Stochastically Transitive Models for Pairwise Comparisons: Statistical and Computational Issues. (arXiv:1510.05610… https://t.co/YNTIgnkiM2,0,2," Abstract: There are various parametric models for analyzing pairwise comparison data, including the Bradley-Terry-Luce (BTL) and Thurstone models, but their reliance on strong parametric assumptions is limiting. In this work, we study a flexible model for pairwise comparisons, under which the probabilities of outcomes are required only to satisfy a natural form of stochastic transitivity. This class includes parametric models including the BTL and Thurstone models as special cases, but is considerably more general. We provide various examples of models in this broader stochastically transitive class for which classical parametric models provide poor fits. Despite this greater flexibility, we show that the matrix of probabilities can be estimated at the same rate as in standard parametric models. On the other hand, unlike in the BTL and Thurstone models, computing the minimax-optimal estimator in the stochastically transitive model is non-trivial, and we explore various computationally tractable alternatives. We show that a simple singular value thresholding algorithm is statistically consistent but does not achieve the minimax rate. We then propose and study algorithms that achieve the minimax rate over interesting sub-classes of the full stochastically transitive class. We complement our theoretical results with thorough numerical simulations. "
735630403944480769,2016-05-26 00:35:30,https://t.co/nYRcMz2Y4b,Learning Nonparametric Forest Graphical Models with Prior Information. (arXiv:1511.03796v2 [stat.ME] UPDATED) https://t.co/nYRcMz2Y4b,0,4," Abstract: We present a framework for incorporating prior information into nonparametric estimation of graphical models. To avoid distributional assumptions, we restrict the graph to be a forest and build on the work of forest density estimation (FDE). We reformulate the FDE approach from a Bayesian perspective, and introduce prior distributions on the graphs. As two concrete examples, we apply this framework to estimating scale-free graphs and learning multiple graphs with similar structures. The resulting algorithms are equivalent to finding a maximum spanning tree of a weighted graph with a penalty term on the connectivity pattern of the graph. We solve the optimization problem via a minorize-maximization procedure with Kruskal's algorithm. Simulations show that the proposed methods outperform competing parametric methods, and are robust to the true data distribution. They also lead to improvement in predictive power and interpretability in two real data sets. "
735630403093024772,2016-05-26 00:35:30,https://t.co/InT5tFpagj,A Theoretically Grounded Application of Dropout in Recurrent Neural Networks. (arXiv:1512.05287v3 [stat.ML] UPDATE… https://t.co/InT5tFpagj,5,10," Abstract: Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning. "
735630402208038914,2016-05-26 00:35:29,https://t.co/BzdHA2r3Wd,Additive Approximations in High Dimensional Nonparametric Regression via the SALSA. (arXiv:1602.00287v3 [stat.ML] … https://t.co/BzdHA2r3Wd,0,4," Abstract: High dimensional nonparametric regression is an inherently difficult problem with known lower bounds depending exponentially in dimension. A popular strategy to alleviate this curse of dimensionality has been to use additive models of \emph{first order}, which model the regression function as a sum of independent functions on each dimension. Though useful in controlling the variance of the estimate, such models are often too restrictive in practical settings. Between non-additive models which often have large variance and first order additive models which have large bias, there has been little work to exploit the trade-off in the middle via additive models of intermediate order. In this work, we propose SALSA, which bridges this gap by allowing interactions between variables, but controls model capacity by limiting the order of interactions. SALSA minimises the residual sum of squares with squared RKHS norm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression with an additive kernel. When the regression function is additive, the excess risk is only polynomial in dimension. Using the Girard-Newton formulae, we efficiently sum over a combinatorial number of terms in the additive expansion. Via a comparison on $15$ real datasets, we show that our method is competitive against $21$ other alternatives. "
735630401255968769,2016-05-26 00:35:29,https://t.co/KZzF6LD259,Preconditioning Kernel Matrices. (arXiv:1602.06693v2 [stat.ML] UPDATED) https://t.co/KZzF6LD259,0,2," Abstract: The computational and storage complexity of kernel machines presents the primary barrier to their scaling to large, modern, datasets. A common way to tackle the scalability issue is to use the conjugate gradient algorithm, which relieves the constraints on both storage (the kernel matrix need not be stored) and computation (both stochastic gradients and parallelization can be used). Even so, conjugate gradient is not without its own issues: the conditioning of kernel matrices is often such that conjugate gradients will have poor convergence in practice. Preconditioning is a common approach to alleviating this issue. Here we propose preconditioned conjugate gradients for kernel machines, and develop a broad range of preconditioners particularly useful for kernel matrices. We describe a scalable approach to both solving kernel machines and learning their hyperparameters. We show this approach is exact in the limit of iterations and outperforms state-of-the-art approximations for a given computational budget. "
735630400236728321,2016-05-26 00:35:29,https://t.co/U0tlcwR7wL,One-Shot Generalization in Deep Generative Models. (arXiv:1603.05106v2 [stat.ML] UPDATED) https://t.co/U0tlcwR7wL,2,15," Abstract: Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples---having seen new examples just once---providing an important class of general-purpose models for one-shot machine learning. "
735630399376855040,2016-05-26 00:35:29,https://t.co/MBNDvdeXAx,Composing graphical models with neural networks for structured representations and fast inference. (arXiv:1603.062… https://t.co/MBNDvdeXAx,5,7," Abstract: We propose a general modeling and inference framework that composes probabilistic graphical models with deep learning methods and combines their respective strengths. Our model family augments graphical structure in latent variables with neural network observation models. For inference, we extend variational autoencoders to use graphical model approximating distributions with recognition networks that output conjugate potentials. All components of these models are learned simultaneously with a single objective, giving a scalable algorithm that leverages stochastic variational inference, natural gradients, graphical model message passing, and the reparameterization trick. We illustrate this framework with several example models and an application to mouse behavioral phenotyping. "
735630398475128833,2016-05-26 00:35:29,https://t.co/vhPZCGBSWw,COCO: A Platform for Comparing Continuous Optimizers in a Black-Box Setting. (arXiv:1603.08785v2 [cs.AI] UPDATED) https://t.co/vhPZCGBSWw,0,2," Abstract: COCO is a platform for Comparing Continuous Optimizers in a black-box setting. It aims at automatizing the tedious and repetitive task of benchmarking numerical optimization algorithms to the greatest possible extent. We present the rationals behind the development of the platform as a general proposition for a guideline towards better benchmarking. We detail underlying fundamental concepts of COCO such as its definition of a problem, the idea of instances, the relevance of target values, and runtime as central performance measure. Finally, we give a quick overview of the basic code structure and the available test suites. "
735630397200027649,2016-05-26 00:35:28,https://t.co/xPReIzke7e,Distributed Clustering of Linear Bandits in Peer to Peer Networks. (arXiv:1604.07706v2 [cs.LG] UPDATED) https://t.co/xPReIzke7e,0,2," Abstract: We provide two distributed confidence ball algorithms for solving linear bandit problems in peer to peer networks with limited communication capabilities. For the first, we assume that all the peers are solving the same linear bandit problem, and prove that our algorithm achieves the optimal asymptotic regret rate of any centralised algorithm that can instantly communicate information between the peers. For the second, we assume that there are clusters of peers solving the same bandit problem within each cluster, and we prove that our algorithm discovers these clusters, while achieving the optimal asymptotic regret rate within each one. Through experiments on several real-world datasets, we demonstrate the performance of proposed algorithms compared to the state-of-the-art. "
735268740284219392,2016-05-25 00:38:22,https://t.co/rpilrb1Ihb,Kernel-based Reconstruction of Graph Signals. (arXiv:1605.07174v1 [stat.ML]) https://t.co/rpilrb1Ihb,0,3," Abstract: A number of applications in engineering, social sciences, physics, and biology involve inference over networks. In this context, graph signals are widely encountered as descriptors of vertex attributes or features in graph-structured data. Estimating such signals in all vertices given noisy observations of their values on a subset of vertices has been extensively analyzed in the literature of signal processing on graphs (SPoG). This paper advocates kernel regression as a framework generalizing popular SPoG modeling and reconstruction and expanding their capabilities. Formulating signal reconstruction as a regression task on reproducing kernel Hilbert spaces of graph signals permeates benefits from statistical learning, offers fresh insights, and allows for estimators to leverage richer forms of prior information than existing alternatives. A number of SPoG notions such as bandlimitedness, graph filters, and the graph Fourier transform are naturally accommodated in the kernel framework. Additionally, this paper capitalizes on the so-called representer theorem to devise simpler versions of existing Thikhonov regularized estimators, and offers a novel probabilistic interpretation of kernel methods on graphs based on graphical models. Motivated by the challenges of selecting the bandwidth parameter in SPoG estimators or the kernel map in kernel-based methods, the present paper further proposes two multi-kernel approaches with complementary strengths. Whereas the first enables estimation of the unknown bandwidth of bandlimited signals, the second allows for efficient graph filter selection. Numerical tests with synthetic as well as real data demonstrate the merits of the proposed methods relative to state-of-the-art alternatives. "
735268739231428608,2016-05-25 00:38:22,https://t.co/an4bSJycLf,Global Optimality of Local Search for Low Rank Matrix Recovery. (arXiv:1605.07221v1 [stat.ML]) https://t.co/an4bSJycLf,0,2," Abstract: We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements. With noisy measurements we show all local minima are very close to a global optimum. Together with a curvature bound at saddle points, this yields a polynomial time global convergence guarantee for stochastic gradient descent {\em from random initialization}. "
735268738082148352,2016-05-25 00:38:22,https://t.co/jeeQJ6PIXO,Convergence guarantees for kernel-based quadrature rules in misspecified settings. (arXiv:1605.07254v1 [stat.ML]) https://t.co/jeeQJ6PIXO,0,2," Abstract: Kernel-based quadrature rules are becoming important in machine learning and statistics, as they achieve super-$\sqrt{n}$ convergence rates in numerical integration, and thus provide alternatives to Monte Carlo integration in challenging settings where integrands are expensive to evaluate or where integrands are high dimensional. These rules are based on the assumption that the integrand has a certain degree of smoothness, which is expressed as that the integrand belongs to a certain reproducing kernel Hilbert space (RKHS). However, this assumption can be violated in practice (e.g., when the integrand is a black box function), and no general theory has been established for the convergence of kernel quadratures in such misspecified settings. Our contribution is in proving that kernel quadratures can be consistent even when the integrand does not belong to the assumed RKHS, i.e., when the integrand is less smooth than assumed. Specifically, we derive convergence rates that depend on the (unknown) lesser smoothness of the integrand, where the degree of smoothness is expressed via powers of RKHSs or via Sobolev spaces. "
735268737125916676,2016-05-25 00:38:22,https://t.co/vZbWShX0HO,Matrix Completion has No Spurious Local Minimum. (arXiv:1605.07272v1 [cs.LG]) https://t.co/vZbWShX0HO,0,4," Abstract: Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for \textit{positive semidefinite} matrix completion has no spurious local minima \--- all local minima must also be global. Therefore, many popular optimization algorithms such as (stochastic) gradient descent can provably solve positive semidefinite matrix completion with \textit{arbitrary} initialization in polynomial time. The result can be generalized to the setting when the observed entries contain noise. We believe that our main proof strategy can be useful for understanding geometric properties of other statistical problems involving partial or noisy observations. "
735268735288782848,2016-05-25 00:38:21,https://t.co/UcfHe9k1pe,Relevant sparse codes with variational information bottleneck. (arXiv:1605.07332v1 [stat.ML]) https://t.co/UcfHe9k1pe,0,5," Abstract: In many applications, it is desirable to extract only the relevant aspects of data. A principled way to do this is the information bottleneck (IB) method, where one seeks a code that maximizes information about a 'relevance' variable, Y, while constraining the information encoded about the original data, X. Unfortunately however, the IB method is computationally demanding when data are high-dimensional and/or non-gaussian. Here we propose an approximate variational scheme for maximizing a lower bound on the IB objective, analogous to variational EM. Using this method, we derive an IB algorithm to recover features that are both relevant and sparse. Finally, we demonstrate how kernelized versions of the algorithm can be used to address a broad range of problems with non-linear relation between X and Y. "
735268733141307392,2016-05-25 00:38:21,https://t.co/z92yVTzMby,Consistency Analysis for the Doubly Stochastic Dirichlet Process. (arXiv:1605.07358v1 [cs.IT]) https://t.co/z92yVTzMby,0,0," Abstract: This technical report proves components consistency for the Doubly Stochastic Dirichlet Process with exponential convergence of posterior probability. We also present the fundamental properties for DSDP as well as inference algorithms. Simulation toy experiment and real-world experiment results for single and multi-cluster also support the consistency proof. This report is also a support document for the paper ""Computationally Efficient Hyperspectral Data Learning Based on the Doubly Stochastic Dirichlet Process"". "
735268732143063040,2016-05-25 00:38:21,https://t.co/wWJaJZR6kO,Riemannian stochastic variance reduced gradient on Grassmann manifold. (arXiv:1605.07367v1 [cs.LG]) https://t.co/wWJaJZR6kO,0,4," Abstract: Stochastic variance reduction algorithms have recently become popular for minimizing the average of a large, but finite, number of loss functions. In this paper, we propose a novel Riemannian extension of the Euclidean stochastic variance reduced gradient algorithm (R-SVRG) to a compact manifold search space. To this end, we show the developments on the Grassmann manifold. The key challenges of averaging, addition, and subtraction of multiple gradients are addressed with notions like logarithm mapping and parallel translation of vectors on the Grassmann manifold. We present a global convergence analysis of the proposed algorithm with decay step-sizes and a local convergence rate analysis under fixed step-size with some natural assumptions. The proposed algorithm is applied on a number of problems on the Grassmann manifold like principal components analysis, low-rank matrix completion, and the Karcher mean computation. In all these cases, the proposed algorithm outperforms the standard Riemannian stochastic gradient descent algorithm. "
735268731018956802,2016-05-25 00:38:20,https://t.co/PEIOXYDBRC,Semiparametric energy-based probabilistic models. (arXiv:1605.07371v1 [q-bio.NC]) https://t.co/PEIOXYDBRC,1,1," Abstract: Probabilistic models can be defined by an energy function, where the probability of each state is proportional to the exponential of the state's negative energy. This paper considers a generalization of energy-based models in which the probability of a state is proportional to an arbitrary positive, strictly decreasing, and twice differentiable function of the state's energy. The precise shape of the nonlinear map from energies to unnormalized probabilities has to be learned from data together with the parameters of the energy function. As a case study we show that the above generalization of a fully visible Boltzmann machine yields an accurate model of neural activity of retinal ganglion cells. We attribute this success to the model's ability to easily capture distributions whose probabilities span a large dynamic range, a possible consequence of latent variables that globally couple the system. Similar features have recently been observed in many datasets, suggesting that our new method has wide applicability. "
735268730150768641,2016-05-25 00:38:20,https://t.co/rAm4sKameX,Refined Lower Bounds for Adversarial Bandits. (arXiv:1605.07416v1 [math.ST]) https://t.co/rAm4sKameX,0,1," Abstract: We provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms. The new results show that recent upper bounds that either (a) hold with high-probability or (b) depend on the total lossof the best arm or (c) depend on the quadratic variation of the losses, are close to tight. Besides this we prove two impossibility results. First, the existence of a single arm that is optimal in every round cannot improve the regret in the worst case. Second, the regret cannot scale with the effective range of the losses. In contrast, both results are possible in the full-information setting. "
735268729295081472,2016-05-25 00:38:20,https://t.co/kDgtRvMPho,Web-scale Topic Models in Spark: An Asynchronous Parameter Server. (arXiv:1605.07422v1 [cs.DC]) https://t.co/kDgtRvMPho,3,4," Abstract: In this paper, we train a Latent Dirichlet Allocation (LDA) topic model on the ClueWeb12 data set, a 27-terabyte Web crawl. We extend Spark, a popular framework for performing large-scale data analysis, with an asynchronous parameter server. Such a parameter server provides a distributed and concurrently accessed parameter space for the model. A Metropolis-Hastings based collapsed Gibbs sampler is implemented using this parameter server achieving an amortized O(1) sampling complexity. We compare our implementation to the default Spark implementations and show that it is significantly faster and more scalable without sacrificing model quality. A topic model with 1,000 topics is trained on the full ClueWeb12 data set, uncovering some of the prevalent themes that appear on the Web. "
735268728229789696,2016-05-25 00:38:20,https://t.co/Off5HwxrAj,Hierarchical Memory Networks. (arXiv:1605.07427v1 [stat.ML]) https://t.co/Off5HwxrAj,3,11," Abstract: Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task. "
735268727244066816,2016-05-25 00:38:19,https://t.co/xZBvi8TSvD,A note on privacy preserving iteratively reweighted least squares. (arXiv:1605.07511v1 [cs.CR]) https://t.co/xZBvi8TSvD,0,1," Abstract: Iteratively reweighted least squares (IRLS) is a widely-used method in machine learning to estimate the parameters in the generalised linear models. In particular, IRLS for L1 minimisation under the linear model provides a closed-form solution in each step, which is a simple multiplication between the inverse of the weighted second moment matrix and the weighted first moment vector. When dealing with privacy sensitive data, however, developing a privacy preserving IRLS algorithm faces two challenges. First, due to the inversion of the second moment matrix, the usual sensitivity analysis in differential privacy incorporating a single datapoint perturbation gets complicated and often requires unrealistic assumptions. Second, due to its iterative nature, a significant cumulative privacy loss occurs. However, adding a high level of noise to compensate for the privacy loss hinders from getting accurate estimates. Here, we develop a practical algorithm that overcomes these challenges and outputs privatised and accurate IRLS solutions. In our method, we analyse the sensitivity of each moments separately and treat the matrix inversion and multiplication as a post-processing step, which simplifies the sensitivity analysis. Furthermore, we apply the {\it{concentrated differential privacy}} formalism, a more relaxed version of differential privacy, which requires adding a significantly less amount of noise for the same level of privacy guarantee, compared to the conventional and advanced compositions of differentially private mechanisms. "
735268726254276608,2016-05-25 00:38:19,https://t.co/ySS9c5ye2K,Inductive quantum learning: Why you are doing it almost right. (arXiv:1605.07541v1 [cs.LG]) https://t.co/ySS9c5ye2K,0,4," Abstract: In supervised learning, an inductive learning algorithm extracts general rules from observed training instances, then the rules are applied to test instances. We show that this splitting of training and application arises naturally, in the classical setting, from a simple independence requirement with a physical interpretation of being non-signalling. Thus, two seemingly different definitions of inductive learning happen to coincide. This follows from very specific properties of classical information, which break down in the quantum setup. We prove a quantum de Finetti theorem for quantum channels, which shows that in the quantum case, the equivalence holds in the asymptotic setting (for large number of test instances). This reveals a natural analogy between classical learning protocols and their quantum counterparts, thus allowing to naturally enquire about standard elements in computational learning theory, such as structural risk minimization, model and sample complexity. "
735268725402828800,2016-05-25 00:38:19,https://t.co/mvghLQjm6e,Sequential Neural Models with Stochastic Layers. (arXiv:1605.07571v1 [stat.ML]) https://t.co/mvghLQjm6e,0,5," Abstract: How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling. "
735268723062362112,2016-05-25 00:38:18,https://t.co/LBBCzWJBe3,Local Minimax Complexity of Stochastic Convex Optimization. (arXiv:1605.07596v1 [stat.ML]) https://t.co/LBBCzWJBe3,0,1," Abstract: We extend the traditional worst-case, minimax analysis of stochastic convex optimization by introducing a localized form of minimax complexity for individual functions. Our main result gives function-specific lower and upper bounds on the number of stochastic subgradient evaluations needed to optimize either the function or its ""hardest local alternative"" to a given numerical precision. The bounds are expressed in terms of a localized and computational analogue of the modulus of continuity that is central to statistical minimax analysis. We show how the computational modulus of continuity can be explicitly calculated in concrete cases, and relates to the curvature of the function at the optimum. We also prove a superefficiency result that demonstrates it is a meaningful benchmark, acting as a computational analogue of the Fisher information in statistical estimation. The nature and practical implications of the results are demonstrated in simulations. "
735268722164785152,2016-05-25 00:38:18,https://t.co/hM8wbtKljP,Posterior Dispersion Indices. (arXiv:1605.07604v1 [stat.ML]) https://t.co/hM8wbtKljP,1,1," Abstract: Probabilistic modeling is cyclical: we specify a model, infer its posterior, and evaluate its performance. Evaluation drives the cycle, as we revise our model based on how it performs. This requires a metric. Traditionally, predictive accuracy prevails. Yet, predictive accuracy does not tell the whole story. We propose to evaluate a model through posterior dispersion. The idea is to analyze how each datapoint fares in relation to posterior uncertainty around the hidden structure. We propose a family of posterior dispersion indices (PDI) that capture this idea. A PDI identifies rich patterns of model mismatch in three real data examples: voting preferences, supermarket shopping, and population genetics. "
735268721262989312,2016-05-25 00:38:18,https://t.co/AprHV8q219,Optimal Stochastic Strongly Convex Optimization with a Logarithmic Number of Projections. (arXiv:1304.5504v6 [cs.L… https://t.co/AprHV8q219,0,3," Abstract: We consider stochastic strongly convex optimization with a complex inequality constraint. This complex inequality constraint may lead to computationally expensive projections in algorithmic iterations of the stochastic gradient descent~(SGD) methods. To reduce the computation costs pertaining to the projections, we propose an Epoch-Projection Stochastic Gradient Descent~(Epro-SGD) method. The proposed Epro-SGD method consists of a sequence of epochs; it applies SGD to an augmented objective function at each iteration within the epoch, and then performs a projection at the end of each epoch. Given a strongly convex optimization and for a total number of $T$ iterations, Epro-SGD requires only $\log(T)$ projections, and meanwhile attains an optimal convergence rate of $O(1/T)$, both in expectation and with a high probability. To exploit the structure of the optimization problem, we propose a proximal variant of Epro-SGD, namely Epro-ORDA, based on the optimal regularized dual averaging method. We apply the proposed methods on real-world applications; the empirical results demonstrate the effectiveness of our methods. "
735268720227037185,2016-05-25 00:38:18,https://t.co/HR4QOL3Uo7,Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models. (arXiv:1412.7461v3 [st… https://t.co/HR4QOL3Uo7,0,1," Abstract: The future predictive performance of a Bayesian model can be estimated using Bayesian cross-validation. In this article, we consider Gaussian latent variable models where the integration over the latent values is approximated using the Laplace method or expectation propagation (EP). We study the properties of several Bayesian leave-one-out (LOO) cross-validation approximations that in most cases can be computed with a small additional cost after forming the posterior approximation given the full data. Our main objective is to assess the accuracy of the approximative LOO cross-validation estimators. That is, for each method (Laplace and EP) we compare the approximate fast computation with the exact brute force LOO computation. Secondarily, we evaluate the accuracy of the Laplace and EP approximations themselves against a ground truth established through extensive Markov chain Monte Carlo simulation. Our empirical results show that the approach based upon a Gaussian approximation to the LOO marginal distribution (the so-called cavity distribution) gives the most accurate and reliable results among the fast methods. "
735268719379816448,2016-05-25 00:38:17,https://t.co/AbU1sSHuNJ,Why Regularized Auto-Encoders learn Sparse Representation?. (arXiv:1505.05561v4 [stat.ML] UPDATED) https://t.co/AbU1sSHuNJ,0,7," Abstract: While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- \textit{Internal Covariate Shift}-- the current solution has certain drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (especially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size $ 1 $ during training. We address these drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call \textit{Normalization Propagation}. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers. "
735268718465458176,2016-05-25 00:38:17,https://t.co/0186fvwhUv,Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays. (arXiv:… https://t.co/0186fvwhUv,2,4," Abstract: We discuss a multiple-play multi-armed bandit (MAB) problem in which several arms are selected at each round. Recently, Thompson sampling (TS), a randomized algorithm with a Bayesian spirit, has attracted much attention for its empirically excellent performance, and it is revealed to have an optimal regret bound in the standard single-play MAB problem. In this paper, we propose the multiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the multiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS for binary rewards has the optimal regret upper bound that matches the regret lower bound provided by Anantharam et al. (1987). Therefore, MP-TS is the first computationally efficient algorithm with optimal regret. A set of computer simulations was also conducted, which compared MP-TS with state-of-the-art algorithms. We also propose a modification of MP-TS, which is shown to have better empirical performance. "
735268717274226689,2016-05-25 00:38:17,https://t.co/oGE8Ph0N3M,On the Sample Complexity of Learning Sparse Graphical Games. (arXiv:1601.07243v2 [cs.GT] UPDATED) https://t.co/oGE8Ph0N3M,0,2," Abstract: We analyze the sample complexity of learning sparse graphical games from purely behavioral data. That is, we assume that we can only observe the players' joint actions and not their payoffs. We analyze the sufficient and necessary number of samples for the correct recovery of the set of pure-strategy Nash equilibria (PSNE) of the true game. Our analysis focuses on sparse directed graphs with $n$ nodes and at most $k$ parents per node. By using VC dimension arguments, we show that if the number of samples is greater than ${O(k n \log^2{n})}$, then maximum likelihood estimation correctly recovers the PSNE with high probability. By using information-theoretic arguments, we show that if the number of samples is less than ${\Omega(k n \log^2{n})}$, then any conceivable method fails to recover the PSNE with arbitrary probability. "
735268716204691456,2016-05-25 00:38:17,https://t.co/lqrmu0giJo,Information-Theoretic Lower Bounds for Recovery of Diffusion Network Structures. (arXiv:1601.07932v2 [cs.LG] UPDAT… https://t.co/lqrmu0giJo,0,3," Abstract: We study the information-theoretic lower bound of the sample complexity of the correct recovery of diffusion network structures. We introduce a discrete-time diffusion model based on the Independent Cascade model for which we obtain a lower bound of order $\Omega(k \log p)$, for directed graphs of $p$ nodes, and at most $k$ parents per node. Next, we introduce a continuous-time diffusion model, for which a similar lower bound of order $\Omega(k \log p)$ is obtained. Our results show that the algorithm of Pouget-Abadie et al. is statistically optimal for the discrete-time regime. Our work also opens the question of whether it is possible to devise an optimal algorithm for the continuous-time regime. "
735268715290300417,2016-05-25 00:38:17,https://t.co/HotqYoNHIJ,Minimum Regret Search for Single- and Multi-Task Optimization. (arXiv:1602.01064v3 [stat.ML] UPDATED) https://t.co/HotqYoNHIJ,0,1," Abstract: We propose minimum regret search (MRS), a novel acquisition function for Bayesian optimization. MRS bears similarities with information-theoretic approaches such as entropy search (ES). However, while ES aims in each query at maximizing the information gain with respect to the global maximum, MRS aims at minimizing the expected simple regret of its ultimate recommendation for the optimum. While empirically ES and MRS perform similar in most of the cases, MRS produces fewer outliers with high simple regret than ES. We provide empirical results both for a synthetic single-task optimization problem as well as for a simulated multi-task robotic control problem. "
735268714191421444,2016-05-25 00:38:16,https://t.co/b49lqlHSzO,A Kronecker-factored approximate Fisher matrix for convolution layers. (arXiv:1602.01407v2 [stat.ML] UPDATED) https://t.co/b49lqlHSzO,0,5," Abstract: Second-order optimization methods such as natural gradient descent have the potential to speed up training of neural networks by correcting for the curvature of the loss function. Unfortunately, the exact natural gradient is impractical to compute for large models, and most approximations either require an expensive iterative procedure or make crude approximations to the curvature. We present Kronecker Factors for Convolution (KFC), a tractable approximation to the Fisher matrix for convolutional networks based on a structured probabilistic model for the distribution over backpropagated derivatives. Similarly to the recently proposed Kronecker-Factored Approximate Curvature (K-FAC), each block of the approximate Fisher matrix decomposes as the Kronecker product of small matrices, allowing for efficient inversion. KFC captures important curvature information while still yielding comparably efficient updates to stochastic gradient descent (SGD). We show that the updates are invariant to commonly used reparameterizations, such as centering of the activations. In our experiments, approximate natural gradient descent with KFC was able to train convolutional networks several times faster than carefully tuned SGD. Furthermore, it was able to train the networks in 10-20 times fewer iterations than SGD, suggesting its potential applicability in a distributed setting. "
735268713205747712,2016-05-25 00:38:16,https://t.co/LLjzZfLatL,Ladder Variational Autoencoders. (arXiv:1602.02282v2 [stat.ML] UPDATED) https://t.co/LLjzZfLatL,0,8," Abstract: Variational Autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers. "
735268712308178944,2016-05-25 00:38:16,https://t.co/Q0EuiRgLzu,Contextual-MDPs for PAC-Reinforcement Learning with Rich Observations. (arXiv:1602.02722v3 [cs.LG] UPDATED) https://t.co/Q0EuiRgLzu,0,3," Abstract: We propose and study a new model for reinforcement learning with rich observations, generalizing contextual bandits to sequential decision making. These models require an agent to take actions based on observations (features) with the goal of achieving long-term performance competitive with a large set of policies. To avoid barriers to sample-efficient learning associated with large observation spaces and general POMDPs, we focus on problems that can be summarized by a small number of hidden states and have long-term rewards that are predictable by a reactive function class. In this setting, we design and analyze a new reinforcement learning algorithm, Least Squares Value Elimination by Exploration. We prove that the algorithm learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. Our result provides theoretical justification for reinforcement learning with function approximation. "
735268711104434176,2016-05-25 00:38:16,https://t.co/37LGJ6YEYW,Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks. (arXiv:1… https://t.co/37LGJ6YEYW,0,2," Abstract: While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- Internal Covariate Shift-- the current solution has certain drawbacks. Specifically, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate for validation due to shifting parameter values (especially during initial training epochs). Also, BN cannot be used with batch-size 1 during training. We address these drawbacks by proposing a non-adaptive normalization technique for removing internal covariate shift, that we call Normalization Propagation. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers. "
735268710018076673,2016-05-25 00:38:15,https://t.co/GfW53Zf6Ev,Estimating parameters of nonlinear systems using~the elitist particle filter based on evolutionary strategies. (ar… https://t.co/GfW53Zf6Ev,0,2," Abstract: In this article, we present the elitist particle filter based on evolutionary strategies (EPFES) as an efficient approach for nonlinear system identification. The EPFES is derived from the frequently-employed state-space model, where the relevant information of the nonlinear system is captured by an unknown state vector. Similar to classical particle filtering, the EPFES consists of a set of particles and respective weights which represent different realizations of the latent state vector and their likelihood of being the solution of the optimization problem. As main innovation, the EPFES includes an evolutionary elitist-particle selection which combines long-term information with instantaneous sampling from an approximated continuous posterior distribution. In this article, we propose two advancements of the previously-published elitist-particle selection process. Further, the EPFES is shown to be a generalization of the widely-used Gaussian particle filter and thus evaluated with respect to the latter for two completely different scenarios: First, we consider the so-called univariate nonstationary growth model with time-variant latent state variable, where the evolutionary selection of elitist particles is evaluated for non-recursively calculated particle weights. Second, the problem of nonlinear acoustic echo cancellation is addressed in a simulated scenario with speech as input signal: By using long-term fitness measures, we highlight the efficacy of the well-generalizing EPFES in estimating the nonlinear system even for large search spaces. Finally, we illustrate similarities between the EPFES and evolutionary algorithms to outline future improvements by fusing the achievements of both fields of research. "
735268708868820994,2016-05-25 00:38:15,https://t.co/sthNuKTOKF,"Copeland Dueling Bandit Problem: Regret Lower Bound, Optimal Algorithm, and Computationally Efficient Algorithm. (… https://t.co/sthNuKTOKF",0,3," Abstract: We study the K-armed dueling bandit problem, a variation of the standard stochastic bandit problem where the feedback is limited to relative comparisons of a pair of arms. The hardness of recommending Copeland winners, the arms that beat the greatest number of other arms, is characterized by deriving an asymptotic regret bound. We propose Copeland Winners Relative Minimum Empirical Divergence (CW-RMED) and derive an asymptotically optimal regret bound for it. However, it is not known whether the algorithm can be efficiently computed or not. To address this issue, we devise an efficient version (ECW-RMED) and derive its asymptotic regret bound. Experimental comparisons of dueling bandit algorithms show that ECW-RMED significantly outperforms existing ones. "
735268707598012417,2016-05-25 00:38:15,https://t.co/HqX7Afi44w,Off-policy evaluation for slate recommendation. (arXiv:1605.04812v2 [cs.LG] UPDATED) https://t.co/HqX7Afi44w,0,3," Abstract: This paper studies the evaluation of policies that recommend an ordered set of items (e.g., a ranking) based on some context---a common scenario in web search, ads and recommender systems. We develop the first practical technique for evaluating page-level metrics of such policies offline using logged past data, alleviating the need for online A/B tests. Our method models the observed quality of the recommended set (e.g., time to success in web search) as an additive decomposition across items. Crucially, the per-item quality is not directly observed or easily modeled from the item's features. A thorough empirical evaluation reveals that this model fits many realistic measures of quality and theoretical analysis shows exponential savings in the amount of required data compared with prior off-policy evaluation approaches. "
734909898325143552,2016-05-24 00:52:28,https://t.co/pRNS2a4FLW,Influence Maximization with Semi-Bandit Feedback. (arXiv:1605.06593v1 [cs.LG]) https://t.co/pRNS2a4FLW,2,0," Abstract: We study a stochastic online problem of learning to influence in a social network with semi-bandit feedback, individual observations of how influenced users influence others. Our problem combines challenges of partial monitoring, because the learning agent only observes the influenced portion of the network, and combinatorial bandits, because the cardinality of the feasible set is exponential in the maximum number of influencers. We propose a computationally efficient UCB-like algorithm for solving our problem, IMLinUCB, and analyze it on forests. Our regret bounds are polynomial in all quantities of interest; reflect the structure of the network; and do not depend on inherently large quantities, such as the reciprocal of the minimum probability of being influenced and the cardinality of the action set. To the best of our knowledge, these are the first such results. IMLinUCB permits linear generalization and therefore is suitable for large-scale problems. We evaluate IMLinUCB on several synthetic problems and observe that the regret of IMLinUCB scales as suggested by our upper bounds. A special form of our problem can be viewed as a linear bandit and we match the regret bounds of LinUCB in this case. "
734909897133985793,2016-05-24 00:52:28,https://t.co/74S83Ue5Oo,Make Workers Work Harder: Decoupled Asynchronous Proximal Stochastic Gradient Descent. (arXiv:1605.06619v1 [math.O… https://t.co/74S83Ue5Oo,0,2," Abstract: Asynchronous parallel optimization algorithms for solving large-scale machine learning problems have drawn significant attention from academia to industry recently. This paper proposes a novel algorithm, decoupled asynchronous proximal stochastic gradient descent (DAP-SGD), to minimize an objective function that is the composite of the average of multiple empirical losses and a regularization term. Unlike the traditional asynchronous proximal stochastic gradient descent (TAP-SGD) in which the master carries much of the computation load, the proposed algorithm off-loads the majority of computation tasks from the master to workers, and leaves the master to conduct simple addition operations. This strategy yields an easy-to-parallelize algorithm, whose performance is justified by theoretical convergence analyses. To be specific, DAP-SGD achieves an $O(\log T/T)$ rate when the step-size is diminishing and an ergodic $O(1/\sqrt{T})$ rate when the step-size is constant, where $T$ is the number of total iterations. "
734909895741411328,2016-05-24 00:52:27,https://t.co/3EPKV3iF4r,Deep Transfer Learning with Joint Adaptation Networks. (arXiv:1605.06636v1 [cs.LG]) https://t.co/3EPKV3iF4r,0,4," Abstract: Deep networks rely on massive amounts of labeled data to learn powerful models. For a target task short of labeled data, transfer learning enables model adaptation from a different source domain. This paper addresses deep transfer learning under a more general scenario that the joint distributions of features and labels may change substantially across domains. Based on the theory of Hilbert space embedding of distributions, a novel joint distribution discrepancy is proposed to directly compare joint distributions across domains, eliminating the need of marginal-conditional factorization. Transfer learning is enabled in deep convolutional networks, where the dataset shifts may linger in multiple task-specific feature layers and the classifier layer. A set of joint adaptation networks are crafted to match the joint distributions of these layers across domains by minimizing the joint distribution discrepancy, which can be trained efficiently using back-propagation. Experiments show that the new approach yields state of the art results on standard domain adaptation datasets. "
734909894223138816,2016-05-24 00:52:27,https://t.co/YoJBWlYb6F,Factored Temporal Sigmoid Belief Networks for Sequence Learning. (arXiv:1605.06715v1 [stat.ML]) https://t.co/YoJBWlYb6F,0,1," Abstract: Deep conditional generative models are developed to simultaneously learn the temporal dependencies of multiple sequences. The model is designed by introducing a three-way weight tensor to capture the multiplicative interactions between side information and sequences. The proposed model builds on the Temporal Sigmoid Belief Network (TSBN), a sequential stack of Sigmoid Belief Networks (SBNs). The transition matrices are further factored to reduce the number of parameters and improve generalization. When side information is not available, a general framework for semi-supervised learning based on the proposed model is constituted, allowing robust sequence classification. Experimental results show that the proposed approach achieves state-of-the-art predictive and classification performance on sequential data, and has the capacity to synthesize sequences, with controlled style transitioning and blending. "
734909892922908672,2016-05-24 00:52:27,https://t.co/uI63HM9LIr,The De-Biased Whittle Likelihood for Second-Order Stationary Stochastic Processes. (arXiv:1605.06718v1 [stat.ME]) https://t.co/uI63HM9LIr,0,0," Abstract: The Whittle likelihood is a computationally efficient pseudo-maximum likelihood inference procedure which is known to produce biased parameter estimates for large classes of time series models. We propose a method for de-biasing Whittle likelihood parameter estimates for second-order stationary stochastic processes. We demonstrate how to compute the de-biased Whittle likelihood in the same $\mathcal{O}(n\log n)$ computational efficiency as standard Whittle likelihood. We prove that the method is consistent, and demonstrate its superior performance in simulation studies. We also demonstrate how the method can be easily combined with standard methods of bias reduction, such as tapering and differencing, to further reduce bias in parameter estimates. "
734909891626848256,2016-05-24 00:52:26,https://t.co/mhT1HglWAY,A Rapid Pattern-Recognition Method for Driving Types Using Clustering-Based Support Vector Machines. (arXiv:1605.0… https://t.co/mhT1HglWAY,0,0," Abstract: A rapid pattern-recognition approach to characterize driver's curve-negotiating behavior is proposed. To shorten the recognition time and improve the recognition of driving styles, a k-means clustering-based support vector machine ( kMC-SVM) method is developed and used for classifying drivers into two types: aggressive and moderate. First, vehicle speed and throttle opening are treated as the feature parameters to reflect the driving styles. Second, to discriminate driver curve-negotiating behaviors and reduce the number of support vectors, the k-means clustering method is used to extract and gather the two types of driving data and shorten the recognition time. Then, based on the clustering results, a support vector machine approach is utilized to generate the hyperplane for judging and predicting to which types the human driver are subject. Lastly, to verify the validity of the kMC-SVM method, a cross-validation experiment is designed and conducted. The research results show that the $ k $MC-SVM is an effective method to classify driving styles with a short time, compared with SVM method. "
734909890506948609,2016-05-24 00:52:26,https://t.co/FdYWU4mAj7,Interpretable Distribution Features with Maximum Testing Power. (arXiv:1605.06796v1 [stat.ML]) https://t.co/FdYWU4mAj7,0,1," Abstract: Two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features). The features are chosen so as to maximize the distinguishability of the distributions, by optimizing a lower bound on test power for a statistical test using these features. The result is a parsimonious and interpretable indication of how and where two distributions differ locally. An empirical estimate of the test power criterion converges with increasing sample size, ensuring the quality of the returned features. In real-world benchmarks on high-dimensional text and image data, linear-time tests using the proposed semimetrics achieve comparable performance to the state-of-the-art quadratic-time maximum mean discrepancy test, while returning human-interpretable features that explain the test results. "
734909889458409476,2016-05-24 00:52:26,https://t.co/2213lFAPFP,Causality on Longitudinal Data: Stable Specification Search in Constrained Structural Equation Modeling. (arXiv:16… https://t.co/2213lFAPFP,1,1," Abstract: A typical problem in causal modeling is the instability of model structure learning, i.e., small changes in finite data can result in completely different optimal models. The present work introduces a novel causal modeling algorithm for longitudinal data, that is robust for finite samples based on recent advances in stability selection using subsampling and selection algorithms. Our approach uses exploratory search but allows incorporation of prior knowledge, e.g., that causal relationships do not go back in time. We represent causal relationships using structural equation models. Models are scored along two objectives: the model fit and the model complexity. Since both objectives are often conflicting we apply a multi-objective evolutionary algorithm to search for Pareto optimal models. To handle the instability of small finite data samples, we repeatedly subsample the data and select those substructures (from the optimal models) that are both stable and parsimonious. These substructures can be visualized through a causal graph. Our more exploratory approach outperforms state-of-the-art alternative approaches on a simulated data set with a known ground truth. We also present the results of our method on three real-world longitudinal data sets on chronic fatigue syndrome, Alzheimer disease, and chronic kidney disease. The findings obtained with our approach are generally in line with results from more hypothesis-driven analyses in earlier studies and do suggest some novel relationships that deserve further research. "
734909888359505921,2016-05-24 00:52:26,https://t.co/hYfLDfkIAW,Smart broadcasting: Do you want to be seen?. (arXiv:1605.06855v1 [cs.SI]) https://t.co/hYfLDfkIAW,0,0," Abstract: Many users in online social networks are constantly trying to gain attention from their followers by broadcasting posts to them. These broadcasters are likely to gain greater attention if their posts can remain visible for a longer period of time among their followers' most recent feeds. Then when to post? In this paper, we study the problem of smart broadcasting using the framework of temporal point processes, where we model users feeds and posts as discrete events occurring in continuous time. Based on such continuous-time model, then choosing a broadcasting strategy for a user becomes a problem of designing the conditional intensity of her posting events. We derive a novel formula which links this conditional intensity with the visibility of the user in her followers' feeds. Furthermore, by exploiting this formula, we develop an efficient convex optimization framework for the when-to-post problem. Our method can find broadcasting strategies that reach a desired visibility level with provable guarantees. We experimented with data gathered from Twitter, and show that our framework can consistently make broadcasters' post more visible than alternatives. "
734909886937636864,2016-05-24 00:52:25,https://t.co/9YxrV1Cj4h,Fast Stochastic Methods for Nonsmooth Nonconvex Optimization. (arXiv:1605.06900v1 [math.OC]) https://t.co/9YxrV1Cj4h,0,3," Abstract: We analyze stochastic algorithms for optimizing nonconvex, nonsmooth finite-sum problems, where the nonconvex part is smooth and the nonsmooth part is convex. Surprisingly, unlike the smooth case, our knowledge of this fundamental problem is very limited. For example, it is not known whether the proximal stochastic gradient method with constant minibatch converges to a stationary point. To tackle this issue, we develop fast stochastic algorithms that provably converge to a stationary point for constant minibatches. Furthermore, using a variant of these algorithms, we show provably faster convergence than batch proximal gradient descent. Finally, we prove global linear convergence rate for an interesting subclass of nonsmooth nonconvex functions, that subsumes several recent works. This paper builds upon our recent series of papers on fast stochastic methods for smooth nonconvex optimization [22, 23], with a novel analysis for nonconvex and nonsmooth functions. "
734909885347958785,2016-05-24 00:52:25,https://t.co/heHgOyZlxS,Learning the Structure of Nonlinear Dynamical Networks: An Information-Theoretic Perspective. (arXiv:1605.06931v1 … https://t.co/heHgOyZlxS,0,6," Abstract: The behaviour of many real-world phenomena can be modelled by nonlinear dynamical systems whereby a latent system state is observed through a filter. We are interested in interacting subsystems of this form, which we model by a set of coupled maps as a synchronous update graph dynamical systems. Specifically, we study the structure learning problem for spatially distributed dynamical systems coupled via a directed acyclic graph. Unlike established structure learning procedures that find locally maximum posterior probabilities of a network structure containing latent variables, our work exploits the properties of dynamical systems to compute globally optimal approximations of these distributions. We arrive at this result by the use of time delay embedding theorems. Taking an information-theoretic perspective, we show that the log-likelihood has an intuitive interpretation in terms of information transfer. "
734909884093857792,2016-05-24 00:52:24,https://t.co/alEux9nKAF,A Sub-Quadratic Exact Medoid Algorithm. (arXiv:1605.06950v1 [stat.ML]) https://t.co/alEux9nKAF,0,1," Abstract: We present a new algorithm, trimed, for obtaining the medoid of a set, that is the element of the set which minimises the mean distance to all other elements. The algorithm is shown to have, under weak assumptions, complexity O(N^(3/2)) in R^d where N is the set size, making it the first sub-quadratic exact medoid algorithm for d>1. Experiments show that it performs very well on spatial network data, frequently requiring two orders of magnitude fewer distances than state-of-the-art approximate algorithms. We show how trimed can be used as a component in an accelerated K-medoids algorithm, and how it can be relaxed to obtain further computational gains with an only minor loss in quality. "
734909882860720128,2016-05-24 00:52:24,https://t.co/aD41wQoGti,Practical Privacy For Expectation Maximization. (arXiv:1605.06995v1 [cs.LG]) https://t.co/aD41wQoGti,0,2," Abstract: The iterative nature of the expectation maximization (EM) algorithm presents a challenge for privacy-preserving estimation, as each iteration increases the amount of noise needed. We propose a practical private EM algorithm that overcomes this challenge using two innovations: (1) a novel moment perturbation formulation for differentially private EM (DP-EM), and (2) the use of two recently developed composition methods to bound the privacy ""cost"" of multiple EM iterations: the moments accountant (MA) and zero-mean concentrated differential privacy (zCDP). Both MA and zCDP bound the moment generating function of the privacy loss random variable and achieve a refined tail bound, which effectively decrease the amount of additive noise. We present empirical results showing the benefits of our approach, as well as similar performance between these two composition methods in the DP-EM setting for Gaussian mixture models. Our approach can be readily extended to many iterative learning algorithms, opening up various exciting future directions. "
734909881300484096,2016-05-24 00:52:24,https://t.co/b1vczltXek,Online Learning with Feedback Graphs Without the Graphs. (arXiv:1605.07018v1 [cs.LG]) https://t.co/b1vczltXek,0,2," Abstract: We study an online learning framework introduced by Mannor and Shamir (2011) in which the feedback is specified by a graph, in a setting where the graph may vary from round to round and is \emph{never fully revealed} to the learner. We show a large gap between the adversarial and the stochastic cases. In the adversarial case, we prove that even for dense feedback graphs, the learner cannot improve upon a trivial regret bound obtained by ignoring any additional feedback besides her own loss. In contrast, in the stochastic case we give an algorithm that achieves $\widetilde \Theta(\sqrt{\alpha T})$ regret over $T$ rounds, provided that the independence numbers of the hidden feedback graphs are at most $\alpha$. We also extend our results to a more general feedback model, in which the learner does not necessarily observe her own loss, and show that, even in simple cases, concealing the feedback graphs might render a learnable problem unlearnable. "
734909879924690944,2016-05-24 00:52:23,https://t.co/joCDpvntEh,Tucker Gaussian Process for Regression and Collaborative Filtering. (arXiv:1605.07025v1 [stat.ML]) https://t.co/joCDpvntEh,0,1," Abstract: We introduce the Tucker Gaussian Process (TGP), a model for regression that regularises a Gaussian Process (GP) towards simpler regression functions for enhanced generalisation performance. We derive it using a novel approach to scalable GP learning, and show that our model is particularly well-suited to grid-structured data and problems where the dependence on covariates is close to being separable. A prime example is collaborative filtering, for which our model provides an effective GP based method that has a low-rank matrix factorisation at its core. We show that TGP generalises classical Bayesian matrix factorisation models, and goes beyond them to give a natural and elegant method for incorporating side information. "
734909878326681600,2016-05-24 00:52:23,https://t.co/I6cS1AjpcZ,Convergence Analysis for Rectangular Matrix Completion Using Burer-Monteiro Factorization and Gradient Descent. (a… https://t.co/I6cS1AjpcZ,0,1," Abstract: We address the rectangular matrix completion problem by lifting the unknown matrix to a positive semidefinite matrix in higher dimension, and optimizing a nonconvex objective over the semidefinite factor using a simple gradient descent scheme. With $O( \mu r^2 \kappa^2 n \max(\mu, \log n))$ random observations of a $n_1 \times n_2$ $\mu$-incoherent matrix of rank $r$ and condition number $\kappa$, where $n = \max(n_1, n_2)$, the algorithm linearly converges to the global optimum with high probability. "
734909876569309184,2016-05-24 00:52:23,https://t.co/hVr9Bsvc4j,Bayesian Model Selection of Stochastic Block Models. (arXiv:1605.07057v1 [stat.ML]) https://t.co/hVr9Bsvc4j,0,3," Abstract: A central problem in analyzing networks is partitioning them into modules or communities. One of the best tools for this is the stochastic block model, which clusters vertices into blocks with statistically homogeneous pattern of links. Despite its flexibility and popularity, there has been a lack of principled statistical model selection criteria for the stochastic block model. Here we propose a Bayesian framework for choosing the number of blocks as well as comparing it to the more elaborate degree- corrected block models, ultimately leading to a universal model selection framework capable of comparing multiple modeling combinations. We will also investigate its connection to the minimum description length principle. "
734909875080310784,2016-05-24 00:52:22,https://t.co/V1eTFBdehO,A Unifying Framework for Sparse Gaussian Process Approximation using Power Expectation Propagation. (arXiv:1605.07… https://t.co/V1eTFBdehO,0,7," Abstract: Gaussian processes (GPs) are flexible distributions over functions that enable high-level assumptions about unknown functions to be encoded in a parsimonious, flexible and general way. Although elegant, the application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. In this paper we develop a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that unifies a large number of these pseudo-point approximations. Unlike much of the previous venerable work in this area, the new framework is built on standard methods for approximate inference (variational free-energy, EP and power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way all of approximation is performed at `inference time' rather than at `modelling time' resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression, classification and state space modelling tasks. "
734909872786046980,2016-05-24 00:52:22,https://t.co/ZYZcuC2txN,Learning Sensor Multiplexing Design through Back-propagation. (arXiv:1605.07078v1 [cs.LG]) https://t.co/ZYZcuC2txN,0,1," Abstract: Recent progress on many imaging and vision tasks has been driven by the use of deep feed-forward neural networks, which are trained by propagating gradients of a loss defined on the final output, back through the network up to the first layer that operates directly on the image. We propose back-propagating one step further---to learn camera sensor designs jointly with networks that carry out inference on the images they capture. In this paper, we specifically consider the design and inference problems in a typical color camera---where the sensor is able to measure only one color channel at each pixel location, and computational inference is required to reconstruct a full color image. We learn the camera sensor's color multiplexing pattern by encoding it as layer whose learnable weights determine which color channel, from among a fixed set, will be measured at each location. These weights are jointly trained with those of a reconstruction network that operates on the corresponding sensor measurements to produce a full color image. Our network achieves significant improvements in accuracy over the traditional Bayer pattern used in most color cameras. It automatically learns to employ a sparse color measurement approach similar to that of a recent design, and moreover, improves upon that design by learning an optimal layout for these measurements. "
734909871083147266,2016-05-24 00:52:21,https://t.co/HneUaoQ3Ug,Optimal Coding in Biological and Artificial Neural Networks. (arXiv:1605.07094v1 [q-bio.NC]) https://t.co/HneUaoQ3Ug,0,2," Abstract: Feature representations in both, biological neural networks in the primate ventral stream and artificial convolutional neural networks trained on object recognition, incresase in complexity and receptive field size with layer depth. Somewhat strikingly, empirical evidence indicates that this analogy extends to the specific representations learned in each layer. This suggests that biological and artificial neural networks share a fundamental organising principle. We shed light on this principle in the framework of optimal coding. Specifically, we first investigate which properties of a code render it robust to transmission over noisy channels and formally prove that for equientropic channels an upper bound on the expected minimum decoding error is attained for codes with maximum marginal entropy. We then show that the pairwise correlation of units in a deep layer of a neural network, that has been trained on an object recognition task, increases when perturbing the distribution of input images, i. e., that the network exhibits properties of an optimally coding system. By analogy, this suggests that the layer-wise similarity of feature representations in biological and artificial neural networks is a result of optimal coding that enables robust transmission of object information over noisy channels. Because we find that in equientropic channels the upper bound on the expected minimum decoding error is independent of the class-conditional entropy, our work further provides a plausible explanation why optimal codes can be learned in unsupervised settings. "
734909869803884545,2016-05-24 00:52:21,https://t.co/fdpAWips7a,Deep Learning without Poor Local Minima. (arXiv:1605.07110v1 [stat.ML]) https://t.co/fdpAWips7a,6,19," Abstract: In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. With no unrealistic assumption, we first prove the following statements for the squared loss function of deep linear neural networks with any depth and any widths: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) there exist ""bad"" saddle points (where the Hessian has no negative eigenvalue) for the deeper networks (with more than three layers), whereas there is no bad saddle point for the shallow networks (with three layers). Moreover, for deep nonlinear neural networks, we prove the same four statements via a reduction to a deep linear model under the independence assumption adopted from recent work. As a result, we present an instance, for which we can answer the following question: how difficult is it to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima). Furthermore, the mathematically proven existence of bad saddle points for deeper models would suggest a possible open problem. We note that even though we have advanced the theoretical foundations of deep learning and non-convex optimization, there is still a gap between theory and practice. "
734909868822368256,2016-05-24 00:52:21,https://t.co/wCf9gwYKzp,Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks. (arXiv:1605.07127v1 [sta… https://t.co/wCf9gwYKzp,0,2," Abstract: We present an algorithm for model-based reinforcement learning that combines Bayesian neural networks (BNNs) with random roll-outs and stochastic optimization for policy learning. The BNNs are trained by minimizing $\alpha$-divergences, allowing us to capture complicated statistical patterns in the transition dynamics, e.g. multi-modality and heteroskedasticity, which are usually missed by other common modeling approaches. We illustrate the performance of our method by solving a challenging benchmark where model-based approaches usually fail and by obtaining promising results in a real-world scenario for controlling a gas turbine. "
734909867702550528,2016-05-24 00:52:21,https://t.co/OrFGN83WGZ,Sub-Gaussian estimators of the mean of a random matrix with heavy-tailed entries. (arXiv:1605.07129v1 [math.ST]) https://t.co/OrFGN83WGZ,0,1," Abstract: Estimation of the covariance matrix has attracted a lot of attention of the statistical research community over the years, partially due to important applications such as Principal Component Analysis. However, frequently used empirical covariance estimator (and its modifications) is very sensitive to outliers in the data. As P. J. Huber wrote in 1964, ""...This raises a question which could have been asked already by Gauss, but which was, as far as I know, only raised a few years ago (notably by Tukey): what happens if the true distribution deviates slightly from the assumed normal one? As is now well known, the sample mean then may have a catastrophically bad performance..."" Motivated by this question, we develop a new estimator of the (element-wise) mean of a random matrix, which includes covariance estimation problem as a special case. Assuming that the entries of a matrix possess only finite second moment, this new estimator admits sub-Gaussian or sub-exponential concentration around the unknown mean in the operator norm. We will explain the key ideas behind our construction, as well as applications to covariance estimation and matrix completion problems. "
734909866481946625,2016-05-24 00:52:20,https://t.co/S5YwrcwOSd,Fairness in Learning: Classic and Contextual Bandits. (arXiv:1605.07139v1 [cs.LG]) https://t.co/S5YwrcwOSd,2,1," Abstract: We introduce the study of fairness in multi-armed bandit problems. Our fairness definition can be interpreted as demanding that given a pool of applicants (say, for college admission or mortgages), a worse applicant is never favored over a better one, despite a learning algorithm's uncertainty over the true payoffs. We prove results of two types. First, in the important special case of the classic stochastic bandits problem (i.e., in which there are no contexts), we provide a provably fair algorithm based on ""chained"" confidence intervals, and provide a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence. When combined with regret bounds for standard non-fair algorithms such as UCB, this proves a strong separation between fair and unfair learning, which extends to the general contextual case. In the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm, and conversely any fair contextual bandit algorithm can be transformed into a KWIK learning algorithm. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms "
734909865164984325,2016-05-24 00:52:20,https://t.co/l6E39yogy9,Actively Learning Hemimetrics with Applications to Eliciting User Preferences. (arXiv:1605.07144v1 [stat.ML]) https://t.co/l6E39yogy9,0,1," Abstract: Motivated by an application of eliciting users' preferences, we investigate the problem of learning hemimetrics, i.e., pairwise distances among a set of $n$ items that satisfy triangle inequalities and non-negativity constraints. In our application, the (asymmetric) distances quantify private costs a user incurs when substituting one item by another. We aim to learn these distances (costs) by asking the users whether they are willing to switch from one item to another for a given incentive offer. Without exploiting structural constraints of the hemimetric polytope, learning the distances between each pair of items requires $\Theta(n^2)$ queries. We propose an active learning algorithm that substantially reduces this sample complexity by exploiting the structural constraints on the version space of hemimetrics. Our proposed algorithm achieves provably-optimal sample complexity for various instances of the task. For example, when the items are embedded into $K$ tight clusters, the sample complexity of our algorithm reduces to $O(n K)$. Extensive experiments on a restaurant recommendation data set support the conclusions of our theoretical analysis. "
734909863919243264,2016-05-24 00:52:20,https://t.co/EuKkfQMWBR,Towards Optimality Conditions for Non-Linear Networks. (arXiv:1605.07145v1 [stat.ML]) https://t.co/EuKkfQMWBR,0,2," Abstract: Training non-linear neural networks is a challenging task, but over the years, various approaches coming from different perspectives have been proposed to improve performance. However, insights into what fundamentally constitutes \textit{optimal} network parameters remains obscure. Similarly, given what properties of data can we hope for a non-linear network to learn is also not well studied. In order to address these challenges, we take a novel approach by analysing neural network from a data generating perspective, where we assume hidden layers generate the observed data. This perspective allows us to connect seemingly disparate approaches explored independently in the machine learning community such as batch normalization, Independent Component Analysis, orthogonal weight initialization, etc, as parts of a bigger picture and provide insights into non-linear networks in terms of properties of parameter and data that lead to better performance. "
734909862702927873,2016-05-24 00:52:19,https://t.co/AL6EnfWU3A,Genetic Architect: Discovering Genomic Structure with Learned Neural Architectures. (arXiv:1605.07156v1 [cs.LG]) https://t.co/AL6EnfWU3A,0,1," Abstract: Each human genome is a 3 billion base pair set of encoding instructions. Decoding the genome using deep learning fundamentally differs from most tasks, as we do not know the full structure of the data and therefore cannot design architectures to suit it. As such, architectures that fit the structure of genomics should be learned not prescribed. Here, we develop a novel search algorithm, applicable across domains, that discovers an optimal architecture which simultaneously learns general genomic patterns and identifies the most important sequence motifs in predicting functional genomic outcomes. The architectures we find using this algorithm succeed at using only RNA expression data to predict gene regulatory structure, learn human-interpretable visualizations of key sequence motifs, and surpass state-of-the-art results on benchmark genomics challenges. "
734909861343952896,2016-05-24 00:52:19,https://t.co/Wi2egqtRMD,Stochastic Modeling and Estimation of Stationary Complex-Valued Signals. (arXiv:1306.5993v3 [stat.ME] UPDATED) https://t.co/Wi2egqtRMD,0,1," Abstract: This paper provides a stochastic modeling framework for the power spectral representations of stationary complex-valued signals. We specify how complex-valued signals can be modeled stochastically in terms of their rotary components, which decompose a bivariate signal according to direction of rotation. The necessary relationships are provided to map between complex-rotary and bivariate-Cartesian representations. We demonstrate how by modeling in rotary components we can infer useful features from application datasets---in particular for capturing the improper or anisotropic structure of a signal---by implementing our methodology on fluid dynamic simulations of turbulence. In addition, we detail how parameters of a chosen stochastic model can be efficiently estimated in the frequency domain, by extending the Whittle likelihood to complex-valued signals. We also provide a new method of testing for complex structure such as impropriety, as well as procedures for model choice and semi-parametric modeling. "
734909859393572866,2016-05-24 00:52:19,https://t.co/FKLQXMHe5O,Completing Low-Rank Matrices with Corrupted Samples from Few Coefficients in General Basis. (arXiv:1506.07615v2 [c… https://t.co/FKLQXMHe5O,0,1," Abstract: Subspace recovery from corrupted and missing data is crucial for various applications in signal processing and information theory. To complete missing values and detect column corruptions, existing robust Matrix Completion (MC) methods mostly concentrate on recovering a low-rank matrix from few corrupted coefficients w.r.t. standard basis, which, however, does not apply to more general basis, e.g., Fourier basis. In this paper, we prove that the range space of an $m\times n$ matrix with rank $r$ can be exactly recovered from few coefficients w.r.t. general basis, though $r$ and the number of corrupted samples are both as high as $O(\min\{m,n\}/\log^3 (m+n))$. Our model covers previous ones as special cases, and robust MC can recover the intrinsic matrix with a higher rank. Moreover, we suggest a universal choice of the regularization parameter, which is $\lambda=1/\sqrt{\log n}$. By our $\ell_{2,1}$ filtering algorithm, which has theoretical guarantees, we can further reduce the computational cost of our model. As an application, we also find that the solutions to extended robust Low-Rank Representation and to our extended robust MC are mutually expressible, so both our theory and algorithm can be applied to the subspace clustering problem with missing values under certain conditions. Experiments verify our theories. "
734909857816547328,2016-05-24 00:52:18,https://t.co/9lGLmfR5R1,Learning Fair Classifiers. (arXiv:1507.05259v4 [stat.ML] UPDATED) https://t.co/9lGLmfR5R1,0,1," Abstract: Automated data-driven decision systems are ubiquitous across a wide variety of online services, from online social networking and e-commerce to e-government. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes have a disproportionally large adverse impact on particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers in a principled manner, by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism on two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control of the level of fairness, often at a minimal cost in terms of accuracy. "
734544190814748676,2016-05-23 00:39:16,https://t.co/HFyVdUrres,Variational hybridization and transformation for large inaccurate noisy-or networks. (arXiv:1605.06181v1 [cs.LG]) https://t.co/HFyVdUrres,0,2," Abstract: Variational inference provides approximations to the computationally intractable posterior distribution in Bayesian networks. A prominent medical application of noisy-or Bayesian network is to infer potential diseases given observed symptoms. Previous studies focus on approximating a handful of complicated pathological cases using variational transformation. Our goal is to use variational transformation as part of a novel hybridized inference for serving reliable and real time diagnosis at web scale. We propose a hybridized inference that allows variational parameters to be estimated without disease posteriors or priors, making the inference faster and much of its computation recyclable. In addition, we propose a transformation ranking algorithm that is very stable to large variances in network prior probabilities, a common issue that arises in medical applications of Bayesian networks. In experiments, we perform comparative study on a large real life medical network and scalability study on a much larger (36,000x) synthesized network. "
734544189636157440,2016-05-23 00:39:16,https://t.co/E9KlkQCc7D,Deep Generative Models with Stick-Breaking Priors. (arXiv:1605.06197v1 [stat.ML]) https://t.co/E9KlkQCc7D,4,18," Abstract: Bayesian nonparametric models are attractive for their data-dependent capacity, but their implementation can be problematic due to computational or analytical obstacles. We make progress on this problem by extending Stochastic Gradient Variational Bayes (Kingma & Welling, 2013), a 'black box' method for approximate posterior inference, to stick-breaking priors (Ishwaran & James, 2001). This innovation allows us to define deep generative models (DGMs) with infinite dimensional latent variables. We experimentally demonstrate that DGMs with Dirichlet process priors learn highly discriminative latent representations that are well suited for semi-supervised settings and often outperform the popular Gaussian alternative. "
734544188382056449,2016-05-23 00:39:16,https://t.co/jxAiN9odB9,Adversarial Delays in Online Strongly-Convex Optimization. (arXiv:1605.06201v1 [cs.LG]) https://t.co/jxAiN9odB9,0,2," Abstract: We consider the problem of strongly-convex online optimization in presence of adversarial delays; in a T-iteration online game, the feedback of the player's query at time t is arbitrarily delayed by an adversary for d_t rounds and delivered before the game ends, at iteration t+d_t-1. Specifically for \algo{online-gradient-descent} algorithm we show it has a simple regret bound of \Oh{\sum_{t=1}^T \log (1+ \frac{d_t}{t})}. This gives a clear and simple bound without resorting any distributional and limiting assumptions on the delays. We further show how this result encompasses and generalizes several of the existing known results in the literature. Specifically it matches the celebrated logarithmic regret \Oh{\log T} when there are no delays (i.e. d_t = 1) and regret bound of \Oh{\tau \log T} for constant delays d_t = \tau. "
734544187106942977,2016-05-23 00:39:16,https://t.co/InkN7xdieB,Convergence of Contrastive Divergence with Annealed Learning Rate in Exponential Family. (arXiv:1605.06220v1 [stat… https://t.co/InkN7xdieB,0,4," Abstract: In our recent paper, we showed that in exponential family, contrastive divergence (CD) with fixed learning rate will give asymptotically consistent estimates \cite{wu2016convergence}. In this paper, we establish consistency and convergence rate of CD with annealed learning rate $\eta_t$. Specifically, suppose CD-$m$ generates the sequence of parameters $\{\theta_t\}_{t \ge 0}$ using an i.i.d. data sample $\mathbf{X}_1^n \sim p_{\theta^*}$ of size $n$, then $\delta_n(\mathbf{X}_1^n) = \limsup_{t \to \infty} \Vert \sum_{s=t_0}^t \eta_s \theta_s / \sum_{s=t_0}^t \eta_s - \theta^* \Vert$ converges in probability to 0 at a rate of $1/\sqrt[3]{n}$. The number ($m$) of MCMC transitions in CD only affects the coefficient factor of convergence rate. Our proof is not a simple extension of the one in \cite{wu2016convergence}. which depends critically on the fact that $\{\theta_t\}_{t \ge 0}$ is a homogeneous Markov chain conditional on the observed sample $\mathbf{X}_1^n$. Under annealed learning rate, the homogeneous Markov property is not available and we have to develop an alternative approach based on super-martingales. Experiment results of CD on a fully-visible $2\times 2$ Boltzmann Machine are provided to demonstrate our theoretical results. "
734544185869602816,2016-05-23 00:39:15,https://t.co/H94NdKoLdo,End-to-End Kernel Learning with Supervised Convolutional Kernel Networks. (arXiv:1605.06265v1 [stat.ML]) https://t.co/H94NdKoLdo,1,12," Abstract: In this paper, we introduce a new image representation based on a multilayer kernel machine. Unlike traditional kernel methods where data representation is decoupled from the prediction task, we learn how to shape the kernel with supervision. We proceed by first proposing improvements of the recently-introduced convolutional kernel networks (CKNs) in the context of unsupervised learning; then, we derive backpropagation rules to take advantage of labeled training data. The resulting model is a new type of convolutional neural network, where optimizing the filters at each layer is equivalent to learning a linear subspace in a reproducing kernel Hilbert space (RKHS). We show that our method achieves reasonably competitive performance for image classification on some standard ""deep learning"" datasets such as CIFAR-10 and SVHN, and also for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks. "
734544184569397248,2016-05-23 00:39:15,https://t.co/fg2dnsh0j0,Piece-wise quadratic lego set for constructing arbitrary error potentials and their fast optimization. (arXiv:1605… https://t.co/fg2dnsh0j0,0,1," Abstract: Most of machine learning approaches have stemmed from the application of minimizing the mean squared distance principle, based on the computationally efficient quadratic optimization methods. However, when faced with high-dimensional and noisy data, the quadratic error functionals demonstrated many weaknesses including high sensitivity to contaminating factors and dimensionality curse. Therefore, a lot of recent applications in machine learning exploited properties of non-quadratic error functionals based on $L_1$ norm or even sub-linear potentials corresponding to quasinorms $L_p$ ($0<p<1$). The back side of these approaches is increase in computational cost for optimization. Till so far, no approaches have been suggested to deal with {\it arbitrary} error functionals, in a flexible and computationally efficient framework. In this paper, we develop a theory and basic universal data approximation algorithms ($k$-means, principal components, principal manifolds and graphs, regularized and sparse regression), based on piece-wise quadratic error potentials of subquadratic growth (PQSQ potentials). We develop a new and universal framework to minimize {\it arbitrary sub-quadratic error potentials} using an algorithm with guaranteed fast convergence to the local or global error minimum. The theory of PQSQ potentials is based on the notion of the cone of minorant functions, and represents a natural approximation formalism based on the application of min-plus algebra. The approach can be applied in most of existing machine learning methods, including methods of data approximation and regularized and sparse regression, leading to the improvement in the computational cost/accuracy trade-off. We demonstrate that on synthetic and real-life datasets PQSQ-based machine learning methods achieve orders of magnitude faster computational performance than the corresponding state-of-the-art methods. "
734544183298543616,2016-05-23 00:39:15,https://t.co/kDoqj37JAi,Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA. (arXiv:1605.06336v1 [stat.ML]) https://t.co/kDoqj37JAi,1,6," Abstract: Nonlinear independent component analysis (ICA) provides an appealing framework for unsupervised feature learning, but the models proposed so far are not identifiable. Here, we first propose a new intuitive principle of unsupervised deep learning from time series which uses the nonstationary structure of the data. Our learning principle, time-contrastive learning (TCL), finds a representation which allows optimal discrimination of time segments (windows). Surprisingly, we show how TCL can be related to a nonlinear ICA model, when ICA is redefined to include temporal nonstationarities. In particular, we show that TCL combined with linear ICA estimates the nonlinear ICA model up to point-wise transformations of the sources, and this solution is unique --- thus providing the first identifiability result for nonlinear ICA which is rigorous, constructive, as well as very general. "
734544182166102017,2016-05-23 00:39:14,https://t.co/mFnXgcbTP6,Learning to Discover Probabilistic Graphical Model Structures. (arXiv:1605.06359v1 [stat.ML]) https://t.co/mFnXgcbTP6,2,2," Abstract: We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. In the setting of Gaussian Graphical Models (GGMs) a popular estimator is a penalized maximum likelihood objective on the precision matrix. Adapting this objective to capture domain-specific knowledge as priors or a new data likelihood requires great effort. In addition, structure recovery is a very indirect consequence of the data-fit term. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired properties. We propose here to leverage this latter source of information in order to learn a function that maps from empirical covariance matrices to estimated graph structures. Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can more directly be tailored to the specific problem of edge structure discovery. We apply this framework to several critical real world problems in structure discovery and show that it can be competitive to standard approaches such as graphical lasso, at a fraction of the execution speed. We use convolutional neural networks to parametrize our estimators due to the compositional block structure of matrix inversion. Experimentally, our learnable graph-discovery method trained on synthetic data generalizes well to different data: identifying relevant edges in real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain competitive (and often superior) performance, compared with analytical methods. "
734544181100711936,2016-05-23 00:39:14,https://t.co/6jxar8oNMJ,Fast $\epsilon$-free Inference of Simulation Models with Bayesian Conditional Density Estimation. (arXiv:1605.0637… https://t.co/6jxar8oNMJ,2,3," Abstract: Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an $\epsilon$-ball around the observed data, which is only correct in the limit $\epsilon\!\rightarrow\!0$. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as $\epsilon\!\rightarrow\!0$, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior. "
734544179964092417,2016-05-23 00:39:14,https://t.co/wNKhoL7gEu,Statistical Inference for Cluster Trees. (arXiv:1605.06416v1 [math.ST]) https://t.co/wNKhoL7gEu,2,2," Abstract: A cluster tree provides a highly-interpretable summary of a density function by representing the hierarchy of its high-density clusters. It is estimated using the empirical tree, which is the cluster tree constructed from a density estimator. This paper addresses the basic question of quantifying our uncertainty by assessing the statistical significance of features of an empirical cluster tree. We first study a variety of metrics that can be used to compare different trees, analyze their properties and assess their suitability for inference. We then propose methods to construct and summarize confidence sets for the unknown true cluster tree. We introduce a partial ordering on cluster trees which we use to prune some of the statistically insignificant features of the empirical tree, yielding interpretable and parsimonious cluster trees. Finally, we illustrate the proposed methods on a variety of synthetic examples and furthermore demonstrate their utility in the analysis of a Graft-versus-Host Disease (GvHD) data set. "
734544178881921024,2016-05-23 00:39:14,https://t.co/MIPMEB4sb0,Quantifying the accuracy of approximate diffusions and Markov chains. (arXiv:1605.06420v1 [math.ST]) https://t.co/MIPMEB4sb0,0,2," Abstract: Markov chains and diffusion processes are indispensable tools in machine learning and statistics that are used for inference, sampling, and modeling. With the growth of large-scale datasets, the computational cost associated with simulating these stochastic processes can be considerable, and many algorithms have been proposed to approximate the underlying Markov chain or diffusion. A fundamental question is how the computational savings trade off against the statistical error incurred due to approximations. This paper develops general results that address this question. We bound the Wasserstein distance between the equilibrium distributions of two diffusions as a function of their mixing rates and the deviation in their drifts. We show that this error bound is tight in simple Gaussian settings. Our general result on continuous diffusions can be discretized to provide insights into the computational--statistical trade-off of Markov chains. As an illustration, we apply our framework to derive finite-sample error bounds of approximate unadjusted Langevin dynamics. We characterize computation-constrained settings where, by using fast-to-compute approximate gradients in the Langevin dynamics, we obtain more accurate samples compared to using the exact gradients. Finally, as an additional application of our approach, we quantify the accuracy of approximate Zig-Zag sampling. Our theoretical analyses are supported by simulation experiments. "
734544177766268928,2016-05-23 00:39:13,https://t.co/SgiZ2vfxkH,Fast Randomized Semi-Supervised Clustering. (arXiv:1605.06422v1 [cs.LG]) https://t.co/SgiZ2vfxkH,1,3," Abstract: We consider the problem of clustering partially labeled data from a minimal number of randomly chosen pairwise comparisons between the items. We introduce an efficient local algorithm based on a power iteration of the non-backtracking operator and study its performance on a simple model. For the case of two clusters, we give bounds on the classification error and show that a small error can be achieved from $O(n)$ randomly chosen measurements, where $n$ is the number of items in the dataset. Our algorithm is therefore efficient both in terms of time and space complexities. We also investigate numerically the performance of the algorithm on synthetic and real world data. "
734544176646377472,2016-05-23 00:39:13,https://t.co/uFsa4flLGi]),Coresets for Scalable Bayesian Logistic Regression. (arXiv:1605.06423v1 [https://t.co/uFsa4flLGi]) https://t.co/eCgLurJ77V,1,4,INDEXERROR
734544175455162368,2016-05-23 00:39:13,https://t.co/bNsFwzzYaH,Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data. (arXiv:1605.06432v1 [st… https://t.co/bNsFwzzYaH,1,10," Abstract: We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions by means of variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction. "
734544172951212032,2016-05-23 00:39:12,https://t.co/xSZU8hT4ja,Structured Prediction Theory and Voted Risk Minimization. (arXiv:1605.06443v1 [stat.ML]) https://t.co/xSZU8hT4ja,0,2," Abstract: We present a general theoretical analysis of structured prediction. By introducing a new complexity measure that explicitly factors in the structure of the output space and the loss function, we are able to derive new data-dependent learning guarantees for a broad family of losses and for hypothesis sets with an arbitrary factor graph decomposition. We extend this theory by leveraging the principle of Voted Risk Minimization (VRM) and showing that learning is possible with complex factor graphs. We both present new learning bounds in this advanced setting as well as derive two new families of algorithms, \emph{Voted Conditional Random Fields} and \emph{Voted Structured Boosting}, which can make use of very complex features and factor graphs without overfitting. Finally, we also validate our theory through experiments on several datasets. "
734544171978117120,2016-05-23 00:39:12,https://t.co/kMt8FhsgmJ,Unreasonable Effectiveness of Learning Neural Nets: Accessible States and Robust Ensembles. (arXiv:1605.06444v1 [s… https://t.co/kMt8FhsgmJ,1,6," Abstract: In artificial neural networks, learning from data is a computationally demanding task in which a large number of connection weights are iteratively tuned through stochastic-gradient-based heuristic processes over a cost-function. It is not well understood how learning occurs in these systems, in particular how they avoid getting trapped in configurations with poor computational performance. Here we study the difficult case of networks with discrete weights, where the optimization landscape is very rough even for simple architectures, and provide theoretical and numerical evidence of the existence of rare - but extremely dense and accessible - regions of configurations in the network weight space. We define a novel measure, which we call the ""robust ensemble"" (RE), which suppresses trapping by isolated configurations and amplifies the role of these dense regions. We analytically compute the RE in some exactly solvable models, and also provide a general algorithmic scheme which is straightforward to implement: define a cost-function given by a sum of a finite number of replicas of the original cost-function, with a constraint centering the replicas around a driving assignment. To illustrate this, we derive several powerful new algorithms, ranging from Markov Chains to message passing to gradient descent processes, where the algorithms target the robust dense states, resulting in substantial improvements in performance. The weak dependence on the number of precision bits of the weights leads us to conjecture that very similar reasoning applies to more conventional neural networks. Analogous algorithmic schemes can also be applied to other optimization problems. "
734544170891763712,2016-05-23 00:39:12,https://t.co/ukBYQpnxfy,Fixed Points of Belief Propagation -- An Analysis via Polynomial Homotopy Continuation. (arXiv:1605.06451v1 [stat.… https://t.co/ukBYQpnxfy,0,3," Abstract: Belief propagation (BP) is an iterative method to perform approximate inference on arbitrary graphical models. Whether BP converges and if the solution is a unique fixed point depends on both, the structure and the parametrization of the model. To understand this dependence we are interested in finding \emph{all} fixed points. In this work, we formulate BP as a set of polynomial equations, the solutions of which correspond to the BP fixed points. We apply the numerical polynomial-homotopy-continuation (NPHC) method to solve such systems. It is commonly believed that uniqueness of BP fixed points implies convergence to this fixed point. Contrary to this conjecture, we find graphs for which BP fails to converge, even though a unique fixed point exists. Moreover, we show that this fixed point gives a good approximation of the exact marginal distribution. "
734544169910317056,2016-05-23 00:39:11,https://t.co/8XwqViXCoF,Virtual Worlds as Proxy for Multi-Object Tracking Analysis. (arXiv:1605.06457v1 [cs.CV]) https://t.co/8XwqViXCoF,0,2," Abstract: Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual KITTI (see this http URL), automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking. "
734544168991756288,2016-05-23 00:39:11,https://t.co/Rr4kcR9QtV,Neural Machine Translation by Jointly Learning to Align and Translate. (arXiv:1409.0473v7 [cs.CL] UPDATED) https://t.co/Rr4kcR9QtV,1,7," Abstract: Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. "
734544167867699200,2016-05-23 00:39:11,https://t.co/udkBTneS5l,A Spectral Algorithm with Additive Clustering for the Recovery of Overlapping Communities in Networks. (arXiv:1506… https://t.co/udkBTneS5l,0,3," Abstract: This paper presents a novel spectral algorithm with additive clustering, designed to identify overlapping communities in networks. The algorithm is based on geometric properties of the spectrum of the expected adjacency matrix in a random graph model that we call stochastic blockmodel withoverlap (SBMO). An adaptive version of the algorithm, that does not require the knowledge of the number of hidden communities, is proved to be consistent under the SBMO when the degrees in the graph are (slightly more than) logarithmic. The algorithm is shown to perform well on simulateddata and on real-world graphs with known overlapping communities. "
734544166680731650,2016-05-23 00:39:11,https://t.co/ABG3k4FquT,Random sampling of bandlimited signals on graphs. (arXiv:1511.05118v2 [cs.SI] UPDATED) https://t.co/ABG3k4FquT,0,3," Abstract: We study the problem of sampling k-bandlimited signals on graphs. We propose two sampling strategies that consist in selecting a small subset of nodes at random. The first strategy is non-adaptive, i.e., independent of the graph structure, and its performance depends on a parameter called the graph coherence. On the contrary, the second strategy is adaptive but yields optimal results. Indeed, no more than O(k log(k)) measurements are sufficient to ensure an accurate and stable recovery of all k-bandlimited signals. This second strategy is based on a careful choice of the sampling distribution, which can be estimated quickly. Then, we propose a computationally efficient decoder to reconstruct k-bandlimited signals from their samples. We prove that it yields accurate reconstructions and that it is also stable to noise. Finally, we conduct several experiments to test these techniques. "
734544165510479872,2016-05-23 00:39:10,https://t.co/E4Qp3EzCNo,ATD: Anomalous Topic Discovery in High Dimensional Discrete Data. (arXiv:1512.06452v2 [stat.ML] UPDATED) https://t.co/E4Qp3EzCNo,0,3," Abstract: We propose an algorithm for detecting patterns exhibited by anomalous clusters in high dimensional discrete data. Unlike most anomaly detection (AD) methods, which detect individual anomalies, our proposed method detects groups (clusters) of anomalies; i.e. sets of points which collectively exhibit abnormal patterns. In many applications this can lead to better understanding of the nature of the atypical behavior and to identifying the sources of the anomalies. Moreover, we consider the case where the atypical patterns exhibit on only a small (salient) subset of the very high dimensional feature space. Individual AD techniques and techniques that detect anomalies using all the features typically fail to detect such anomalies, but our method can detect such instances collectively, discover the shared anomalous patterns exhibited by them, and identify the subsets of salient features. In this paper, we focus on detecting anomalous topics in a batch of text documents, developing our algorithm based on topic models. Results of our experiments show that our method can accurately detect anomalous topics and salient features (words) under each such topic in a synthetic data set and two real-world text corpora and achieves better performance compared to both standard group AD and individual AD techniques. All required code to reproduce our experiments is available from this https URL "
734544164004745216,2016-05-23 00:39:10,https://t.co/Sn7GhthFWV,A Latent-Variable Lattice Model. (arXiv:1512.07587v5 [cs.LG] UPDATED) https://t.co/Sn7GhthFWV,0,2," Abstract: Markov random field (MRF) learning is intractable, and its approximation algorithms are computationally expensive. We target a small subset of MRF that is used frequently in computer vision. We characterize this subset with three concepts: Lattice, Homogeneity, and Inertia; and design a non-markov model as an alternative. Our goal is robust learning from small datasets. Our learning algorithm uses vector quantization and, at time complexity O(U log U) for a dataset of U pixels, is much faster than that of general-purpose MRF. "
734544162738081792,2016-05-23 00:39:10,https://t.co/g8I02ouGhn,R\'enyi Divergence Variational Inference. (arXiv:1602.02311v2 [stat.ML] UPDATED) https://t.co/g8I02ouGhn,0,2," Abstract: This paper introduces the variational R\'enyi bound (VR) that extends traditional variational inference to R\'enyi's alpha-divergences. This new family of variational methods unifies a number of existing approaches, and enables a smooth interpolation from the evidence lower-bound to the log (marginal) likelihood that is controlled by the value of alpha that parametrises the divergence. The reparameterization trick, Monte Carlo approximation and stochastic optimisation methods are deployed to obtain a tractable and unified framework for optimisation. We further consider negative alpha values and propose a novel variational inference method as a new special case in the proposed framework. Experiments on Bayesian neural networks and variational auto-encoders demonstrate the wide applicability of the VR bound. "
734544161400033280,2016-05-23 00:39:09,https://t.co/zKaHNJ4WKi,Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series. (arXiv:1602.07109v4 [stat.ML]… https://t.co/zKaHNJ4WKi,0,9, Abstract: Approximate variational inference has shown to be a powerful tool for modeling unknown complex probability distributions. Recent advances in the field allow us to learn probabilistic models of sequences that actively exploit spatial and temporal structure. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line. 
734544159978164224,2016-05-23 00:39:09,https://t.co/tQfUclwpBX,Optimal Black-Box Reductions Between Optimization Objectives. (arXiv:1603.05642v3 [math.OC] UPDATED) https://t.co/tQfUclwpBX,0,6," Abstract: The diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods, finely tuned to the specific regression or classification task at hand. We reduce the complexity of algorithm design for machine learning by reductions: we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity in applications. Furthermore, unlike existing results, our new reductions are OPTIMAL and more PRACTICAL. We show how these new reductions give rise to new and faster running times on training linear classifiers for various families of loss functions, and conclude with experiments showing their successes also in practice. "
734544158921240580,2016-05-23 00:39:09,https://t.co/URHhK9guUt,Katyusha: The First Truly Accelerated Stochastic Gradient Method. (arXiv:1603.05953v2 [math.OC] UPDATED) https://t.co/URHhK9guUt,4,9," Abstract: We introduce $\mathtt{Katyusha}$, the first direct, primal-only stochastic gradient method that has a provably accelerated convergence rate in convex optimization. In contrast, previous methods are based on dual coordinate descent which are more restrictive, or based on outer-inner loops which make them ""blind"" to the underlying stochastic nature of the optimization process. $\mathtt{Katyusha}$ is the first algorithm that incorporates acceleration directly into stochastic gradient updates. Unlike previous results, $\mathtt{Katyusha}$ obtains an optimal convergence rate. It also supports proximal updates, non-Euclidean norm smoothness, non-uniform sampling, and mini-batch sampling. When applied to interesting classes of convex objectives, including smooth objectives (e.g., Lasso, Logistic Regression), strongly-convex objectives (e.g., SVM), and non-smooth objectives (e.g., L1SVM), $\mathtt{Katyusha}$ improves the best known convergence rates. The main ingredient behind our result is $\textit{Katyusha momentum}$, a novel ""negative momentum on top of momentum"" that can be incorporated into a variance-reduction based algorithm and speed it up. As a result, since variance reduction has been successfully applied to a fast growing list of practical problems, our paper suggests that in each of such cases, one had better hurry up and give Katyusha a hug. "
734544157889441794,2016-05-23 00:39:09,https://t.co/mjXzEoR6p3,Localized Lasso for High-Dimensional Regression. (arXiv:1603.06743v2 [stat.ML] UPDATED) https://t.co/mjXzEoR6p3,1,5," Abstract: We introduce the localized Lasso, which is suited for learning models that are both interpretable and have a high predictive power in problems with high dimensionality $d$ and small sample size $n$. More specifically, we consider a function defined by local sparse models, one at each data point. We introduce sample-wise network regularization to borrow strength across the models, and sample-wise exclusive group sparsity (a.k.a., $\ell_{1,2}$ norm) to introduce diversity into the choice of feature sets in the local models. The local models are interpretable in terms of similarity of their sparsity patterns. The cost function is convex, and thus has a globally optimal solution. Moreover, we propose a simple yet efficient iterative least-squares based optimization procedure for the localized Lasso, which does not need a tuning parameter, and is guaranteed to converge to a globally optimal solution. The solution is empirically shown to outperform alternatives for both simulated and genomic personalized medicine data. "
734544156786319360,2016-05-23 00:39:08,https://t.co/ZtdyZdRvkl,Online Optimization of Smoothed Piecewise Constant Functions. (arXiv:1604.01999v2 [cs.LG] UPDATED) https://t.co/ZtdyZdRvkl,0,7," Abstract: We study online optimization of smoothed piecewise constant functions over the domain [0, 1). This is motivated by the problem of adaptively picking parameters of learning algorithms as in the recently introduced framework by Gupta and Roughgarden (2016). Majority of the machine learning literature has focused on Lipschitz-continuous functions or functions with bounded gradients. 1 This is with good reason---any learning algorithm suffers linear regret even against piecewise constant functions that are chosen adversarially, arguably the simplest of non-Lipschitz continuous functions. The smoothed setting we consider is inspired by the seminal work of Spielman and Teng (2004) and the recent work of Gupta and Roughgarden---in this setting, the sequence of functions may be chosen by an adversary, however, with some uncertainty in the location of discontinuities. We give algorithms that achieve sublinear regret in the full information and bandit settings. "
734544154752094208,2016-05-23 00:39:08,https://t.co/DNZmWhvYIA,A Network-based End-to-End Trainable Task-oriented Dialogue System. (arXiv:1604.04562v2 [cs.CL] UPDATED) https://t.co/DNZmWhvYIA,0,3," Abstract: Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring labelled datasets and solving a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable dialogue system along with a new way of collecting task-oriented dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain. "
733460872861863936,2016-05-20 00:54:33,https://t.co/jL9wJBjIXN,Supervised Learning with Quantum-Inspired Tensor Networks. (arXiv:1605.05775v1 [stat.ML]) https://t.co/jL9wJBjIXN,0,6, Abstract: Tensor networks are efficient representations of high-dimensional tensors which have been very successful for physics and mathematics applications. We demonstrate how algorithms for optimizing such networks can be adapted to supervised learning tasks by using matrix product states (tensor trains) to parameterize models for classifying images. For the MNIST data set we obtain less than 1% test set classification error. We discuss how the tensor network form imparts additional structure to the learned model and suggest a possible generative interpretation. 
733460871586832384,2016-05-20 00:54:33,https://t.co/NGZIGOYWk7,The Quality of the Covariance Selection Through Detection Problem and AUC Bounds. (arXiv:1605.05776v1 [cs.IT]) https://t.co/NGZIGOYWk7,0,3," Abstract: We consider the problem of quantifying the quality of a model selection problem for a graphical model. We discuss this by formulating the problem as a detection problem. Model selection problems usually minimize a distance between the original distribution and the model distribution. For the special case of Gaussian distributions, the model selection problem simplifies to the covariance selection problem which is widely discussed in literature by Dempster [2] where the likelihood criterion is maximized or equivalently the Kullback-Leibler (KL) divergence is minimized to compute the model covariance matrix. While this solution is optimal for Gaussian distributions in the sense of the KL divergence, it is not optimal when compared with other information divergences and criteria such as Area Under the Curve (AUC). In this paper, we analytically compute upper and lower bounds for the AUC and discuss the quality of model selection problem using the AUC and its bounds as an accuracy measure in detection problem. We define the correlation approximation matrix (CAM) and show that analytical computation of the KL divergence, the AUC and its bounds only depend on the eigenvalues of CAM. We also show the relationship between the AUC, the KL divergence and the ROC curve by optimizing with respect to the ROC curve. In the examples provided, we pick tree structures as the simplest graphical models. We perform simulations on fully-connected graphs and compute the tree structured models by applying the widely used Chow-Liu algorithm [3]. Examples show that the quality of tree approximation models are not good in general based on information divergences, the AUC and its bounds when the number of nodes in the graphical model is large. We show both analytically and by simulations that the 1-AUC for the tree approximation model decays exponentially as the dimension of graphical model increases. "
733460870404005889,2016-05-20 00:54:33,https://t.co/XKsM7MxKGB,Efficient Nonparametric Smoothness Estimation. (arXiv:1605.05785v1 [math.ST]) https://t.co/XKsM7MxKGB,0,3," Abstract: Sobolev quantities (norms, inner products, and distances) of probability density functions are important in the theory of nonparametric statistics, but have rarely been used in practice, partly due to a lack of practical estimators. They also include, as special cases, $L^2$ quantities which are used in many applications. We propose and analyze a family of estimators for Sobolev quantities of unknown probability density functions. We bound the bias and variance of our estimators over finite samples, finding that they are generally minimax rate-optimal. Our estimators are significantly more computationally tractable than previous estimators, and exhibit a statistical/computational trade-off allowing them to adapt to computational constraints. We also draw theoretical connections to recent work on fast two-sample testing. Finally, we empirically validate our estimators on synthetic data. "
733460869338660868,2016-05-20 00:54:32,https://t.co/mhbj5ZHILA,Recurrent Exponential-Family Harmoniums without Backprop-Through-Time. (arXiv:1605.05799v1 [cs.LG]) https://t.co/mhbj5ZHILA,0,8," Abstract: Exponential-family harmoniums (EFHs), which extend restricted Boltzmann machines (RBMs) from Bernoulli random variables to other exponential families (Welling et al., 2005), are generative models that can be trained with unsupervised-learning techniques, like contrastive divergence (Hinton et al. 2006; Hinton, 2002), as density estimators for static data. Methods for extending RBMs--and likewise EFHs--to data with temporal dependencies have been proposed previously (Sutskever and Hinton, 2007; Sutskever et al., 2009), the learning procedure being validated by qualitative assessment of the generative model. Here we propose and justify, from a very different perspective, an alternative training procedure, proving sufficient conditions for optimal inference under that procedure. The resulting algorithm can be learned with only forward passes through the data--backprop-through-time is not required, as in previous approaches. The proof exploits a recent result about information retention in density estimators (Makin and Sabes, 2015), and applies it to a ""recurrent EFH"" (rEFH) by induction. Finally, we demonstrate optimality by simulation, testing the rEFH: (1) as a filter on training data generated with a linear dynamical system, the position of which is noisily reported by a population of ""neurons"" with Poisson-distributed spike counts; and (2) with the qualitative experiments proposed by Sutskever et al. (2009). "
733460868185260032,2016-05-20 00:54:32,https://t.co/Ga8O60IyJH,False Discovery Rate Control and Statistical Quality Assessment of Annotators in Crowdsourced Ranking. (arXiv:1605… https://t.co/Ga8O60IyJH,0,3," Abstract: With the rapid growth of crowdsourcing platforms it has become easy and relatively inexpensive to collect a dataset labeled by multiple annotators in a short time. However due to the lack of control over the quality of the annotators, some abnormal annotators may be affected by position bias which can potentially degrade the quality of the final consensus labels. In this paper we introduce a statistical framework to model and detect annotator's position bias in order to control the false discovery rate (FDR) without a prior knowledge on the amount of biased annotators - the expected fraction of false discoveries among all discoveries being not too high, in order to assure that most of the discoveries are indeed true and replicable. The key technical development relies on some new knockoff filters adapted to our problem and new algorithms based on the Inverse Scale Space dynamics whose discretization is potentially suitable for large scale crowdsourcing data analysis. Our studies are supported by experiments with both simulated examples and real-world data. The proposed framework provides us a useful tool for quantitatively studying annotator's abnormal behavior in crowdsourcing data arising from machine learning, sociology, computer vision, multimedia, etc. "
733460866943721473,2016-05-20 00:54:32,https://t.co/0kX4Q1c7UO,Bayesian Variable Selection for Globally Sparse Probabilistic PCA. (arXiv:1605.05918v1 [stat.ML]) https://t.co/0kX4Q1c7UO,2,5," Abstract: Sparse versions of principal component analysis (PCA) have imposed themselves as simple, yet powerful ways of selecting relevant features of high-dimensional data in an unsupervised manner. However, when several sparse principal components are computed, the interpretation of the selected variables is difficult since each axis has its own sparsity pattern and has to be interpreted separately. To overcome this drawback, we propose a Bayesian procedure called globally sparse probabilistic PCA (GSPPCA) that allows to obtain several sparse components with the same sparsity pattern. This allows the practitioner to identify the original variables which are relevant to describe the data. To this end, using Roweis' probabilistic interpretation of PCA and a Gaussian prior on the loading matrix, we provide the first exact computation of the marginal likelihood of a Bayesian PCA model. To avoid the drawbacks of discrete model selection, a simple relaxation of this framework is presented. It allows to find a path of models using a variational expectation-maximization algorithm. The exact marginal likelihood is then maximized over this path. This approach is illustrated on real and synthetic data sets. In particular, using unlabeled microarray data, GSPPCA infers much more relevant gene subsets than traditional sparse PCA algorithms. "
733460864959840257,2016-05-20 00:54:31,https://t.co/Tg3smFRwJt,Randomized Primal-Dual Proximal Block Coordinate Updates. (arXiv:1605.05969v1 [math.OC]) https://t.co/Tg3smFRwJt,0,4," Abstract: In this paper we propose a randomized primal-dual proximal block coordinate updating framework for a general multi-block convex optimization model with coupled objective function and linear constraints. Assuming mere convexity, we establish its $O(1/t)$ convergence rate in terms of the objective value and feasibility measure. The framework includes several existing algorithms as special cases such as a primal-dual method for bilinear saddle-point problems (PD-S), the proximal Jacobian ADMM (Prox-JADMM) and a randomized variant of the ADMM method for multi-block convex optimization. Our analysis recovers and/or strengthens the convergence properties of several existing algorithms. For example, for PD-S our result leads to the same order of convergence rate without the previously assumed boundedness condition on the constraint sets, and for Prox-JADMM the new result provides convergence rate in terms of the objective value and the feasibility violation. It is well known that the original ADMM may fail to converge when the number of blocks exceeds two. Our result shows that if an appropriate randomization procedure is invoked to select the updating blocks, then a sublinear rate of convergence in expectation can be guaranteed for multi-block ADMM, without assuming any strong convexity. The new approach is also extended to solve problems where only a stochastic approximation of the (sub-)gradient of the objective is available, and we establish an $O(1/\sqrt{t})$ convergence rate of the extended approach for solving stochastic programming. "
733460863638634497,2016-05-20 00:54:31,https://t.co/py6ArWyB0f,A Multi-Batch L-BFGS Method for Machine Learning. (arXiv:1605.06049v1 [math.OC]) https://t.co/py6ArWyB0f,3,8," Abstract: The question of how to parallelize the stochastic gradient descent (SGD) method has received much attention in the literature. In this paper, we focus instead on batch methods that use a sizeable fraction of the training set at each iteration to facilitate parallelism, and that employ second-order information. In order to improve the learning process, we follow a multi-batch approach in which the batch changes at each iteration. This can cause difficulties because L-BFGS employs gradient differences to update the Hessian approximations, and when these gradients are computed using different data points the process can be unstable. This paper shows how to perform stable quasi-Newton updating in the multi-batch setting, illustrates the behavior of the algorithm in a distributed computing platform, and studies its convergence properties for both the convex and nonconvex cases. "
733460862460039170,2016-05-20 00:54:31,https://t.co/WbYkJCiNsX,Blind system identification using kernel-based methods. (arXiv:1412.4056v2 [cs.SY] UPDATED) https://t.co/WbYkJCiNsX,0,3," Abstract: We propose a new method for blind system identification. Resorting to a Gaussian regression framework, we model the impulse response of the unknown linear system as a realization of a Gaussian process. The structure of the covariance matrix (or kernel) of such a process is given by the stable spline kernel, which has been recently introduced for system identification purposes and depends on an unknown hyperparameter. We assume that the input can be linearly described by few parameters. We estimate these parameters, together with the kernel hyperparameter and the noise variance, using an empirical Bayes approach. The related optimization problem is efficiently solved with a novel iterative scheme based on the Expectation-Maximization method. In particular, we show that each iteration consists of a set of simple update rules. We show, through some numerical experiments, very promising performance of the proposed method. "
733460860559990787,2016-05-20 00:54:30,https://t.co/ouMUL919ge,Character-based Neural Machine Translation. (arXiv:1603.00810v2 [cs.CL] UPDATED) https://t.co/ouMUL919ge,1,10," Abstract: Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task. "
733094830029570048,2016-05-19 00:40:02,https://t.co/Pg9D4gzRNO,Orthogonal symmetric non-negative matrix factorization under the stochastic block model. (arXiv:1605.05349v1 [stat… https://t.co/Pg9D4gzRNO,0,7," Abstract: We present a method based on the orthogonal symmetric non-negative matrix tri-factorization of the normalized Laplacian matrix for community detection in complex networks. While the exact factorization of a given order may not exist and is NP hard to compute, we obtain an approximate factorization by solving an optimization problem. We establish the connection of the factors obtained through the factorization to a non-negative basis of an invariant subspace of the estimated matrix, drawing parallel with the spectral clustering. Using such factorization for clustering in networks is motivated by analyzing a block-diagonal Laplacian matrix with the blocks representing the connected components of a graph. The method is shown to be consistent for community detection in graphs generated from the stochastic block model and the degree corrected stochastic block model. Simulation results and real data analysis show the effectiveness of these methods under a wide variety of situations, including sparse and highly heterogeneous graphs where the usual spectral clustering is known to fail. Our method also performs better than the state of the art in popular benchmark network datasets, e.g., the political web blogs and the karate club data. "
733094829123620865,2016-05-19 00:40:02,https://t.co/Be8RApwTJ2,Optimization Beyond Prediction: Prescriptive Price Optimization. (arXiv:1605.05422v1 [math.OC]) https://t.co/Be8RApwTJ2,0,3," Abstract: This paper addresses a novel data science problem, prescriptive price optimization, which derives the optimal price strategy to maximize future profit/revenue on the basis of massive predictive formulas produced by machine learning. The prescriptive price optimization first builds sales forecast formulas of multiple products, on the basis of historical data, which reveal complex relationships between sales and prices, such as price elasticity of demand and cannibalization. Then, it constructs a mathematical optimization problem on the basis of those predictive formulas. We present that the optimization problem can be formulated as an instance of binary quadratic programming (BQP). Although BQP problems are NP-hard in general and computationally intractable, we propose a fast approximation algorithm using a semi-definite programming (SDP) relaxation, which is closely related to the Goemans-Williamson's Max-Cut approximation. Our experiments on simulation and real retail datasets show that our prescriptive price optimization simultaneously derives the optimal prices of tens/hundreds products with practical computational time, that potentially improve 8.2% of gross profit of those products. "
733094827450093568,2016-05-19 00:40:01,https://t.co/s4wPOay0Vq,Learning activation functions from data using cubic spline interpolation. (arXiv:1605.05509v1 [stat.ML]) https://t.co/s4wPOay0Vq,2,6," Abstract: Neural networks require a careful design in order to perform properly on a given task. In particular, selecting a good activation function (possibly in a data-dependent fashion) is a crucial step, which remains an open problem in the research community. Despite a large amount of investigations, most current implementations simply select one fixed function from a small set of candidates, which is not adapted during training, and is shared among all neurons throughout the different layers. However, neither two of these assumptions can be supposed optimal in practice. In this paper, we present a principled way to have data-dependent adaptation of the activation functions, which is performed independently for each neuron. This is achieved by leveraging over past and present advances on cubic spline interpolation, allowing for local adaptation of the functions around their regions of use. The resulting algorithm is relatively cheap to implement, and overfitting is counterbalanced by the inclusion of a novel damping criterion, which penalizes unwanted oscillations from a predefined shape. Experimental results validate the proposal over two well-known benchmarks. "
733094826258882560,2016-05-19 00:40:01,https://t.co/XXgjDceuvS,ABC random forests for Bayesian parameter inference. (arXiv:1605.05537v1 [stat.ME]) https://t.co/XXgjDceuvS,4,6," Abstract: Approximate Bayesian Computation (ABC) has grown into a standard methodology to handle Bayesian inference in models associated with intractable likelihood functions. Most ABC implementations require the selection of a summary statistic as the data itself is too large to be compared to simulated realisations from the assumed model. The dimension of this statistic is generally quite large. Furthermore, the tolerance level that governs the acceptance or rejection of parameter values needs to be calibrated and the range of calibration techniques available so far is mostly based on asymptotic arguments. We propose here to conduct Bayesian inference based on an arbitrarily large vector of summary statistics without imposing a selection of the relevant components and bypassing the derivation of a tolerance. The approach relies on the random forest methodology of Breiman (2001) when applied to regression. We advocate the derivation of a new random forest for each component of the parameter vector. Correlations between parameter components are handled by separate random forests. When compared with standard ABC solutions, this technology offers significant gains in terms of robustness to the choice of the summary statistics and of computing time. All methods designed here have been added to the abcrf R library. "
733094825139011584,2016-05-19 00:40:01,https://t.co/SPLV0rkLJP,A Distributed Quaternion Kalman Filter With Applications to Fly-by-Wire Systems. (arXiv:1605.05588v1 [cs.SY]) https://t.co/SPLV0rkLJP,0,4," Abstract: The introduction of automated flight control and management systems have made possible aircraft designs that sacrifice arodynamic stability in order to incorporate stealth technology intro their shape, operate more efficiently, and are highly maneuverable. Therefore, modern flight management systems are reliant on multiple redundant sensors to monitor and control the rotations of the aircraft. To this end, a novel distributed quaternion Kalman filtering algorithm is developed for tracking the rotation and orientation of an aircraft in the three-dimensional space. The algorithm is developed to distribute computation among the sensors in a manner that forces them to consent to a unique solution while being robust to sensor and link failure, a desirable characteristic for flight management systems. In addition, the underlying quaternion-valued state space model allows to avoid problems associated with gimbal lock. The performance of the developed algorithm is verified through simulations. "
733094823717142533,2016-05-19 00:40:00,https://t.co/uFsa4flLGi]),Gaussian variational approximation with sparse precision matrix. (arXiv:1605.05622v1 [https://t.co/uFsa4flLGi]) https://t.co/g30rOjYWuZ,0,3,INDEXERROR
733094822542757888,2016-05-19 00:40:00,https://t.co/ukFqIEJG77,Online Algorithms For Parameter Mean And Variance Estimation In Dynamic Regression Models. (arXiv:1605.05697v1 [st… https://t.co/ukFqIEJG77,0,3," Abstract: We study the problem of estimating the parameters of a regression model from a set of observations, each consisting of a response and a predictor. The response is assumed to be related to the predictor via a regression model of unknown parameters. Often, in such models the parameters to be estimated are assumed to be constant. Here we consider the more general scenario where the parameters are allowed to evolve over time, a more natural assumption for many applications. We model these dynamics via a linear update equation with additive noise that is often used in a wide range of engineering applications, particularly in the well-known and widely used Kalman filter (where the system state it seeks to estimate maps to the parameter values here). We derive an approximate algorithm to estimate both the mean and the variance of the parameter estimates in an online fashion for a generic regression model. This algorithm turns out to be equivalent to the extended Kalman filter. We specialize our algorithm to the multivariate exponential family distribution to obtain a generalization of the generalized linear model (GLM). Because the common regression models encountered in practice such as logistic, exponential and multinomial all have observations modeled through an exponential family distribution, our results are used to easily obtain algorithms for online mean and variance parameter estimation for all these regression models in the context of time-dependent parameters. Lastly, we propose to use these algorithms in the contextual multi-armed bandit scenario, where so far model parameters are assumed static and observations univariate and Gaussian or Bernoulli. Both of these restrictions can be relaxed using the algorithms described here, which we combine with Thompson sampling to show the resulting performance on a simulation. "
733094821632585729,2016-05-19 00:40:00,https://t.co/eXdxscPIkP,Generalized Min-Max Kernel and Generalized Consistent Weighted Sampling. (arXiv:1605.05721v1 [cs.LG]) https://t.co/eXdxscPIkP,0,2," Abstract: We propose the ""generalized min-max"" (GMM) kernel as a measure of data similarity, where data vectors can have both positive and negative entries. GMM is positive definite as there is an associate hashing method named ""generalized consistent weighted sampling"" (GCWS) which linearizes this (nonlinear) kernel. A natural competitor of GMM is the radial basis function (RBF) kernel, whose corresponding hashing method is known as the ""random Fourier features"" (RFF). An extensive experimental study on classifications of {50} publicly available datasets demonstrates that both the GMM and RBF kernels can often substantially improve over linear classifiers. Furthermore, the GCWS hashing method typically requires substantially fewer samples than (the normalized) RFF in order to achieve similar classification accuracies. To understand the property of random Fourier features (RFF), we derive the theoretical variances of RFF and its normalized version (which we name as NRFF). Overall, the relative (to the expectation) variances of RFF and NRFF are substantially larger than the relative variance of GCWS. This helps explain the superb empirical results of GCWS compared to RFF (NRFF). We expect that GMM and GCWS will be adopted in practice for large-scale statistical machine learning applications and efficient near neighbor search (as GMM generates discrete hash values). "
733094820793733120,2016-05-19 00:40:00,https://t.co/LhXgHA0wDe,Characteristic Kernels and Infinitely Divisible Distributions. (arXiv:1403.7304v2 [stat.ML] UPDATED) https://t.co/LhXgHA0wDe,0,4," Abstract: We connect shift-invariant characteristic kernels to infinitely divisible distributions on $\mathbb{R}^{d}$. Characteristic kernels play an important role in machine learning applications with their kernel means to distinguish any two probability measures. The contribution of this paper is two-fold. First, we show, using the L\'evy-Khintchine formula, that any shift-invariant kernel given by a bounded, continuous and symmetric probability density function (pdf) of an infinitely divisible distribution on $\mathbb{R}^d$ is characteristic. We also present some closure property of such characteristic kernels under addition, pointwise product, and convolution. Second, in developing various kernel mean algorithms, it is fundamental to compute the following values: (i) kernel mean values $m_P(x)$, $x \in \mathcal{X}$, and (ii) kernel mean RKHS inner products ${\left\langle m_P, m_Q \right\rangle_{\mathcal{H}}}$, for probability measures $P, Q$. If $P, Q$, and kernel $k$ are Gaussians, then computation (i) and (ii) results in Gaussian pdfs that is tractable. We generalize this Gaussian combination to more general cases in the class of infinitely divisible distributions. We then introduce a {\it conjugate} kernel and {\it convolution trick}, so that the above (i) and (ii) have the same pdf form, expecting tractable computation at least in some cases. As specific instances, we explore $\alpha$-stable distributions and a rich class of generalized hyperbolic distributions, where the Laplace, Cauchy and Student-t distributions are included. "
733094819845840898,2016-05-19 00:39:59,https://t.co/g4EeVfnG0K,A new kernel-based approach for overparameterized Hammerstein system identification. (arXiv:1504.08190v2 [cs.SY] U… https://t.co/g4EeVfnG0K,0,2," Abstract: In this paper we propose a new identification scheme for Hammerstein systems, which are dynamic systems consisting of a static nonlinearity and a linear time-invariant dynamic system in cascade. We assume that the nonlinear function can be described as a linear combination of $p$ basis functions. We reconstruct the $p$ coefficients of the nonlinearity together with the first $n$ samples of the impulse response of the linear system by estimating an $np$-dimensional overparameterized vector, which contains all the combinations of the unknown variables. To avoid high variance in these estimates, we adopt a regularized kernel-based approach and, in particular, we introduce a new kernel tailored for Hammerstein system identification. We show that the resulting scheme provides an estimate of the overparameterized vector that can be uniquely decomposed as the combination of an impulse response and $p$ coefficients of the static nonlinearity. We also show, through several numerical experiments, that the proposed method compares very favorably with two standard methods for Hammerstein system identification. "
733094818910470144,2016-05-19 00:39:59,https://t.co/EIRDQg6G03,On the estimation of initial conditions in kernel-based system identification. (arXiv:1504.08196v2 [cs.SY] UPDATED) https://t.co/EIRDQg6G03,0,2," Abstract: Recent developments in system identification have brought attention to regularized kernel-based methods, where, adopting the recently introduced stable spline kernel, prior information on the unknown process is enforced. This reduces the variance of the estimates and thus makes kernel-based methods particularly attractive when few input-output data samples are available. In such cases however, the influence of the system initial conditions may have a significant impact on the output dynamics. In this paper, we specifically address this point. We propose three methods that deal with the estimation of initial conditions using different types of information. The methods consist in various mixed maximum likelihood--a posteriori estimators which estimate the initial conditions and tune the hyperparameters characterizing the stable spline kernel. To solve the related optimization problems, we resort to the expectation-maximization method, showing that the solutions can be attained by iterating among simple update steps. Numerical experiments show the advantages, in terms of accuracy in reconstructing the system impulse response, of the proposed strategies, compared to other kernel-based schemes not accounting for the effect initial conditions. "
733094817991929856,2016-05-19 00:39:59,https://t.co/OzcTtDtZFB,Causality on Cross-Sectional Data: Stable Specification Search in Constrained Structural Equation Modeling. (arXiv… https://t.co/OzcTtDtZFB,0,4," Abstract: Causal modeling has long been an attractive topic for many researchers and in recent decades there has seen a surge in theoretical development and discovery algorithms. Generally discovery algorithms can be divided into two approaches: constraint-based and score-based. The constraint-based approach is able to detect common causes of the observed variables but the use of independence tests makes it less reliable. The score-based approach produces a result that is easier to interpret as it also measures the reliability of the inferred causal relationships, but it is unable to detect common confounders of the observed variables. A drawback of both score-based and constrained-based approaches is the inherent instability in structure estimation. With finite samples small changes in the data can lead to completely different optimal structures. The present work introduces a new hypothesis-free score-based causal discovery algorithm, called stable specification search, that is robust for finite samples based on recent advances in stability selection using subsampling and selection algorithms. Structure search is performed over Structural Equation Models. Our approach uses exploratory search but allows incorporation of prior background knowledge. We validated our approach on one simulated data set, which we compare to the known ground truth, and two real-world data sets for Chronic Fatigue Syndrome and Attention Deficit Hyperactivity Disorder, which we compare to earlier medical studies. The results on the simulated data set show significant improvement over alternative approaches and the results on the real-word data sets show consistency with the hypothesis driven models constructed by medical experts. "
733094817006227457,2016-05-19 00:39:59,https://t.co/vzAutbH1gQ,Variational Gaussian Copula Inference. (arXiv:1506.05860v3 [stat.ML] UPDATED) https://t.co/vzAutbH1gQ,1,5," Abstract: We utilize copulas to constitute a unified framework for constructing and optimizing variational proposals in hierarchical Bayesian models. For models with continuous and non-Gaussian hidden variables, we propose a semiparametric and automated variational Gaussian copula approach, in which the parametric Gaussian copula family is able to preserve multivariate posterior dependence, and the nonparametric transformations based on Bernstein polynomials provide ample flexibility in characterizing the univariate marginal posteriors. "
732732475814907904,2016-05-18 00:40:10,https://t.co/PhEWWDkFeF,Classification of Big Data with Application to Imaging Genetics. (arXiv:1605.04932v1 [physics.data-an]) https://t.co/PhEWWDkFeF,0,3," Abstract: Big data applications, such as medical imaging and genetics, typically generate datasets that consist of few observations n on many more variables p, a scenario that we denote as p>>n. Traditional data processing methods are often insufficient for extracting information out of big data. This calls for the development of new algorithms that can deal with the size, complexity, and the special structure of such datasets. In this paper, we consider the problem of classifying p>>n data and propose a classification method based on linear discriminant analysis (LDA). Traditional LDA depends on the covariance estimate of the data, but when p>>n the sample covariance estimate is singular. The proposed method estimates the covariance by using a sparse version of noisy principal component analysis (nPCA). The use of sparsity in this setting aims at automatically selecting variables that are relevant for classification. In experiments, the new method is compared to state-of-the art methods for big data problems using both simulated datasets and imaging genetics datasets. "
732732474132959232,2016-05-18 00:40:10,https://t.co/Kp1DaL2RpP,Probing the Geometry of Data with Diffusion Fr\'echet Functions. (arXiv:1605.04955v1 [stat.ML]) https://t.co/Kp1DaL2RpP,1,5," Abstract: Many complex ecosystems, such as those formed by multiple microbial taxa, involve intricate interactions amongst various sub-communities. The most basic relationships are frequently modeled as co-occurrence networks in which the nodes represent the various players in the community and the weighted edges encode levels of interaction. In this setting, the composition of a community may be viewed as a probability distribution on the nodes of the network. This paper develops methods for modeling the organization of such data, as well as their Euclidean counterparts, across spatial scales. Using the notion of diffusion distance, we introduce diffusion Fr\'echet functions and diffusion Fr\'echet vectors associated with probability distributions on Euclidean spaces and the vertex set of a weighted network, respectively. We prove that these functional statistics are stable with respect to the Wasserstein distance between probability measures, thus yielding robust descriptors of their shapes. We apply the methodology to investigate bacterial communities in the human gut, seeking to characterize divergence from intestinal homeostasis in patients with Clostridium difficile infection (CDI) and the effects of fecal microbiota transplantation, a treatment used in CDI patients that has proven to be significantly more effective than traditional treatment with antibiotics. The proposed method proves useful in deriving a biomarker that might help elucidate the mechanisms that drive these processes. "
732732472161669121,2016-05-18 00:40:09,https://t.co/riYVqBdBx9,Incremental Object Recognition in Robotics with Extension to New Classes in Constant Time. (arXiv:1605.05045v1 [st… https://t.co/riYVqBdBx9,0,4," Abstract: We consider object recognition in the context of lifelong learning, where a robotic agent learns to discriminate between a growing number of object classes as it accumulates experience about the environment. We propose an incremental variant of the Regularized Least Squares for Classification (RLSC) algorithm, and exploit its structure to seamlessly add new classes to the learned model. The presented algorithm addresses the problem of having unbalanced proportion of training examples per class, which occurs when new objects are presented to the system for the first time. We evaluate our algorithm on both a machine learning benchmark dataset and a challenging object recognition task in a robotic setting. Empirical evidence on both problems shows that our approach is significantly faster than its batch counterparts while achieving comparable or better classification performance when classes are unbalanced. "
732732469607342080,2016-05-18 00:40:08,https://t.co/idiHRkDqCh,Biologically Inspired Radio Signal Feature Extraction with Sparse Denoising Autoencoders. (arXiv:1605.05239v1 [sta… https://t.co/idiHRkDqCh,0,9," Abstract: Automatic modulation classification (AMC) is an important task for modern communication systems; however, it is a challenging problem when signal features and precise models for generating each modulation may be unknown. We present a new biologically-inspired AMC method without the need for models or manually specified features --- thus removing the requirement for expert prior knowledge. We accomplish this task using regularized stacked sparse denoising autoencoders (SSDAs). Our method selects efficient classification features directly from raw in-phase/quadrature (I/Q) radio signals in an unsupervised manner. These features are then used to construct higher-complexity abstract features which can be used for automatic modulation classification. We demonstrate this process using a dataset generated with a software defined radio, consisting of random input bits encoded in 100-sample segments of various common digital radio modulations. Our results show correct classification rates of > 99% at 7.5 dB signal-to-noise ratio (SNR) and > 92% at 0 dB SNR in a 6-way classification test. Our experiments demonstrate a dramatically new and broadly applicable mechanism for performing AMC and related tasks without the need for expert-defined or modulation-specific signal information. "
732732467250106369,2016-05-18 00:40:08,https://t.co/ML21KpJDPY,Learning Convolutional Neural Networks for Graphs. (arXiv:1605.05273v1 [cs.LG]) https://t.co/ML21KpJDPY,3,12," Abstract: Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient. "
732732465152950273,2016-05-18 00:40:07,https://t.co/7cbhVw20aF,Exact Simulation of Noncircular or Improper Complex-Valued Stationary Gaussian Processes using Circulant Embedding… https://t.co/7cbhVw20aF,0,3," Abstract: This paper provides an algorithm for simulating improper (or noncircular) complex-valued stationary Gaussian processes. The technique utilizes recently developed methods for multivariate Gaussian processes from the circulant embedding literature. The method can be performed in $\mathcal{O}(nlog_2n)$ operations, where n is the length of the desired sequence. The method is exact, except when eigenvalues of prescribed circulant matrices are negative. We evaluate the performance of the algorithm empirically, and provide a practical example where the method is guaranteed to be exact for all $n$, with an improper fractional Gaussian noise process. "
732732462607011840,2016-05-18 00:40:07,https://t.co/cdOxbFAG9Y,Minimax Lower Bounds for Kronecker-Structured Dictionary Learning. (arXiv:1605.05284v1 [cs.IT]) https://t.co/cdOxbFAG9Y,0,3," Abstract: Dictionary learning is the problem of estimating the collection of atomic elements that provide a sparse representation of measured/collected signals or data. This paper finds fundamental limits on the sample complexity of estimating dictionaries for tensor data by proving a lower bound on the minimax risk. This lower bound depends on the dimensions of the tensor and parameters of the generative model. The focus of this paper is on second-order tensor data, with the underlying dictionaries constructed by taking the Kronecker product of two smaller dictionaries and the observed data generated by sparse linear combinations of dictionary atoms observed through white Gaussian noise. In this regard, the paper provides a general lower bound on the minimax risk and also adapts the proof techniques for equivalent results using sparse and Gaussian coefficient models. The reported results suggest that the sample complexity of dictionary learning for tensor data can be significantly lower than that for unstructured data. "
732732460128178176,2016-05-18 00:40:06,https://t.co/v3f5Fz2IaF,Convex Optimization for Linear Query Processing under Approximate Differential Privacy. (arXiv:1602.04302v3 [cs.DB… https://t.co/v3f5Fz2IaF,2,4," Abstract: Differential privacy enables organizations to collect accurate aggregates over sensitive data with strong, rigorous guarantees on individuals' privacy. Previous work has found that under differential privacy, computing multiple correlated aggregates as a batch, using an appropriate \emph{strategy}, may yield higher accuracy than computing each of them independently. However, finding the best strategy that maximizes result accuracy is non-trivial, as it involves solving a complex constrained optimization program that appears to be non-linear and non-convex. Hence, in the past much effort has been devoted in solving this non-convex optimization program. Existing approaches include various sophisticated heuristics and expensive numerical solutions. None of them, however, guarantees to find the optimal solution of this optimization problem. This paper points out that under ($\epsilon$, $\delta$)-differential privacy, the optimal solution of the above constrained optimization problem in search of a suitable strategy can be found, rather surprisingly, by solving a simple and elegant convex optimization program. Then, we propose an efficient algorithm based on Newton's method, which we prove to always converge to the optimal solution with linear global convergence rate and quadratic local convergence rate. Empirical evaluations demonstrate the accuracy and efficiency of the proposed solution. "
732369897846677504,2016-05-17 00:39:25,https://t.co/29yxDiwHrX,Proceedings of the 5th Workshop on Machine Learning and Interpretation in Neuroimaging (MLINI) at NIPS 2015. (arXi… https://t.co/29yxDiwHrX,0,7," Abstract: This volume is a collection of contributions from the 5th Workshop on Machine Learning and Interpretation in Neuroimaging (MLINI) at the Neural Information Processing Systems (NIPS 2015) conference. Modern multivariate statistical methods developed in the rapidly growing field of machine learning are being increasingly applied to various problems in neuroimaging, from cognitive state detection to clinical diagnosis and prognosis. Multivariate pattern analysis methods are designed to examine complex relationships between high-dimensional signals, such as brain images, and outcomes of interest, such as the category of a stimulus, a type of a mental state of a subject, or a specific mental disorder. Such techniques are in contrast with the traditional mass-univariate approaches that dominated neuroimaging in the past and treated each individual imaging measurement in isolation. We believe that machine learning has a prominent role in shaping how questions in neuroscience are framed, and that the machine-learning mind set is now entering modern psychology and behavioral studies. It is also equally important that practical applications in these fields motivate a rapidly evolving line or research in the machine learning community. In parallel, there is an intense interest in learning more about brain function in the context of rich naturalistic environments and scenes. Efforts to go beyond highly specific paradigms that pinpoint a single function, towards schemes for measuring the interaction with natural and more varied scene are made. The goal of the workshop is to pinpoint the most pressing issues and common challenges across the neuroscience, neuroimaging, psychology and machine learning fields, and to sketch future directions and open questions in the light of novel methodology. "
732369896265404416,2016-05-17 00:39:24,https://t.co/LRS8hxpvCf,Monotone Retargeting for Unsupervised Rank Aggregation with Object Features. (arXiv:1605.04465v1 [stat.ML]) https://t.co/LRS8hxpvCf,0,2," Abstract: Learning the true ordering between objects by aggregating a set of expert opinion rank order lists is an important and ubiquitous problem in many applications ranging from social choice theory to natural language processing and search aggregation. We study the problem of unsupervised rank aggregation where no ground truth ordering information in available, neither about the true preference ordering between any set of objects nor about the quality of individual rank lists. Aggregating the often inconsistent and poor quality rank lists in such an unsupervised manner is a highly challenging problem, and standard consensus-based methods are often ill-defined, and difficult to solve. In this manuscript we propose a novel framework to bypass these issues by using object attributes to augment the standard rank aggregation framework. We design algorithms that learn joint models on both rank lists and object features to obtain an aggregated rank ordering that is more accurate and robust, and also helps weed out rank lists of dubious validity. We validate our techniques on synthetic datasets where our algorithm is able to estimate the true rank ordering even when the rank lists are corrupted. Experiments on three real datasets, MQ2008, MQ2008 and OHSUMED, show that using object features can result in significant improvement in performance over existing rank aggregation methods that do not use object information. Furthermore, when at least some of the rank lists are of high quality, our methods are able to effectively exploit their high expertise to output an aggregated rank ordering of great accuracy. "
732369894675775488,2016-05-17 00:39:24,https://t.co/Eshi07rDiZ,Generalized Linear Models for Aggregated Data. (arXiv:1605.04466v1 [stat.ML]) https://t.co/Eshi07rDiZ,1,3," Abstract: Databases in domains such as healthcare are routinely released to the public in aggregated form. Unfortunately, naive modeling with aggregated data may significantly diminish the accuracy of inferences at the individual level. This paper addresses the scenario where features are provided at the individual level, but the target variables are only available as histogram aggregates or order statistics. We consider a limiting case of generalized linear modeling when the target variables are only known up to permutation, and explore how this relates to permutation testing; a standard technique for assessing statistical dependency. Based on this relationship, we propose a simple algorithm to estimate the model parameters and individual level inferences via alternating imputation and standard generalized linear model fitting. Our results suggest the effectiveness of the proposed approach when, in the original data, permutation testing accurately ascertains the veracity of the linear relationship. The framework is extended to general histogram data with larger bins - with order statistics such as the median as a limiting case. Our experimental results on simulated data and aggregated healthcare data suggest a diminishing returns property with respect to the granularity of the histogram - when a linear relationship holds in the original data, the targets can be predicted accurately given relatively coarse histograms. "
732369892796706816,2016-05-17 00:39:23,https://t.co/uyko0mC4qY,Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient. (arXiv… https://t.co/uyko0mC4qY,0,2," Abstract: This work focuses on dynamic regret of online convex optimization that compares the performance of online learning to a clairvoyant who knows the sequence of loss functions in advance and hence selects the minimizer of the loss function at each step. By assuming that the clairvoyant moves slowly (i.e., the minimizers change slowly), we present several improved variation-based upper bounds of the dynamic regret under the true and noisy gradient feedback, which are {\it optimal} in light of the presented lower bounds. The key to our analysis is to explore a regularity metric that measures the temporal changes in the clairvoyant's minimizers, to which we refer as {\it path variation}. Firstly, we present a general lower bound in terms of the path variation, and then show that under full information or gradient feedback we are able to achieve an optimal dynamic regret. Secondly, we present a lower bound with noisy gradient feedback and then show that we can achieve optimal dynamic regrets under a stochastic gradient feedback and two-point bandit feedback. Moreover, for a sequence of smooth loss functions that admit a small variation in the gradients, our dynamic regret under the two-point bandit feedback matches what is achieved with full information. "
732369890422706176,2016-05-17 00:39:23,https://t.co/yzkApBsZiM,Alternating optimization method based on nonnegative matrix factorizations for deep neural networks. (arXiv:1605.0… https://t.co/yzkApBsZiM,0,12," Abstract: The backpropagation algorithm for calculating gradients has been widely used in computation of weights for deep neural networks (DNNs). This method requires derivatives of objective functions and has some difficulties finding appropriate parameters such as learning rate. In this paper, we propose a novel approach for computing weight matrices of fully-connected DNNs by using two types of semi-nonnegative matrix factorizations (semi-NMFs). In this method, optimization processes are performed by calculating weight matrices alternately, and backpropagation (BP) is not used. We also present a method to calculate stacked autoencoder using a NMF. The output results of the autoencoder are used as pre-training data for DNNs. The experimental results show that our method using three types of NMFs attains similar error rates to the conventional DNNs with BP. "
732369888447221760,2016-05-17 00:39:22,https://t.co/1mKO1ykqQZ,Wavelet Scattering Regression of Quantum Chemical Energies. (arXiv:1605.04654v1 [math.CA]) https://t.co/1mKO1ykqQZ,0,5," Abstract: We introduce multiscale invariant dictionaries to estimate quantum chemical energies of organic molecules, from training databases. Molecular energies are invariant to isometric atomic displacements, and are Lipschitz continuous to molecular deformations. Similarly to density functional theory (DFT), the molecule is represented by an electronic density function. A multiscale invariant dictionary is calculated with wavelet scattering invariants. It cascades a first wavelet transform which separates scales, with a second wavelet transform which computes interactions across scales. Sparse scattering regressions give state of the art results over two databases of organic planar molecules. On these databases, the regression error is of the order of the error produced by DFT codes, but at a fraction of the computational cost. "
732369886714986497,2016-05-17 00:39:22,https://t.co/o7sO1ijw5y,Solve-Select-Scale: A Three Step Process For Sparse Signal Estimation. (arXiv:1605.04657v1 [cs.IT]) https://t.co/o7sO1ijw5y,0,2," Abstract: In the theory of compressed sensing (CS), the sparsity $\|x\|_0$ of the unknown signal $\mathbf{x} \in \mathcal{R}^n$ is of prime importance and the focus of reconstruction algorithms has mainly been either $\|x\|_0$ or its convex relaxation (via $\|x\|_1$). However, it is typically unknown in practice and has remained a challenge when nothing about the size of the support is known. As pointed recently, $\|x\|_0$ might not be the best metric to minimize directly, both due to its inherent complexity as well as its noise performance. Recently a novel stable measure of sparsity $s(\mathbf{x}) := \|\mathbf{x}\|_1^2/\|\mathbf{x}\|_2^2$ has been investigated by Lopes \cite{Lopes2012}, which is a sharp lower bound on $\|\mathbf{x}\|_0$. The estimation procedure for this measure uses only a small number of linear measurements, does not rely on any sparsity assumptions, and requires very little computation. The usage of the quantity $s(\mathbf{x})$ in sparse signal estimation problems has not received much importance yet. We develop the idea of incorporating $s(\mathbf{x})$ into the signal estimation framework. We also provide a three step algorithm to solve problems of the form $\mathbf{Ax=b}$ with no additional assumptions on the original signal $\mathbf{x}$. "
732369885066579968,2016-05-17 00:39:22,https://t.co/bE5DVca6x2,A Critical Examination of RESCAL for Completion of Knowledge Bases with Transitive Relations. (arXiv:1605.04672v1 … https://t.co/bE5DVca6x2,0,2," Abstract: Link prediction in large knowledge graphs has received a lot of attention recently because of its importance for inferring missing relations and for completing and improving noisily extracted knowledge graphs. Over the years a number of machine learning researchers have presented various models for predicting the presence of missing relations in a knowledge base. Although all the previous methods are presented with empirical results that show high performance on select datasets, there is almost no previous work on understanding the connection between properties of a knowledge base and the performance of a model. In this paper we analyze the RESCAL method and prove that it can not encode asymmetric transitive relations in knowledge bases. "
732369883065909248,2016-05-17 00:39:21,https://t.co/sthJDjhjuL,Geometry Aware Mappings for High Dimensional Sparse Factors. (arXiv:1605.04764v1 [cs.LG]) https://t.co/sthJDjhjuL,1,3," Abstract: While matrix factorisation models are ubiquitous in large scale recommendation and search, real time application of such models requires inner product computations over an intractably large set of item factors. In this manuscript we present a novel framework that uses the inverted index representation to exploit structural properties of sparse vectors to significantly reduce the run time computational cost of factorisation models. We develop techniques that use geometry aware permutation maps on a tessellated unit sphere to obtain high dimensional sparse embeddings for latent factors with sparsity patterns related to angular closeness of the original latent factors. We also design several efficient and deterministic realisations within this framework and demonstrate with experiments that our techniques lead to faster run time operation with minimal loss of accuracy. "
732369880859742208,2016-05-17 00:39:21,https://t.co/HqX7Afi44w,Off-policy evaluation for slate recommendation. (arXiv:1605.04812v1 [cs.LG]) https://t.co/HqX7Afi44w,0,3," Abstract: This paper studies the evaluation of policies that recommend an ordered set of items (e.g., a ranking) based on some context---a common scenario in web search, ads and recommender systems. We develop the first practical technique for evaluating page-level metrics of such policies offline using logged past data, alleviating the need for online A/B tests. Our method models the observed quality of the recommended set (e.g., time to success in web search) as an additive decomposition across items. Crucially, the per-item quality is not directly observed or easily modeled from the item's features. A thorough empirical evaluation reveals that this model fits many realistic measures of quality and theoretical analysis shows exponential savings in the amount of required data compared with prior off-policy evaluation approaches. "
732369879299440641,2016-05-17 00:39:20,https://t.co/jJSy7eCWXL,Online Optimization for Large-Scale Max-Norm Regularization. (arXiv:1406.3190v4 [stat.ML] UPDATED) https://t.co/jJSy7eCWXL,0,4," Abstract: Max-norm regularizer has been extensively studied in the last decade as it promotes an effective low-rank estimation for the underlying data. However, such max-norm regularized problems are typically formulated and solved in a batch manner, which prevents it from processing big data due to possible memory budget. In this paper, hence, we propose an online algorithm that is scalable to large-scale setting. Particularly, we consider the matrix decomposition problem as an example, although a simple variant of the algorithm and analysis can be adapted to other important problems such as matrix completion. The crucial technique in our implementation is to reformulating the max-norm to an equivalent matrix factorization form, where the factors consist of a (possibly overcomplete) basis component and a coefficients one. In this way, we may maintain the basis component in the memory and optimize over it and the coefficients for each sample alternatively. Since the memory footprint of the basis component is independent of the sample size, our algorithm is appealing when manipulating a large collection of samples. We prove that the sequence of the solutions (i.e., the basis component) produced by our algorithm converges to a stationary point of the expected loss function asymptotically. Numerical study demonstrates encouraging results for the efficacy and robustness of our algorithm compared to the widely used nuclear norm solvers. "
732369875977568257,2016-05-17 00:39:19,https://t.co/zDUBrQF1aY,Projected Nesterov's Proximal-Gradient Algorithm for Sparse Signal Reconstruction with a Convex Constraint. (arXiv… https://t.co/zDUBrQF1aY,0,5," Abstract: We develop a projected Nesterov's proximal-gradient (PNPG) approach for sparse signal reconstruction that combines adaptive step size with Nesterov's momentum acceleration. The objective function that we wish to minimize is the sum of a convex differentiable data-fidelity (negative log-likelihood (NLL)) term and a convex regularization term. We apply sparse signal regularization where the signal belongs to a closed convex set within the closure of the domain of the NLL; the convex-set constraint facilitates flexible NLL domains and accurate signal recovery. Signal sparsity is imposed using the $\ell_1$-norm penalty on the signal's linear transform coefficients or gradient map, respectively. The PNPG approach employs projected Nesterov's acceleration step with restart and an inner iteration to compute the proximal mapping. We propose an adaptive step-size selection scheme to obtain a good local majorizing function of the NLL and reduce the time spent backtracking. Thanks to step-size adaptation, PNPG does not require Lipschitz continuity of the gradient of the NLL. We present an integrated derivation of the momentum acceleration and its $\mathcal{O}(k^{-2})$ convergence-rate and iterate convergence proofs, which account for adaptive step-size selection, inexactness of the iterative proximal mapping, and the convex-set constraint. The tuning of PNPG is largely application-independent. Tomographic and compressed-sensing reconstruction experiments with Poisson generalized linear and Gaussian linear measurement models demonstrate the performance of the proposed approach. "
732369874039779329,2016-05-17 00:39:19,https://t.co/gYUY2EbvPG,Antisocial Behavior in Online Discussion Communities. (arXiv:1504.00680v2 [cs.SI] UPDATED) https://t.co/gYUY2EbvPG,0,3," Abstract: User contributions in the form of posts, comments, and votes are essential to the success of online communities. However, allowing user participation also invites undesirable behavior such as trolling. In this paper, we characterize antisocial behavior in three large online discussion communities by analyzing users who were banned from these communities. We find that such users tend to concentrate their efforts in a small number of threads, are more likely to post irrelevantly, and are more successful at garnering responses from other users. Studying the evolution of these users from the moment they join a community up to when they get banned, we find that not only do they write worse than other users over time, but they also become increasingly less tolerated by the community. Further, we discover that antisocial behavior is exacerbated when community feedback is overly harsh. Our analysis also reveals distinct groups of users with different levels of antisocial behavior that can change over time. We use these insights to identify antisocial users early on, a task of high practical importance to community maintainers. "
732369872206856192,2016-05-17 00:39:18,https://t.co/ArJ6tw5tJl,Data-driven Sequential Monte Carlo in Probabilistic Programming. (arXiv:1512.04387v2 [cs.AI] UPDATED) https://t.co/ArJ6tw5tJl,2,4," Abstract: Most of Markov Chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC) algorithms in existing probabilistic programming systems suboptimally use only model priors as proposal distributions. In this work, we describe an approach for training a discriminative model, namely a neural network, in order to approximate the optimal proposal by using posterior estimates from previous runs of inference. We show an example that incorporates a data-driven proposal for use in a non-parametric model in the Anglican probabilistic programming system. Our results show that data-driven proposals can significantly improve inference performance so that considerably fewer particles are necessary to perform a good posterior estimation. "
732369870180978688,2016-05-17 00:39:18,https://t.co/OwzqWnyweB,On Distributed Cooperative Decision-Making in Multiarmed Bandits. (arXiv:1512.06888v2 [cs.SY] UPDATED) https://t.co/OwzqWnyweB,0,2," Abstract: We study the explore-exploit tradeoff in distributed cooperative decision-making using the context of the multiarmed bandit (MAB) problem. For the distributed cooperative MAB problem, we design the cooperative UCB algorithm that comprises two interleaved distributed processes: (i) running consensus algorithms for estimation of rewards, and (ii) upper-confidence-bound-based heuristics for selection of arms. We rigorously analyze the performance of the cooperative UCB algorithm and characterize the influence of communication graph structure on the decision-making performance of the group. "
732369868415246337,2016-05-17 00:39:18,https://t.co/ho0JvGOeOz,"Implementing a Bayes Filter in a Neural Circuit: The Case of Unknown, Nonlinear Stimulus Dynamics. (arXiv:1512.078… https://t.co/ho0JvGOeOz",1,5," Abstract: In order to interact intelligently with objects in the world, animals must first transform neural population responses into estimates of the dynamic, unknown stimuli which caused them. The Bayesian solution to this problem is known as a Bayes filter, which applies Bayes' rule to combine population responses with the predictions of an internal model. In this paper we present a method for learning to approximate a Bayes filter when the stimulus dynamics are unknown. To do this we use the inferential properties of probabilistic population codes to compute Bayes' rule, and train a neural network to compute approximate predictions by the method of maximum likelihood. In particular, we perform stochastic gradient descent on the negative log-likelihood with a novel approximation of the gradient. We demonstrate our methods on a finite-state, a linear, and a nonlinear filtering problem, and show how the hidden layer of the neural network develops tuning curves which are consistent with findings in experimental neuroscience. "
732369862861979652,2016-05-17 00:39:16,https://t.co/j6x6fh15KM,Better safe than sorry: Risky function exploitation through safe optimization. (arXiv:1602.01052v2 [stat.AP] UPDAT… https://t.co/j6x6fh15KM,0,2," Abstract: Exploration-exploitation of functions, that is learning and optimizing a mapping between inputs and expected outputs, is ubiquitous to many real world situations. These situations sometimes require us to avoid certain outcomes at all cost, for example because they are poisonous, harmful, or otherwise dangerous. We test participants' behavior in scenarios in which they have to find the optimum of a function while at the same time avoid outputs below a certain threshold. In two experiments, we find that Safe-Optimization, a Gaussian Process-based exploration-exploitation algorithm, describes participants' behavior well and that participants seem to care firstly whether a point is safe and then try to pick the optimal point from all such safe points. This means that their trade-off between exploration and exploitation can be seen as an intelligent, approximate, and homeostasis-driven strategy. "
732009260956192768,2016-05-16 00:46:22,https://t.co/X57yaNBXL8,Transfer Hashing with Privileged Information. (arXiv:1605.04034v1 [cs.LG]) https://t.co/X57yaNBXL8,1,3," Abstract: Most existing learning to hash methods assume that there are sufficient data, either labeled or unlabeled, on the domain of interest (i.e., the target domain) for training. However, this assumption cannot be satisfied in some real-world applications. To address this data sparsity issue in hashing, inspired by transfer learning, we propose a new framework named Transfer Hashing with Privileged Information (THPI). Specifically, we extend the standard learning to hash method, Iterative Quantization (ITQ), in a transfer learning manner, namely ITQ+. In ITQ+, a new slack function is learned from auxiliary data to approximate the quantization error in ITQ. We developed an alternating optimization approach to solve the resultant optimization problem for ITQ+. We further extend ITQ+ to LapITQ+ by utilizing the geometry structure among the auxiliary data for learning more precise binary codes in the target domain. Extensive experiments on several benchmark datasets verify the effectiveness of our proposed approaches through comparisons with several state-of-the-art baselines. "
732009259152642048,2016-05-16 00:46:22,https://t.co/4IZwnuAN7X,Cross-Domain Visual Matching via Generalized Similarity Measure and Feature Learning. (arXiv:1605.04039v1 [cs.CV]) https://t.co/4IZwnuAN7X,0,5," Abstract: Cross-domain visual data matching is one of the fundamental problems in many real-world vision tasks, e.g., matching persons across ID photos and surveillance videos. Conventional approaches to this problem usually involves two steps: i) projecting samples from different domains into a common space, and ii) computing (dis-)similarity in this space based on a certain distance. In this paper, we present a novel pairwise similarity measure that advances existing models by i) expanding traditional linear projections into affine transformations and ii) fusing affine Mahalanobis distance and Cosine similarity by a data-driven combination. Moreover, we unify our similarity measure with feature representation learning via deep convolutional neural networks. Specifically, we incorporate the similarity measure matrix into the deep architecture, enabling an end-to-end way of model optimization. We extensively evaluate our generalized similarity model in several challenging cross-domain matching tasks: person re-identification under different views and face verification over different modalities (i.e., faces from still images and videos, older and younger faces, and sketch and photo portraits). The experimental results demonstrate superior performance of our model over other state-of-the-art methods. "
732009257793683456,2016-05-16 00:46:21,https://t.co/mJOav7G0S6,Wisdom of Crowds cluster ensemble. (arXiv:1605.04074v1 [stat.ML]) https://t.co/mJOav7G0S6,1,4," Abstract: The Wisdom of Crowds is a phenomenon described in social science that suggests four criteria applicable to groups of people. It is claimed that, if these criteria are satisfied, then the aggregate decisions made by a group will often be better than those of its individual members. Inspired by this concept, we present a novel feedback framework for the cluster ensemble problem, which we call Wisdom of Crowds Cluster Ensemble (WOCCE). Although many conventional cluster ensemble methods focusing on diversity have recently been proposed, WOCCE analyzes the conditions necessary for a crowd to exhibit this collective wisdom. These include decentralization criteria for generating primary results, independence criteria for the base algorithms, and diversity criteria for the ensemble members. We suggest appropriate procedures for evaluating these measures, and propose a new measure to assess the diversity. We evaluate the performance of WOCCE against some other traditional base algorithms as well as state-of-the-art ensemble methods. The results demonstrate the efficiency of WOCCE's aggregate decision-making compared to other algorithms. "
732009254392107008,2016-05-16 00:46:20,https://t.co/j8sUaIgE8R,Barzilai-Borwein Step Size for Stochastic Gradient Descent. (arXiv:1605.04131v1 [math.OC]) https://t.co/j8sUaIgE8R,0,4," Abstract: One of the major issues in stochastic gradient descent (SGD) methods is how to choose an appropriate step size while running the algorithm. Since the traditional line search technique does not apply for stochastic optimization algorithms, the common practice in SGD is either to use a diminishing step size, or to tune a fixed step size by hand, which can be time consuming in practice. In this paper, we propose to use the Barzilai-Borwein (BB) method to automatically compute step sizes for SGD and its variant: stochastic variance reduced gradient (SVRG) method, which leads to two algorithms: SGD-BB and SVRG-BB. We prove that SVRG-BB converges linearly for strongly convex objective functions. As a by-product, we prove the linear convergence result of SVRG with Option I proposed in [10], whose convergence result is missing in the literature. Numerical experiments on standard data sets show that the performance of SGD-BB and SVRG-BB is comparable to and sometimes even better than SGD and SVRG with best-tuned step sizes, and is superior to some advanced SGD variants. "
732009252328472579,2016-05-16 00:46:20,https://t.co/IIpxIc2Ryc,Stochastic Optimization Techniques for Quantification Performance Measures. (arXiv:1605.04135v1 [stat.ML]) https://t.co/IIpxIc2Ryc,0,2," Abstract: The estimation of class prevalence, i.e., the fraction of a population that belongs to a certain class, is a very useful tool in data analytics and learning, and finds applications in many domains such as sentiment analysis, epidemiology, etc. For example, in sentiment analysis, the objective is often not to estimate whether a specific text conveys a positive or a negative sentiment, but rather estimate the overall distribution of positive and negative sentiments during an event window. A popular way of performing the above task, often dubbed quantification, is to use supervised learning to train a prevalence estimator from labeled data. Contemporary literature cites several performance measures used to measure the success of such prevalence estimators. In this paper we propose the first online stochastic algorithms for directly optimizing these quantification-specific performance measures. We also provide algorithms that optimize hybrid performance measures that seek to balance quantification and classification performance. Our algorithms present a significant advancement in the theory of multivariate optimization and we show, by a rigorous theoretical analysis, that they exhibit optimal convergence. We also report extensive experiments on benchmark and real data sets which demonstrate that our methods significantly outperform existing optimization techniques used for these performance measures. "
732009250701053952,2016-05-16 00:46:20,https://t.co/AhZBk8yJcP,ABtree: An Algorithm for Subgroup-Based Treatment Assignment. (arXiv:1605.04262v1 [stat.ML]) https://t.co/AhZBk8yJcP,0,3," Abstract: Given two possible treatments, there may exist subgroups who benefit greater from one treatment than the other. This problem is relevant to the field of marketing, where treatments may correspond to different ways of selling a product. It is similarly relevant to the field of public policy, where treatments may correspond to specific government programs. And finally, personalized medicine is a field wholly devoted to understanding which subgroups of individuals will benefit from particular medical treatments. We present a computationally fast tree-based method, ABtree, for treatment effect differentiation. Unlike other methods, ABtree specifically produces decision rules for optimal treatment assignment on a per-individual basis. The treatment choices are selected for maximizing the overall occurrence of a desired binary outcome, conditional on a set of covariates. In this poster, we present the methodology on tree growth and pruning, and show performance results when applied to simulated data as well as real data. "
732009248964673536,2016-05-16 00:46:19,https://t.co/WZSWnNuhn2,Tensor decomposition with generalized lasso penalties. (arXiv:1502.06930v3 [stat.ME] UPDATED) https://t.co/WZSWnNuhn2,0,7," Abstract: We present an approach for penalized tensor decomposition (PTD) that estimates smoothly varying latent factors in multi-way data. This generalizes existing work on sparse tensor decomposition and penalized matrix decompositions, in a manner parallel to the generalized lasso for regression and smoothing problems. Our approach presents many nontrivial challenges at the intersection of modeling and computation, which are studied in detail. An efficient coordinate-wise optimization algorithm for (PTD) is presented, and its convergence properties are characterized. The method is applied both to simulated data and real data on flu hospitalizations in Texas. These results show that our penalized tensor decomposition can offer major improvements on existing methods for analyzing multi-way data that exhibit smooth spatial or temporal features. "
732009247299514370,2016-05-16 00:46:19,https://t.co/SZZIlfZPjj,High Dimensional Bayesian Optimisation and Bandits via Additive Models. (arXiv:1503.01673v3 [stat.ML] UPDATED) https://t.co/SZZIlfZPjj,0,10," Abstract: Bayesian Optimisation (BO) is a technique used in optimising a $D$-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on $D$ even though the function depends on all $D$ dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive. "
732009246062215168,2016-05-16 00:46:18,https://t.co/24rjTKvUEJ,String and Membrane Gaussian Processes. (arXiv:1507.06977v2 [stat.ML] UPDATED) https://t.co/24rjTKvUEJ,0,4," Abstract: In this paper we introduce a novel framework for making exact nonparametric Bayesian inference on latent functions, that is particularly suitable for Big Data tasks. Firstly, we introduce a class of stochastic processes we refer to as string Gaussian processes (string GPs), which are not to be mistaken for Gaussian processes operating on text. We construct string GPs so that their finite-dimensional marginals exhibit suitable local conditional independence structures, which allow for scalable, distributed, and flexible nonparametric Bayesian inference, without resorting to approximations, and while ensuring some mild global regularity constraints. Furthermore, string GP priors naturally cope with heterogeneous input data, and the gradient of the learned latent function is readily available for explanatory analysis. Secondly, we provide some theoretical results relating our approach to the standard GP paradigm. In particular, we prove that some string GPs are Gaussian processes, which provides a complementary global perspective on our framework. Finally, we derive a scalable and distributed MCMC scheme for supervised learning tasks under string GP priors. The proposed MCMC scheme has computational time complexity $\mathcal{O}(N)$ and memory requirement $\mathcal{O}(dN)$, where $N$ is the data size and $d$ the dimension of the input space. We illustrate the efficacy of the proposed approach on several synthetic and real-world datasets, including a dataset with $6$ millions input points and $8$ attributes. "
732009245110091776,2016-05-16 00:46:18,https://t.co/N0zJ5zlRnG,Unbiased Bayesian Inference for Population Markov Jump Processes via Random Truncations. (arXiv:1509.08327v2 [stat… https://t.co/N0zJ5zlRnG,0,4," Abstract: We consider continuous time Markovian processes where populations of individual agents interact stochastically according to kinetic rules. Despite the increasing prominence of such models in fields ranging from biology to smart cities, Bayesian inference for such systems remains challenging, as these are continuous time, discrete state systems with potentially infinite state-space. Here we propose a novel efficient algorithm for joint state / parameter posterior sampling in population Markov Jump processes. We introduce a class of pseudo-marginal sampling algorithms based on a random truncation method which enables a principled treatment of infinite state spaces. Extensive evaluation on a number of benchmark models shows that this approach achieves considerable savings compared to state of the art methods, retaining accuracy and fast convergence. We also present results on a synthetic biology data set showing the potential for practical usefulness of our work. "
732009244095074304,2016-05-16 00:46:18,https://t.co/wYimkkhD67,Modeling and Estimation of Discrete-Time Reciprocal Processes via Probabilistic Graphical Models. (arXiv:1603.0441… https://t.co/wYimkkhD67,0,4," Abstract: Reciprocal processes are acausal generalizations of Markov processes introduced by Bernstein in 1932. In the literature, a significant amount of attention has been focused on developing dynamical models for reciprocal processes. In this paper, we provide a probabilistic graphical model for reciprocal processes. This leads to a principled solution of the smoothing problem via message passing algorithms. For the finite state space case, convergence analysis is revisited via the Hilbert metric. "
732009241385566209,2016-05-16 00:46:17,https://t.co/GwQpaVieQ6,Fast methods for training Gaussian processes on large data sets. (arXiv:1604.01250v2 [stat.ML] UPDATED) https://t.co/GwQpaVieQ6,0,15," Abstract: Gaussian process regression (GPR) is a non-parametric Bayesian technique for interpolating or fitting data. The main barrier to further uptake of this powerful tool rests in the computational costs associated with the matrices which arise when dealing with large data sets. Here, we derive some simple results which we have found useful for speeding up the learning stage in the GPR algorithm, and especially for performing Bayesian model comparison between different covariance functions. We apply our techniques to both synthetic and real data and quantify the speed-up relative to using nested sampling to numerically evaluate model evidences. "
730919096351625216,2016-05-13 00:34:27,https://t.co/uOcqmRDmyJ,EEF: Exponentially Embedded Families with Class-Specific Features for Classification. (arXiv:1605.03631v1 [stat.ML… https://t.co/uOcqmRDmyJ,1,4," Abstract: In this letter, we present a novel exponentially embedded families (EEF) based classification method, in which the probability density function (PDF) on raw data is estimated from the PDF on features. With the PDF construction, we show that class-specific features can be used in the proposed classification method, instead of a common feature subset for all classes as used in conventional approaches. We apply the proposed EEF classifier for text categorization as a case study and derive an optimal Bayesian classification rule with class-specific feature selection based on the Information Gain (IG) score. The promising performance on real-life data sets demonstrates the effectiveness of the proposed approach and indicates its wide potential applications. "
730919094808154113,2016-05-13 00:34:26,https://t.co/m0ggsQHcJh,Learning Representations for Counterfactual Inference. (arXiv:1605.03661v1 [stat.ML]) https://t.co/m0ggsQHcJh,3,5," Abstract: Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, ""Would this patient have lower blood sugar had she received a different medication?"". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art. "
730919093394649088,2016-05-13 00:34:26,https://t.co/OpTj1Ts6l6,Subspace Perspective on Canonical Correlation Analysis: Dimension Reduction and Minimax Rates. (arXiv:1605.03662v1… https://t.co/OpTj1Ts6l6,0,6," Abstract: Canonical correlation analysis (CCA) is a fundamental statistical tool for exploring the correlation structure between two sets of random variables. In this paper, motivated by recent success of applying CCA to learn low dimensional representations of high dimensional objects, we propose to quantify the estimation loss of CCA by the excess prediction loss defined through a prediction-after-dimension-reduction framework. Such framework suggests viewing CCA estimation as estimating the subspaces spanned by the canonical variates. Interestedly, the proposed error metrics derived from the excess prediction loss turn out to be closely related to the principal angles between the subspaces spanned by the population and sample canonical variates respectively. We characterize the non-asymptotic minimax rates under the proposed metrics, especially the dependency of the minimax rates on the key quantities including the dimensions, the condition number of the covariance matrices, the canonical correlations and the eigen-gap, with minimal assumptions on the joint covariance matrix. To the best of our knowledge, this is the first finite sample result that captures the effect of the canonical correlations on the minimax rates. "
730919091461066752,2016-05-13 00:34:25,https://t.co/nn1F1VDkjA,Tensor Train polynomial models via Riemannian optimization. (arXiv:1605.03795v1 [stat.ML]) https://t.co/nn1F1VDkjA,0,4," Abstract: Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K. "
730919088906764288,2016-05-13 00:34:25,https://t.co/y0U6YcTf16,Detecting Relative Anomaly. (arXiv:1605.03805v1 [stat.ML]) https://t.co/y0U6YcTf16,0,3," Abstract: System states that are anomalous from the perspective of a domain expert occur frequently in some anomaly detection problems. The performance of commonly used unsupervised anomaly detection methods may suffer in that setting, because they use frequency as a proxy for anomaly. We propose a novel concept for anomaly detection, called relative anomaly detection. It is tailored to be robust towards anomalies that occur frequently, by taking into account their location relative to the most typical observations. The approaches we develop are computationally feasible even for large data sets, and they allow real-time detection. We illustrate using data sets of potential scraping attempts and Wi-Fi channel utilization, both from Google, Inc. "
730919086306263041,2016-05-13 00:34:24,https://t.co/SOwLwDjxUI,Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model. (arXiv:1605.03835v1 [cs.CL]) https://t.co/SOwLwDjxUI,0,3," Abstract: Recent advances in conditional recurrent language modelling have mainly focused on network architectures (e.g., attention mechanism), learning algorithms (e.g., scheduled sampling and sequence-level training) and novel applications (e.g., image/video description generation, speech recognition, etc.) On the other hand, we notice that decoding algorithms/strategies have not been investigated as much, and it has become standard to use greedy or beam search. In this paper, we propose a novel decoding strategy motivated by an earlier observation that nonlinear hidden layers of a deep neural network stretch the data manifold. The proposed strategy is embarrassingly parallelizable without any communication overhead, while improving an existing decoding algorithm. We extensively evaluate it with attention-based neural machine translation on the task of En->Cz translation. "
730919084234276865,2016-05-13 00:34:24,https://t.co/t9kWBo8yls,Asymptotic sequential Rademacher complexity of a finite function class. (arXiv:1605.03843v1 [cs.LG]) https://t.co/t9kWBo8yls,0,2," Abstract: For a finite function class we describe the large sample limit of the sequential Rademacher complexity in terms of the viscosity solution of a $G$-heat equation. In the language of Peng's sublinear expectation theory, the same quantity equals to the expected value of the largest order statistics of a multidimensional $G$-normal random variable. We illustrate this result by deriving upper and lower bounds for the asymptotic sequential Rademacher complexity. "
730919082695000064,2016-05-13 00:34:23,https://t.co/WmQ8OQbbzn,Context-dependent feature analysis with random forests. (arXiv:1605.03848v1 [stat.ML]) https://t.co/WmQ8OQbbzn,2,3," Abstract: In many cases, feature selection is often more complicated than identifying a single subset of input variables that would together explain the output. There may be interactions that depend on contextual information, i.e., variables that reveal to be relevant only in some specific circumstances. In this setting, the contribution of this paper is to extend the random forest variable importances framework in order (i) to identify variables whose relevance is context-dependent and (ii) to characterize as precisely as possible the effect of contextual information on these variables. The usage and the relevance of our framework for highlighting context-dependent variables is illustrated on both artificial and real datasets. "
730919080971112448,2016-05-13 00:34:23,https://t.co/CYdES70yI6,An Empirical-Bayes Score for Discrete Bayesian Networks. (arXiv:1605.03884v1 [stat.ML]) https://t.co/CYdES70yI6,0,2," Abstract: Bayesian network structure learning is often performed in a Bayesian setting, by evaluating candidate structures using their posterior probabilities for a given data set. Score-based algorithms then use those posterior probabilities as an objective function and return the maximum a posteriori network as the learned model. For discrete Bayesian networks, the canonical choice for a posterior score is the Bayesian Dirichlet equivalent uniform (BDeu) marginal likelihood with a uniform (U) graph prior (Heckerman et al., 1995). Its favourable theoretical properties descend from assuming a uniform prior both on the space of the network structures and on the space of the parameters of the network. In this paper, we revisit the limitations of these assumptions; and we introduce an alternative set of assumptions and the resulting score: the Bayesian Dirichlet sparse (BDs) empirical Bayes marginal likelihood with a marginal uniform (MU) graph prior. We evaluate its performance in an extensive simulation study, showing that MU+BDs is more accurate than U+BDeu both in learning the structure of the network and in predicting new observations, while not being computationally more complex to estimate. "
730919079100461056,2016-05-13 00:34:22,https://t.co/9tSMeMBNLd,Competitive analysis of the top-K ranking problem. (arXiv:1605.03933v1 [cs.DS]) https://t.co/9tSMeMBNLd,0,3," Abstract: Motivated by applications in recommender systems, web search, social choice and crowdsourcing, we consider the problem of identifying the set of top $K$ items from noisy pairwise comparisons. In our setting, we are non-actively given $r$ pairwise comparisons between each pair of $n$ items, where each comparison has noise constrained by a very general noise model called the strong stochastic transitivity (SST) model. We analyze the competitive ratio of algorithms for the top-$K$ problem. In particular, we present a linear time algorithm for the top-$K$ problem which has a competitive ratio of $\tilde{O}(\sqrt{n})$; i.e. to solve any instance of top-$K$, our algorithm needs at most $\tilde{O}(\sqrt{n})$ times as many samples needed as the best possible algorithm for that instance (in contrast, all previous known algorithms for the top-$K$ problem have competitive ratios of $\tilde{\Omega}(n)$ or worse). We further show that this is tight: any algorithm for the top-$K$ problem has competitive ratio at least $\tilde{\Omega}(\sqrt{n})$. "
730919076810371073,2016-05-13 00:34:22,https://t.co/wTuMlbluwz,Empirical Similarity for Absent Data Generation in Imbalanced Classification. (arXiv:1508.01235v2 [stat.ML] UPDATE… https://t.co/wTuMlbluwz,0,2," Abstract: When the training data in a two-class classification problem is overwhelmed by one class, most classification techniques fail to correctly identify the data points belonging to the underrepresented class. We propose Similarity-based Imbalanced Classification (SBIC) that learns patterns in the training data based on an empirical similarity function. To take the imbalanced structure of the training data into account, SBIC utilizes the concept of absent data, i.e. data from the minority class which can help better find the boundary between the two classes. SBIC simultaneously optimizes the weights of the empirical similarity function and finds the locations of absent data points. As such, SBIC uses an embedded mechanism for synthetic data generation which does not modify the training dataset, but alters the algorithm to suit imbalanced datasets. Therefore, SBIC uses the ideas of both major schools of thoughts in imbalanced classification: Like cost-sensitive approaches SBIC operates on an algorithm level to handle imbalanced structures; and similar to synthetic data generation approaches, it utilizes the properties of unobserved data points from the minority class. The application of SBIC to imbalanced datasets suggests it is comparable to, and in some cases outperforms, other commonly used classification techniques for imbalanced datasets. "
730556652307550208,2016-05-12 00:34:13,https://t.co/sO8COB4HyB,Generalized Sparse Precision Matrix Selection for Fitting Multivariate Gaussian Random Fields to Large Data Sets. … https://t.co/sO8COB4HyB,0,5," Abstract: This paper generalizes the Sparse Precision matrix Selection (SPS) algorithm, proposed by Davanloo et al. (2015) for estimating scalar Gaussian Random Field (GRF) models, to the multivariate, second-order stationary case under a separable covariance function. Theoretical convergence rates for the estimated covariance matrix and for the estimated parameters of the correlation function are established. Numerical simulation results validate our theoretical findings. Data segmentation is used to handle large data sets. "
730556651233771521,2016-05-12 00:34:13,https://t.co/FzQ7xBR4vY,High dimensional thresholded regression and shrinkage effect. (arXiv:1605.03306v1 [stat.ME]) https://t.co/FzQ7xBR4vY,0,2," Abstract: High-dimensional sparse modeling via regularization provides a powerful tool for analyzing large-scale data sets and obtaining meaningful, interpretable models. The use of nonconvex penalty functions shows advantage in selecting important features in high dimensions, but the global optimality of such methods still demands more understanding. In this paper, we consider sparse regression with hard-thresholding penalty, which we show to give rise to thresholded regression. This approach is motivated by its close connection with the $L_0$-regularization, which can be unrealistic to implement in practice but of appealing sampling properties, and its computational advantage. Under some mild regularity conditions allowing possibly exponentially growing dimensionality, we establish the oracle inequalities of the resulting regularized estimator, as the global minimizer, under various prediction and variable selection losses, as well as the oracle risk inequalities of the hard-thresholded estimator followed by a further $L_2$-regularization. The risk properties exhibit interesting shrinkage effects under both estimation and prediction losses. We identify the optimal choice of the ridge parameter, which is shown to have simultaneous advantages to both the $L_2$-loss and prediction loss. These new results and phenomena are evidenced by simulation and real data examples. "
730556650319454208,2016-05-12 00:34:13,https://t.co/n3vo2Dxnin,Asymptotic equivalence of regularization methods in thresholded parameter space. (arXiv:1605.03310v1 [stat.ME]) https://t.co/n3vo2Dxnin,0,3," Abstract: High-dimensional data analysis has motivated a spectrum of regularization methods for variable selection and sparse modeling, with two popular classes of convex ones and concave ones. A long debate has been on whether one class dominates the other, an important question both in theory and to practitioners. In this paper, we characterize the asymptotic equivalence of regularization methods, with general penalty functions, in a thresholded parameter space under the generalized linear model setting, where the dimensionality can grow up to exponentially with the sample size. To assess their performance, we establish the oracle inequalities, as in Bickel, Ritov and Tsybakov (2009), of the global minimizer for these methods under various prediction and variable selection losses. These results reveal an interesting phase transition phenomenon. For polynomially growing dimensionality, the $L_1$-regularization method of Lasso and concave methods are asymptotically equivalent, having the same convergence rates in the oracle inequalities. For exponentially growing dimensionality, concave methods are asymptotically equivalent but have faster convergence rates than the Lasso. We also establish a stronger property of the oracle risk inequalities of the regularization methods, as well as the sampling properties of computable solutions. Our new theoretical results are illustrated and justified by simulation and real data examples. "
730556649467969536,2016-05-12 00:34:12,https://t.co/4OIHIfBlAC,The constrained Dantzig selector with enhanced consistency. (arXiv:1605.03311v1 [stat.ME]) https://t.co/4OIHIfBlAC,0,2," Abstract: The Dantzig selector has received popularity for many applications such as compressed sensing and sparse modeling, thanks to its computational efficiency as a linear programming problem and its nice sampling properties. Existing results show that it can recover sparse signals mimicking the accuracy of the ideal procedure, up to a logarithmic factor of the dimensionality. Such a factor has been shown to hold for many regularization methods. An important question is whether this factor can be reduced to a logarithmic factor of the sample size in ultra-high dimensions under mild regularity conditions. To provide an affirmative answer, in this paper we suggest the constrained Dantzig selector, which has more flexible constraints and parameter space. We prove that the suggested method can achieve convergence rates within a logarithmic factor of the sample size of the oracle rates and improved sparsity, under a fairly weak assumption on the signal strength. Such improvement is significant in ultra-high dimensions. This method can be implemented efficiently through sequential linear programming. Numerical studies confirm that the sample size needed for a certain level of accuracy in these problems can be much reduced. "
730556648473919488,2016-05-12 00:34:12,https://t.co/PWh2X61Igp,Innovated scalable efficient estimation in ultra-large Gaussian graphical models. (arXiv:1605.03313v1 [stat.ME]) https://t.co/PWh2X61Igp,0,3," Abstract: Large-scale precision matrix estimation is of fundamental importance yet challenging in many contemporary applications for recovering Gaussian graphical models. In this paper, we suggest a new approach of innovated scalable efficient estimation (ISEE) for estimating large precision matrix. Motivated by the innovated transformation, we convert the original problem into that of large covariance matrix estimation. The suggested method combines the strengths of recent advances in high-dimensional sparse modeling and large covariance matrix estimation. Compared to existing approaches, our method is scalable and can deal with much larger precision matrices with simple tuning. Under mild regularity conditions, we establish that this procedure can recover the underlying graphical structure with significant probability and provide efficient estimation of link strengths. Both computational and theoretical advantages of the procedure are evidenced through simulation and real data examples. "
730556647509233665,2016-05-12 00:34:12,https://t.co/pZHirA5YOU,Interaction pursuit in high-dimensional multi-response regression via distance correlation. (arXiv:1605.03315v1 [s… https://t.co/pZHirA5YOU,0,2," Abstract: Feature interactions can contribute to a large proportion of variation in many prediction models. In the era of big data, the coexistence of high dimensionality in both responses and covariates poses unprecedented challenges in identifying important interactions. In this paper, we suggest a two-stage interaction identification method, called the interaction pursuit via distance correlation (IPDC), in the setting of high-dimensional multi-response interaction models that exploits feature screening applied to transformed variables with distance correlation followed by feature selection. Such a procedure is computationally efficient, generally applicable beyond the heredity assumption, and effective even when the number of responses diverges with the sample size. Under mild regularity conditions, we show that this method enjoys nice theoretical properties including the sure screening property, support union recovery, and oracle inequalities in prediction and estimation for both interactions and main effects. The advantages of our method are supported by several simulation studies and real data analysis. "
730556646531997696,2016-05-12 00:34:12,https://t.co/odxplaHoBp,Tuning parameter selection in high dimensional penalized likelihood. (arXiv:1605.03321v1 [stat.ME]) https://t.co/odxplaHoBp,0,2," Abstract: Determining how to appropriately select the tuning parameter is essential in penalized likelihood methods for high-dimensional data analysis. We examine this problem in the setting of penalized likelihood methods for generalized linear models, where the dimensionality of covariates p is allowed to increase exponentially with the sample size n. We propose to select the tuning parameter by optimizing the generalized information criterion (GIC) with an appropriate model complexity penalty. To ensure that we consistently identify the true model, a range for the model complexity penalty is identified in GIC. We find that this model complexity penalty should diverge at the rate of some power of $\log p$ depending on the tail probability behavior of the response variables. This reveals that using the AIC or BIC to select the tuning parameter may not be adequate for consistently identifying the true model. Based on our theoretical study, we propose a uniform choice of the model complexity penalty and show that the proposed approach consistently identifies the true model among candidate models with asymptotic probability one. We justify the performance of the proposed procedure by numerical simulations and a gene expression data analysis. "
730556645571497984,2016-05-12 00:34:12,https://t.co/Neheed72XG,Unsupervised Semantic Action Discovery from Video Collections. (arXiv:1605.03324v1 [cs.CV]) https://t.co/Neheed72XG,3,6," Abstract: Human communication takes many forms, including speech, text and instructional videos. It typically has an underlying structure, with a starting point, ending, and certain objective steps between them. In this paper, we consider instructional videos where there are tens of millions of them on the Internet. We propose a method for parsing a video into such semantic steps in an unsupervised way. Our method is capable of providing a semantic ""storyline"" of the video composed of its objective steps. We accomplish this using both visual and language cues in a joint generative model. Our method can also provide a textual description for each of the identified semantic steps and video segments. We evaluate our method on a large number of complex YouTube videos and show that our method discovers semantically correct instructions for a variety of tasks. "
730556644598423554,2016-05-12 00:34:11,https://t.co/2ihJt29bl1,Asymptotic properties for combined $L_1$ and concave regularization. (arXiv:1605.03335v1 [stat.ME]) https://t.co/2ihJt29bl1,0,4," Abstract: Two important goals of high-dimensional modeling are prediction and variable selection. In this article, we consider regularization with combined $L_1$ and concave penalties, and study the sampling properties of the global optimum of the suggested method in ultra-high dimensional settings. The $L_1$-penalty provides the minimum regularization needed for removing noise variables in order to achieve oracle prediction risk, while concave penalty imposes additional regularization to control model sparsity. In the linear model setting, we prove that the global optimum of our method enjoys the same oracle inequalities as the lasso estimator and admits an explicit bound on the false sign rate, which can be asymptotically vanishing. Moreover, we establish oracle risk inequalities for the method and the sampling properties of computable solutions. Numerical studies suggest that our method yields more stable estimates than using a concave penalty alone. "
730556642639630336,2016-05-12 00:34:11,https://t.co/tnnRYbprRX,Active Uncertainty Calibration in Bayesian ODE Solvers. (arXiv:1605.03364v1 [cs.NA]) https://t.co/tnnRYbprRX,0,2," Abstract: There is resurging interest, in statistics and machine learning, in solvers for ordinary differential equations (ODEs) that return probability measures instead of point estimates. Recently, Conrad et al. introduced a sampling-based class of methods that are 'well-calibrated' in a specific sense. But the computational cost of these methods is significantly above that of classic methods. On the other hand, Schober et al. pointed out a precise connection between classic Runge-Kutta ODE solvers and Gaussian filters, which gives only a rough probabilistic calibration, but at negligible cost overhead. By formulating the solution of ODEs as approximate inference in linear Gaussian SDEs, we investigate a range of probabilistic ODE solvers, that bridge the trade-off between computational cost and probabilistic calibration, and identify the inaccurate gradient measurement as the crucial source of uncertainty. We propose the novel filtering-based method Bayesian Quadrature filtering (BQF) which uses Bayesian quadrature to actively learn the imprecision in the gradient measurement by collecting multiple gradient evaluations. "
730556641347833856,2016-05-12 00:34:11,https://t.co/6yZfTONjU5,Random forests for survival analysis using maximally selected rank statistics. (arXiv:1605.03391v1 [stat.ML]) https://t.co/6yZfTONjU5,1,5," Abstract: The most popular approach for analyzing survival data is the Cox regression model. The Cox model may, however, be misspecified, and its proportionality assumption is not always fulfilled. An alternative approach is random forests for survival outcomes. The standard split criterion for random survival forests is the log-rank test statistics, which favors splitting variables with many possible split points. Conditional inference forests avoid this split point selection bias. However, linear rank statistics are utilized in current software for conditional inference forests to select the optimal splitting variable, which cannot detect non-linear effects in the independent variables. We therefore use maximally selected rank statistics for split point selection in random forests for survival analysis. As in conditional inference forests, p-values for association between split points and survival time are minimized. We describe several p-value approximations and the implementation of the proposed random forest approach. A simulation study demonstrates that unbiased split point selection is possible. However, there is a trade-off between unbiased split point selection and runtime. In benchmark studies of prediction performance on simulated and real datasets the new method performs better than random survival forests if informative dichotomous variables are combined with uninformative variables with more categories and better than conditional inference forests if non-linear covariate effects are included. In a runtime comparison the method proves to be computationally faster than both alternatives, if a simple p-value approximation is used. "
730556640261509120,2016-05-12 00:34:10,https://t.co/NnnVP5l8Ig,A constrained L1 minimization approach for estimating multiple Sparse Gaussian or Nonparanormal Graphical Models. … https://t.co/NnnVP5l8Ig,0,3," Abstract: Identifying context-specific entity networks from aggregated data is an important task, arising often in bioinformatics and neuroimaging. Computationally, this task can be formulated as jointly estimating multiple different, but related, sparse Undirected Graphical Models (UGM) from aggregated samples across several contexts. Previous joint-UGM studies have mostly focused on sparse Gaussian Graphical Models (sGGMs) and can't identify context-specific edge patterns directly. We, therefore, propose a novel approach, SIMULE (detecting Shared and Individual parts of MULtiple graphs Explicitly) to learn multi-UGM via a constrained L1 minimization. SIMULE automatically infers both specific edge patterns that are unique to each context and shared interactions preserved among all the contexts. Through the L1 constrained formulation, this problem is cast as multiple independent subtasks of linear programming that can be solved efficiently in parallel. In addition to Gaussian data, SIMULE can also handle multivariate Nonparanormal data that greatly relaxes the normality assumption that many real-world applications do not follow. We provide a novel theoretical proof showing that SIMULE achieves a consistent result at the rate O(log(Kp)/n_{tot}). On multiple synthetic datasets and two biomedical datasets, SIMULE shows significant improvement over state-of-the-art multi-sGGM and single-UGM baselines. "
730556639397482501,2016-05-12 00:34:10,https://t.co/KUsaAPMbNv,Minimizing Finite Sums with the Stochastic Average Gradient. (arXiv:1309.2388v2 [math.OC] UPDATED) https://t.co/KUsaAPMbNv,0,2," Abstract: We propose the stochastic average gradient (SAG) method for optimizing the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method's iteration cost is independent of the number of terms in the sum. However, by incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate than black-box SG methods. The convergence rate is improved from O(1/k^{1/2}) to O(1/k) in general, and when the sum is strongly-convex the convergence rate is improved from the sub-linear O(1/k) to a linear convergence rate of the form O(p^k) for p \textless{} 1. Further, in many cases the convergence rate of the new method is also faster than black-box deterministic gradient methods, in terms of the number of gradient evaluations. Numerical experiments indicate that the new algorithm often dramatically outperforms existing SG and deterministic gradient methods, and that the performance may be further improved through the use of non-uniform sampling strategies. "
730556638348902401,2016-05-12 00:34:10,https://t.co/H8W785HKTi,Collaborative Filtering Bandits. (arXiv:1502.03473v6 [cs.LG] UPDATED) https://t.co/H8W785HKTi,0,4," Abstract: Classical collaborative filtering, and content-based filtering methods try to learn a static recommendation model given training data. These approaches are far from ideal in highly dynamic recommendation domains such as news recommendation and computational advertisement, where the set of items and users is very fluid. In this work, we investigate an adaptive clustering technique for content recommendation based on exploration-exploitation strategies in contextual multi-armed bandit settings. Our algorithm takes into account the collaborative effects that arise due to the interaction of the users with the items, by dynamically grouping users based on the items under consideration and, at the same time, grouping items based on the similarity of the clusterings induced over the users. The resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods. We provide an empirical analysis on medium-size real-world datasets, showing scalability and increased prediction performance (as measured by click-through rate) over state-of-the-art methods for clustering bandits. We also provide a regret analysis within a standard linear stochastic noise setting. "
730556637333884930,2016-05-12 00:34:10,https://t.co/OauN7BQE6y,Linear Shape Deformation Models with Local Support Using Graph-based Structured Matrix Factorisation. (arXiv:1510.… https://t.co/OauN7BQE6y,0,4," Abstract: Representing 3D shape deformations by linear models in high-dimensional space has many applications in computer vision and medical imaging, such as shape-based interpolation or segmentation. Commonly, using Principal Components Analysis a low-dimensional (affine) subspace of the high-dimensional shape space is determined. However, the resulting factors (the most dominant eigenvectors of the covariance matrix) have global support, i.e. changing the coefficient of a single factor deforms the entire shape. In this paper, a method to obtain deformation factors with local support is presented. The benefits of such models include better flexibility and interpretability as well as the possibility of interactively deforming shapes locally. For that, based on a well-grounded theoretical motivation, we formulate a matrix factorisation problem employing sparsity and graph-based regularisation terms. We demonstrate that for brain shapes our method outperforms the state of the art in local support models with respect to generalisation ability and sparse shape reconstruction, whereas for human body shapes our method gives more realistic deformations. "
730556636327231490,2016-05-12 00:34:09,https://t.co/cEBL4DIh1F,Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning. (arXiv:1510.08906v3 [stat.ML] UPDATED) https://t.co/cEBL4DIh1F,0,3," Abstract: Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound $\tilde O(\frac{|\mathcal S|^2 |\mathcal A| H^2}{\epsilon^2} \ln\frac 1 \delta)$ and a lower PAC bound $\tilde \Omega(\frac{|\mathcal S| |\mathcal A| H^2}{\epsilon^2} \ln \frac 1 {\delta + c})$ that match up to log-terms and an additional linear dependency on the number of states $|\mathcal S|$. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein's inequality to improve on previous bounds for episodic finite-horizon MDPs which have a time-horizon dependency of at least $H^3$. "
730556634993446916,2016-05-12 00:34:09,https://t.co/CVhH8VQ2FD,Common Variable Learning and Invariant Representation Learning using Siamese Neural Networks. (arXiv:1512.08806v3 … https://t.co/CVhH8VQ2FD,0,3," Abstract: We consider the statistical problem of learning common source of variability in data which are synchronously captured by multiple sensors, and demonstrate that Siamese neural networks can be naturally applied to this problem. This approach is useful in particular in exploratory, data-driven applications, where neither a model nor label information is available. In recent years, many researchers have successfully applied Siamese neural networks to obtain an embedding of data which corresponds to a ""semantic similarity"". We present an interpretation of this ""semantic similarity"" as learning of equivalence classes. We discuss properties of the embedding obtained by Siamese networks and provide empirical results that demonstrate the ability of Siamese networks to learn common variability. "
730196548735348736,2016-05-11 00:43:18,https://t.co/gicA0KPHbq,Identification of refugee influx patterns in Greece via model-theoretic analysis of daily arrivals. (arXiv:1605.02… https://t.co/gicA0KPHbq,0,3," Abstract: The refugee crisis is perhaps the single most challenging problem for Europe today. Hundreds of thousands of people have already traveled across dangerous sea passages from Turkish shores to Greek islands, resulting in thousands of dead and missing, despite the best rescue efforts from both sides. One of the main reasons is the total lack of any early warning-alerting system, which could provide some preparation time for the prompt and effective deployment of resources at the hot zones. This work is such an attempt for a systemic analysis of the refugee influx in Greece, aiming at (a) the statistical and signal-level characterization of the smuggling networks and (b) the formulation and preliminary assessment of such models for predictive purposes, i.e., as the basis of such an early warning-alerting protocol. To our knowledge, this is the first-ever attempt to design such a system, since this refugee crisis itself and its geographical properties are unique (intense event handling, little or no warning). The analysis employs a wide range of statistical, signal-based and matrix factorization (decomposition) techniques, including linear & linear-cosine regression, spectral analysis, ARMA, SVD, Probabilistic PCA, ICA, K-SVD for Dictionary Learning, as well as fractal dimension analysis. It is established that the behavioral patterns of the smuggling networks closely match (as expected) the regular burst and pause periods of store-and-forward networks in digital communications. There are also major periodic trends in the range of 6.2-6.5 days and strong correlations in lags of four or more days, with distinct preference in the Sunday-Monday 48-hour time frame. These results show that such models can be used successfully for short-term forecasting of the influx intensity, producing an invaluable operational asset for planners, decision-makers and first-responders. "
730196547959443456,2016-05-11 00:43:18,https://t.co/EbCyEFumK6,Decoding Stacked Denoising Autoencoders. (arXiv:1605.02832v1 [cs.LG]) https://t.co/EbCyEFumK6,5,14," Abstract: Data representation in a stacked denoising autoencoder is investigated. Decoding is a simple technique for translating a stacked denoising autoencoder into a composition of denoising autoencoders in the ground space. In the infinitesimal limit, a composition of denoising autoencoders is reduced to a continuous denoising autoencoder, which is rich in analytic properties and geometric interpretation. For example, the continuous denoising autoencoder solves the backward heat equation and transports each data point so as to decrease entropy of the data distribution. Together with ridgelet analysis, an integral representation of a stacked denoising autoencoder is derived. "
730196547103817728,2016-05-11 00:43:17,https://t.co/6K6Z6MHpt6,Modeling Short Over-Dispersed Spike-Train Data: A Hierarchical Parametric Empirical Bayes Framework. (arXiv:1605.0… https://t.co/6K6Z6MHpt6,0,3," Abstract: In this letter, a Hierarchical Parametric Empirical Bayes model is proposed to model spike count data. We have integrated Generalized Linear Models (GLMs) and empirical Bayes theory to simultaneously provide three advantages: (1) a model of over-dispersion of spike count values; (2) reduced MSE in estimation when compared to using the maximum likelihood method for GLMs; and (3) an efficient alternative to inference with fully Bayes estimators. We apply the model to study both simulated data and experimental neural data from the retina. The simulation results indicate that the new model can estimate both the weights of connections among neural populations and the output firing rates (mean spike count) efficiently and accurately. The results from the retinal datasets show that the proposed model outperforms both standard Poisson and Negative Binomial GLMs in terms of the prediction log-likelihood of held-out datasets. "
730196546243923968,2016-05-11 00:43:17,https://t.co/Iy4MusHiTr,Learning theory estimates with observations from general stationary stochastic processes. (arXiv:1605.02887v1 [sta… https://t.co/Iy4MusHiTr,0,3," Abstract: This paper investigates the supervised learning problem with observations drawn from certain general stationary stochastic processes. Here by \emph{general}, we mean that many stationary stochastic processes can be included. We show that when the stochastic processes satisfy a generalized Bernstein-type inequality, a unified treatment on analyzing the learning schemes with various mixing processes can be conducted and a sharp oracle inequality for generic regularized empirical risk minimization schemes can be established. The obtained oracle inequality is then applied to derive convergence rates for several learning schemes such as empirical risk minimization (ERM), least squares support vector machines (LS-SVMs) using given generic kernels, and SVMs using Gaussian kernels for both least squares and quantile regression. It turns out that for i.i.d.~processes, our learning rates for ERM recover the optimal rates. On the other hand, for non-i.i.d.~processes including geometrically $\alpha$-mixing Markov processes, geometrically $\alpha$-mixing processes with restricted decay, $\phi$-mixing processes, and (time-reversed) geometrically $\mathcal{C}$-mixing processes, our learning rates for SVMs with Gaussian kernels match, up to some arbitrarily small extra term in the exponent, the optimal rates. For the remaining cases, our rates are at least close to the optimal rates. As a by-product, the assumed generalized Bernstein-type inequality also provides an interpretation of the so-called ""effective number of observations"" for various mixing processes. "
730196545392525312,2016-05-11 00:43:17,https://t.co/UhPRMoFGMt,An efficient K-means algorithm for Massive Data. (arXiv:1605.02989v1 [stat.ML]) https://t.co/UhPRMoFGMt,0,4," Abstract: Due to the progressive growth of the amount of data available in a wide variety of scientific fields, it has become more difficult to ma- nipulate and analyze such information. Even though datasets have grown in size, the K-means algorithm remains as one of the most popular clustering methods, in spite of its dependency on the initial settings and high computational cost, especially in terms of distance computations. In this work, we propose an efficient approximation to the K-means problem intended for massive data. Our approach recursively partitions the entire dataset into a small number of sub- sets, each of which is characterized by its representative (center of mass) and weight (cardinality), afterwards a weighted version of the K-means algorithm is applied over such local representation, which can drastically reduce the number of distances computed. In addition to some theoretical properties, experimental results indicate that our method outperforms well-known approaches, such as the K-means++ and the minibatch K-means, in terms of the relation between number of distance computations and the quality of the approximation. "
730196544608194561,2016-05-11 00:43:17,https://t.co/oPAzxp4Ige,Destination Prediction by Trajectory Distribution Based Model. (arXiv:1605.03027v1 [stat.ML]) https://t.co/oPAzxp4Ige,0,3," Abstract: In this paper we propose a new method to predict the final destination of vehicle trips based on their initial partial trajectories. We first review how we obtained clustering of trajectories that describes user behaviour. Then, we explain how we model main traffic flow patterns by a mixture of 2d Gaussian distributions. This yielded a density based clustering of locations, which produces a data driven grid of similar points within each pattern. We present how this model can be used to predict the final destination of a new trajectory based on their first locations using a two step procedure: We first assign the new trajectory to the clusters it mot likely belongs. Secondly, we use characteristics from trajectories inside these clusters to predict the final destination. Finally, we present experimental results of our methods for classification of trajectories and final destination prediction on datasets of timestamped GPS-Location of taxi trips. We test our methods on two different datasets, to assess the capacity of our method to adapt automatically to different subsets. "
730196543505113088,2016-05-11 00:43:17,https://t.co/qSOazB06ZY,A note on the statistical view of matrix completion. (arXiv:1605.03040v1 [stat.ML]) https://t.co/qSOazB06ZY,1,5," Abstract: A very simple interpretation of matrix completion problem is introduced based on statistical models. Combined with the well-known results from missing data analysis, such interpretation indicates that matrix completion is still a valid and principled estimation procedure even without the missing completely at random (MCAR) assumption, which almost all of the current theoretical studies of matrix completion assume. "
730196542712381441,2016-05-11 00:43:16,https://t.co/P6U3BGy5vB,Kernel-Based Structural Equation Models for Topology Identification of Directed Networks. (arXiv:1605.03122v1 [sta… https://t.co/P6U3BGy5vB,0,3," Abstract: Structural equation models (SEMs) have been widely adopted for inference of causal interactions in complex networks. Recent examples include unveiling topologies of hidden causal networks over which processes such as spreading diseases, or rumors propagate. The appeal of SEMs in these settings stems from their simplicity and tractability, since they typically assume linear dependencies among observable variables. Acknowledging the limitations inherent to adopting linear models, the present paper advocates nonlinear SEMs, which account for (possible) nonlinear dependencies among network nodes. The advocated approach leverages kernels as a powerful encompassing framework for nonlinear modeling, and an efficient estimator with affordable tradeoffs is put forth. Interestingly, pursuit of the novel kernel-based approach yields a convex regularized estimator that promotes edge sparsity, and is amenable to proximal-splitting optimization methods. To this end, solvers with complementary merits are developed by leveraging the alternating direction method of multipliers, and proximal gradient iterations. Experiments conducted on simulated data demonstrate that the novel approach outperforms linear SEMs with respect to edge detection errors. Furthermore, tests on a real gene expression dataset unveil interesting new edges that were not revealed by linear SEMs, which could shed more light on regulatory behavior of human genes. "
730196541722472448,2016-05-11 00:43:16,https://t.co/Zx1mY49dJj,Communication Lower Bounds for Statistical Estimation Problems via a Distributed Data Processing Inequality. (arXi… https://t.co/Zx1mY49dJj,1,2," Abstract: We study the tradeoff between the statistical error and communication cost of distributed statistical estimation problems in high dimensions. In the distributed sparse Gaussian mean estimation problem, each of the $m$ machines receives $n$ data points from a $d$-dimensional Gaussian distribution with unknown mean $\theta$ which is promised to be $k$-sparse. The machines communicate by message passing and aim to estimate the mean $\theta$. We provide a tight (up to logarithmic factors) tradeoff between the estimation error and the number of bits communicated between the machines. This directly leads to a lower bound for the distributed \textit{sparse linear regression} problem: to achieve the statistical minimax error, the total communication is at least $\Omega(\min\{n,d\}m)$, where $n$ is the number of observations that each machine receives and $d$ is the ambient dimension. These lower results improve upon [Sha14,SD'14] by allowing multi-round iterative communication model. We also give the first optimal simultaneous protocol in the dense case for mean estimation. As our main technique, we prove a \textit{distributed data processing inequality}, as a generalization of usual data processing inequalities, which might be of independent interest and useful for other problems. "
730196540640366592,2016-05-11 00:43:16,https://t.co/zga80f1ocq,Clustering subgaussian mixtures by semidefinite programming. (arXiv:1602.06612v2 [stat.ML] UPDATED) https://t.co/zga80f1ocq,0,3," Abstract: We introduce a model-free relax-and-round algorithm for k-means clustering based on a semidefinite relaxation due to Peng and Wei. The algorithm interprets the SDP output as a denoised version of the original data and then rounds this output to a hard clustering. We provide a generic method for proving performance guarantees for this algorithm, and we analyze the algorithm in the context of subgaussian mixture models. We also study the fundamental limits of estimating Gaussian centers by k-means clustering in order to compare our approximation guarantee to the theoretically optimal k-means clustering solution. "
730196538488700929,2016-05-11 00:43:15,https://t.co/hS5zvv5Jte,A Selection of Giant Radio Sources from NVSS. (arXiv:1603.06895v2 [astro-ph.GA] UPDATED) https://t.co/hS5zvv5Jte,0,2," Abstract: Results of the application of pattern recognition techniques to the problem of identifying Giant Radio Sources (GRS) from the data in the NVSS catalog are presented and issues affecting the process are explored. Decision-tree pattern recognition software was applied to training set source pairs developed from known NVSS large angular size radio galaxies. The full training set consisted of 51,195 source pairs, 48 of which were known GRS for which each lobe was primarily represented by a single catalog component. The source pairs had a maximum separation of 20 arc minutes and a minimum component area of 1.87 square arc minutes at the 1.4 mJy level. The importance of comparing resulting probability distributions of the training and application sets for cases of unknown class ratio is demonstrated. The probability of correctly ranking a randomly selected (GRS, non-GRS) pair from the best of the tested classifiers was determined to be 97.8 +/- 1.5%. The best classifiers were applied to the over 870,000 candidate pairs from the entire catalog. Images of higher ranked sources were visually screened and a table of over sixteen hundred candidates, including morphological annotation, is presented. These systems include doubles and triples, Wide-Angle Tail (WAT) and Narrow-Angle Tail (NAT), S- or Z-shaped systems, and core-jets and resolved cores. While some resolved lobe systems are recovered with this technique, generally it is expected that such systems would require a different approach. "
729852045553979392,2016-05-10 01:54:22,https://t.co/36YCmYpXAR,Distributed Learning with Infinitely Many Hypotheses. (arXiv:1605.02105v1 [math.OC]) https://t.co/36YCmYpXAR,0,6," Abstract: We consider a distributed learning setup where a network of agents sequentially access realizations of a set of random variables with unknown distributions. The network objective is to find a parametrized distribution that best describes their joint observations in the sense of the Kullback-Leibler divergence. Apart from recent efforts in the literature, we analyze the case of countably many hypotheses and the case of a continuum of hypotheses. We provide non-asymptotic bounds for the concentration rate of the agents' beliefs around the correct hypothesis in terms of the number of agents, the network parameters, and the learning abilities of the agents. Additionally, we provide a novel motivation for a general set of distributed Non-Bayesian update rules as instances of the distributed stochastic mirror descent algorithm. "
729852043888824320,2016-05-10 01:54:21,https://t.co/TAvGvGHBms,Likelihood Inflating Sampling Algorithm. (arXiv:1605.02113v1 [stat.ML]) https://t.co/TAvGvGHBms,0,3," Abstract: Markov Chain Monte Carlo (MCMC) sampling from a posterior distribution corresponding to a massive data set can be computationally prohibitive since producing one sample requires a number of operations that is linear in the data size. In this paper, we introduce a new communication-free parallel method, the Likelihood Inflating Sampling Algorithm (LISA), that significantly reduces computational costs by randomly splitting the dataset into smaller subsets and running MCMC methods independently and in parallel on each subset using different processors. Each processor will draw sub-samples from sub-posterior distributions that are defined by ""inflating"" the likelihood function and the sub-samples are then combined using the importance re-sampling method to perform approximate full-data posterior samples. We test our method on several examples including the important case of Bayesian Additive Regression Trees (BART) using both simulated and real datasets. The method we propose shows significant efficiency gains over the existing Consensus Monte Carlo of Scott et al. (2013). "
729852042240462850,2016-05-10 01:54:21,https://t.co/bwo848UHGA,Matching models across abstraction levels with Gaussian Processes. (arXiv:1605.02190v1 [stat.ML]) https://t.co/bwo848UHGA,0,2," Abstract: Biological systems are often modelled at different levels of abstraction depending on the particular aims/resources of a study. Such different models often provide qualitatively concordant predictions over specific parametrisations, but it is generally unclear whether model predictions are quantitatively in agreement, and whether such agreement holds for different parametrisations. Here we present a generally applicable statistical machine learning methodology to automatically reconcile the predictions of different models across abstraction levels. Our approach is based on defining a correction map, a random function which modifies the output of a model in order to match the statistics of the output of a different model of the same system. We use two biological examples to give a proof-of-principle demonstration of the methodology, and discuss its advantages and potential further applications. "
729852040726327296,2016-05-10 01:54:21,https://t.co/oHFoUMYapr,A Bayesian Group Sparse Multi-Task Regression Model for Imaging Genetics. (arXiv:1605.02234v1 [stat.ME]) https://t.co/oHFoUMYapr,0,3," Abstract: Motivation: Recent advances in technology for brain imaging and high-throughput genotyping have motivated studies examining the influence of genetic variation on brain structure. Wang et al. (Bioinformatics, 2012) have developed an approach for the analysis of imaging genomic studies using penalized multi-task regression with regularization based on a novel group $l_{2,1}$-norm penalty which encourages structured sparsity at both the gene level and SNP level. While incorporating a number of useful features, the proposed method only furnishes a point estimate of the regression coefficients; techniques for conducting statistical inference are not provided. A new Bayesian method is proposed here to overcome this limitation. Results: We develop a Bayesian hierarchical modeling formulation where the posterior mode corresponds to the estimator proposed by Wang et al. (Bioinformatics, 2012), and an approach that allows for full posterior inference including the construction of interval estimates for the regression parameters. We show that the proposed hierarchical model can be expressed as a three-level Gaussian scale mixture and this representation facilitates the use of a Gibbs sampling algorithm for posterior simulation. Simulation studies demonstrate that the interval estimates obtained using our approach achieve adequate coverage probabilities that outperform those obtained from the nonparametric bootstrap. Our proposed methodology is applied to the analysis of neuroimaging and genetic data collected as part of the Alzheimer's Disease Neuroimaging Initiative (ADNI), and this analysis of the ADNI cohort demonstrates clearly the value added of incorporating interval estimation beyond only point estimation when relating SNPs to brain imaging endophenotypes. "
729852038574706688,2016-05-10 01:54:20,https://t.co/2WlFsLWeEM,Rate-Distortion Bounds on Bayes Risk in Supervised Learning. (arXiv:1605.02268v1 [cs.IT]) https://t.co/2WlFsLWeEM,0,3," Abstract: An information-theoretic framework is presented for estimating the number of labeled samples needed to train a classifier in a parametric Bayesian setting. Ideas from rate-distortion theory are used to derive bounds on the average $L_1$ or $L_\infty$ distance between the learned classifier and the true maximum a posteriori classifier---which are well-established surrogates for the excess classification error due to imperfect learning---in terms of the differential entropy of the posterior distribution, the Fisher information of the parametric family, and the number of training samples available. The maximum {\em a posteriori} classifier is viewed as a random source, labeled training data are viewed as a finite-rate encoding of the source, and the $L_1$ or $L_\infty$ Bayes risk is viewed as the average distortion. The result is a complementary framework to the well-known probably approximately correct (PAC) framework. PAC bounds characterize worst-case learning performance of a family of classifiers whose complexity is captured by the Vapnik-Chervonenkis (VC) dimension. The rate-distortion framework, on the other hand, characterizes the average-case performance of a family of data distributions in terms of a quantity called the interpolation dimension, which represents the complexity of the family of data distributions. The resulting bounds do not suffer from the pessimism typical of the PAC framework, particularly when the training set is small. The framework also naturally accommodates multi-class settings. Furthermore, Monte Carlo methods provide accurate estimates of the bounds even for complicated distributions. The effectiveness of this framework is demonstrated in both a binary and multi-class Gaussian setting. "
729852036414615552,2016-05-10 01:54:20,https://t.co/Jf7gEQYSj5,On-Average KL-Privacy and its equivalence to Generalization for Max-Entropy Mechanisms. (arXiv:1605.02277v1 [stat.… https://t.co/Jf7gEQYSj5,0,2," Abstract: We define On-Average KL-Privacy and present its properties and connections to differential privacy, generalization and information-theoretic quantities including max-information and mutual information. The new definition significantly weakens differential privacy, while preserving its minimalistic design features such as composition over small group and multiple queries as well as closeness to post-processing. Moreover, we show that On-Average KL-Privacy is **equivalent** to generalization for a large class of commonly-used tools in statistics and machine learning that samples from Gibbs distributions---a class of distributions that arises naturally from the maximum entropy principle. In addition, a byproduct of our analysis yields a lower bound for generalization error in terms of mutual information which reveals an interesting interplay with known upper bounds that use the same quantity. "
729852034451648512,2016-05-10 01:54:19,https://t.co/hcjIexAzzE,Information Recovery in Shuffled Graphs via Graph Matching. (arXiv:1605.02315v1 [stat.ML]) https://t.co/hcjIexAzzE,0,2," Abstract: In a number of methodologies for joint inference across graphs, it is assumed that an explicit vertex correspondence is a priori known across the vertex sets of the graphs. While this assumption is often reasonable, in practice these correspondences may be unobserved and/or errorfully observed, and graph matching---aligning a pair of graphs to minimize their edge disagreements---is used to align the graphs before performing subsequent inference. Herein, we explore the duality between the loss of mutual information due to an errorfully observed vertex correspondence and the ability of graph matching algorithms to recover the true correspondence across graphs. We then demonstrate the practical effect that graph shuffling---and matching---can have on subsequent inference, with examples from two sample graph hypothesis testing and joint graph clustering. "
729852032593612800,2016-05-10 01:54:19,https://t.co/zvoQyU2vhx,Structured Nonconvex and Nonsmooth Optimization: Algorithms and Iteration Complexity Analysis. (arXiv:1605.02408v1… https://t.co/zvoQyU2vhx,0,6," Abstract: Nonconvex optimization problems are frequently encountered in much of statistics, business, science and engineering, but they are not yet widely recognized as a technology. A reason for this relatively low degree of popularity is the lack of a well developed system of theory and algorithms to support the applications, as is the case for its convex counterpart. This paper aims to take one step in the direction of disciplined nonconvex optimization. In particular, we consider in this paper some constrained nonconvex optimization models in block decision variables, with or without coupled affine constraints. In the case of no coupled constraints, we show a sublinear rate of convergence to an $\epsilon$-stationary solution in the form of variational inequality for a generalized conditional gradient method, where the convergence rate is shown to be dependent on the H\""olderian continuity of the gradient of the smooth part of the objective. For the model with coupled affine constraints, we introduce corresponding $\epsilon$-stationarity conditions, and propose two proximal-type variants of the ADMM to solve such a model, assuming the proximal ADMM updates can be implemented for all the block variables except for the last block, for which either a gradient step or a majorization-minimization step is implemented. We show an iteration complexity bound of $O(1/\epsilon^2)$ to reach an $\epsilon$-stationary solution for both algorithms. Moreover, we show that the same iteration complexity of a proximal BCD method follows immediately. Numerical results are provided to illustrate the efficacy of the proposed algorithms for tensor robust PCA. "
729852031318503425,2016-05-10 01:54:18,https://t.co/Tw3hKYQJWM,Randomized Kaczmarz for Rank Aggregation from Pairwise Comparisons. (arXiv:1605.02470v1 [cs.LG]) https://t.co/Tw3hKYQJWM,0,3," Abstract: We revisit the problem of inferring the overall ranking among entities in the framework of Bradley-Terry-Luce (BTL) model, based on available empirical data on pairwise preferences. By a simple transformation, we can cast the problem as that of solving a noisy linear system, for which a ready algorithm is available in the form of the randomized Kaczmarz method. This scheme is provably convergent, has excellent empirical performance, and is amenable to on-line, distributed and asynchronous variants. Convergence, convergence rate, and error analysis of the proposed algorithm are presented and several numerical experiments are conducted whose results validate our theoretical findings. "
729852029682724865,2016-05-10 01:54:18,https://t.co/ejIF4XsjAz,Clustering Time Series and the Surprising Robustness of HMMs. (arXiv:1605.02531v1 [cs.IT]) https://t.co/ejIF4XsjAz,1,5," Abstract: Suppose that we are given a time series where consecutive samples are believed to come from a probabilistic source, that the source changes from time to time and that the total number of sources is fixed. Our objective is to estimate the distributions of the sources. A standard approach to this problem is to model the data as a hidden Markov model (HMM). However, since the data often lacks the Markov or the stationarity properties of an HMM, one can ask whether this approach is still suitable or perhaps another approach is required. In this paper we show that a maximum likelihood HMM estimator can be used to approximate the source distributions in a much larger class of models than HMMs. Specifically, we propose a natural and fairly general non-stationary model of the data, where the only restriction is that the sources do not change too often. Our main result shows that for this model, a maximum-likelihood HMM estimator produces the correct second moment of the data, and the results can be extended to higher moments. "
729852027841433601,2016-05-10 01:54:18,https://t.co/VEYt4ShSlf,Exact ICL maximization in a non-stationary temporal extension of the stochastic block model for dynamic networks. … https://t.co/VEYt4ShSlf,0,3," Abstract: The stochastic block model (SBM) is a flexible probabilistic tool that can be used to model interactions between clusters of nodes in a network. However, it does not account for interactions of time varying intensity between clusters. The extension of the SBM developed in this paper addresses this shortcoming through a temporal partition: assuming interactions between nodes are recorded on fixed-length time intervals, the inference procedure associated with the model we propose allows to cluster simultaneously the nodes of the network and the time intervals. The number of clusters of nodes and of time intervals, as well as the memberships to clusters, are obtained by maximizing an exact integrated complete-data likelihood, relying on a greedy search approach. Experiments on simulated and real data are carried out in order to assess the proposed methodology. "
729852026381840384,2016-05-10 01:54:17,https://t.co/n2iaQfaca3,Mean Absolute Percentage Error for regression models. (arXiv:1605.02541v1 [stat.ML]) https://t.co/n2iaQfaca3,0,2," Abstract: We study in this paper the consequences of using the Mean Absolute Percentage Error (MAPE) as a measure of quality for regression models. We prove the existence of an optimal MAPE model and we show the universal consistency of Empirical Risk Minimization based on the MAPE. We also show that finding the best model under the MAPE is equivalent to doing weighted Mean Absolute Error (MAE) regression, and we apply this weighting strategy to kernel regression. The behavior of the MAPE kernel regression is illustrated on simulated data. "
729852024536305664,2016-05-10 01:54:17,https://t.co/xGFBFxqUNM,Dynamic Decomposition of Spatiotemporal Neural Signals. (arXiv:1605.02609v1 [q-bio.NC]) https://t.co/xGFBFxqUNM,0,3," Abstract: Neural signals are characterized by rich temporal and spatiotemporal dynamics that reflect the organization of cortical networks. Theoretical research has shown how neural networks can operate at different dynamic ranges that correspond to specific types of information processing. Here we present a data analysis framework that uses a linearized model of these dynamic states in order to decompose the measured neural signal into a series of components that capture both rhythmic and non-rhythmic neural activity. The method is based on stochastic differential equations and Gaussian process regression. Through computer simulations and analysis of magnetoencephalographic data, we demonstrate the efficacy of the method in identifying meaningful modulations of oscillatory signals corrupted by structured temporal and spatiotemporal noise. These results suggest that the method is particularly suitable for the analysis and interpretation of complex temporal and spatiotemporal neural signals. "
729852022820876288,2016-05-10 01:54:16,https://t.co/P2CkVD50c2,Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering. (arXiv:1605.02633v1 [cs.LG]) https://t.co/P2CkVD50c2,0,2," Abstract: State-of-the-art subspace clustering methods are based on expressing each data point as a linear combination of other data points while regularizing the matrix of coefficients with $\ell_1$, $\ell_2$ or nuclear norms. $\ell_1$ regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad theoretical conditions, but the clusters may not be connected. $\ell_2$ and nuclear norm regularization often improve connectivity, but give a subspace-preserving affinity only for independent subspaces. Mixed $\ell_1$, $\ell_2$ and nuclear norm regularizations offer a balance between the subspace-preserving and connectedness properties, but this comes at the cost of increased computational complexity. This paper studies the geometry of the elastic net regularizer (a mixture of the $\ell_1$ and $\ell_2$ norms) and uses it to derive a provably correct and scalable active set method for finding the optimal coefficients. Our geometric analysis also provides a theoretical justification and a geometric interpretation for the balance between the connectedness (due to $\ell_2$ regularization) and subspace-preserving (due to $\ell_1$ regularization) properties for elastic net subspace clustering. Our experiments show that the proposed active set method not only achieves state-of-the-art clustering performance, but also efficiently handles large-scale datasets. "
729852020761493506,2016-05-10 01:54:16,https://t.co/Hkd6T63ACL,Why (and How) Avoid Orthogonal Procrustes in Regularized Multivariate Analysis. (arXiv:1605.02674v1 [stat.ML]) https://t.co/Hkd6T63ACL,0,4," Abstract: Multivariate Analysis (MVA) comprises a family of well-known methods for feature extraction that exploit correlations among input variables of the data representation. One important property that is enjoyed by most such methods is uncorrelation among the extracted features. Recently, regularized versions of MVA methods have appeared in the literature, mainly with the goal to gain interpretability of the solution. In these cases, the solutions can no longer be obtained in a closed manner, and it is frequent to recur to the iteration of two steps, one of them being an orthogonal Procrustes problem. This letter shows that the Procrustes solution is not optimal from the perspective of the overall MVA method, and proposes an alternative approach based on the solution of an eigenvalue problem. Our method ensures the preservation of several properties of the original methods, most notably the uncorrelation of the extracted features, as demonstrated theoretically and through a collection of selected experiments. "
729852018890711041,2016-05-10 01:54:15,https://t.co/LsRF2rNBRn,Inference of High-dimensional Autoregressive Generalized Linear Models. (arXiv:1605.02693v1 [stat.ML]) https://t.co/LsRF2rNBRn,0,3," Abstract: Vector autoregressive models characterize a variety of time series in which linear combinations of current and past observations can be used to accurately predict future observations. For instance, each element of an observation vector could correspond to a different node in a network, and the parameters of an autoregressive model would correspond to the impact of the network structure on the time series evolution. Often these models are used successfully in practice to learn the structure of social, epidemiological, financial, or biological neural networks. However, little is known about statistical guarantees of estimates of such models in non-Gaussian settings. This paper addresses the inference of the autoregressive parameters and associated network structure within a generalized linear model framework that includes Poisson and Bernoulli autoregressive processes. At the heart of this analysis is a sparsity-regularized maximum likelihood estimator. While sparsity-regularization is well-studied in the statistics and machine learning communities, those analysis methods cannot be applied to autoregressive generalized linear models because of the correlations and potential heteroscedasticity inherent in the observations. Sample complexity bounds are derived using a combination of martingale concentration inequalities and modified covering techniques originally proposed for high-dimensional linear regression analysis. These bounds, which are supported by several simulation studies, characterize the impact of various network parameters on estimator performance. "
729852016466526210,2016-05-10 01:54:15,https://t.co/BOiPA4EBTD,A Theoretical Analysis of Deep Neural Networks for Texture Classification. (arXiv:1605.02699v1 [cs.CV]) https://t.co/BOiPA4EBTD,1,4," Abstract: We investigate the use of Deep Neural Networks for the classification of image datasets where texture features are important for generating class-conditional discriminative representations. To this end, we first derive the size of the feature space for some standard textural features extracted from the input dataset and then use the theory of Vapnik-Chervonenkis dimension to show that hand-crafted feature extraction creates low-dimensional representations which help in reducing the overall excess error rate. As a corollary to this analysis, we derive for the first time upper bounds on the VC dimension of Convolutional Neural Network as well as Dropout and Dropconnect networks and the relation between excess error rate of Dropout and Dropconnect networks. The concept of intrinsic dimension is used to validate the intuition that texture-based datasets are inherently higher dimensional as compared to handwritten digits or other object recognition datasets and hence more difficult to be shattered by neural networks. We then derive the mean distance from the centroid to the nearest and farthest sampling points in an n-dimensional manifold and show that the Relative Contrast of the sample data vanishes as dimensionality of the underlying vector space tends to infinity. "
729852015325646848,2016-05-10 01:54:15,https://t.co/zXktjfkiUY,Stochastic Variance Reduced Optimization for Nonconvex Sparse Learning. (arXiv:1605.02711v1 [cs.LG]) https://t.co/zXktjfkiUY,0,4," Abstract: We propose a stochastic variance reduced optimization algorithm for solving sparse learning problems with cardinality constraints. Sufficient conditions are provided, under which the proposed algorithm enjoys strong linear convergence guarantees and optimal estimation accuracy in high dimensions. We further extend the proposed algorithm to an asynchronous parallel variant with a near linear speedup. Numerical experiments demonstrate the efficiency of our algorithm in terms of both parameter estimation and computational performance. "
729852013509480448,2016-05-10 01:54:14,https://t.co/XIfsl8RYh2,Efficient Reinforcement Learning in Deterministic Systems with Value Function Generalization. (arXiv:1307.4847v3 [… https://t.co/XIfsl8RYh2,0,2," Abstract: We consider the problem of reinforcement learning over episodes of a finite-horizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efficient exploration and value function generalization. We establish that when the true value function lies within a given hypothesis class, OCP selects optimal actions over all but at most K episodes, where K is the eluder dimension of the given hypothesis class. We establish further efficiency and asymptotic performance guarantees that apply even if the true value function does not lie in the given hypothesis class, for the special case where the hypothesis class is the span of pre-specified indicator functions over disjoint sets. We also discuss the computational complexity of OCP and present computational results involving two illustrative examples. "
729852012037345280,2016-05-10 01:54:14,https://t.co/YW93HwRe3g,Recurrent Neural Network Training with Dark Knowledge Transfer. (arXiv:1505.04630v5 [stat.ML] UPDATED) https://t.co/YW93HwRe3g,2,8," Abstract: Recurrent neural networks (RNNs), particularly long short-term memory (LSTM), have gained much attention in automatic speech recognition (ASR). Although some successful stories have been reported, training RNNs remains highly challenging, especially with limited training data. Recent research found that a well-trained model can be used as a teacher to train other child models, by using the predictions generated by the teacher model as supervision. This knowledge transfer learning has been employed to train simple neural nets with a complex one, so that the final performance can reach a level that is infeasible to obtain by regular training. In this paper, we employ the knowledge transfer learning approach to train RNNs (precisely LSTM) using a deep neural network (DNN) model as the teacher. This is different from most of the existing research on knowledge transfer learning, since the teacher (DNN) is assumed to be weaker than the child (RNN); however, our experiments on an ASR task showed that it works fairly well: without applying any tricks on the learning scheme, this approach can train RNNs successfully even with limited training data. "
729852010187640832,2016-05-10 01:54:13,https://t.co/QObK9TKmpF,$M$-Statistic for Kernel Change-Point Detection. (arXiv:1507.01279v3 [cs.LG] UPDATED) https://t.co/QObK9TKmpF,0,3," Abstract: Detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning. Kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach. However, none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic. Such characterization is crucial for setting the detection threshold, to control the significance level in the offline case as well as the false alarm rate (captured by the average run length) in the online case. In this paper we focus on the scenario when the amount of background data is large, and propose two related computationally efficient kernel-based statistics for change-point detection, which we call ""$M$-statistics"". A novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-of-measure. Such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner, without the need to resort to the more expensive simulations such as bootstrapping. Moreover, our $M$-statistic can be applied to high-dimensional data by choosing a proper kernel. We show that our methods perform well in both synthetic and real world data. "
729852008820256770,2016-05-10 01:54:13,https://t.co/pfY8eZLVAG,Fast Discrete Distribution Clustering Using Wasserstein Barycenter with Sparse Support. (arXiv:1510.00012v2 [stat.… https://t.co/pfY8eZLVAG,0,2," Abstract: In a variety of research areas, the weighted bag of vectors and the histogram are widely used descriptors for complex objects. Both can be expressed as discrete distributions. D2-clustering pursues the minimum total within-cluster variation for a set of discrete distributions subject to the Kantorovich-Wasserstein metric. D2-clustering has a severe scalability issue, the bottleneck being the computation of a centroid distribution, called Wasserstein barycenter, that minimizes its sum of squared distances to the cluster members. In this paper, we develop a modified Bregman ADMM approach for computing the approximate discrete Wasserstein barycenter of large clusters. In the case when the support points of the barycenters are unknown and of low cardinality, our method achieves high accuracy empirically at a much reduced computational cost. The strengths and weaknesses of our method and its alternatives are examined through experiments; and scenarios for their respective usage are recommended. Moreover, we develop both serial and parallelized versions of the algorithm. By experimenting with large-scale data, we demonstrate the computational efficiency of the new methods and investigate their convergence properties and numerical stability. The clustering results obtained on several datasets in different domains are highly competitive in comparison with some widely used methods' in the corresponding areas. "
729836873632370688,2016-05-10 00:54:05,https://t.co/NjPpG5O82u,The Power of Depth for Feedforward Neural Networks. (arXiv:1512.03965v4 [cs.LG] UPDATED) https://t.co/NjPpG5O82u,0,12," Abstract: We show that there is a simple (approximately radial) function on $\reals^d$, expressible by a small 3-layer feedforward neural networks, which cannot be approximated by any 2-layer network, to more than a certain constant accuracy, unless its width is exponential in the dimension. The result holds for virtually all known activation functions, including rectified linear units, sigmoids and thresholds, and formally demonstrates that depth -- even if increased by 1 -- can be exponentially more valuable than width for standard feedforward neural networks. Moreover, compared to related results in the context of Boolean functions, our result requires fewer assumptions, and the proof techniques and construction are very different. "
729836872323747840,2016-05-10 00:54:04,https://t.co/NUPDNg8qfo,Learning the kernel matrix via predictive low-rank approximations. (arXiv:1601.04366v2 [cs.LG] UPDATED) https://t.co/NUPDNg8qfo,1,5," Abstract: Efficient and accurate low-rank approximations of multiple data sources are essential in the era of big data. The scaling of kernel-based learning algorithms to large datasets is limited by the O(n^2) computation and storage complexity of the full kernel matrix, which is required by most of the recent kernel learning algorithms. We present the Mklaren algorithm to approximate multiple kernel matrices learn a regression model, which is entirely based on geometrical concepts. The algorithm does not require access to full kernel matrices yet it accounts for the correlations between all kernels. It uses Incomplete Cholesky decomposition, where pivot selection is based on least-angle regression in the combined, low-dimensional feature space. The algorithm has linear complexity in the number of data points and kernels. When explicit feature space induced by the kernel can be constructed, a mapping from the dual to the primal Ridge regression weights is used for model interpretation. The Mklaren algorithm was tested on eight standard regression datasets. It outperforms contemporary kernel matrix approximation approaches when learning with multiple kernels. It identifies relevant kernels, achieving highest explained variance than other multiple kernel learning methods for the same number of iterations. Test accuracy, equivalent to the one using full kernel matrices, was achieved with at significantly lower approximation ranks. A difference in run times of two orders of magnitude was observed when either the number of samples or kernels exceeds 3000. "
729836871346470914,2016-05-10 00:54:04,https://t.co/ODf3srMEeF,Building Machines That Learn and Think Like People. (arXiv:1604.00289v2 [cs.AI] UPDATED) https://t.co/ODf3srMEeF,1,5," Abstract: Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models. "
729836870444720130,2016-05-10 00:54:04,https://t.co/pnhF2G9uus,Triplet Probabilistic Embedding for Face Verification and Clustering. (arXiv:1604.05417v2 [cs.CV] UPDATED) https://t.co/pnhF2G9uus,0,3," Abstract: Despite significant progress made over the past twenty five years, unconstrained face verification remains a challenging problem. This paper proposes an approach that couples a deep CNN-based approach with a low-dimensional discriminative embedding learned using triplet probability constraints to solve the unconstrained face verification problem. Aside from yielding performance improvements, this embedding provides significant advantages in terms of memory and for post-processing operations like subject specific clustering. Experiments on the challenging IJB-A dataset show that the proposed algorithm performs comparably or better than the state of the art methods in verification and identification metrics, while requiring much less training data and training time. The superior performance of the proposed method on the CFP dataset shows that the representation learned by our deep CNN is robust to extreme pose variation. Furthermore, we demonstrate the robustness of the deep features to challenges including age, pose, blur and clutter by performing simple clustering experiments on both IJB-A and LFW datasets. "
729471225001578496,2016-05-09 00:41:07,https://t.co/RAAK8gVnyd,Rank Ordered Autoencoders. (arXiv:1605.01749v1 [cs.LG]) https://t.co/RAAK8gVnyd,0,3," Abstract: A new method for the unsupervised learning of sparse representations using autoencoders is proposed and implemented by ordering the output of the hidden units by their activation value and progressively reconstructing the input in this order. This can be done efficiently in parallel with the use of cumulative sums and sorting only slightly increasing the computational costs. Minimizing the difference of this progressive reconstruction with respect to the input can be seen as minimizing the number of active output units required for the reconstruction of the input. The model thus learns to reconstruct optimally using the least number of active output units. This leads to high sparsity without the need for extra hyperparameters, the amount of sparsity is instead implicitly learned by minimizing this progressive reconstruction error. Results of the trained model are given for patches of the CIFAR10 dataset, showing rapid convergence of features and extremely sparse output activations while maintaining a minimal reconstruction error and showing extreme robustness to overfitting. Additionally the reconstruction as function of number of active units is presented which shows the autoencoder learns a rank order code over the input where the highest ranked units correspond to the highest decrease in reconstruction error. "
729471224099811328,2016-05-09 00:41:07,https://t.co/W5WlB5fQfL,Clustering on the Edge: Learning Structure in Graphs. (arXiv:1605.01779v1 [stat.ML]) https://t.co/W5WlB5fQfL,0,8," Abstract: With the recent popularity of graphical clustering methods, there has been an increased focus on the information between samples. We show how learning cluster structure using edge features naturally and simultaneously determines the most likely number of clusters and addresses data scale issues. These results are particularly useful in instances where (a) there are a large number of clusters and (b) we have some labeled edges. Applications in this domain include image segmentation, community discovery and entity resolution. Our model is an extension of the planted partition model and our solution uses results of correlation clustering, which achieves a partition O(log(n))-close to the log-likelihood of the true clustering. "
729471223076376576,2016-05-09 00:41:07,https://t.co/NswXiw4pbq,Energy Disaggregation for Real-Time Building Flexibility Detection. (arXiv:1605.01939v1 [stat.ML]) https://t.co/NswXiw4pbq,0,2," Abstract: Energy is a limited resource which has to be managed wisely, taking into account both supply-demand matching and capacity constraints in the distribution grid. One aspect of the smart energy management at the building level is given by the problem of real-time detection of flexible demand available. In this paper we propose the use of energy disaggregation techniques to perform this task. Firstly, we investigate the use of existing classification methods to perform energy disaggregation. A comparison is performed between four classifiers, namely Naive Bayes, k-Nearest Neighbors, Support Vector Machine and AdaBoost. Secondly, we propose the use of Restricted Boltzmann Machine to automatically perform feature extraction. The extracted features are then used as inputs to the four classifiers and consequently shown to improve their accuracy. The efficiency of our approach is demonstrated on a real database consisting of detailed appliance-level measurements with high temporal resolution, which has been used for energy disaggregation in previous studies, namely the REDD. The results show robustness and good generalization capabilities to newly presented buildings with at least 96% accuracy. "
729471222141038592,2016-05-09 00:41:06,https://t.co/kb7srxSGXr,Weighted SGD for $\ell_p$ Regression with Randomized Preconditioning. (arXiv:1502.03571v3 [math.OC] UPDATED) https://t.co/kb7srxSGXr,0,3," Abstract: In recent years, stochastic gradient descent (SGD) methods and randomized linear algebra (RLA) algorithms have been applied to many large-scale problems in machine learning and data analysis. We aim to bridge the gap between these two methods in solving constrained overdetermined linear regression problems---e.g., $\ell_2$ and $\ell_1$ regression problems. We propose a hybrid algorithm named pwSGD that uses RLA techniques for preconditioning and constructing an importance sampling distribution, and then performs an SGD-like iterative process with weighted sampling on the preconditioned system. We prove that pwSGD inherits faster convergence rates that only depend on the lower dimension of the linear system, while maintaining low computation complexity. Particularly, when solving $\ell_1$ regression with size $n$ by $d$, pwSGD returns an approximate solution with $\epsilon$ relative error in the objective value in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d)/\epsilon^2)$ time. This complexity is uniformly better than that of RLA methods in terms of both $\epsilon$ and $d$ when the problem is unconstrained. For $\ell_2$ regression, pwSGD returns an approximate solution with $\epsilon$ relative error in the objective value and the solution vector measured in prediction norm in $\mathcal{O}(\log n \cdot \text{nnz}(A) + \text{poly}(d) \log(1/\epsilon) /\epsilon)$ time. We also provide lower bounds on the coreset complexity for more general regression problems, indicating that still new ideas will be needed to extend similar RLA preconditioning ideas to weighted SGD algorithms for more general regression problems. Finally, the effectiveness of such algorithms is illustrated numerically on both synthetic and real datasets. "
729471221142818816,2016-05-09 00:41:06,https://t.co/FAQTRRs0Kz,Interpretable Classification Models for Recidivism Prediction. (arXiv:1503.07810v4 [stat.ML] UPDATED) https://t.co/FAQTRRs0Kz,0,3," Abstract: We investigate a long-debated question, which is how to create predictive models of recidivism that are sufficiently accurate, transparent, and interpretable to use for decision-making. This question is complicated as these models are used to support different decisions, from sentencing, to determining release on probation, to allocating preventative social services. Each use case might have an objective other than classification accuracy, such as a desired true positive rate (TPR) or false positive rate (FPR). Each (TPR, FPR) pair is a point on the receiver operator characteristic (ROC) curve. We use popular machine learning methods to create models along the full ROC curve on a wide range of recidivism prediction problems. We show that many methods (SVM, Ridge Regression) produce equally accurate models along the full ROC curve. However, methods that designed for interpretability (CART, C5.0) cannot be tuned to produce models that are accurate and/or interpretable. To handle this shortcoming, we use a new method known as SLIM (Supersparse Linear Integer Models) to produce accurate, transparent, and interpretable models along the full ROC curve. These models can be used for decision-making for many different use cases, since they are just as accurate as the most powerful black-box machine learning models, but completely transparent, and highly interpretable. "
729471220182290432,2016-05-09 00:41:06,https://t.co/aO7mH0tZLN,Information Recovery from Pairwise Measurements. (arXiv:1504.01369v4 [cs.IT] UPDATED) https://t.co/aO7mH0tZLN,0,5," Abstract: This paper is concerned with jointly recovering $n$ node-variables $\left\{ x_{i}\right\}_{1\leq i\leq n}$ from a collection of pairwise difference measurements. Imagine we acquire a few observations taking the form of $x_{i}-x_{j}$; the observation pattern is represented by a measurement graph $\mathcal{G}$ with an edge set $\mathcal{E}$ such that $x_{i}-x_{j}$ is observed if and only if $(i,j)\in\mathcal{E}$. To account for noisy measurements in a general manner, we model the data acquisition process by a set of channels with given input/output transition measures. Employing information-theoretic tools applied to channel decoding problems, we develop a \emph{unified} framework to characterize the fundamental recovery criterion, which accommodates general graph structures, alphabet sizes, and channel transition measures. In particular, our results isolate a family of \emph{minimum} \emph{channel divergence measures} to characterize the degree of measurement corruption, which together with the size of the minimum cut of $\mathcal{G}$ dictates the feasibility of exact information recovery. For various homogeneous graphs, the recovery condition depends almost only on the edge sparsity of the measurement graph irrespective of other graphical metrics; alternatively, the minimum sample complexity required for these graphs scales like \[ \text{minimum sample complexity }\asymp\frac{n\log n}{\mathsf{Hel}_{1/2}^{\min}} \] for certain information metric $\mathsf{Hel}_{1/2}^{\min}$ defined in the main text, as long as the alphabet size is not super-polynomial in $n$. We apply our general theory to three concrete applications, including the stochastic block model, the outlier model, and the haplotype assembly problem. Our theory leads to order-wise tight recovery conditions for all these scenarios. "
729471219217633283,2016-05-09 00:41:06,https://t.co/3UWVysmxfi,Market forecasting using Hidden Markov Models. (arXiv:1504.07829v2 [stat.ML] UPDATED) https://t.co/3UWVysmxfi,1,7," Abstract: Working on the daily closing prices and logreturns, in this paper we deal with the use of Hidden Markov Models (HMMs) to forecast the price of the EUR/USD Futures. The aim of our work is to understand how the HMMs describe different financial time series depending on their structure. Subsequently, we analyse the forecasting methods exposed in the previous literature, putting on evidence their pros and cons. "
729471218059972609,2016-05-09 00:41:05,https://t.co/Uh1qgCip8D,Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit. (arXiv:1507.01238v3 [cs.CV] UPDATED) https://t.co/Uh1qgCip8D,0,6," Abstract: Subspace clustering methods based on $\ell_1$, $\ell_2$ or nuclear norm regularization have become very popular due to their simplicity, theoretical guarantees and empirical success. However, the choice of the regularizer can greatly impact both theory and practice. For instance, $\ell_1$ regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad conditions (e.g., arbitrary subspaces and corrupted data). However, it requires solving a large scale convex optimization problem. On the other hand, $\ell_2$ and nuclear norm regularization provide efficient closed form solutions, but require very strong assumptions to guarantee a subspace-preserving affinity, e.g., independent subspaces and uncorrupted data. In this paper we study a subspace clustering method based on orthogonal matching pursuit. We show that the method is both computationally efficient and guaranteed to give a subspace-preserving affinity under broad conditions. Experiments on synthetic data verify our theoretical analysis, and applications in handwritten digit and face clustering show that our approach achieves the best trade off between accuracy and efficiency. "
729471217011412992,2016-05-09 00:41:05,https://t.co/2sYUuL1VIz,Convergence of Contrastive Divergence Algorithm in Exponential Family. (arXiv:1603.05729v2 [stat.ML] UPDATED) https://t.co/2sYUuL1VIz,0,3," Abstract: This paper studies the convergence properties of contrastive divergence algorithm for parameter inference in exponential family, by relating it to Markov chain theory and stochastic stability literature. We prove that, under mild conditions and given a finite data sample $X_1,\dots,X_n \sim p_{\theta^*}$ i.i.d. in an event with probability approaching to 1, the sequence $\{\theta_t\}_{t \ge 0}$ generated by CD algorithm is a positive Harris recurrent chain, and thus processes an unique invariant distribution $\pi_n$. The invariant distribution concentrates around the Maximum Likelihood Estimate at a speed arbitrarily slower than $\sqrt{n}$, and the number of steps in Markov Chain Monte Carlo only affects the coefficient factor of the concentration rate. Finally we conclude that as $n \to \infty$, $$\limsup_{t \to \infty} \left\Vert \frac{1}{t} \sum_{s=1}^t \theta_s - \theta^*\right\Vert \overset{p}{\to} 0.$$ "
729471215782510592,2016-05-09 00:41:05,https://t.co/giyIySQPCs,Regret Analysis of the Anytime Optimally Confident UCB Algorithm. (arXiv:1603.08661v2 [cs.LG] UPDATED) https://t.co/giyIySQPCs,0,7," Abstract: I introduce and analyse an anytime version of the Optimally Confident UCB (OCUCB) algorithm designed for minimising the cumulative regret in finite-armed stochastic bandits with subgaussian noise. The new algorithm is simple, intuitive (in hindsight) and comes with the strongest finite-time regret guarantees for a horizon-free algorithm so far. I also show a finite-time lower bound that nearly matches the upper bound. "
728384438711812096,2016-05-06 00:42:37,https://t.co/ppXR4WSI06,Sampling Requirements for Stable Autoregressive Estimation. (arXiv:1605.01436v1 [cs.IT]) https://t.co/ppXR4WSI06,0,4," Abstract: We consider the problem of estimating the parameters of a linear autoregressive model with sub-Gaussian innovations from a limited sequence of consecutive observations. Assuming that the parameters are compressible, we analyze the performance of the $\ell_1$-regularized least squares as well as a greedy estimator of the parameters and characterize the sampling trade-offs required for stable recovery in the non-asymptotic regime. Our results extend those of compressed sensing for linear models where the covariates are i.i.d. and independent of the observation history to autoregressive processes with highly inter-dependent covariates. We also derive sufficient conditions on the sparsity level that guarantee the minimax optimality of the $\ell_1$-regularized least squares estimate. Applying these techniques to simulated data as well as real-world datasets from crude oil prices and traffic speed data confirm our predicted theoretical performance gains in terms of estimation accuracy and model selection. "
728384437776482305,2016-05-06 00:42:37,https://t.co/dzwP5uZYvN,Sampling from strongly log-concave distributions with the Unadjusted Langevin Algorithm. (arXiv:1605.01559v1 [math… https://t.co/dzwP5uZYvN,0,4," Abstract: We consider in this paper the problem of sampling a probability distribution $\pi$ having a density with respect to the Lebesgue measure on $\mathbb{R}^d$, known up to a normalisation factor $x \mapsto \mathrm{e}^{-U(x)}/\int_{\mathbb{R}^d} \mathrm{e}^{-U(y)} \mathrm{d} y$. Under the assumption that $U$ is continuously differentiable, $\nabla U$ is globally Lipshitz and $U$ is strongly convex, we obtain non-asymptotic bounds for the convergence to stationarity in Wasserstein distances of the sampling method based on the Euler discretization of the Langevin stochastic differential equation for both constant and decreasing step sizes. The dependence on the dimension of the state space of the obtained bounds is studied to demonstrate the applicability of this method in the high dimensional setting. The convergence of an appropriately weighted empirical measure is also investigated and bounds for the mean square error and exponential deviation inequality for Lipschitz functions are reported. Some numerical results are presented to illustrate our findings. "
728384436773982209,2016-05-06 00:42:37,https://t.co/iTSjmq00Gr,Observational-Interventional Priors for Dose-Response Learning. (arXiv:1605.01573v1 [stat.ML]) https://t.co/iTSjmq00Gr,0,2," Abstract: Controlled interventions provide the most direct source of information for learning causal effects. In particular, a dose-response curve can be learned by varying the treatment level and observing the corresponding outcomes. However, interventions can be expensive and time-consuming. Observational data, where the treatment is not controlled by a known mechanism, is sometimes available. Under some strong assumptions, observational data allows for the estimation of dose-response curves. Estimating such curves nonparametrically is hard: sample sizes for controlled interventions may be small, while in the observational case a large number of measured confounders may need to be marginalized. In this paper, we introduce a hierarchical Gaussian process prior that constructs a distribution over the dose-response curve by learning from observational data, and reshapes the distribution with a nonparametric affine transform learned from controlled interventions. This function composition from different sources is shown to speed-up learning, which we demonstrate with a thorough sensitivity analysis and an application to modeling the effect of therapy on cognitive skills of premature infants. "
728384435930927104,2016-05-06 00:42:36,https://t.co/TMMFj9vb0g,The IBM Speaker Recognition System: Recent Advances and Error Analysis. (arXiv:1605.01635v1 [cs.CL]) https://t.co/TMMFj9vb0g,0,4," Abstract: We present the recent advances along with an error analysis of the IBM speaker recognition system for conversational speech. Some of the key advancements that contribute to our system include: a nearest-neighbor discriminant analysis (NDA) approach (as opposed to LDA) for intersession variability compensation in the i-vector space, the application of speaker and channel-adapted features derived from an automatic speech recognition (ASR) system for speaker recognition, and the use of a DNN acoustic model with a very large number of output units (~10k senones) to compute the frame-level soft alignments required in the i-vector estimation process. We evaluate these techniques on the NIST 2010 SRE extended core conditions (C1-C9), as well as the 10sec-10sec condition. To our knowledge, results achieved by our system represent the best performances published to date on these conditions. For example, on the extended tel-tel condition (C5) the system achieves an EER of 0.59%. To garner further understanding of the remaining errors (on C5), we examine the recordings associated with the low scoring target trials, where various issues are identified for the problematic recordings/trials. Interestingly, it is observed that correcting the pathological recordings not only improves the scores for the target trials but also for the nontarget trials. "
728384434597208064,2016-05-06 00:42:36,https://t.co/qcns1DPrqG,The embedding dimension of Laplacian eigenfunction maps. (arXiv:1605.01643v1 [stat.ML]) https://t.co/qcns1DPrqG,0,4," Abstract: Any closed, connected Riemannian manifold $M$ can be smoothly embedded by its Laplacian eigenfunction maps into $\mathbb{R}^m$ for some $m$. We call the smallest such $m$ the maximal embedding dimension of $M$. We show that the maximal embedding dimension of $M$ is bounded from above by a constant depending only on the dimension of $M$, a lower bound for injectivity radius, a lower bound for Ricci curvature, and a volume bound. We interpret this result for the case of surfaces isometrically immersed in $\mathbb{R}^3$, showing that the maximal embedding dimension only depends on bounds for the Gaussian curvature, mean curvature, and surface area. Furthermore, we consider the relevance of these results for shape registration. "
728384433691217920,2016-05-06 00:42:36,https://t.co/5UW3uOxtUX,A Tight Bound of Hard Thresholding. (arXiv:1605.01656v1 [stat.ML]) https://t.co/5UW3uOxtUX,0,2," Abstract: This paper is concerned with the hard thresholding technique which sets all but the $k$ largest absolute elements to zero. We establish a tight bound that quantitatively characterizes the deviation of the thresholded solution from a given signal. Our theoretical result is universal in the sense that it holds for all choices of parameters, and the underlying analysis only depends on fundamental arguments in mathematical optimization. We discuss the implications for the literature: Compressed Sensing. On account of the crucial estimate, we bridge the connection between restricted isometry property (RIP) and the sparsity parameter of $k$ for a vast volume of hard thresholding based algorithms, which renders an improvement on the RIP condition especially when the true sparsity is unknown. This suggests that in essence, many more kinds of sensing matrices or fewer measurements are admissible for the data acquisition procedure. Machine Learning. In terms of large-scale machine learning, a significant yet challenging problem is producing sparse solutions in online setting. In stark contrast to prior works that attempted the $\ell_1$ relaxation for promoting sparsity, we present a novel algorithm which performs hard thresholding in each iteration to ensure such parsimonious solutions. Equipped with the developed bound for hard thresholding, we prove global linear convergence for a number of prevalent statistical models under mild assumptions, even though the problem turns out to be non-convex. "
728384432869130241,2016-05-06 00:42:36,https://t.co/sthNuKTOKF,"Copeland Dueling Bandit Problem: Regret Lower Bound, Optimal Algorithm, and Computationally Efficient Algorithm. (… https://t.co/sthNuKTOKF",0,3," Abstract: We study the K-armed dueling bandit problem, a variation of the standard stochastic bandit problem where the feedback is limited to relative comparisons of a pair of arms. The hardness of recommending Copeland winners, the arms that beat the greatest number of other arms, is characterized by deriving an asymptotic regret bound. We propose Copeland Winners Relative Minimum Empirical Divergence (CW-RMED) and derive an asymptotically optimal regret bound for it. However, it is not known whether the algorithm can be efficiently computed or not. To address this issue, we devise an efficient version (ECW-RMED) and derive its asymptotic regret bound. Experimental comparisons of dueling bandit algorithms show that ECW-RMED significantly outperforms existing ones. "
728384432072167425,2016-05-06 00:42:35,https://t.co/CjhTRs9hvC,A note on adjusting $R^2$ for using with cross-validation. (arXiv:1605.01703v1 [cs.LG]) https://t.co/CjhTRs9hvC,0,3, Abstract: We show how to adjust the coefficient of determination ($R^2$) when used for measuring predictive accuracy via leave-one-out cross-validation. 
728384431053017088,2016-05-06 00:42:35,https://t.co/9b1sNXQTsY,Hamiltonian Monte Carlo Acceleration Using Surrogate Functions with Random Bases. (arXiv:1506.05555v4 [sta… https://t.co/9b1sNXQTsY,0,2," Abstract: For big data analysis, high computational cost for Bayesian methods often limits their applications in practice. In recent years, there have been many attempts to improve computational efficiency of Bayesian inference. Here we propose an efficient and scalable computational technique for a state-of-the-art Markov Chain Monte Carlo (MCMC) methods, namely, Hamiltonian Monte Carlo (HMC). The key idea is to explore and exploit the structure and regularity in parameter space for the underlying probabilistic model to construct an effective approximation of its geometric properties. To this end, we build a surrogate function to approximate the target distribution using properly chosen random bases and an efficient optimization process. The resulting method provides a flexible, scalable, and efficient sampling algorithm, which converges to the correct target distribution. We show that by choosing the basis functions and optimization process differently, our method can be related to other approaches for the construction of surrogate functions such as generalized additive models or Gaussian process models. Experiments based on simulated and real data show that our approach leads to substantially more efficient sampling algorithms compared to existing state-of-the art methods. "
728384428360269824,2016-05-06 00:42:35,https://t.co/4WXjFMqmRm,Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization. (arXiv:1604.03… https://t.co/4WXjFMqmRm,0,7," Abstract: Recently, {\it stochastic momentum} methods have been widely adopted in training deep neural networks. However, their convergence analysis is still underexplored at the moment, in particular for non-convex optimization. This paper fills the gap between practice and theory by developing a basic convergence analysis of two stochastic momentum methods, namely stochastic heavy-ball method and the stochastic variant of Nesterov's accelerated gradient method. We hope that the basic convergence results developed in this paper can serve the reference to the convergence of stochastic momentum methods and also serve the baselines for comparison in future development of stochastic momentum methods. The novelty of convergence analysis presented in this paper is a unified framework, revealing more insights about the similarities and differences between different stochastic momentum methods and stochastic gradient method. The unified framework exhibits a continuous change from the gradient method to Nesterov's accelerated gradient method and finally the heavy-ball method incurred by a free parameter, which can help explain a similar change observed in the testing error convergence behavior for deep learning. Furthermore, our empirical results for optimizing deep neural networks demonstrate that the stochastic variant of Nesterov's accelerated gradient method achieves a good tradeoff (between speed of convergence in training error and robustness of convergence in testing error) among the three stochastic methods. "
728384427492003840,2016-05-06 00:42:34,https://t.co/jXibrgYhKg,Finding Common Characteristics Among NBA Playoff and Championship Teams: A Machine Learning Approach. (arXiv:1604.… https://t.co/jXibrgYhKg,1,3," Abstract: In this paper, we employ machine learning techniques to analyze sixteen seasons of NBA regular season data from every team to determine the common characteristics among NBA playoff teams. Each team was characterized by 42 predictor variables and one binary response variable taking on a value of ""TRUE"" if a team had made the playoffs, and value of ""FALSE"" if a team had missed the playoffs. After fitting an initial classification tree to this problem, this tree was then pruned which decreased the test error rate. Further to this, a random forest of classification trees was grown which provided a very accurate model from which a variable importance plot was generated to determine which predictor variables had the greatest influence on the response variable. The result of this work was the conclusion that the most important factors in characterizing a team's playoff eligibility are the opponent field goal percentage and the opponent points per game. This seems to suggest that \emph{defensive} factors as opposed to offensive factors are the most important characteristics shared among NBA playoff teams. We also perform a classification analysis to determine common characteristics among NBA championship teams. Using an artificial neural network structure, we show that championship teams must be able to have very strong defensive characteristics, in particular, strong perimeter defense characteristics in combination with an effective half-court offense that generates high-percentage two-point shots. A key part of this offensive strategy must also be the ability to draw fouls. This analysis will hopefully dispel the rising notion that an offense geared towards shooting many three point shots is a sufficient and necessary condition for an NBA team to be successful in qualifying for the playoffs and winning a championship. "
728384426644783104,2016-05-06 00:42:34,https://t.co/G9BSJkmsEz,Bayesian inference in hierarchical models by combining independent posteriors. (arXiv:1603.09272v2 [stat.C… https://t.co/G9BSJkmsEz,1,4," Abstract: Hierarchical models are versatile tools for joint modeling of data sets arising from different, but related, sources. Fully Bayesian inference may, however, become computationally prohibitive if the source-specific data models are complex, or if the number of sources is very large. To facilitate computation, we propose an approach, where inference is first made independently for the parameters of each data set, whereupon the obtained posterior samples are used as observed data in a substitute hierarchical model, based on a scaled likelihood function. Compared to direct inference in a full hierarchical model, the approach has the advantage of being able to speed up convergence by breaking down the initial large inference problem into smaller individual subproblems with better convergence properties. Moreover it enables parallel processing of the possibly complex inferences of the source-specific parameters, which may otherwise create a computational bottleneck if processed jointly as part of a hierarchical model. The approach is illustrated with both simulated and real data. "
728020829854965760,2016-05-05 00:37:46,https://t.co/BcwroG14xa,Decentralized Dynamic Discriminative Dictionary Learning. (arXiv:1605.01107v1 [stat.ML]) https://t.co/BcwroG14xa,0,3," Abstract: We consider discriminative dictionary learning in a distributed online setting, where a network of agents aims to learn a common set of dictionary elements of a feature space and model parameters while sequentially receiving observations. We formulate this problem as a distributed stochastic program with a non-convex objective and present a block variant of the Arrow-Hurwicz saddle point algorithm to solve it. Using Lagrange multipliers to penalize the discrepancy between them, only neighboring nodes exchange model information. We show that decisions made with this saddle point algorithm asymptotically achieve a first-order stationarity condition on average. "
728020828277886977,2016-05-05 00:37:46,https://t.co/8zWEhU3tiY,An evaluation of randomized machine learning methods for redundant data: Predicting short and medium-term suicide … https://t.co/8zWEhU3tiY,0,3," Abstract: Accurate prediction of suicide risk in mental health patients remains an open problem. Existing methods including clinician judgments have acceptable sensitivity, but yield many false positives. Exploiting administrative data has a great potential, but the data has high dimensionality and redundancies in the recording processes. We investigate the efficacy of three most effective randomized machine learning techniques random forests, gradient boosting machines, and deep neural nets with dropout in predicting suicide risk. Using a cohort of mental health patients from a regional Australian hospital, we compare the predictive performance with popular traditional approaches clinician judgments based on a checklist, sparse logistic regression and decision trees. The randomized methods demonstrated robustness against data redundancies and superior predictive performance on AUC and F-measure. "
728020827074072576,2016-05-05 00:37:45,https://t.co/nk4Sfov9X8,Linear Bandit algorithms using the Bootstrap. (arXiv:1605.01185v1 [stat.ML]) https://t.co/nk4Sfov9X8,0,5," Abstract: This study presents two new algorithms for solving linear stochastic bandit problems. The proposed methods use an approach from non-parametric statistics called bootstrapping to create confidence bounds. This is achieved without making any assumptions about the distribution of noise in the underlying system. We present the X-Random and X-Fixed bootstrap bandits which correspond to the two well-known approaches for conducting bootstraps on models, in the literature. The proposed methods are compared to other popular solutions for linear stochastic bandit problems, namely, OFUL, LinUCB and Thompson Sampling. The comparisons are carried out using a simulation study on a hierarchical probability meta-model, built from published data of experiments, which are run on real systems. The model representing the response surfaces is conceptualized as a Bayesian Network which is presented with varying degrees of noise for the simulations. One of the proposed methods, X-Random bootstrap, performs better than the baselines in-terms of cumulative regret across various degrees of noise and different number of trials. In certain settings the cumulative regret of this method is less than half of the best baseline. The X-Fixed bootstrap performs comparably in most situations and particularly well when the number of trials is low. The study concludes that these algorithms could be a preferred alternative for solving linear bandit problems, especially when the distribution of the noise in the system is unknown. "
728020825744547840,2016-05-05 00:37:45,https://t.co/W2GtEPil0N,IISCNLP at SemEval-2016 Task 2: Interpretable STS with ILP based Multiple Chunk Aligner. (arXiv:1605.01194v1 [cs.C… https://t.co/W2GtEPil0N,0,2," Abstract: Interpretable semantic textual similarity (iSTS) task adds a crucial explanatory layer to pairwise sentence similarity. We address various components of this task: chunk level semantic alignment along with assignment of similarity type and score for aligned chunks with a novel system presented in this paper. We propose an algorithm, iMATCH, for the alignment of multiple non-contiguous chunks based on Integer Linear Programming (ILP). Similarity type and score assignment for pairs of chunks is done using a supervised multiclass classification technique based on Random Forrest Classifier. Results show that our algorithm iMATCH has low execution time and outperforms most other participating systems in terms of alignment score. Of the three datasets, we are top ranked for answer- students dataset in terms of overall score and have top alignment score for headlines dataset in the gold chunks track. "
728020824498786305,2016-05-05 00:37:45,https://t.co/b995kQOajk,A Bayesian Approach to Policy Recognition and State Representation Learning. (arXiv:1605.01278v1 [stat.ML]) https://t.co/b995kQOajk,0,2," Abstract: Learning from demonstration (LfD) is the process of building behavioral models of a task from demonstrations provided by an expert. These models can be used e.g. for system control by generalizing the expert demonstrations to previously unencountered situations. Most LfD methods, however, make strong assumptions about the expert behavior, e.g. they assume the existence of a deterministic optimal ground truth policy or require direct monitoring of the expert's controls, which limits their practical use as part of a general system identification framework. In this work, we consider the LfD problem in a more general setting where we allow for arbitrary stochastic expert policies, without reasoning about the quality of the demonstrations. In particular, we focus on the problem of policy recognition, which is to extract a system's latent control policy from observed system behavior. Following a Bayesian methodology allows us to consider various sources of uncertainty about the expert behavior, including the latent expert controls, to model the full posterior distribution of expert controllers. Further, we show that the same methodology can be applied in a nonparametric context to reason about the complexity of the state representation used by the expert and to learn task-appropriate partitionings of the system state space. "
728020823395733504,2016-05-05 00:37:44,https://t.co/toxFbP9wsb,Multi Level Monte Carlo methods for a class of ergodic stochastic differential equations. (arXiv:1605.01384v1 [mat… https://t.co/toxFbP9wsb,0,4," Abstract: We develop a framework that allows the use of the multi-level Monte Carlo (MLMC) methodology (Giles 2015) to calculate expectations with respect to the invariant measures of ergodic SDEs. In that context, we study the (over-damped) Langevin equations with strongly convex potential. We show that, when appropriate contracting couplings for the numerical integrators are available, one can obtain a time-uniform estimates of the MLMC variance in stark contrast to the majority of the results in the MLMC literature. As a consequence, one can approximate expectations with respect to the invariant measure in an unbiased way without the need of a Metropolis- Hastings step. In addition, a root mean square error of $\mathcal{O}(\epsilon)$ is achieved with $\mathcal{O}(\epsilon^{-2})$ complexity on par with Markov Chain Monte Carlo (MCMC) methods, which however can be computationally intensive when applied to large data sets. Finally, we present a multilevel version of the recently introduced Stochastic Gradient Langevin (SGLD) method (Welling and Teh, 2011) built for large datasets applications. We show that this is the first stochastic gradient MCMC method with complexity $\mathcal{O}(\epsilon^{-2}|\log {\epsilon}|^{3})$, which is asymptotically an order $\epsilon$ lower than the $ \mathcal{O}(\epsilon^{-3})$ complexity of all stochastic gradient MCMC methods that are currently available. Numerical experiments confirm our theoretical findings. "
728020822208724992,2016-05-05 00:37:44,https://t.co/07TeJibgZj,The Hidden Convexity of Spectral Clustering. (arXiv:1403.0667v3 [cs.LG] UPDATED) https://t.co/07TeJibgZj,0,3," Abstract: In recent years, spectral clustering has become a standard method for data analysis used in a broad range of applications. In this paper we propose a new class of algorithms for multiway spectral clustering based on optimization of a certain ""contrast function"" over the unit sphere. These algorithms, partly inspired by certain Independent Component Analysis techniques, are simple, easy to implement and efficient. Geometrically, the proposed algorithms can be interpreted as hidden basis recovery by means of function optimization. We give a complete characterization of the contrast functions admissible for provable basis recovery. We show how these conditions can be interpreted as a ""hidden convexity"" of our optimization problem on the sphere; interestingly, we use efficient convex maximization rather than the more common convex minimization. We also show encouraging experimental results on real and simulated data. "
728020821206290433,2016-05-05 00:37:44,https://t.co/MeIP43Aefg,Adaptive Penalized Estimation of Directed Acyclic Graphs From Categorical Data. (arXiv:1403.2310v3 [stat.ME] UPDAT… https://t.co/MeIP43Aefg,0,3," Abstract: We develop in this article a penalized likelihood method to estimate sparse Bayesian networks from categorical data. The structure of a Bayesian network is represented by a directed acyclic graph (DAG). We model the conditional distribution of a node given its parents by multi-logit regression and estimate the structure of a DAG via maximizing a regularized likelihood. The adaptive group Lasso penalty is employed to encourage sparsity by selecting grouped dummy variables encoding the level of a factor. We develop a blockwise coordinate descent algorithm to solve the penalized likelihood problem subject to the acyclicity constraint of a DAG. When intervention data are available, our method may construct a causal network, in which a directed edge represents a causal relation. We apply our method to various simulated networks and a real biological network. The results show that our method is very competitive, compared to other existing methods, in DAG estimation from both interventional and high-dimensional observational data. We also establish consistency in parameter and structure estimation for our method when the number of nodes is fixed. "
728020819822129152,2016-05-05 00:37:44,https://t.co/7Pa5RRNFDR,Asymptotic Theory for Random Forests. (arXiv:1405.0352v2 [math.ST] UPDATED) https://t.co/7Pa5RRNFDR,0,3," Abstract: Random forests have proven to be reliable predictive algorithms in many application areas. Not much is known, however, about the statistical properties of random forests. Several authors have established conditions under which their predictions are consistent, but these results do not provide practical estimates of random forest errors. In this paper, we analyze a random forest model based on subsampling, and show that random forest predictions are asymptotically normal provided that the subsample size s scales as s(n)/n = o(log(n)^{-d}), where n is the number of training examples and d is the number of features. Moreover, we show that the asymptotic variance can consistently be estimated using an infinitesimal jackknife for bagged ensembles recently proposed by Efron (2014). In other words, our results let us both characterize and estimate the error-distribution of random forest predictions, thus taking a step towards making random forests tools for statistical inference instead of just black-box predictive algorithms. "
728020818584834048,2016-05-05 00:37:43,https://t.co/30CwbzZ3X7,New insights and perspectives on the natural gradient method. (arXiv:1412.1193v6 [cs.LG] UPDATED) https://t.co/30CwbzZ3X7,0,8," Abstract: Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of approximate 2nd-order optimization method, where the Fisher information matrix used to compute the natural gradient direction can be viewed as an approximation of the Hessian. This perspective turns out to have significant implications for how to design a practical and robust version of the method. Among our various other contributions is a thorough analysis of the convergence speed of natural gradient descent and more general stochastic methods, a critical examination of the oft-used ""empirical"" approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by the method, which we show still holds for certain other choices of the curvature matrix, but notably not the Hessian. "
728020817234280452,2016-05-05 00:37:43,https://t.co/j4PRvYNQLU,Optimizing Neural Networks with Kronecker-factored Approximate Curvature. (arXiv:1503.05671v6 [cs.LG] UPDATED) https://t.co/j4PRvYNQLU,0,4," Abstract: We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-Factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network's Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC's approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix. "
728020815896293378,2016-05-05 00:37:43,https://t.co/Wc5BdHB613,Randomer Forests. (arXiv:1506.03410v2 [stat.ML] UPDATED) https://t.co/Wc5BdHB613,0,2," Abstract: Random forests (RF) is a popular general purpose classifier that has been shown to outperform many other classifiers on a variety of datasets. The widespread use of random forests can be attributed to several factors, some of which include its excellent empirical performance, scale and unit invariance, robustness to outliers, time and space complexity, and interpretability. While RF has many desirable qualities, one drawback is its sensitivity to rotations and other operations that ""mix"" variables. In this work, we establish a generalized forest building scheme, linear threshold forests. Random forests and many other currently existing decision forest algorithms can be viewed as special cases of this scheme. With this scheme in mind, we propose a few special cases which we call randomer forests (RerFs). RerFs are linear threshold forest that exhibit all of the nice properties of RF, in addition to approximate affine invariance. In simulated datasets designed for RF to do well, we demonstrate that RerF outperforms RF. We also demonstrate that one particular variant of RerF is approximately affine invariant. Lastly, in an evaluation on 121 benchmark datasets, we observe that RerF outperforms RF. We therefore putatively propose that RerF be considered a replacement for RF as the general purpose classifier of choice. Open source code is available at this http URL "
728020814449233920,2016-05-05 00:37:42,https://t.co/gJL3P2Fr5n,Ethnicity sensitive author disambiguation using semi-supervised learning. (arXiv:1508.07744v2 [cs.DL] UPDATED) https://t.co/gJL3P2Fr5n,0,2," Abstract: Author name disambiguation in bibliographic databases is the problem of grouping together scientific publications written by the same person, accounting for potential homonyms and/or synonyms. Among solutions to this problem, digital libraries are increasingly offering tools for authors to manually curate their publications and claim those that are theirs. Indirectly, these tools allow for the inexpensive collection of large annotated training data, which can be further leveraged to build a complementary automated disambiguation system capable of inferring patterns for identifying publications written by the same person. Building on more than 1 million publicly released crowdsourced annotations, we propose an automated author disambiguation solution exploiting this data (i) to learn an accurate classifier for identifying coreferring authors and (ii) to guide the clustering of scientific publications by distinct authors in a semi-supervised way. To the best of our knowledge, our analysis is the first to be carried out on data of this size and coverage. With respect to the state of the art, we validate the general pipeline used in most existing solutions, and improve by: (i) proposing phonetic-based blocking strategies, thereby increasing recall; and (ii) adding strong ethnicity-sensitive features for learning a linkage function, thereby tailoring disambiguation to non-Western author names whenever necessary. "
728020813308383233,2016-05-05 00:37:42,https://t.co/jmCPIT4g1W,Gated Graph Sequence Neural Networks. (arXiv:1511.05493v3 [cs.LG] UPDATED) https://t.co/jmCPIT4g1W,0,4," Abstract: Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures. "
728020811626496001,2016-05-05 00:37:42,https://t.co/o59WJAsqpi,RSG: Beating SG without Smoothness and/or Strong Convexity. (arXiv:1512.03107v9 [math.OC] UPDATED) https://t.co/o59WJAsqpi,0,3," Abstract: In this paper, we study the efficiency of a {\bf R}estarted {\bf S}ub{\bf G}radient (RSG) method that periodically restarts the standard subgradient method (SG). We show that, when applied to a broad class of convex optimization problems, RSG method can find an $\epsilon$-optimal solution with a low complexity than SG method. In particular, we first show that RSG can reduce the dependence of SG's iteration complexity on the distance between the initial solution and the optimal set to that between the $\epsilon$-level set and the optimal set. In addition, we show the advantages of RSG over SG in solving three different families of convex optimization problems. (a) For the problems whose epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with local quadratic growth property, RSG has an $O(\frac{1}{\epsilon}\log(\frac{1}{\epsilon}))$ iteration complexity. (c) For the problems that admit a local Kurdyka-\L ojasiewicz property with a power constant of $\beta\in[0,1)$, RSG has an $O(\frac{1}{\epsilon^{2\beta}}\log(\frac{1}{\epsilon}))$ iteration complexity. On the contrary, with only the standard analysis, the iteration complexity of SG is known to be $O(\frac{1}{\epsilon^2})$ for these three classes of problems. The novelty of our analysis lies at exploiting the lower bound of the first-order optimality residual at the $\epsilon$-level set. It is this novelty that allows us to explore the local properties of functions (e.g., local quadratic growth property, local Kurdyka-\L ojasiewicz property, more generally local error bounds) to develop the improved convergence of RSG. We demonstrate the effectiveness of the proposed algorithms on several machine learning tasks including regression and classification. "
728020810137538560,2016-05-05 00:37:41,https://t.co/BovXsygzLm,Classical Statistics and Statistical Learning in Imaging Neuroscience. (arXiv:1603.01857v2 [stat.ML] UPDATED) https://t.co/BovXsygzLm,1,6," Abstract: Neuroimaging research has predominantly drawn conclusions based on classical statistics, including null-hypothesis testing, t-tests, and ANOVA. Throughout recent years, statistical learning methods enjoy increasing popularity, including cross-validation, pattern classification, and sparsity-inducing regression. These two methodological families used for neuroimaging data analysis can be viewed as two extremes of a continuum. Yet, they originated from different historical contexts, build on different theories, rest on different assumptions, evaluate different outcome metrics, and permit different conclusions. This paper portrays commonalities and differences between classical statistics and statistical learning with their relation to neuroimaging research. The conceptual implications are illustrated in three common analysis scenarios. It is thus tried to resolve possible confusion between classical hypothesis testing and data-guided model estimation by discussing their ramifications for the neuroimaging access to neurobiology. "
727659662414364672,2016-05-04 00:42:37,https://t.co/w7FaoloWbj,Efficient Distributed Estimation of Inverse Covariance Matrices. (arXiv:1605.00758v1 [stat.ME]) https://t.co/w7FaoloWbj,1,3," Abstract: In distributed systems, communication is a major concern due to issues such as its vulnerability or efficiency. In this paper, we are interested in estimating sparse inverse covariance matrices when samples are distributed into different machines. We address communication efficiency by proposing a method where, in a single round of communication, each machine transfers a small subset of the entries of the inverse covariance matrix. We show that, with this efficient distributed method, the error rates can be comparable with estimation in a non-distributed setting, and correct model selection is still possible. Practical performance is shown through simulations. "
727659661630054401,2016-05-04 00:42:37,https://t.co/5qQbeWmg5f,Temporal Clustering of Time Series via Threshold Autoregressive Models: Application to Commodity Prices. (arXiv:16… https://t.co/5qQbeWmg5f,2,8," Abstract: This study aimed to find temporal clusters for several commodity prices using the threshold non-linear autoregressive model. It is expected that the process of determining the commodity groups that are time-dependent will advance the current knowledge about the dynamics of co-moving and coherent prices, and can serve as a basis for multivariate time series analyses. The clustering of commodity prices was examined using the proposed clustering approach based on time series models to incorporate the time varying properties of price series into the clustering scheme. Accordingly, the primary aim in this study was grouping time series according to the similarity between their Data Generating Mechanisms (DGMs) rather than comparing pattern similarities in the time series traces. The approximation to the DGM of each series was accomplished using threshold autoregressive models, which are recognized for their ability to represent nonlinear features in time series, such as abrupt changes, time-irreversibility and regime-shifting behavior. Through the use of the proposed approach, one can determine and monitor the set of co-moving time series variables across the time dimension. Furthermore, generating a time varying commodity price index and sub-indexes can become possible. Consequently, we conducted a simulation study to assess the effectiveness of the proposed clustering approach and the results are presented for both the simulated and real data sets. "
727659660526882817,2016-05-04 00:42:36,https://t.co/GXHnN4uqDd,Dictionary Learning for Massive Matrix Factorization. (arXiv:1605.00937v1 [stat.ML]) https://t.co/GXHnN4uqDd,4,7," Abstract: Sparse matrix factorization is a popular tool to obtain interpretable data decompositions, which are also effective to perform data completion or denoising. Its applicability to large datasets has been addressed with online and randomized methods, that reduce the complexity in one of the matrix dimension, but not in both of them. In this paper, we tackle very large matrices in both dimensions. We propose a new factoriza-tion method that scales gracefully to terabyte-scale datasets, that could not be processed by previous algorithms in a reasonable amount of time. We demonstrate the efficiency of our approach on massive functional Magnetic Resonance Imaging (fMRI) data, and on matrix completion problems for recommender systems, where we obtain significant speed-ups compared to state-of-the art coordinate descent methods. "
727659659599974400,2016-05-04 00:42:36,https://t.co/fVwkAKW0Em,Personalized Risk Scoring for Critical Care Patients using Mixtures of Gaussian Process Experts. (arXiv:1605.00959… https://t.co/fVwkAKW0Em,0,3," Abstract: We develop a personalized real time risk scoring algorithm that provides timely and granular assessments for the clinical acuity of ward patients based on their (temporal) lab tests and vital signs. Heterogeneity of the patients population is captured via a hierarchical latent class model. The proposed algorithm aims to discover the number of latent classes in the patients population, and train a mixture of Gaussian Process (GP) experts, where each expert models the physiological data streams associated with a specific class. Self-taught transfer learning is used to transfer the knowledge of latent classes learned from the domain of clinically stable patients to the domain of clinically deteriorating patients. For new patients, the posterior beliefs of all GP experts about the patient's clinical status given her physiological data stream are computed, and a personalized risk score is evaluated as a weighted average of those beliefs, where the weights are learned from the patient's hospital admission information. Experiments on a heterogeneous cohort of 6,313 patients admitted to Ronald Regan UCLA medical center show that our risk score outperforms the currently deployed risk scores, such as MEWS and Rothman scores. "
727659658677264384,2016-05-04 00:42:36,https://t.co/B2csfTyhLx,"Exact post-selection inference, with application to the lasso. (arXiv:1311.6238v8 [math.ST] UPDATED) https://t.co/B2csfTyhLx",0,6, Abstract: We develop a general approach to valid inference after model selection. At the core of our framework is a result that characterizes the distribution of a post-selection estimator conditioned on the selection event. We specialize the approach to model selection by the lasso to form valid confidence intervals for the selected coefficients and test whether all relevant variables have been included in the model. 
727659657758683136,2016-05-04 00:42:36,https://t.co/FMT7drBgty,Provable Sparse Tensor Decomposition. (arXiv:1502.01425v3 [stat.ML] UPDATED) https://t.co/FMT7drBgty,1,5," Abstract: We propose a novel sparse tensor decomposition method, namely Tensor Truncated Power (TTP) method, that incorporates variable selection into the estimation of decomposition components. The sparsity is achieved via an efficient truncation step embedded in the tensor power iteration. Our method applies to a broad family of high dimensional latent variable models, including high dimensional Gaussian mixture and mixtures of sparse regressions. A thorough theoretical investigation is further conducted. In particular, we show that the final decomposition estimator is guaranteed to achieve a local statistical rate, and further strengthen it to the global statistical rate by introducing a proper initialization procedure. In high dimensional regimes, the obtained statistical rate significantly improves those shown in the existing non-sparse decomposition methods. The empirical advantages of TTP are confirmed in extensive simulated results and two real applications of click-through rate prediction and high-dimensional gene clustering. "
727659656752025600,2016-05-04 00:42:36,https://t.co/bNEQx4woEH,Provable Bayesian Inference via Particle Mirror Descent. (arXiv:1506.03101v2 [cs.LG] UPDATED) https://t.co/bNEQx4woEH,0,8," Abstract: Bayesian methods are appealing in their flexibility in modeling complex data and ability in capturing uncertainty in parameters. However, when Bayes' rule does not result in tractable closed-form, most approximate inference algorithms lack either scalability or rigorous guarantees. To tackle this challenge, we propose a simple yet provable algorithm, \emph{Particle Mirror Descent} (PMD), to iteratively approximate the posterior density. PMD is inspired by stochastic functional mirror descent where one descends in the density space using a small batch of data points at each iteration, and by particle filtering where one uses samples to approximate a function. We prove result of the first kind that, with $m$ particles, PMD provides a posterior density estimator that converges in terms of $KL$-divergence to the true posterior in rate $O(1/\sqrt{m})$. We demonstrate competitive empirical performances of PMD compared to several approximate inference algorithms in mixture models, logistic regression, sparse Gaussian processes and latent Dirichlet allocation on large scale datasets. "
727659655946768385,2016-05-04 00:42:35,https://t.co/4iBdD3FtqG,Neural Simpletrons - Minimalistic Directed Generative Networks for Learning with Few Labels. (arXiv:1506.08448v3 [… https://t.co/4iBdD3FtqG,0,5," Abstract: Classifiers for the semi-supervised setting often combine strong supervised models with additional learning objectives to make use of unlabeled data. This results in powerful though very complex models that are hard to train and that demand additional labels for optimal parameter tuning, which are often not given when labeled data is very sparse. We here study a minimalistic multi-layer generative neural network for semi-supervised learning in a form and setting as similar to standard discriminative networks as possible. Based on normalized Poisson mixtures, we derive compact and local learning and neural activation rules. Learning and inference in the network can be scaled using standard deep learning tools for parallelized GPU implementation. With the single objective of likelihood optimization, both labeled and unlabeled data are naturally incorporated into learning. Empirical evaluations on standard benchmarks show, that for datasets with few labels the derived minimalistic network improves on all classical deep learning approaches and is competitive with their recent variants without the need of additional labels for parameter tuning. Furthermore, we find that the studied network is the best performing monolithic (`non-hybrid') system for few labels, and that it can be applied in the limit of very few labels, where no other system has been reported to operate so far. "
727659654893977601,2016-05-04 00:42:35,https://t.co/QXvB3hVxL4,Learning to Aggregate Information for Sequential Inferences. (arXiv:1508.07964v2 [stat.ML] UPDATED) https://t.co/QXvB3hVxL4,1,4," Abstract: We consider the problem of training a binary sequential classifier under an error rate constraint. It is well known that for known densities, accumulating the likelihood ratio statistics is time optimal under a fixed error rate constraint. For the case of unknown densities, we formulate the learning for sequential detection problem as a constrained density ratio estimation problem. Specifically, we show that the problem can be posed as a convex optimization problem using a Reproducing Kernel Hilbert Space representation for the log-density ratio function. The proposed binary sequential classifier is tested on synthetic data set and UC Irvine human activity recognition data set, together with previous approaches for density ratio estimation. Our empirical results show that the classifier trained through the proposed technique achieves smaller average sampling cost than previous classifiers proposed in the literature for the same error rate. "
727298558777561089,2016-05-03 00:47:43,https://t.co/bXz6Bd4mf0,Deep Convolutional Neural Networks on Cartoon Functions. (arXiv:1605.00031v1 [cs.IT]) https://t.co/bXz6Bd4mf0,2,5," Abstract: Wiatowski and B\""olcskei, 2015, proved that deformation stability and vertical translation invariance of deep convolutional neural network-based feature extractors are guaranteed by the network structure per se rather than the specific convolution kernels and non-linearities. While the translation invariance result applies to square-integrable functions, the deformation stability bound holds for band-limited functions only. Many signals of practical relevance (such as natural images) exhibit, however, sharp and curved discontinuities and are hence not band-limited. The main contribution of this paper is a deformation stability result that takes these structural properties into account. Specifically, we establish deformation stability bounds for the class of cartoon functions introduced by Donoho, 2001. "
727298557418561540,2016-05-03 00:47:43,https://t.co/bRZAxNHdjw,Kernel Balancing: A flexible non-parametric weighting procedure for estimating causal effects. (arXiv:1605.00155v1… https://t.co/bRZAxNHdjw,0,3," Abstract: In the absence of unobserved confounders, matching and weighting methods are widely used to estimate causal quantities including the Average Treatment Effect on the Treated (ATT). Unfortunately, these methods do not necessarily achieve their goal of making the multivariate distribution of covariates for the control group identical to that of the treated, leaving some (potentially multivariate) functions of the covariates with different means between the two groups. When these ""imbalanced"" functions influence the non-treatment potential outcome, the conditioning on observed covariates fails, and ATT estimates may be biased. Kernel balancing, introduced here, targets a weaker requirement for unbiased ATT estimation, specifically, that the expected non-treatment potential outcome for the treatment and control groups are equal. The conditional expectation of the non-treatment potential outcome is assumed to fall in the space of functions associated with a choice of kernel, implying a set of basis functions in which this regression surface is linear. Weights are then chosen on the control units such that the treated and control group have equal means on these basis functions. As a result, the expectation of the non-treatment potential outcome must also be equal for the treated and control groups after weighting, allowing unbiased ATT estimation by subsequent difference in means or an outcome model using these weights. Moreover, the weights produced are (1) precisely those that equalize a particular kernel-based approximation of the multivariate distribution of covariates for the treated and control, and (2) equivalent to a form of stabilized inverse propensity score weighting, though it does not require assuming any model of the treatment assignment mechanism. An R package, KBAL, is provided to implement this approach. "
727298556185468929,2016-05-03 00:47:43,https://t.co/8LviylK8kH,Further properties of the forward-backward envelope with applications to difference-of-convex programming. (arXiv:… https://t.co/8LviylK8kH,0,3," Abstract: In this paper, we further study the forward-backward envelope first introduced in [28] and [30] for problems whose objective is the sum of a proper closed convex function and a twice continuously differentiable possibly nonconvex function with Lipschitz continuous gradient. We derive sufficient conditions on the original problem for the corresponding forward-backward envelope to be a level-bounded and Kurdyka-{\L}ojasiewicz function with an exponent of $\frac12$; these results are important for the efficient minimization of the forward-backward envelope by classical optimization algorithms. In addition, we demonstrate how to minimize some difference-of-convex regularized least squares problems by minimizing a suitably constructed forward-backward envelope. Our preliminary numerical results on randomly generated instances of large-scale $\ell_{1-2}$ regularized least squares problems [37] illustrate that an implementation of this approach with a limited-memory BFGS scheme usually outperforms standard first-order methods such as the nonmonotone proximal gradient method in [35]. "
727298555153661953,2016-05-03 00:47:42,https://t.co/S7vqs2VrAf,Text-mining the NeuroSynth corpus using Deep Boltzmann Machines. (arXiv:1605.00223v1 [cs.LG]) https://t.co/S7vqs2VrAf,0,4," Abstract: Large-scale automated meta-analysis of neuroimaging data has recently established itself as an important tool in advancing our understanding of human brain function. This research has been pioneered by NeuroSynth, a database collecting both brain activation coordinates and associated text across a large cohort of neuroimaging research papers. One of the fundamental aspects of such meta-analysis is text-mining. To date, word counts and more sophisticated methods such as Latent Dirichlet Allocation have been proposed. In this work we present an unsupervised study of the NeuroSynth text corpus using Deep Boltzmann Machines (DBMs). The use of DBMs yields several advantages over the aforementioned methods, principal among which is the fact that it yields both word and document embeddings in a high-dimensional vector space. Such embeddings serve to facilitate the use of traditional machine learning techniques on the text corpus. The proposed DBM model is shown to learn embeddings with a clear semantic structure. "
727298554096680960,2016-05-03 00:47:42,https://t.co/ufFARFTXeo,Directional Statistics in Machine Learning: a Brief Review. (arXiv:1605.00316v1 [stat.ML]) https://t.co/ufFARFTXeo,1,13," Abstract: The modern data analyst must cope with data encoded in various forms, vectors, matrices, strings, graphs, or more. Consequently, statistical and machine learning models tailored to different data encodings are important. We focus on data encoded as normalized vectors, so that their ""direction"" is more important than their magnitude. Specifically, we consider high-dimensional vectors that lie either on the surface of the unit hypersphere or on the real projective plane. For such data, we briefly review common mathematical models prevalent in machine learning, while also outlining some technical aspects, software, applications, and open mathematical challenges. "
727298553043935236,2016-05-03 00:47:42,https://t.co/gLJcNQ7XnT,Contrastive Structured Anomaly Detection for Gaussian Graphical Models. (arXiv:1605.00355v1 [stat.ML]) https://t.co/gLJcNQ7XnT,0,2," Abstract: Gaussian graphical models (GGMs) are probabilistic tools of choice for analyzing conditional dependencies between variables in complex systems. Finding changepoints in the structural evolution of a GGM is therefore essential to detecting anomalies in the underlying system modeled by the GGM. In order to detect structural anomalies in a GGM, we consider the problem of estimating changes in the precision matrix of the corresponding Gaussian distribution. We take a two-step approach to solving this problem:- (i) estimating a background precision matrix using system observations from the past without any anomalies, and (ii) estimating a foreground precision matrix using a sliding temporal window during anomaly monitoring. Our primary contribution is in estimating the foreground precision using a novel contrastive inverse covariance estimation procedure. In order to accurately learn only the structural changes to the GGM, we maximize a penalized log-likelihood where the penalty is the $l_1$ norm of difference between the foreground precision being estimated and the already learned background precision. We modify the alternating direction method of multipliers (ADMM) algorithm for sparse inverse covariance estimation to perform contrastive estimation of the foreground precision matrix. Our results on simulated GGM data show significant improvement in precision and recall for detecting structural changes to the GGM, compared to a non-contrastive sliding window baseline. "
727298551873691648,2016-05-03 00:47:41,https://t.co/I88NFcKjZC,Highly Accurate Prediction of Jobs Runtime Classes. (arXiv:1605.00388v1 [stat.ML]) https://t.co/I88NFcKjZC,0,3," Abstract: Separating the short jobs from the long is a known technique to improve scheduling performance. In this paper we describe a method we developed for accurately predicting the runtimes classes of the jobs to enable this separation. Our method uses the fact that the runtimes can be represented as a mixture of overlapping Gaussian distributions, in order to train a CART classifier to provide the prediction. The threshold that separates the short jobs from the long jobs is determined during the evaluation of the classifier to maximize prediction accuracy. Our results indicate overall accuracy of 90% for the data set used in our study, with sensitivity and specificity both above 90%. "
727298550917419008,2016-05-03 00:47:41,https://t.co/PdB6vmhFbs,Recovery of non-linear cause-effect relationships from linearly mixed neuroimaging data. (arXiv:1605.00391v1 [stat… https://t.co/PdB6vmhFbs,0,5," Abstract: Causal inference concerns the identification of cause-effect relationships between variables. However, often only linear combinations of variables constitute meaningful causal variables. For example, recovering the signal of a cortical source from electroencephalography requires a well-tuned combination of signals recorded at multiple electrodes. We recently introduced the MERLiN (Mixture Effect Recovery in Linear Networks) algorithm that is able to recover, from an observed linear mixture, a causal variable that is a linear effect of another given variable. Here we relax the assumption of this cause-effect relationship being linear and present an extended algorithm that can pick up non-linear cause-effect relationships. Thus, the main contribution is an algorithm (and ready to use code) that has broader applicability and allows for a richer model class. Furthermore, a comparative analysis indicates that the assumption of linear cause-effect relationships is not restrictive in analysing electroencephalographic data. "
727298549885591552,2016-05-03 00:47:41,https://t.co/TyqlsnzoNt,Fuzzy clustering of distribution-valued data using adaptive L2 Wasserstein distances. (arXiv:1605.00513v1 [stat.ML… https://t.co/TyqlsnzoNt,0,3," Abstract: Distributional (or distribution-valued) data are a new type of data arising from several sources and are considered as realizations of distributional variables. A new set of fuzzy c-means algorithms for data described by distributional variables is proposed. The algorithms use the $L2$ Wasserstein distance between distributions as dissimilarity measures. Beside the extension of the fuzzy c-means algorithm for distributional data, and considering a decomposition of the squared $L2$ Wasserstein distance, we propose a set of algorithms using different automatic way to compute the weights associated with the variables as well as with their components, globally or cluster-wise. The relevance weights are computed in the clustering process introducing product-to-one constraints. The relevance weights induce adaptive distances expressing the importance of each variable or of each component in the clustering process, acting also as a variable selection method in clustering. We have tested the proposed algorithms on artificial and real-world data. Results confirm that the proposed methods are able to better take into account the cluster structure of the data with respect to the standard fuzzy c-means, with non-adaptive distances. "
727298548891537409,2016-05-03 00:47:41,https://t.co/xfigrRYhED,Linear-time Outlier Detection via Sensitivity. (arXiv:1605.00519v1 [stat.ML]) https://t.co/xfigrRYhED,0,5," Abstract: Outliers are ubiquitous in modern data sets. Distance-based techniques are a popular non-parametric approach to outlier detection as they require no prior assumptions on the data generating distribution and are simple to implement. Scaling these techniques to massive data sets without sacrificing accuracy is a challenging task. We propose a novel algorithm based on the intuition that outliers have a significant influence on the quality of divergence-based clustering solutions. We propose sensitivity - the worst-case impact of a data point on the clustering objective - as a measure of outlierness. We then prove that influence, a (non-trivial) upper-bound on the sensitivity, can be computed by a simple linear time algorithm. To scale beyond a single machine, we propose a communication efficient distributed algorithm. In an extensive experimental evaluation, we demonstrate the effectiveness and establish the statistical significance of the proposed approach. In particular, it outperforms the most popular distance-based approaches while being several orders of magnitude faster. "
727298547893309440,2016-05-03 00:47:41,https://t.co/9u7y9MqfA3,"Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning. (arXiv:1605.00529v1 [stat.ML]) https://t.co/9u7y9MqfA3",0,3," Abstract: Faced with massive data, is it possible to trade off (statistical) risk, and (computational) space and time? This challenge lies at the heart of large-scale machine learning. Using k-means clustering as a prototypical unsupervised learning problem, we show how we can strategically summarize the data (control space) in order to trade off risk and time when data is generated by a probabilistic model. Our summarization is based on coreset constructions from computational geometry. We also develop an algorithm, TRAM, to navigate the space/time/data/risk tradeoff in practice. In particular, we show that for a fixed risk (or data size), as the data size increases (resp. risk increases) the running time of TRAM decreases. Our extensive experiments on real data sets demonstrate the existence and practical utility of such tradeoffs, not only for k-means but also for Gaussian Mixture Models. "
727298546823749632,2016-05-03 00:47:40,https://t.co/8jnVzcZBUB,Graph Clustering Bandits for Recommendation. (arXiv:1605.00596v1 [stat.ML]) https://t.co/8jnVzcZBUB,0,4," Abstract: We investigate an efficient context-dependent clustering technique for recommender systems based on exploration-exploitation strategies through multi-armed bandits over multiple users. Our algorithm dynamically groups users based on their observed behavioral similarity during a sequence of logged activities. In doing so, the algorithm reacts to the currently served user by shaping clusters around him/her but, at the same time, it explores the generation of clusters over users which are not currently engaged. We motivate the effectiveness of this clustering policy, and provide an extensive empirical analysis on real-world datasets, showing scalability and improved prediction performance over state-of-the-art methods for sequential clustering of users in multi-armed bandit scenarios. "
727298545540337664,2016-05-03 00:47:40,https://t.co/9s8hqhemEB,Algorithms for Learning Sparse Additive Models with Interactions in High Dimensions. (arXiv:1605.00609v1 [cs.LG]) https://t.co/9s8hqhemEB,0,7," Abstract: A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is a Sparse Additive Model (SPAM), if it is of the form $f(\mathbf{x}) = \sum_{l \in \mathcal{S}}\phi_{l}(x_l)$ where $\mathcal{S} \subset [d]$, $|\mathcal{S}| \ll d$. Assuming $\phi$'s, $\mathcal{S}$ to be unknown, there exists extensive work for estimating $f$ from its samples. In this work, we consider a generalized version of SPAMs, that also allows for the presence of a sparse number of second order interaction terms. For some $\mathcal{S}_1 \subset [d], \mathcal{S}_2 \subset {[d] \choose 2}$, with $|\mathcal{S}_1| \ll d, |\mathcal{S}_2| \ll d^2$, the function $f$ is now assumed to be of the form: $\sum_{p \in \mathcal{S}_1}\phi_{p} (x_p) + \sum_{(l,l^{\prime}) \in \mathcal{S}_2}\phi_{(l,l^{\prime})} (x_l,x_{l^{\prime}})$. Assuming we have the freedom to query $f$ anywhere in its domain, we derive efficient algorithms that provably recover $\mathcal{S}_1,\mathcal{S}_2$ with finite sample bounds. Our analysis covers the noiseless setting where exact samples of $f$ are obtained, and also extends to the noisy setting where the queries are corrupted with noise. For the noisy setting in particular, we consider two noise models namely: i.i.d Gaussian noise and arbitrary but bounded noise. Our main methods for identification of $\mathcal{S}_2$ essentially rely on estimation of sparse Hessian matrices, for which we provide two novel compressed sensing based schemes. Once $\mathcal{S}_1, \mathcal{S}_2$ are known, we show how the individual components $\phi_p$, $\phi_{(l,l^{\prime})}$ can be estimated via additional queries of $f$, with uniform error bounds. Lastly, we provide simulation results on synthetic data that validate our theoretical findings. "
727298544378503168,2016-05-03 00:47:40,https://t.co/cMGk8Qt5GV,On b-bit min-wise hashing for large-scale regression and classification with sparse data. (arXiv:1308.1269v3 [math… https://t.co/cMGk8Qt5GV,0,4," Abstract: Large-scale regression problems where both the number of variables, $p$, and the number of observations, $n$, may be large and in the order of millions or more, are becoming increasingly more common. Typically the data are sparse: only a fraction of a percent of the entries in the design matrix are non-zero. Nevertheless, often the only computationally feasible approach is to perform dimension reduction to obtain a new design matrix with far fewer columns, and then work with this compressed data. $b$-bit min-wise hashing (Li and Konig, 2011) is a promising dimension reduction scheme for sparse matrices. In this work we study the prediction error of procedures which perform regression in the new lower-dimensional space after applying the method. For both linear and logistic models we show that the average prediction error vanishes asymptotically as long as $q \|\beta^*\|_2^2 /n \rightarrow 0$, where $q$ is the average number of non-zero entries in each row of the design matrix and $\beta^*$ is the coefficient of the linear predictor. We also show that ordinary least squares or ridge regression applied to the reduced data in a sense amounts to a non-parametric regression and can in fact allow us fit more flexible models. We obtain non-asymptotic prediction error bounds for interaction models and for models where an unknown row normalisation must be applied before the signal is linear in the predictors. "
727298543166304256,2016-05-03 00:47:39,https://t.co/C2evC2DG8j,"Adaptive Concentration of Regression Trees, with Application to Random Forests. (arXiv:1503.06388v3 [math.ST] UPDA… https://t.co/C2evC2DG8j",1,3," Abstract: We study the convergence of the predictive surface of regression trees and forests. To support our analysis we introduce a notion of adaptive concentration for regression trees. This approach breaks tree training into a model selection phase in which we pick the tree splits, followed by a model fitting phase where we find the best regression model consistent with these splits. We then show that the fitted regression tree concentrates around the optimal predictor with the same splits: as d and n get large, the discrepancy is with high probability bounded on the order of sqrt(log(d) log(n)/k) uniformly over the whole regression surface, where d is the dimension of the feature space, n is the number of training examples, and k is the minimum leaf size for each tree. We also provide rate-matching lower bounds for this adaptive concentration statement. From a practical perspective, our result enables us to prove consistency results for adaptively grown forests in high dimensions, and to carry out valid post-selection inference in the sense of Berk et al. [2013] for subgroups defined by tree leaves. "
727298542155513856,2016-05-03 00:47:39,https://t.co/pB9bEBINne,Computational Cost Reduction in Learned Transform Classifications. (arXiv:1504.06779v2 [cs.CV] UPDATED) https://t.co/pB9bEBINne,0,2," Abstract: We present a theoretical analysis and empirical evaluations of a novel set of techniques for computational cost reduction of classifiers that are based on learned transform and soft-threshold. By modifying optimization procedures for dictionary and classifier training, as well as the resulting dictionary entries, our techniques allow to reduce the bit precision and to replace each floating-point multiplication by a single integer bit shift. We also show how the optimization algorithms in some dictionary training methods can be modified to penalize higher-energy dictionaries. We applied our techniques with the classifier Learning Algorithm for Soft-Thresholding, testing on the datasets used in its original paper. Our results indicate it is feasible to use solely sums and bit shifts of integers to classify at test time with a limited reduction of the classification accuracy. These low power operations are a valuable trade off in FPGA implementations as they increase the classification throughput while decrease both energy consumption and manufacturing cost. "
727298541085929476,2016-05-03 00:47:39,https://t.co/3VafoH2Thu,The Information Sieve. (arXiv:1507.02284v2 [stat.ML] UPDATED) https://t.co/3VafoH2Thu,1,4," Abstract: We introduce a new framework for unsupervised learning of representations based on a novel hierarchical decomposition of information. Intuitively, data is passed through a series of progressively fine-grained sieves. Each layer of the sieve recovers a single latent factor that is maximally informative about multivariate dependence in the data. The data is transformed after each pass so that the remaining unexplained information trickles down to the next layer. Ultimately, we are left with a set of latent factors explaining all the dependence in the original data and remainder information consisting of independent noise. We present a practical implementation of this framework for discrete variables and apply it to a variety of fundamental tasks in unsupervised learning including independent component analysis, lossy and lossless compression, and predicting missing values in data. "
727298539815055360,2016-05-03 00:47:39,https://t.co/XNMiC1PpL9,Strong Coresets for Hard and Soft Bregman Clustering with Applications to Exponential Family Mixtures. (arXiv:1508… https://t.co/XNMiC1PpL9,1,3," Abstract: Coresets are efficient representations of data sets such that models trained on the coreset are provably competitive with models trained on the original data set. As such, they have been successfully used to scale up clustering models such as K-Means and Gaussian mixture models to massive data sets. However, until now, the algorithms and the corresponding theory were usually specific to each clustering problem. We propose a single, practical algorithm to construct strong coresets for a large class of hard and soft clustering problems based on Bregman divergences. This class includes hard clustering with popular distortion measures such as the Squared Euclidean distance, the Mahalanobis distance, KL-divergence and Itakura-Saito distance. The corresponding soft clustering problems are directly related to popular mixture models due to a dual relationship between Bregman divergences and Exponential family distributions. Our theoretical results further imply a randomized polynomial-time approximation scheme for hard clustering. We demonstrate the practicality of the proposed algorithm in an empirical evaluation. "
727298538640658432,2016-05-03 00:47:38,https://t.co/vH60sR4hHl,Robust Elastic Net Regression. (arXiv:1511.04690v2 [cs.LG] UPDATED) https://t.co/vH60sR4hHl,0,2," Abstract: We propose a robust elastic net (REN) model for high-dimensional sparse regression and give its performance guarantees (both the statistical error bound and the optimization bound). A simple idea of trimming the inner product is applied to the elastic net model. Specifically, we robustify the covariance matrix by trimming the inner product based on the intuition that the trimmed inner product can not be significant affected by a bounded number of arbitrarily corrupted points (outliers). The REN model can also derive two interesting special cases: robust Lasso and robust soft thresholding. Comprehensive experimental results show that the robustness of the proposed model consistently outperforms the original elastic net and matches the performance guarantees nicely. "
727298537315250177,2016-05-03 00:47:38,https://t.co/y3eQ7aHFUM,Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks. (arXiv:1511.06390v2 [s… https://t.co/y3eQ7aHFUM,0,8," Abstract: In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method - which we dub categorical generative adversarial networks (or CatGAN) - on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM). "
727298535587233792,2016-05-03 00:47:38,https://t.co/DdsbDFICbC,Kernel-based Tests for Joint Independence. (arXiv:1603.00285v2 [math.ST] UPDATED) https://t.co/DdsbDFICbC,0,4," Abstract: We investigate the problem of testing whether $d$ random variables, which may or may not be continuous, are jointly (or mutually) independent. Our method builds on ideas of the two variable Hilbert-Schmidt independence criterion (HSIC) but allows for an arbitrary number of variables. We embed the $d$-dimensional joint distribution and the product of the marginals into a reproducing kernel Hilbert space and define the $d$-variable Hilbert-Schmidt independence criterion (dHSIC) as the squared distance between the embeddings. In the population case, the value of dHSIC is zero if and only if the $d$ variables are jointly independent, as long as the kernel is characteristic. Based on an empirical estimate of dHSIC, we define three different non-parametric hypothesis tests: a permutation test, a bootstrap test and a test based on a Gamma approximation. We prove that the permutation test achieves the significance level and that the bootstrap test achieves pointwise asymptotic significance level as well as pointwise asymptotic consistency (i.e., it is able to detect any type of fixed dependence in the large sample limit). The Gamma approximation does not come with these guarantees; however, it is computationally very fast and for small $d$, it performs well in practice. Finally, we apply the test to a problem in causal discovery. "
727298534337318915,2016-05-03 00:47:37,https://t.co/tnl0wmznpq,Gaussian Process Domain Experts for Model Adaptation in Facial Behavior Analysis. (arXiv:1604.02917v2 [stat.ML] UP… https://t.co/tnl0wmznpq,0,2," Abstract: We present a novel approach for supervised domain adaptation that is based upon the probabilistic framework of Gaussian processes (GPs). Specifically, we introduce domain-specific GPs as local experts for facial expression classification from face images. The adaptation of the classifier is facilitated in probabilistic fashion by conditioning the target expert on multiple source experts. Furthermore, in contrast to existing adaptation approaches, we also learn a target expert from available target data solely. Then, a single and confident classifier is obtained by combining the predictions from multiple experts based on their confidence. Learning of the model is efficient and requires no retraining/reweighting of the source classifiers. We evaluate the proposed approach on two publicly available datasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression classification. To this end, we perform adaptation of two contextual factors: 'where' (view) and 'who' (subject). We show in our experiments that the proposed approach consistently outperforms both source and target classifiers, while using as few as 30 target examples. It also outperforms the state-of-the-art approaches for supervised domain adaptation. "
726932327369195520,2016-05-02 00:32:27,https://t.co/omGtSblB7C,Optimal Transport vs. Fisher-Rao distance between Copulas for Clustering Multivariate Time Series. (arXiv:1604.086… https://t.co/omGtSblB7C,0,4," Abstract: We present a methodology for clustering N objects which are described by multivariate time series, i.e. several sequences of real-valued random variables. This clustering methodology leverages copulas which are distributions encoding the dependence structure between several random variables. To take fully into account the dependence information while clustering, we need a distance between copulas. In this work, we compare renowned distances between distributions: the Fisher-Rao geodesic distance, related divergences and optimal transport, and discuss their advantages and disadvantages. Applications of such methodology can be found in the clustering of financial assets. A tutorial, experiments and implementation for reproducible research can be found at www.datagrapple.com/Tech. "
726932326228353024,2016-05-02 00:32:26,https://t.co/jvkZG5fhxs,Sparse Generalized Eigenvalue Problem: Optimal Statistical Rates via Truncated Rayleigh Flow. (arXiv:1604.08697v1 … https://t.co/jvkZG5fhxs,0,5," Abstract: Sparse generalized eigenvalue problem plays a pivotal role in a large family of high-dimensional learning tasks, including sparse Fisher's discriminant analysis, canonical correlation analysis, and sufficient dimension reduction. However, the theory of sparse generalized eigenvalue problem remains largely unexplored. In this paper, we exploit a non-convex optimization perspective to study this problem. In particular, we propose the truncated Rayleigh flow method (Rifle) to estimate the leading generalized eigenvector and show that it converges linearly to a solution with the optimal statistical rate of convergence. Our theory involves two key ingredients: (i) a new analysis of the gradient descent method on non-convex objective functions, as well as (ii) a fine-grained characterization of the evolution of sparsity patterns along the solution path. Thorough numerical studies are provided to back up our theory. Finally, we apply our proposed method in the context of sparse sufficient dimension reduction to two gene expression data sets. "
726932325108428800,2016-05-02 00:32:26,https://t.co/i9YuIBeshp,Learning Compact Structural Representations for Audio Events Using Regressor Banks. (arXiv:1604.08716v1 [cs.SD]) https://t.co/i9YuIBeshp,0,3," Abstract: We introduce a new learned descriptor for audio signals which is efficient for event representation. The entries of the descriptor are produced by evaluating a set of regressors on the input signal. The regressors are class-specific and trained using the random regression forests framework. Given an input signal, each regressor estimates the onset and offset positions of the target event. The estimation confidence scores output by a regressor are then used to quantify how the target event aligns with the temporal structure of the corresponding category. Our proposed descriptor has two advantages. First, it is compact, i.e. the dimensionality of the descriptor is equal to the number of event classes. Second, we show that even simple linear classification models, trained on our descriptor, yield better accuracies on audio event classification task than not only the nonlinear baselines but also the state-of-the-art results. "
726932323934031872,2016-05-02 00:32:26,https://t.co/xbOOUBKlej,Towards Conceptual Compression. (arXiv:1604.08772v1 [stat.ML]) https://t.co/xbOOUBKlej,2,8," Abstract: We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'. "
726932322856083456,2016-05-02 00:32:26,https://t.co/9pu6VjjTVF,The Z-loss: a shift and scale invariant classification loss belonging to the Spherical Family. (arXiv:1604.08859v1… https://t.co/9pu6VjjTVF,0,4," Abstract: Despite being the standard loss function to train multi-class neural networks, the log-softmax has two potential limitations. First, it involves computations that scale linearly with the number of output classes, which can restrict the size of problems we are able to tackle with current hardware. Second, it remains unclear how close it matches the task loss such as the top-k error rate or other non-differentiable evaluation metrics which we aim to optimize ultimately. In this paper, we introduce an alternative classification loss function, the Z-loss, which is designed to address these two issues. Unlike the log-softmax, it has the desirable property of belonging to the spherical loss family (Vincent et al., 2015), a class of loss functions for which training can be performed very efficiently with a complexity independent of the number of output classes. We show experimentally that it significantly outperforms the other spherical loss functions previously investigated. Furthermore, we show on a word language modeling task that it also outperforms the log-softmax with respect to certain ranking scores, such as top-k scores, suggesting that the Z-loss has the flexibility to better match the task loss. These qualities thus makes the Z-loss an appealing candidate to train very efficiently large output networks such as word-language models or other extreme classification problems. On the One Billion Word (Chelba et al., 2014) dataset, we are able to train a model with the Z-loss 40 times faster than the log-softmax and more than 4 times faster than the hierarchical softmax. "
726932321354547201,2016-05-02 00:32:25,https://t.co/cRj44vxOVB,"Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables. (arXiv:1604.08880v1 [cs.… https://t.co/cRj44vxOVB",1,12," Abstract: Human activity recognition (HAR) in ubiquitous computing is beginning to adopt deep learning to substitute for well-established analysis techniques that rely on hand-crafted feature extraction and classification techniques. From these isolated applications of custom deep architectures it is, however, difficult to gain an overview of their suitability for problems ranging from the recognition of manipulative gestures to the segmentation and identification of physical activities like running or ascending stairs. In this paper we rigorously explore deep, convolutional, and recurrent approaches across three representative datasets that contain movement data captured with wearable sensors. We describe how to train recurrent approaches in this setting, introduce a novel regularisation approach, and illustrate how they outperform the state-of-the-art on a large benchmark dataset. Across thousands of recognition experiments with randomly sampled model configurations we investigate the suitability of each model for different tasks in HAR, explore the impact of hyperparameters using the fANOVA framework, and provide guidelines for the practitioner who wants to apply deep learning in their problem setting. "
726932319840260096,2016-05-02 00:32:25,https://t.co/TuMwmtF7xH,An efficient and expressive similarity measure for relational clustering using neighbourhood trees. (arXiv:1604.08… https://t.co/TuMwmtF7xH,0,5," Abstract: Clustering is an underspecified task: there are no universal criteria for what makes a good clustering. This is especially true for relational data, where similarity can be based on the features of individuals, the relationships between them, or a mix of both. Existing methods for relational clustering have strong and often implicit biases in this respect. In this paper, we introduce a novel similarity measure for relational data. It is the first measure to incorporate a wide variety of types of similarity, including similarity of attributes, similarity of relational context, and proximity in a hypergraph. We experimentally evaluate how using this similarity affects the quality of clustering on very different types of datasets. The experiments demonstrate that (a) using this similarity in standard clustering methods consistently gives good results, whereas other measures work well only on datasets that match their bias; and (b) on most datasets, the novel similarity outperforms even the best among the existing ones. "
726932318498263041,2016-05-02 00:32:25,https://t.co/bTd2oVmW8r,Minimum Spectral Connectivity Projection Pursuit for Unsupervised Classification. (arXiv:1509.01546v2 [stat.ML] UP… https://t.co/bTd2oVmW8r,0,5," Abstract: We study the problem of determining the optimal low dimensional projection for maximising the separability of a binary partition of an unlabelled dataset, as measured by spectral graph theory. This is achieved by finding projections which minimise the second eigenvalue of the Laplacian matrices of the projected data, which corresponds to a non-convex, non-smooth optimisation problem. We show that the optimal univariate projection based on spectral connectivity converges to the vector normal to the maximum margin hyperplane through the data, as the scaling parameter is reduced to zero. This establishes a connection between connectivity as measured by spectral graph theory and maximal Euclidean separation. It also allows us to apply our methodology to the problem of finding large margin linear separators. The computational cost associated with each eigen-problem is quadratic in the number of data. To mitigate this problem, we propose an approximation method using microclusters with provable approximation error bounds. We evaluate the performance of the proposed method on a large collection of benchmark datasets and find that it compares favourably with existing methods for projection pursuit and dimension reduction for unsupervised data partitioning. "
726932317181190145,2016-05-02 00:32:24,https://t.co/OLDjGNDU2V,Without-Replacement Sampling for Stochastic Gradient Methods: Convergence Results and Application to Distributed O… https://t.co/OLDjGNDU2V,0,3," Abstract: Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled \emph{with} replacement. In practice, however, sampling \emph{without} replacement is very common, easier to implement in many cases, and often performs better. In this paper, we provide competitive convergence guarantees for without-replacement sampling, under various scenarios, for three types of algorithms: Any algorithm with online regret guarantees, stochastic gradient descent, and SVRG. A useful application of our SVRG analysis is a nearly-optimal algorithm for regularized least squares in a distributed setting, in terms of both communication complexity and runtime complexity, when the data is randomly partitioned and the condition number can be as large as the data size per machine (up to logarithmic factors). Our proof techniques combine ideas from stochastic optimization, adversarial online learning, and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems. "
726932315654504451,2016-05-02 00:32:24,https://t.co/FY4Ug4rWLz,Fast Low-Rank Matrix Learning with Nonconvex Regularization. (arXiv:1512.00984v1 [cs.NA] CROSS LISTED) https://t.co/FY4Ug4rWLz,0,3," Abstract: Low-rank modeling has a lot of important applications in machine learning, computer vision and social network analysis. While the matrix rank is often approximated by the convex nuclear norm, the use of nonconvex low-rank regularizers has demonstrated better recovery performance. However, the resultant optimization problem is much more challenging. A very recent state-of-the-art is based on the proximal gradient algorithm. However, it requires an expensive full SVD in each proximal step. In this paper, we show that for many commonly-used nonconvex low-rank regularizers, a cutoff can be derived to automatically threshold the singular values obtained from the proximal operator. This allows the use of power method to approximate the SVD efficiently. Besides, the proximal operator can be reduced to that of a much smaller matrix projected onto this leading subspace. Convergence, with a rate of O(1/T) where T is the number of iterations, can be guaranteed. Extensive experiments are performed on matrix completion and robust principal component analysis. The proposed method achieves significant speedup over the state-of-the-art. Moreover, the matrix solution obtained is more accurate and has a lower rank than that of the traditional nuclear norm regularizer. "
725844709034332160,2016-04-29 00:30:38,https://t.co/Yp2hXXSvq4,Streaming View Learning. (arXiv:1604.08291v1 [stat.ML]) https://t.co/Yp2hXXSvq4,0,4," Abstract: An underlying assumption in conventional multi-view learning algorithms is that all views can be simultaneously accessed. However, due to various factors when collecting and pre-processing data from different views, the streaming view setting, in which views arrive in a streaming manner, is becoming more common. By assuming that the subspaces of a multi-view model trained over past views are stable, here we fine tune their combination weights such that the well-trained multi-view model is compatible with new views. This largely overcomes the burden of learning new view functions and updating past view functions. We theoretically examine convergence issues and the influence of streaming views in the proposed algorithm. Experimental results on real-world datasets suggest that studying the streaming views problem in multi-view learning is significant and that the proposed algorithm can effectively handle streaming views in different applications. "
725844706500947969,2016-04-29 00:30:38,https://t.co/j1z1pmar0O,Sequential Bayesian optimal experimental design via approximate dynamic programming. (arXiv:1604.08320v1 [stat.ME]) https://t.co/j1z1pmar0O,0,8," Abstract: The design of multiple experiments is commonly undertaken via suboptimal strategies, such as batch (open-loop) design that omits feedback or greedy (myopic) design that does not account for future effects. This paper introduces new strategies for the optimal design of sequential experiments. First, we rigorously formulate the general sequential optimal experimental design (sOED) problem as a dynamic program. Batch and greedy designs are shown to result from special cases of this formulation. We then focus on sOED for parameter inference, adopting a Bayesian formulation with an information theoretic design objective. To make the problem tractable, we develop new numerical approaches for nonlinear design with continuous parameter, design, and observation spaces. We approximate the optimal policy by using backward induction with regression to construct and refine value function approximations in the dynamic program. The proposed algorithm iteratively generates trajectories via exploration and exploitation to improve approximation accuracy in frequently visited regions of the state space. Numerical results are verified against analytical solutions in a linear-Gaussian setting. Advantages over batch and greedy design are then demonstrated on a nonlinear source inversion problem where we seek an optimal policy for sequential sensing. "
725844704005345280,2016-04-29 00:30:37,https://t.co/tR8Ov3Agvb,Two Differentially Private Rating Collection Mechanisms for Recommender Systems. (arXiv:1604.08402v1 [stat.ML]) https://t.co/tR8Ov3Agvb,0,4," Abstract: We design two mechanisms for the recommender system to collect user ratings. One is modified Laplace mechanism, and the other is randomized response mechanism. We prove that they are both differentially private and preserve the data utility. "
725844701035810816,2016-04-29 00:30:36,https://t.co/kTnH9VSrme,A Probabilistic Adaptive Search System for Exploring the Face Space. (arXiv:1604.08524v1 [stat.ML]) https://t.co/kTnH9VSrme,1,4," Abstract: Face recall is a basic human cognitive process performed routinely, e.g., when meeting someone and determining if we have met that person before. Assisting a subject during face recall by suggesting candidate faces can be challenging. One of the reasons is that the search space - the face space - is quite large and lacks structure. A commercial application of face recall is facial composite systems - such as Identikit, PhotoFIT, and CD-FIT - where a witness searches for an image of a face that resembles his memory of a particular offender. The inherent uncertainty and cost in the evaluation of the objective function, the large size and lack of structure of the search space, and the unavailability of the gradient concept makes this problem inappropriate for traditional optimization methods. In this paper we propose a novel evolutionary approach for searching the face space that can be used as a facial composite system. The approach is inspired by methods of Bayesian optimization and differs from other applications in the use of the skew-normal distribution as its acquisition function. This choice of acquisition function provides greater granularity, with regularized, conservative, and realistic results. "
725844698049437696,2016-04-29 00:30:36,https://t.co/A8NeWrslLS,Robust subspace recovery by Tyler's M-estimator. (arXiv:1206.1386v3 [stat.ML] UPDATED) https://t.co/A8NeWrslLS,0,3," Abstract: This paper considers the problem of robust subspace recovery: given a set of $N$ points in $\mathbb{R}^D$, if many lie in a $d$-dimensional subspace, then can we recover the underlying subspace? We show that Tyler's M-estimator can be used to recover the underlying subspace, if the percentage of the inliers is larger than $d/D$ and the data points lie in general position. Empirically, Tyler's M-estimator compares favorably with other convex subspace recovery algorithms in both simulations and experiments on real data sets. "
725844694257799168,2016-04-29 00:30:35,https://t.co/3YaphHy0qi,Scalable Discrete Sampling as a Multi-Armed Bandit Problem. (arXiv:1506.09039v3 [stat.ML] UPDATED) https://t.co/3YaphHy0qi,0,4," Abstract: Drawing a sample from a discrete distribution is one of the building components for Monte Carlo methods. Like other sampling algorithms, discrete sampling suffers from the high computational burden in large-scale inference problems. We study the problem of sampling a discrete random variable with a high degree of dependency that is typical in large-scale Bayesian inference and graphical models, and propose an efficient approximate solution with a subsampling approach. We make a novel connection between the discrete sampling and Multi-Armed Bandits problems with a finite reward population and provide three algorithms with theoretical guarantees. Empirical evaluations show the robustness and efficiency of the approximate algorithms in both synthetic and real-world large-scale problems. "
725844686049517568,2016-04-29 00:30:33,https://t.co/KKmgEtXaBr,Fast k-means with accurate bounds. (arXiv:1602.02514v5 [stat.ML] UPDATED) https://t.co/KKmgEtXaBr,0,7," Abstract: We propose a novel accelerated exact k-means algorithm, which performs better than the current state-of-the-art low-dimensional algorithm in 18 of 22 experiments, running up to 3 times faster. We also propose a general improvement of existing state-of-the-art accelerated exact k-means algorithms through better estimates of the distance bounds used to reduce the number of distance calculations, and get a speedup in 36 of 44 experiments, up to 1.8 times faster. We have conducted experiments with our own implementations of existing methods to ensure homogeneous evaluation of performance, and we show that our implementations perform as well or better than existing available implementations. Finally, we propose simplified variants of standard approaches and show that they are faster than their fully-fledged counterparts in 59 of 62 experiments. "
725500325164146688,2016-04-28 01:42:11,https://t.co/oa9bqmjLqv,Evaluating the effect of topic consideration in identifying communities of rating-based social networks. (arXiv:16… https://t.co/oa9bqmjLqv,0,3," Abstract: Finding meaningful communities in social network has attracted the attentions of many researchers. The community structure of complex networks reveals both their organization and hidden relations among their constituents. Most of the researches in the field of community detection mainly focus on the topological structure of the network without performing any content analysis. Nowadays, real world social networks are containing a vast range of information including shared objects, comments, following information, etc. In recent years, a number of researches have proposed approaches which consider both the contents that are interchanged in the networks and the topological structures of the networks in order to find more meaningful communities. In this research, the effect of topic analysis in finding more meaningful communities in social networking sites in which the users express their feelings toward different objects (like movies) by the means of rating is demonstrated by performing extensive experiments. "
725500324056817664,2016-04-28 01:42:11,https://t.co/8HPW2g3m1I,Distributed Flexible Nonlinear Tensor Factorization. (arXiv:1604.07928v1 [cs.LG]) https://t.co/8HPW2g3m1I,0,5," Abstract: Tensor factorization is a powerful tool to analyse multi-way data. Compared with traditional multi-linear methods, nonlinear tensor factorization models are capable of capturing more complex relationships in the data. However, they are computationally expensive and may suffer severe learning bias in case of extreme data sparsity. To overcome these limitations, in this paper we propose a distributed, flexible nonlinear tensor factorization model. Our model can effectively avoid the expensive computations and structural restrictions of the Kronecker-product in existing TGP formulations, allowing an arbitrary subset of tensorial entries to be selected to contribute to the training. At the same time, we derive a tractable and tight variational evidence lower bound (ELBO) that enables highly decoupled, parallel computations and high-quality inference. Based on the new bound, we develop a distributed inference algorithm in the MapReduce framework, which is key-value-free and can fully exploit the memory cache mechanism in fast MapReduce systems such as SPARK. Experimental results fully demonstrate the advantages of our method over several state-of-the-art approaches, in terms of both predictive performance and computational efficiency. Moreover, our approach shows a promising potential in the application of Click-Through-Rate (CTR) prediction for online advertising. "
725500322928549888,2016-04-28 01:42:10,https://t.co/xIT1KSXCVK,UBL: an R package for Utility-based Learning. (arXiv:1604.08079v1 [cs.MS]) https://t.co/xIT1KSXCVK,0,3," Abstract: This document describes the R package UBL that allows the use of several methods for handling utility-based learning problems. Classification and regression problems that assume non-uniform costs and/or benefits pose serious challenges to predictive analytic tasks. In the context of meteorology, finance, medicine, ecology, among many other, specific domain information concerning the preference bias of the users must be taken into account to enhance the models predictive performance. To deal with this problem, a large number of techniques was proposed by the research community for both classification and regression tasks. The main goal of UBL package is to facilitate the utility-based predictive analytic task by providing a set of methods to deal with this type of problems in the R environment. It is a versatile tool that provides mechanisms to handle both regression and classification (binary and multiclass) tasks. Moreover, UBL package allows the user to specify his domain preferences, but it also provides some automatic methods that try to infer those preference bias from the domain, considering some common known settings. "
725485206791159809,2016-04-28 00:42:06,https://t.co/uFsa4flLGi]),An ABC interpretation of the multiple auxiliary variable method. (arXiv:1604.08102v1 [https://t.co/uFsa4flLGi]) https://t.co/FqBIQF3p5z,4,4,INDEXERROR
725485204920504320,2016-04-28 00:42:06,https://t.co/oIi3lcl1jB,Interpretable Deep Neural Networks for Single-Trial EEG Classification. (arXiv:1604.08201v1 [cs.NE]) https://t.co/oIi3lcl1jB,1,11," Abstract: Background: In cognitive neuroscience the potential of Deep Neural Networks (DNNs) for solving complex classification tasks is yet to be fully exploited. The most limiting factor is that DNNs as notorious 'black boxes' do not provide insight into neurophysiological phenomena underlying a decision. Layer-wise Relevance Propagation (LRP) has been introduced as a novel method to explain individual network decisions. New Method: We propose the application of DNNs with LRP for the first time for EEG data analysis. Through LRP the single-trial DNN decisions are transformed into heatmaps indicating each data point's relevance for the outcome of the decision. Results: DNN achieves classification accuracies comparable to those of CSP-LDA. In subjects with low performance subject-to-subject transfer of trained DNNs can improve the results. The single-trial LRP heatmaps reveal neurophysiologically plausible patterns, resembling CSP-derived scalp maps. Critically, while CSP patterns represent class-wise aggregated information, LRP heatmaps pinpoint neural patterns to single time points in single trials. Comparison with Existing Method(s): We compare the classification performance of DNNs to that of linear CSP-LDA on two data sets related to motor-imaginery BCI. Conclusion: We have demonstrated that DNN is a powerful non-linear tool for EEG analysis. With LRP a new quality of high-resolution assessment of neural activity can be reached. LRP is a potential remedy for the lack of interpretability of DNNs that has limited their utility in neuroscientific applications. The extreme specificity of the LRP-derived heatmaps opens up new avenues for investigating neural activity underlying complex perception or decision-related processes. "
725485203645423616,2016-04-28 00:42:06,https://t.co/Dv8DBB4ZWo,"Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle. (a… https://t.co/Dv8DBB4ZWo",0,5," Abstract: While machine learning has proven to be a powerful data-driven solution to many real-life problems, its use in sensitive domains has been limited due to privacy concerns. A popular approach known as **differential privacy** offers provable privacy guarantees, but it is often observed in practice that it could substantially hamper learning accuracy. In this paper we study the learnability (whether a problem can be learned by any algorithm) under Vapnik's general learning setting with differential privacy constraint, and reveal some intricate relationships between privacy, stability and learnability. In particular, we show that a problem is privately learnable **if an only if** there is a private algorithm that asymptotically minimizes the empirical risk (AERM). In contrast, for non-private learning AERM alone is not sufficient for learnability. This result suggests that when searching for private learning algorithms, we can restrict the search to algorithms that are AERM. In light of this, we propose a conceptual procedure that always finds a universally consistent algorithm whenever the problem is learnable under privacy constraint. We also propose a generic and practical algorithm and show that under very general conditions it privately learns a wide class of learning problems. Lastly, we extend some of the results to the more practical $(\epsilon,\delta)$-differential privacy and establish the existence of a phase-transition on the class of problems that are approximately privately learnable with respect to how small $\delta$ needs to be. "
725485202580070404,2016-04-28 00:42:05,https://t.co/ZkjMoVdkPN,Influence Maximization with Bandits. (arXiv:1503.00024v4 [cs.SI] UPDATED) https://t.co/ZkjMoVdkPN,0,2," Abstract: We consider the problem of \emph{influence maximization}, the problem of maximizing the number of people that become aware of a product by finding the `best' set of `seed' users to expose the product to. Most prior work on this topic assumes that we know the probability of each user influencing each other user, or we have data that lets us estimate these influences. However, this information is typically not initially available or is difficult to obtain. To avoid this assumption, we adopt a combinatorial multi-armed bandit paradigm that estimates the influence probabilities as we sequentially try different seed sets. We establish bounds on the performance of this procedure under the existing edge-level feedback as well as a novel and more realistic node-level feedback. Beyond our theoretical results, we describe a practical implementation and experimentally demonstrate its efficiency and effectiveness on four real datasets. "
725485201409888256,2016-04-28 00:42:05,https://t.co/dS6uXN058T,Tensor vs Matrix Methods: Robust Tensor Decomposition under Block Sparse Perturbations. (arXiv:1510.04747v7 [cs.LG… https://t.co/dS6uXN058T,1,11," Abstract: Robust tensor CP decomposition involves decomposing a tensor into low rank and sparse components. We propose a novel non-convex iterative algorithm with guaranteed recovery. It alternates between low-rank CP decomposition through gradient ascent (a variant of the tensor power method), and hard thresholding of the residual. We prove convergence to the globally optimal solution under natural incoherence conditions on the low rank component, and bounded level of sparse perturbations. We compare our method with natural baselines which apply robust matrix PCA either to the {\em flattened} tensor, or to the matrix slices of the tensor. Our method can provably handle a far greater level of perturbation when the sparse tensor is block-structured. This naturally occurs in many applications such as the activity detection task in videos. Our experiments validate these findings. Thus, we establish that tensor methods can tolerate a higher level of gross corruptions compared to matrix methods. "
725485200461955072,2016-04-28 00:42:05,https://t.co/p0P9hy07DR,Change Detection in Multivariate Datastreams: Likelihood and Detectability Loss. (arXiv:1510.04850v3 [stat.ML] UPD… https://t.co/p0P9hy07DR,0,5," Abstract: We address the problem of detecting changes in multivariate datastreams, and we investigate the intrinsic difficulty that change-detection methods have to face when the data dimension scales. In particular, we consider a general approach where changes are detected by comparing the distribution of the log-likelihood of the datastream over different time windows. Despite the fact that this approach constitutes the frame of several change-detection methods, its effectiveness when data dimension scales has never been investigated, which is indeed the goal of our paper. We show that the magnitude of the change can be naturally measured by the symmetric Kullback-Leibler divergence between the pre- and post-change distributions, and that the detectability of a change of a given magnitude worsens when the data dimension increases. This problem, which we refer to as \emph{detectability loss}, is due to the linear relationship between the variance of the log-likelihood and the data dimension. We analytically derive the detectability loss on Gaussian-distributed datastreams, and empirically demonstrate that this problem holds also on real-world datasets and that can be harmful even at low data-dimensions (say, 10). "
725485199224627202,2016-04-28 00:42:05,https://t.co/64UxhaPqHK,Train and Test Tightness of LP Relaxations in Structured Prediction. (arXiv:1511.01419v3 [stat.ML] UPDATED) https://t.co/64UxhaPqHK,0,4," Abstract: Structured prediction is used in areas such as computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation to the striking observation that approximations based on linear programming (LP) relaxations are often tight on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that tightness generalizes from train to test data. "
725485197530128384,2016-04-28 00:42:04,https://t.co/4NacWmisFl,Mixtures of Sparse Autoregressive Networks. (arXiv:1511.04776v4 [stat.ML] UPDATED) https://t.co/4NacWmisFl,0,5," Abstract: We consider high-dimensional distribution estimation through autoregressive networks. By combining the concepts of sparsity, mixtures and parameter sharing we obtain a simple model which is fast to train and which achieves state-of-the-art or better results on several standard benchmark datasets. Specifically, we use an L1-penalty to regularize the conditional distributions and introduce a procedure for automatic parameter sharing between mixture components. Moreover, we propose a simple distributed representation which permits exact likelihood evaluations since the latent variables are interleaved with the observable variables and can be easily integrated out. Our model achieves excellent generalization performance and scales well to extremely high dimensions. "
725485195852402688,2016-04-28 00:42:04,https://t.co/69RvDpaKBm,"Simple, Robust and Optimal Ranking from Pairwise Comparisons. (arXiv:1512.08949v2 [cs.LG] UPDATED) https://t.co/69RvDpaKBm",0,6," Abstract: We consider data in the form of pairwise comparisons of n items, with the goal of precisely identifying the top k items for some value of k < n, or alternatively, recovering a ranking of all the items. We analyze the Copeland counting algorithm that ranks the items in order of the number of pairwise comparisons won, and show it has three attractive features: (a) its computational efficiency leads to speed-ups of several orders of magnitude in computation time as compared to prior work; (b) it is robust in that theoretical guarantees impose no conditions on the underlying matrix of pairwise-comparison probabilities, in contrast to some prior work that applies only to the BTL parametric model; and (c) it is an optimal method up to constant factors, meaning that it achieves the information-theoretic limits for recovering the top k-subset. We extend our results to obtain sharp guarantees for approximate recovery under the Hamming distortion metric, and more generally, to any arbitrary error requirement that satisfies a simple and natural monotonicity condition. "
725128373224230913,2016-04-27 01:04:11,https://t.co/aseIh6TONc,Conversational Markers of Constructive Discussions. (arXiv:1604.07407v1 [cs.CL]) https://t.co/aseIh6TONc,0,3," Abstract: Group discussions are essential for organizing every aspect of modern life, from faculty meetings to senate debates, from grant review panels to papal conclaves. While costly in terms of time and organization effort, group discussions are commonly seen as a way of reaching better decisions compared to solutions that do not require coordination between the individuals (e.g. voting)---through discussion, the sum becomes greater than the parts. However, this assumption is not irrefutable: anecdotal evidence of wasteful discussions abounds, and in our own experiments we find that over 30% of discussions are unproductive. We propose a framework for analyzing conversational dynamics in order to determine whether a given task-oriented discussion is worth having or not. We exploit conversational patterns reflecting the flow of ideas and the balance between the participants, as well as their linguistic choices. We apply this framework to conversations naturally occurring in an online collaborative world exploration game developed and deployed to support this research. Using this setting, we show that linguistic cues and conversational patterns extracted from the first 20 seconds of a team discussion are predictive of whether it will be a wasteful or a productive one. "
725128372108537861,2016-04-27 01:04:10,https://t.co/FN9QocmAcl,Learning Local Dependence In Ordered Data. (arXiv:1604.07451v1 [math.ST]) https://t.co/FN9QocmAcl,1,2," Abstract: In many applications, data come with a natural ordering. This ordering can often induce local dependence among nearby variables. However, in complex data, the width of this dependence may vary, making simple assumptions such as a constant neighborhood size unrealistic. We propose a framework for learning this local dependence based on estimating the inverse of the Cholesky factor of the covariance matrix. Penalized maximum likelihood estimation of this matrix yields a simple regression interpretation for local dependence in which variables are predicted by their neighbors. Our proposed method involves solving a convex, penalized Gaussian likelihood problem with a hierarchical group lasso penalty. The problem decomposes into independent subproblems which can be solved efficiently in parallel using first-order methods. Our method yields a sparse, symmetric, positive definite estimator of the precision matrix, encoding a Gaussian graphical model. We derive theoretical results not found in existing methods attaining this structure. In particular, our conditions for signed support recovery and estimation consistency rates in multiple norms are as mild as those in a regression problem. Empirical results show our method performing favorably compared to existing methods. We apply our method to genomic data to flexibly model linkage disequilibrium. Our method is also applied to improve the performance of discriminant analysis in sound recording classification. "
725128371181633537,2016-04-27 01:04:10,https://t.co/nlLoR1SNit,Dynamic Pricing with Demand Covariates. (arXiv:1604.07463v1 [stat.ML]) https://t.co/nlLoR1SNit,0,3," Abstract: We consider a firm that sells products over $T$ periods without knowing the demand function. The firm sequentially sets prices to earn revenue and to learn the underlying demand function simultaneously. A natural heuristic for this problem, commonly used in practice, is greedy iterative least squares (GILS). At each time period, GILS estimates the demand as a linear function of the price by applying least squares to the set of prior prices and realized demands. Then a price that maximizes the revenue, given the estimated demand function, is used for the next time period. The performance is measured by the regret, which is the expected revenue loss from the optimal (oracle) pricing policy when the demand function is known. Recently, den Boer and Zwart (2014) and Keskin and Zeevi (2014) demonstrated that GILS is sub-optimal. They introduced algorithms which integrate forced price dispersion with GILS and achieve asymptotically optimal performance. In this paper, we consider this dynamic pricing problem in a data-rich environment. In particular, we assume that the firm knows the expected demand under a particular price from historical data, and in each period, before setting the price, the firm has access to extra information (demand covariates) which may be predictive of the demand. We prove that in this setting GILS achieves asymptotically optimal regret of order $\log(T)$. We also show the following surprising result: in the original dynamic pricing problem of den Boer and Zwart (2014) and Keskin and Zeevi (2014), inclusion of any set of covariates in GILS as potential demand covariates (even though they could carry no information) would make GILS asymptotically optimal. We validate our results via extensive numerical simulations on synthetic and real data sets. "
725128369856192512,2016-04-27 01:04:10,https://t.co/lu31g7zKeU,Nonparametric Bayesian Negative Binomial Factor Analysis. (arXiv:1604.07464v1 [stat.ME]) https://t.co/lu31g7zKeU,0,5," Abstract: A common approach to analyze an attribute-instance count matrix, an element of which represents how many times an attribute appears in an instance, is to factorize it under the Poisson likelihood. We show its limitation in capturing the tendency for an attribute present in an instance to both repeat itself and excite related ones. To address this limitation, we construct negative binomial factor analysis (NBFA) to factorize the matrix under the negative binomial likelihood, and relate it to a Dirichlet-multinomial distribution based mixed-membership model. To support countably infinite factors, we propose the hierarchical gamma-negative binomial process. By exploiting newly proved connections between discrete distributions, we construct two blocked and a collapsed Gibbs sampler that all adaptively truncate their number of factors, and demonstrate that the blocked Gibbs sampler developed under a compound Poisson representation converges fast and has low computational complexity. Example results show that NBFA has a distinct mechanism in adjusting its number of inferred factors according to the instance lengths, and provides clear advantages in parsimonious representation, predictive power, and computational complexity over previously proposed discrete latent variable models, which either completely ignore burstiness, or model only the burstiness of the attributes but not that of the factors. "
725128368845393921,2016-04-27 01:04:10,https://t.co/wpkPt1gerH,Deep Multi-fidelity Gaussian Processes. (arXiv:1604.07484v1 [cs.LG]) https://t.co/wpkPt1gerH,0,4, Abstract: We develop a novel multi-fidelity framework that goes far beyond the classical AR(1) Co-kriging scheme of Kennedy and O'Hagan (2000). Our method can handle general discontinuous cross-correlations among systems with different levels of fidelity. A combination of multi-fidelity Gaussian Processes (AR(1) Co-kriging) and deep neural networks enables us to construct a method that is immune to discontinuities. We demonstrate the effectiveness of the new technology using standard benchmark problems designed to resemble the outputs of complicated high- and low-fidelity codes. 
725128367901675520,2016-04-27 01:04:09,https://t.co/PzOqq0pXU7,A New Approach in Persian Handwritten Letters Recognition Using Error Correcting Output Coding. (arXiv:1604.07554v… https://t.co/PzOqq0pXU7,0,3," Abstract: Classification Ensemble, which uses the weighed polling of outputs, is the art of combining a set of basic classifiers for generating high-performance, robust and more stable results. This study aims to improve the results of identifying the Persian handwritten letters using Error Correcting Output Coding (ECOC) ensemble method. Furthermore, the feature selection is used to reduce the costs of errors in our proposed method. ECOC is a method for decomposing a multi-way classification problem into many binary classification tasks; and then combining the results of the subtasks into a hypothesized solution to the original problem. Firstly, the image features are extracted by Principal Components Analysis (PCA). After that, ECOC is used for identification the Persian handwritten letters which it uses Support Vector Machine (SVM) as the base classifier. The empirical results of applying this ensemble method using 10 real-world data sets of Persian handwritten letters indicate that this method has better results in identifying the Persian handwritten letters than other ensemble methods and also single classifications. Moreover, by testing a number of different features, this paper found that we can reduce the additional cost in feature selection stage by using this method. "
725128366697881604,2016-04-27 01:04:09,https://t.co/xPReIzke7e,Distributed Clustering of Linear Bandits in Peer to Peer Networks. (arXiv:1604.07706v1 [cs.LG]) https://t.co/xPReIzke7e,0,3," Abstract: We provide two distributed confidence ball algorithms for solving linear bandit problems in peer to peer networks with limited communication capabilities. For the first, we assume that all the peers are solving the same linear bandit problem, and prove that our algorithm achieves the optimal asymptotic regret rate of any centralised algorithm that can instantly communicate information between the peers. For the second, we assume that there are clusters of peers solving the same bandit problem within each cluster, and we prove that our algorithm discovers these clusters, while achieving the optimal asymptotic regret rate within each one. Through experiments on several real-world datasets, we demonstrate the performance of proposed algorithms compared to the state-of-the-art. "
725128365766750208,2016-04-27 01:04:09,https://t.co/zTAhx5w4Ag,Condorcet's Jury Theorem for Consensus Clustering. (arXiv:1604.07711v1 [stat.ML]) https://t.co/zTAhx5w4Ag,0,3," Abstract: Condorcet's Jury Theorem has been invoked for ensemble classifiers to indicate that the combination of many classifiers can have better predictive performance than a single classifier. Such a theoretical underpinning is unknown for consensus clustering. This article extends Condorcet's Jury Theorem to the mean partition approach under the additional assumptions that a unique ground-truth partition exists and sample partitions are drawn from a sufficiently small ball containing the ground-truth. As an implication of practical relevance, we question the claim that the quality of consensus clustering depends on the diversity of the sample partitions. Instead, we conjecture that limiting the diversity of the mean partitions is necessary for controlling the quality. "
725128364793696258,2016-04-27 01:04:09,https://t.co/NuEmqzwjpm,Scale Normalization. (arXiv:1604.07796v1 [cs.NE]) https://t.co/NuEmqzwjpm,0,4," Abstract: One of the difficulties of training deep neural networks is caused by improper scaling between layers. Scaling issues introduce exploding / gradient problems, and have typically been addressed by careful scale-preserving initialization. We investigate the value of preserving scale, or isometry, beyond the initial weights. We propose two methods of maintaing isometry, one exact and one stochastic. Preliminary experiments show that for both determinant and scale-normalization effectively speeds up learning. Results suggest that isometry is important in the beginning of learning, and maintaining it leads to faster learning. "
725128363833221126,2016-04-27 01:04:08,https://t.co/JFPM04ipVr,Sparse PCA via Covariance Thresholding. (arXiv:1311.5179v5 [math.ST] UPDATED) https://t.co/JFPM04ipVr,0,7," Abstract: In sparse principal component analysis we are given noisy observations of a low-rank matrix of dimension $n\times p$ and seek to reconstruct it under additional sparsity assumptions. In particular, we assume here each of the principal components $\mathbf{v}_1,\dots,\mathbf{v}_r$ has at most $s_0$ non-zero entries. We are particularly interested in the high dimensional regime wherein $p$ is comparable to, or even much larger than $n$. In an influential paper, \cite{johnstone2004sparse} introduced a simple algorithm that estimates the support of the principal vectors $\mathbf{v}_1,\dots,\mathbf{v}_r$ by the largest entries in the diagonal of the empirical covariance. This method can be shown to identify the correct support with high probability if $s_0\le K_1\sqrt{n/\log p}$, and to fail with high probability if $s_0\ge K_2 \sqrt{n/\log p}$ for two constants $0<K_1,K_2<\infty$. Despite a considerable amount of work over the last ten years, no practical algorithm exists with provably better support recovery guarantees. Here we analyze a covariance thresholding algorithm that was recently proposed by \cite{KrauthgamerSPCA}. On the basis of numerical simulations (for the rank-one case), these authors conjectured that covariance thresholding correctly recover the support with high probability for $s_0\le K\sqrt{n}$ (assuming $n$ of the same order as $p$). We prove this conjecture, and in fact establish a more general guarantee including higher-rank as well as $n$ much smaller than $p$. Recent lower bounds \cite{berthet2013computational, ma2015sum} suggest that no polynomial time algorithm can do significantly better. The key technical component of our analysis develops new bounds on the norm of kernel random matrices, in regimes that were not considered before. "
725128362881064961,2016-04-27 01:04:08,https://t.co/1Rohw8lBOj,Neural network-based clustering using pairwise constraints. (arXiv:1511.06321v5 [cs.LG] UPDATED) https://t.co/1Rohw8lBOj,0,9," Abstract: This paper presents a neural network-based end-to-end clustering framework. We design a novel strategy to utilize the contrastive criteria for pushing data-forming clusters directly from raw data, in addition to learning a feature embedding suitable for such clustering. The network is trained with weak labels, specifically partial pairwise relationships between data instances. The cluster assignments and their probabilities are then obtained at the output layer by feed-forwarding the data. The framework has the interesting characteristic that no cluster centers need to be explicitly specified, thus the resulting cluster distribution is purely data-driven and no distance metrics need to be predefined. The experiments show that the proposed approach beats the conventional two-stage method (feature embedding with k-means) by a significant margin. It also compares favorably to the performance of the standard cross entropy loss for classification. Robustness analysis also shows that the method is largely insensitive to the number of clusters. Specifically, we show that the number of dominant clusters is close to the true number of clusters even when a large k is used for clustering. "
725128361916375040,2016-04-27 01:04:08,https://t.co/Nfr8IXIVQK,Regularizing RNNs by Stabilizing Activations. (arXiv:1511.08400v7 [cs.NE] UPDATED) https://t.co/Nfr8IXIVQK,0,5," Abstract: We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms. This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modeling and phoneme recognition, and outperforming weight noise and dropout. We achieve competitive performance (18.6\% PER) on the TIMIT phoneme recognition task for RNNs evaluated without beam search or an RNN transducer. With this penalty term, IRNN can achieve similar performance to LSTM on language modeling, although adding the penalty term to the LSTM results in superior performance. Our penalty term also prevents the exponential growth of IRNN's activations outside of their training horizon, allowing them to generalize to much longer sequences. "
725128360804884480,2016-04-27 01:04:08,https://t.co/CVhH8VQ2FD,Common Variable Learning and Invariant Representation Learning using Siamese Neural Networks. (arXiv:1512.08806v2 … https://t.co/CVhH8VQ2FD,0,6," Abstract: We consider the statistical problem of learning common source of variability in data which are synchronously captured by multiple sensors, and demonstrate that Siamese neural networks can be naturally applied to this problem. This approach is useful in particular in exploratory, data-driven applications, where neither a model nor label information is available. In recent years, many researchers have successfully applied Siamese neural networks to obtain an embedding of data which corresponds to a ""semantic similarity"". We present an interpretation of this ""semantic similarity"" as learning of equivalence classes. We discuss properties of the embedding obtained by Siamese networks and provide empirical results that demonstrate the ability of Siamese networks to learn common variability. "
724760500299505665,2016-04-26 00:42:23,https://t.co/yQLbmqydev,Non-convex Global Minimization and False Discovery Rate Control for the TREX. (arXiv:1604.06815v1 [stat.ML]) https://t.co/yQLbmqydev,0,5," Abstract: The TREX is a recently introduced method for performing sparse high-dimensional regression. Despite its statistical promise as an alternative to the lasso, square-root lasso, and scaled lasso, the TREX is computationally challenging in that it requires solving a non-convex optimization problem. This paper shows a remarkable result: despite the non-convexity of the TREX problem, there exists a polynomial-time algorithm that is guaranteed to find the global minimum. This result adds the TREX to a very short list of non-convex optimization problems that can be globally optimized (principal components analysis being a famous example). After deriving and developing this new approach, we demonstrate that (i) the ability of the preexisting TREX heuristic to reach the global minimum is strongly dependent on the difficulty of the underlying statistical problem, (ii) the new polynomial-time algorithm for TREX permits a novel variable ranking and selection scheme, (iii) this scheme can be incorporated into a rule that controls the false discovery rate (FDR) of included features in the model. To achieve this last aim, we provide an extension of the results of Barber & Candes (2015) to establish that the knockoff filter framework can be applied to the TREX. This investigation thus provides both a rare case study of a heuristic for non-convex optimization and a novel way of exploiting non-convexity for statistical inference. "
724760499053813760,2016-04-26 00:42:23,https://t.co/sk6KFAkoJE,"Jacques Lacan's Registers of the Psychoanalytic Field, Applied using Geometric Data Analysis to Edgar Allan Poe's … https://t.co/sk6KFAkoJE",0,4," Abstract: In a first investigation, a Lacan-motivated template of the Poe story is fitted to the data. A segmentation of the storyline is used in order to map out the diachrony. Based on this, it will be shown how synchronous aspects, potentially related to Lacanian registers, can be sought. This demonstrates the effectiveness of an approach based on a model template of the storyline narrative. In a second and more comprehensive investigation, we develop an approach for revealing, that is, uncovering, Lacanian register relationships. Objectives of this work include the wide and general application of our methodology. This methodology is strongly based on the ""letting the data speak"" Correspondence Analysis analytics platform of Jean-Paul Benz\'ecri, that is also the geometric data analysis, both qualitative and quantitative analytics, developed by Pierre Bourdieu. "
724760497095086081,2016-04-26 00:42:22,https://t.co/x7TywAZQbW,Agnostic Estimation of Mean and Covariance. (arXiv:1604.06968v1 [cs.DS]) https://t.co/x7TywAZQbW,0,1," Abstract: We consider the problem of estimating the mean and covariance of a distribution from iid samples in $\mathbb{R}^n$, in the presence of an $\eta$ fraction of malicious noise; this is in contrast to much recent work where the noise itself is assumed to be from a distribution of known type. The agnostic problem includes many interesting special cases, e.g., learning the parameters of a single Gaussian (or finding the best-fit Gaussian) when $\eta$ fraction of data is adversarially corrupted, agnostically learning a mixture of Gaussians, agnostic ICA, etc. We present polynomial-time algorithms to estimate the mean and covariance with error guarantees in terms of information-theoretic lower bounds. As a corollary, we also obtain an agnostic algorithm for Singular Value Decomposition. "
724760495799021569,2016-04-26 00:42:22,https://t.co/OVOL0dgpl4,Fast-and-Light Stochastic ADMM. (arXiv:1604.07070v1 [cs.LG]) https://t.co/OVOL0dgpl4,0,4," Abstract: The alternating direction method of multipliers (ADMM) is a powerful optimization solver in machine learning. Recently, stochastic ADMM has been integrated with variance reduction methods for stochastic gradient, leading to SAG-ADMM and SDCA-ADMM that have fast convergence rates and low iteration complexities. However, their space requirements can still be high. In this paper, we propose an integration of ADMM with the method of stochastic variance reduced gradient (SVRG). Unlike another recent integration attempt called SCAS-ADMM, the proposed algorithm retains the fast convergence benefits of SAG-ADMM and SDCA-ADMM, but is more advantageous in that its storage requirement is very low, even independent of the sample size $n$. We also extend the proposed method for nonconvex problems, and obtain a convergence rate of $O(1/T)$. Experimental results demonstrate that it is as fast as SAG-ADMM and SDCA-ADMM, much faster than SCAS-ADMM, and can be used on much bigger data sets. "
724760494209380352,2016-04-26 00:42:21,https://t.co/NeFRWwO5lz,Semi-supervised Vocabulary-informed Learning. (arXiv:1604.07093v1 [cs.CV]) https://t.co/NeFRWwO5lz,0,4," Abstract: Despite significant progress in object categorization, in recent years, a number of important challenges remain, mainly, ability to learn from limited labeled data and ability to recognize object classes within large, potentially open, set of labels. Zero-shot learning is one way of addressing these challenges, but it has only been shown to work with limited sized class vocabularies and typically requires separation between supervised and unsupervised classes, allowing former to inform the latter but not vice versa. We propose the notion of semi-supervised vocabulary-informed learning to alleviate the above mentioned challenges and address problems of supervised, zero-shot and open set recognition using a unified framework. Specifically, we propose a maximum margin framework for semantic manifold-based recognition that incorporates distance constraints from (both supervised and unsupervised) vocabulary atoms, ensuring that labeled samples are projected closest to their correct prototypes, in the embedding space, than to others. We show that resulting model shows improvements in supervised, zero-shot, and large open set recognition, with up to 310K class vocabulary on AwA and ImageNet datasets. "
724760492640751617,2016-04-26 00:42:21,https://t.co/9oyPo85yBR,Double Thompson Sampling for Dueling Bandits. (arXiv:1604.07101v1 [cs.LG]) https://t.co/9oyPo85yBR,0,3," Abstract: In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for dueling bandit problems. As indicated by its name, D-TS selects both the first and the second candidates according to Thompson Sampling. Specifically, D-TS maintains a posterior distribution for the preference matrix, and chooses the pair of arms for comparison by sampling twice from the posterior distribution. This simple algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as its special case. For general Copeland dueling bandits, we show that D-TS achieves $O(K^2 \log T)$ regret. For Condorcet dueling bandits, we further simplify the D-TS algorithm and show that the simplified D-TS algorithm achieves $O(K \log T + K^2 \log \log T)$ regret. Simulation results based on both synthetic and real-world data demonstrate the efficiency of the proposed D-TS algorithm. "
724760491051110401,2016-04-26 00:42:21,https://t.co/oV051DnnAy,Neural Random Forests. (arXiv:1604.07143v1 [stat.ML]) https://t.co/oV051DnnAy,2,11," Abstract: Given an ensemble of randomized regression trees, it is possible to restructure them as a collection of multilayered neural networks with particular connection weights. Following this principle, we reformulate the random forest method of Breiman (2001) into a neural network setting, and in turn propose two new hybrid procedures that we call neural random forests. Both predictors exploit prior knowledge of regression trees for their architecture, have less parameters to tune than standard networks, and less restrictions on the geometry of the decision boundaries. Consistency results are proved, and substantial numerical evidence is provided on both synthetic and real data sets to assess the excellent performance of our methods in a large variety of prediction problems. "
724760488882622464,2016-04-26 00:42:20,https://t.co/fSxb7dtSAh,Weighted Spectral Cluster Ensemble. (arXiv:1604.07178v1 [cs.LG]) https://t.co/fSxb7dtSAh,0,3," Abstract: Clustering explores meaningful patterns in the non-labeled data sets. Cluster Ensemble Selection (CES) is a new approach, which can combine individual clustering results for increasing the performance of the final results. Although CES can achieve better final results in comparison with individual clustering algorithms and cluster ensemble methods, its performance can be dramatically affected by its consensus diversity metric and thresholding procedure. There are two problems in CES: 1) most of the diversity metrics is based on heuristic Shannon's entropy and 2) estimating threshold values are really hard in practice. The main goal of this paper is proposing a robust approach for solving the above mentioned problems. Accordingly, this paper develops a novel framework for clustering problems, which is called Weighted Spectral Cluster Ensemble (WSCE), by exploiting some concepts from community detection arena and graph based clustering. Under this framework, a new version of spectral clustering, which is called Two Kernels Spectral Clustering, is used for generating graphs based individual clustering results. Further, by using modularity, which is a famous metric in the community detection, on the transformed graph representation of individual clustering results, our approach provides an effective diversity estimation for individual clustering results. Moreover, this paper introduces a new approach for combining the evaluated individual clustering results without the procedure of thresholding. Experimental study on varied data sets demonstrates that the prosed approach achieves superior performance to state-of-the-art methods. "
724760487594983425,2016-04-26 00:42:20,https://t.co/Wi9Vur1MmA,Fast nonlinear embeddings via structured matrices. (arXiv:1604.07356v1 [stat.ML]) https://t.co/Wi9Vur1MmA,0,4," Abstract: We present a new paradigm for speeding up randomized computations of several frequently used functions in machine learning. In particular, our paradigm can be applied for improving computations of kernels based on random embeddings. Above that, the presented framework covers multivariate randomized functions. As a byproduct, we propose an algorithmic approach that also leads to a significant reduction of space complexity. Our method is based on careful recycling of Gaussian vectors into structured matrices that share properties of fully random matrices. The quality of the proposed structured approach follows from combinatorial properties of the graphs encoding correlations between rows of these structured matrices. Our framework covers as special cases already known structured approaches such as the Fast Johnson-Lindenstrauss Transform, but is much more general since it can be applied also to highly nonlinear embeddings. We provide strong concentration results showing the quality of the presented paradigm. "
724760485854351360,2016-04-26 00:42:19,https://t.co/GW5Q5Qtdut,Matrix completion with column manipulation: Near-optimal sample-robustness-rank tradeoffs. (arXiv:1102.2254v2 [sta… https://t.co/GW5Q5Qtdut,0,2," Abstract: This paper considers the problem of matrix completion when some number of the columns are completely and arbitrarily corrupted, potentially by a malicious adversary. It is well-known that standard algorithms for matrix completion can return arbitrarily poor results, if even a single column is corrupted. One direct application comes from robust collaborative filtering. Here, some number of users are so-called manipulators who try to skew the predictions of the algorithm by calibrating their inputs to the system. In this paper, we develop an efficient algorithm for this problem based on a combination of a trimming procedure and a convex program that minimizes the nuclear norm and the $\ell_{1,2}$ norm. Our theoretical results show that given a vanishing fraction of observed entries, it is nevertheless possible to complete the underlying matrix even when the number of corrupted columns grows. Significantly, our results hold without any assumptions on the locations or values of the observed entries of the manipulated columns. Moreover, we show by an information-theoretic argument that our guarantees are nearly optimal in terms of the fraction of sampled entries on the authentic columns, the fraction of corrupted columns, and the rank of the underlying matrix. Our results therefore sharply characterize the tradeoffs between sample, robustness and rank in matrix completion. "
724760484780580864,2016-04-26 00:42:19,https://t.co/5hdPoNUu4N,A Probabilistic $\ell_1$ Method for Clustering High Dimensional Data. (arXiv:1504.01294v2 [math.ST] UPDATED) https://t.co/5hdPoNUu4N,0,3," Abstract: In general, the clustering problem is NP-hard, and global optimality cannot be established for non-trivial instances. For high-dimensional data, distance-based methods for clustering or classification face an additional difficulty, the unreliability of distances in very high-dimensional spaces. We propose a distance-based iterative method for clustering data in very high-dimensional space, using the $\ell_1$-metric that is less sensitive to high dimensionality than the Euclidean distance. For $K$ clusters in $\mathbb{R}^n$, the problem decomposes to $K$ problems coupled by probabilities, and an iteration reduces to finding $Kn$ weighted medians of points on a line. The complexity of the algorithm is linear in the dimension of the data space, and its performance was observed to improve significantly as the dimension increases. "
724760483782336512,2016-04-26 00:42:19,https://t.co/DHdf7aWGTh,Learning to Select Pre-Trained Deep Representations with Bayesian Evidence Framework. (arXiv:1506.02565v4 [cs.CV] … https://t.co/DHdf7aWGTh,1,2," Abstract: We propose a Bayesian evidence framework to facilitate transfer learning from pre-trained deep convolutional neural networks (CNNs). Our framework is formulated on top of a least squares SVM (LS-SVM) classifier, which is simple and fast in both training and testing, and achieves competitive performance in practice. The regularization parameters in LS-SVM is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing CNN out of multiple candidates for transfer learning; the evidence is optimized efficiently by employing Aitken's delta-squared process, which accelerates convergence of fixed point update. The proposed Bayesian evidence framework also provides a good solution to identify the best ensemble of heterogeneous CNNs through a greedy algorithm. Our Bayesian evidence framework for transfer learning is tested on 12 visual recognition datasets and illustrates the state-of-the-art performance consistently in terms of prediction accuracy and modeling efficiency. "
724760482800869376,2016-04-26 00:42:19,https://t.co/gVrywYfQCv,Global Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation. (arXiv:1506.07405v2 [cs.N… https://t.co/gVrywYfQCv,0,3," Abstract: It has been observed in a variety of contexts that gradient descent methods have great success in solving low-rank matrix factorization problems, despite the relevant problem formulation being non-convex. We tackle a particular instance of this scenario, where we seek the $d$-dimensional subspace spanned by a streaming data matrix. We apply the natural first order incremental gradient descent method, constraining the gradient method to the Grassmannian. In this paper, we propose an adaptive step size scheme that is greedy for the noiseless case, that maximizes the improvement of our metric of convergence at each data index $t$, and yields an expected improvement for the noisy case. We show that, with noise-free data, this method converges from any random initialization to the global minimum of the problem. For noisy data, we provide the expected convergence rate of the proposed algorithm per iteration. "
724760481857191937,2016-04-26 00:42:18,https://t.co/rB126us7zb,Learning vector autoregressive models with focalised Granger-causality graphs. (arXiv:1507.01978v2 [cs.LG] UPDATED) https://t.co/rB126us7zb,0,2," Abstract: We consider the problem of learning models for forecasting multiple time-series systems together with discovering the leading indicators that serve as good predictors for the system. We model the systems by linear vector autoregressive models (VAR) and link the discovery of leading indicators to inferring sparse graphs of Granger-causality. We propose new problem formulations and develop two new methods to learn such models, gradually increasing the complexity of assumptions and approaches. While the first method assumes common structures across the whole system, our second method uncovers model clusters based on the Granger-causality and leading indicators together with learning the model parameters. We study the performance of our methods on a comprehensive set of experiments and confirm their efficacy and their advantages over state-of-the-art sparse VAR and graphical Granger learning methods. "
724760480724717572,2016-04-26 00:42:18,https://t.co/fFErguBNdv,Robust Sparse Blind Source Separation. (arXiv:1507.02216v2 [stat.AP] UPDATED) https://t.co/fFErguBNdv,0,2," Abstract: Blind Source Separation is a widely used technique to analyze multichannel data. In many real-world applications, its results can be significantly hampered by the presence of unknown outliers. In this paper, a novel algorithm coined rGMCA (robust Generalized Morphological Component Analysis) is introduced to retrieve sparse sources in the presence of outliers. It explicitly estimates the sources, the mixing matrix, and the outliers. It also takes advantage of the estimation of the outliers to further implement a weighting scheme, which provides a highly robust separation procedure. Numerical experiments demonstrate the efficiency of rGMCA to estimate the mixing matrix in comparison with standard BSS techniques. "
724760479122464768,2016-04-26 00:42:18,https://t.co/SAqF7xtiW1,When Are Nonconvex Problems Not Scary?. (arXiv:1510.06096v2 [math.OC] UPDATED) https://t.co/SAqF7xtiW1,0,4," Abstract: In this note, we focus on smooth nonconvex optimization problems that obey: (1) all local minimizers are also global; and (2) around any saddle point or local maximizer, the objective has a negative directional curvature. Concrete applications such as dictionary learning, generalized phase retrieval, and orthogonal tensor decomposition are known to induce such structures. We describe a second-order trust-region algorithm that provably converges to a global minimizer efficiently, without special initializations. Finally we highlight alternatives, and open problems in this direction. "
724760478094888960,2016-04-26 00:42:18,https://t.co/iiYHnzyreR,Optimization as Estimation with Gaussian Processes in Bandit Settings. (arXiv:1510.06423v3 [stat.ML] UPDATED) https://t.co/iiYHnzyreR,0,2," Abstract: Recently, there has been rising interest in Bayesian optimization -- the optimization of an unknown function with assumptions usually expressed by a Gaussian Process (GP) prior. We study an optimization strategy that directly uses an estimate of the argmax of the function. This strategy offers both practical and theoretical advantages: no tradeoff parameter needs to be selected, and, moreover, we establish close connections to the popular GP-UCB and GP-PI strategies. Our approach can be understood as automatically and adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We illustrate the effects of this adaptive tuning via bounds on the regret as well as an extensive empirical evaluation on robotics and vision tasks, demonstrating the robustness of this strategy for a range of performance criteria. "
724760477126037504,2016-04-26 00:42:17,https://t.co/9fTjdKDLlr,A note on the evaluation of generative models. (arXiv:1511.01844v3 [stat.ML] UPDATED) https://t.co/9fTjdKDLlr,0,8," Abstract: Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided. "
724760476006096896,2016-04-26 00:42:17,https://t.co/su5rcHuumg,Efficient AUC Optimization for Information Ranking Applications. (arXiv:1511.05202v3 [cs.IR] UPDATED) https://t.co/su5rcHuumg,0,2," Abstract: Adequate evaluation of an information retrieval system to estimate future performance is a crucial task. Area under the ROC curve (AUC) is widely used to evaluate the generalization of a retrieval system. However, the objective function optimized in many retrieval systems is the error rate and not the AUC value. This paper provides an efficient and effective non-linear approach to optimize AUC using additive regression trees, with a special emphasis on the use of multi-class AUC (MAUC) because multiple relevance levels are widely used in many ranking applications. Compared to a conventional linear approach, the performance of the non-linear approach is comparable on binary-relevance benchmark datasets and is better on multi-relevance benchmark datasets. "
724760474835947520,2016-04-26 00:42:17,https://t.co/2yIzhwWxW9,Performance Limits of Online Stochastic Sub-Gradient Learning. (arXiv:1511.07902v2 [stat.ML] UPDATED) https://t.co/2yIzhwWxW9,0,2," Abstract: This work examines the performance of stochastic sub-gradient learning strategies under weaker conditions than usually considered in the literature. The conditions are shown to be automatically satisfied by several important cases of interest including the construction of Linear-SVM, LASSO, and Total-Variation denoising formulations. In comparison, these problems do not satisfy the traditional assumptions automatically and, therefore, conclusions derived based on these earlier assumptions are not directly applicable to these problems. The analysis establishes that stochastic sub-gradient strategies can attain exponential convergence rates, as opposed to sub-linear rates, to the steady-state. A realizable exponential-weighting procedure is proposed to smooth the intermediate iterates by the sub-gradient procedure and to guarantee the established performance bounds in terms of convergence rate and excessive risk performance. Both single-agent and multi-agent scenarios are studied, where the latter case assumes that a collection of agents are interconnected by a topology and can only interact locally with their neighbors. The theoretical conclusions are illustrated by several examples and simulations, including comparisons with the FISTA procedure. "
724760473829269505,2016-04-26 00:42:17,https://t.co/6bkxfKHFfo,Practical Algorithms for Learning Near-Isometric Linear Embeddings. (arXiv:1601.00062v2 [stat.ML] UPDATED) https://t.co/6bkxfKHFfo,0,4," Abstract: We propose two practical non-convex approaches for learning near-isometric, linear embeddings of finite sets of data points. Given a set of training points $\mathcal{X}$, we consider the secant set $S(\mathcal{X})$ that consists of all pairwise difference vectors of $\mathcal{X}$, normalized to lie on the unit sphere. The problem can be formulated as finding a symmetric and positive semi-definite matrix $\boldsymbol{\Psi}$ that preserves the norms of all the vectors in $S(\mathcal{X})$ up to a distortion parameter $\delta$. Motivated by non-negative matrix factorization, we reformulate our problem into a Frobenius norm minimization problem, which is solved by the Alternating Direction Method of Multipliers (ADMM) and develop an algorithm, FroMax. Another method solves for a projection matrix $\boldsymbol{\Psi}$ by minimizing the restricted isometry property (RIP) directly over the set of symmetric, postive semi-definite matrices. Applying ADMM and a Moreau decomposition on a proximal mapping, we develop another algorithm, NILE-Pro, for dimensionality reduction. FroMax is shown to converge faster for smaller $\delta$ while NILE-Pro converges faster for larger $\delta$. Both non-convex approaches are then empirically demonstrated to be more computationally efficient than prior convex approaches for a number of applications in machine learning and signal processing. "
724760472516448256,2016-04-26 00:42:16,https://t.co/tsv8q6TcjV,Dimensionality-Dependent Generalization Bounds for $k$-Dimensional Coding Schemes. (arXiv:1601.00238v2 [stat.ML] U… https://t.co/tsv8q6TcjV,0,3," Abstract: The $k$-dimensional coding schemes refer to a collection of methods that attempt to represent data using a set of representative $k$-dimensional vectors, and include non-negative matrix factorization, dictionary learning, sparse coding, $k$-means clustering and vector quantization as special cases. Previous generalization bounds for the reconstruction error of the $k$-dimensional coding schemes are mainly dimensionality independent. A major advantage of these bounds is that they can be used to analyze the generalization error when data is mapped into an infinite- or high-dimensional feature space. However, many applications use finite-dimensional data features. Can we obtain dimensionality-dependent generalization bounds for $k$-dimensional coding schemes that are tighter than dimensionality-independent bounds when data is in a finite-dimensional feature space? The answer is positive. In this paper, we address this problem and derive a dimensionality-dependent generalization bound for $k$-dimensional coding schemes by bounding the covering number of the loss function class induced by the reconstruction error. The bound is of order $\mathcal{O}\left(\left(mk\ln(mkn)/n\right)^{\lambda_n}\right)$, where $m$ is the dimension of features, $k$ is the number of the columns in the linear implementation of coding schemes, $n$ is the size of sample, $\lambda_n>0.5$ when $n$ is finite and $\lambda_n=0.5$ when $n$ is infinite. We show that our bound can be tighter than previous results, because it avoids inducing the worst-case upper bound on $k$ of the loss function and converges faster. The proposed generalization bound is also applied to some specific coding schemes to demonstrate that the dimensionality-dependent bound is an indispensable complement to these dimensionality-independent generalization bounds. "
724760470385733632,2016-04-26 00:42:16,https://t.co/IckJnzzgZK,A Minimalistic Approach to Sum-Product Network Learning for Real Applications. (arXiv:1602.04259v3 [cs.AI] UPDATED) https://t.co/IckJnzzgZK,0,3," Abstract: Sum-Product Networks (SPNs) are a class of expressive yet tractable hierarchical graphical models. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identifying similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete and there is no missing data. We introduce a practical, simplified version of LearnSPN, MiniSPN, that runs faster and can handle missing data and heterogeneous features common in real applications. We demonstrate the performance of MiniSPN on standard benchmark datasets and on two datasets from Google's Knowledge Graph exhibiting high missingness rates and a mix of discrete and continuous features. "
724395851020378112,2016-04-25 00:33:24,https://t.co/NgAwasfJbu,Stabilized Sparse Online Learning for Sparse Data. (arXiv:1604.06498v1 [stat.ML]) https://t.co/NgAwasfJbu,0,4," Abstract: Stochastic gradient descent (SGD) is commonly used for optimization in large-scale machine learning problems. Langford et al. (2009) introduce a sparse online learning method to induce sparsity via truncated gradient. With high-dimensional sparse data, however, the method suffers from slow convergence and high variance due to the heterogeneity in feature sparsity. To mitigate this issue, we introduce a stabilized truncated stochastic gradient descent algorithm. We employ a soft-thresholding scheme on the weight vector where the imposed shrinkage is adaptive to the amount of information available in each feature. The variability in the resulted sparse weight vector is further controlled by stability selection integrated with the informative truncation. To facilitate better convergence, we adopt an annealing strategy on the truncation rate, which leads to a balanced trade-off between exploration and exploitation in learning a sparse weight vector. Numerical experiments show that our algorithm compares favorably with the original algorithm in terms of prediction accuracy, achieved sparsity and stability. "
724395849808240640,2016-04-25 00:33:23,https://t.co/3z24zT9Oba,The Mean Partition Theorem of Consensus Clustering. (arXiv:1604.06626v1 [cs.LG]) https://t.co/3z24zT9Oba,0,5," Abstract: To devise efficient solutions for approximating a mean partition in consensus clustering, Dimitriadou et al. [3] presented a necessary condition of optimality for a consensus function based on least square distances. We show that their result is pivotal for deriving interesting properties of consensus clustering beyond optimization. For this, we present the necessary condition of optimality in a slightly stronger form in terms of the Mean Partition Theorem and extend it to the Expected Partition Theorem. To underpin its versatility, we show three examples that apply the Mean Partition Theorem: (i) equivalence of the mean partition and optimal multiple alignment, (ii) construction of profiles and motifs, and (iii) relationship between consensus clustering and cluster stability. "
724395847996313600,2016-04-25 00:33:23,https://t.co/8a8OetoZr9,$\gamma$-regression: Robust and Sparse Regression based on $\gamma$-divergence. (arXiv:1604.06637v1 [stat.ME]) https://t.co/8a8OetoZr9,1,3," Abstract: In high-dimensional data, many sparse regression methods have been proposed. However, they may not be robust against outliers. Recently, the use of density power weight has been studied for robust parameter estimation and the corresponding divergences have been discussed. One of such divergences is the $\gamma$-divergence and the robust estimator using the $\gamma$-divergence is known for having a strong robustness. In this paper, we consider the robust and sparse regression based on $\gamma$-divergence. We extend the $\gamma$-divergence to the regression problem and show that it has a strong robustness under heavy contamination even when outliers are heterogeneous. The loss function is constructed by an empirical estimate of the $\gamma$-divergence with sparse regularization and the parameter estimate is defined as the minimizer of the loss function. To obtain the robust and sparse estimate, we propose an efficient update algorithm which has a monotone decreasing property of the loss function. Particularly, we discuss a linear regression problem with $L_1$ regularization in detail. In numerical experiments and real data analyses, we see that the proposed method outperforms past robust and sparse methods. "
724395847069384704,2016-04-25 00:33:23,https://t.co/kse9vngedA,An improved chromosome formulation for genetic algorithms applied to variable selection with the inclusion of inte… https://t.co/kse9vngedA,0,4," Abstract: Genetic algorithms are a well-known method for tackling the problem of variable selection. As they are non-parametric and can use a large variety of fitness functions, they are well-suited as a variable selection wrapper that can be applied to many different models. In almost all cases, the chromosome formulation used in these genetic algorithms consists of a binary vector of length n for n potential variables indicating the presence or absence of the corresponding variables. While the aforementioned chromosome formulation has exhibited good performance for relatively small n, there are potential problems when the size of n grows very large, especially when interaction terms are considered. We introduce a modification to the standard chromosome formulation that allows for better scalability and model sparsity when interaction terms are included in the predictor search space. Experimental results show that the indexed chromosome formulation demonstrates improved computational efficiency and sparsity on high-dimensional datasets with interaction terms compared to the standard chromosome formulation. "
724395846058577920,2016-04-25 00:33:22,https://t.co/sYk2Zr18JY,Developing an ICU scoring system with interaction terms using a genetic algorithm. (arXiv:1604.06730v1 [cs.LG]) https://t.co/sYk2Zr18JY,0,3," Abstract: ICU mortality scoring systems attempt to predict patient mortality using predictive models with various clinical predictors. Examples of such systems are APACHE, SAPS and MPM. However, most such scoring systems do not actively look for and include interaction terms, despite physicians intuitively taking such interactions into account when making a diagnosis. One barrier to including such terms in predictive models is the difficulty of using most variable selection methods in high-dimensional datasets. A genetic algorithm framework for variable selection with logistic regression models is used to search for two-way interaction terms in a clinical dataset of adult ICU patients, with separate models being built for each category of diagnosis upon admittance to the ICU. The models had good discrimination across all categories, with a weighted average AUC of 0.84 (>0.90 for several categories) and the genetic algorithm was able to find several significant interaction terms, which may be able to provide greater insight into mortality prediction for health practitioners. The GA selected models had improved performance against stepwise selection and random forest models, and provides greater flexibility in terms of variable selection by being able to optimize over any modeler-defined model performance metric instead of a specific variable importance metric. "
724395844246614016,2016-04-25 00:33:22,https://t.co/8JtRK3mnYr,Learning a Tree-Structured Ising Model in Order to Make Predictions. (arXiv:1604.06749v1 [math.ST]) https://t.co/8JtRK3mnYr,0,4," Abstract: We study the problem of learning a tree graphical model from samples such that low-order marginals are accurate. We define a distance (""small set TV"" or ssTV) between distributions $P$ and $Q$ by taking the maximum, over all subsets $\mathcal{S}$ of a given size, of the total variation between the marginals of P and Q on $\mathcal{S}$. Approximating a distribution to within small ssTV allows making predictions based on partial observations. Focusing on pairwise marginals and tree-structured Ising models on $p$ nodes with maximum edge strength $\beta$, we prove that $\max\{e^{2\beta}\log p, \eta^{-2}\log(p/\eta)\} $ i.i.d. samples suffices to get a distribution (from the same class) with ssTV at most $\eta$ from the one generating the samples. "
724395842946359296,2016-04-25 00:33:22,https://t.co/nMW1JHnZZF,Thompson Sampling is Asymptotically Optimal in General Environments. (arXiv:1602.07905v1 [cs.LG] CROSS LISTED) https://t.co/nMW1JHnZZF,2,12," Abstract: We discuss a variant of Thompson sampling for nonparametric reinforcement learning in a countable classes of general stochastic environments. These environments can be non-Markov, non-ergodic, and partially observable. We show that Thompson sampling learns the environment class in the sense that (1) asymptotically its value converges to the optimal value in mean and (2) given a recoverability assumption regret is sublinear. "
724395840807288833,2016-04-25 00:33:21,https://t.co/ZgxYqZWEcP,Loss Bounds and Time Complexity for Speed Priors. (arXiv:1604.03343v1 [cs.LG] CROSS LISTED) https://t.co/ZgxYqZWEcP,0,3," Abstract: This paper establishes for the first time the predictive performance of speed priors and their computational complexity. A speed prior is essentially a probability distribution that puts low probability on strings that are not efficiently computable. We propose a variant to the original speed prior (Schmidhuber, 2002), and show that our prior can predict sequences drawn from probability measures that are estimable in polynomial time. Our speed prior is computable in doubly-exponential time, but not in polynomial time. On a polynomial time computable sequence our speed prior is computable in exponential time. We show better upper complexity bounds for Schmidhuber's speed prior under the same conditions, and that it predicts deterministic sequences that are computable in polynomial time; however, we also show that it is not computable in polynomial time, and the question of its predictive properties for stochastic sequences remains open. "
723313656797253633,2016-04-22 00:53:08,https://t.co/L2oG1j2FEm,Dynamic matrix factorization with social influence. (arXiv:1604.06194v1 [stat.ML]) https://t.co/L2oG1j2FEm,2,6," Abstract: Matrix factorization is a key component of collaborative filtering-based recommendation systems because it allows us to complete sparse user-by-item ratings matrices under a low-rank assumption that encodes the belief that similar users give similar ratings and that similar items garner similar ratings. This paradigm has had immeasurable practical success, but it is not the complete story for understanding and inferring the preferences of people. First, peoples' preferences and their observable manifestations as ratings evolve over time along general patterns of trajectories. Second, an individual person's preferences evolve over time through influence of their social connections. In this paper, we develop a unified process model for both types of dynamics within a state space approach, together with an efficient optimization scheme for estimation within that model. The model combines elements from recent developments in dynamic matrix factorization, opinion dynamics and social learning, and trust-based recommendation. The estimation builds upon recent advances in numerical nonlinear optimization. Empirical results on a large-scale data set from the Epinions website demonstrate consistent reduction in root mean squared error by consideration of the two types of dynamics. "
723313655799005184,2016-04-22 00:53:08,https://t.co/baf0PxhtO3,Markov models for ocular fixation locations in the presence and absence of colour. (arXiv:1604.06335v1 [stat.AP]) https://t.co/baf0PxhtO3,0,2," Abstract: We propose to model the fixation locations of the human eye when observing a still image by a Markovian point process in R 2 . Our approach is data driven using k-means clustering of the fixation locations to identify distinct salient regions of the image, which in turn correspond to the states of our Markov chain. Bayes factors are computed as model selection criterion to determine the number of clusters. Furthermore, we demonstrate that the behaviour of the human eye differs from this model when colour information is removed from the given image. "
723313654876221441,2016-04-22 00:53:08,https://t.co/G8UIKalm0V,Robust Estimators in High Dimensions without the Computational Intractability. (arXiv:1604.06443v1 [cs.DS]) https://t.co/G8UIKalm0V,0,3," Abstract: We study high-dimensional distribution learning in an agnostic setting where an adversary is allowed to arbitrarily corrupt an $\varepsilon$-fraction of the samples. Such questions have a rich history spanning statistics, machine learning and theoretical computer science. Even in the most basic settings, the only known approaches are either computationally inefficient or lose dimension-dependent factors in their error guarantees. This raises the following question:Is high-dimensional agnostic distribution learning even possible, algorithmically? In this work, we obtain the first computationally efficient algorithms with dimension-independent error guarantees for agnostically learning several fundamental classes of high-dimensional distributions: (1) a single Gaussian, (2) a product distribution on the hypercube, (3) mixtures of two product distributions (under a natural balancedness condition), and (4) mixtures of spherical Gaussians. Our algorithms achieve error that is independent of the dimension, and in many cases scales nearly-linearly with the fraction of adversarially corrupted samples. Moreover, we develop a general recipe for detecting and correcting corruptions in high-dimensions, that may be applicable to many other problems. "
723313653424992256,2016-04-22 00:53:08,https://t.co/NqAQxEumkX,Sketching for Sequential Change-Point Detection. (arXiv:1505.06770v2 [cs.LG] UPDATED) https://t.co/NqAQxEumkX,0,6," Abstract: We study sequential change-point detection using sketches (linear projections) of high-dimensional signal vectors, by presenting the sketching procedures that are derived based on the generalized likelihood ratio statistic. We consider both fixed and time-varying projections, and derive theoretical approximations to two fundamental performance metrics: the average run length (ARL) and the expected detection delay (EDD); these approximations are shown to be highly accurate by numerical simulations. We also characterize the performance of the procedure when the projection is a Gaussian random projection or a sparse 0-1 matrix (in particular, an expander graph). Finally, we demonstrate the good performance of the sketching performance using simulation and real-data examples on solar flare detection and failure detection in power networks. "
723313651948634113,2016-04-22 00:53:07,https://t.co/WBp18ZezZj,Bayesian Approximate Kernel Regression with Variable Selection. (arXiv:1508.01217v2 [stat.ME] UPDATED) https://t.co/WBp18ZezZj,1,6," Abstract: Nonlinear kernel regression models are often used in statistics and machine learning due to greater accuracy than linear models. Variable selection for kernel regression models is a challenge partly because, unlike the linear regression setting, there is no clear concept of an effect size for regression coefficients. In this paper, we propose a novel framework that provides an analog of the effect size of each explanatory variable for Bayesian kernel regression models when the kernel is shift-invariant---for example the Gaussian kernel. We use function analytic properties of shift-invariant reproducing kernel Hilbert spaces (RKHS) to define a linear vector space that (1) captures nonlinear structure and (2) can be projected onto the original explanatory variables. The projection onto the original explanatory variables serves as the analog of effect sizes. The specific function analytic property we use is that shift-invariant kernel functions can be approximated via random Fourier bases. Based on the random Fourier expansion we propose a computationally efficient class of Bayesian approximate kernel regression (BAKR) models for both nonlinear regression and binary classification for which one can compute an analog of effect sizes. By adapting some classical results in compressive sensing we state conditions under which BAKR can recover a sparse set of effect sizes, simultaneous variable selection and regression. We illustrate the utility of BAKR by examining, in some detail, two important problems in statistical genetics: genomic selection (predicting phenotype from genotype) and association mapping (inference of significant variables or loci). State-of-the-art methods for genomic selection and association mapping are based on kernel regression and linear models, respectively. BAKR is the first method that is competitive in both settings. "
723313650770055168,2016-04-22 00:53:07,https://t.co/JUAs2WjeOk,Sparse group factor analysis for biclustering of multiple data sources. (arXiv:1512.08808v2 [cs.LG] UPDATED) https://t.co/JUAs2WjeOk,0,4," Abstract: Motivation: Modelling methods that find structure in data are necessary with the current large volumes of genomic data, and there have been various efforts to find subsets of genes exhibiting consistent patterns over subsets of treatments. These biclustering techniques have focused on one data source, often gene expression data. We present a Bayesian approach for joint biclustering of multiple data sources, extending a recent method Group Factor Analysis (GFA) to have a biclustering interpretation with additional sparsity assumptions. The resulting method enables data-driven detection of linear structure present in parts of the data sources. Results: Our simulation studies show that the proposed method reliably infers bi-clusters from heterogeneous data sources. We tested the method on data from the NCI-DREAM drug sensitivity prediction challenge, resulting in an excellent prediction accuracy. Moreover, the predictions are based on several biclusters which provide insight into the data sources, in this case on gene expression, DNA methylation, protein abundance, exome sequence, functional connectivity fingerprints and drug sensitivity. "
723313649075556353,2016-04-22 00:53:07,https://t.co/QRt8QsYb7d,Stationary signal processing on graphs. (arXiv:1601.02522v3 [cs.DS] UPDATED) https://t.co/QRt8QsYb7d,0,3," Abstract: Graphs are a central tool in machine learning and information processing as they allow to conveniently capture the structure of complex datasets. In this context, it is of high importance to develop flexible models of signals defined over graphs or networks. In this paper, we generalize the traditional concept of wide sense stationarity to signals defined over the vertices of arbitrary weighted undirected graphs. We show that stationarity is intimately linked to statistical invariance under a localization operator reminiscent of translation. We prove that stationary graph signals are characterized by a well-defined Power Spectral Density that can be efficiently estimated even for large graphs. We leverage this new concept to derive Wiener-type estimation procedures of noisy and partially observed signals and illustrate the performance of this new model for denoising and regression. "
722947447429931008,2016-04-21 00:37:57,https://t.co/4VMk5Mw6ZR,Trading-Off Cost of Deployment Versus Accuracy in Learning Predictive Models. (arXiv:1604.05819v1 [stat.ML]) https://t.co/4VMk5Mw6ZR,1,4," Abstract: Predictive models are finding an increasing number of applications in many industries. As a result, a practical means for trading-off the cost of deploying a model versus its effectiveness is needed. Our work is motivated by risk prediction problems in healthcare. Cost-structures in domains such as healthcare are quite complex, posing a significant challenge to existing approaches. We propose a novel framework for designing cost-sensitive structured regularizers that is suitable for problems with complex cost dependencies. We draw upon a surprising connection to boolean circuits. In particular, we represent the problem costs as a multi-layer boolean circuit, and then use properties of boolean circuits to define an extended feature vector and a group regularizer that exactly captures the underlying cost structure. The resulting regularizer may then be combined with a fidelity function to perform model prediction, for example. For the challenging real-world application of risk prediction for sepsis in intensive care units, the use of our regularizer leads to models that are in harmony with the underlying cost structure and thus provide an excellent prediction accuracy versus cost tradeoff. "
722947445525782528,2016-04-21 00:37:57,https://t.co/SBxNbcnIY6,A Factorization Machine Framework for Testing Bigram Embeddings in Knowledgebase Completion. (arXiv:1604.05878v1 [… https://t.co/SBxNbcnIY6,4,10," Abstract: Embedding-based Knowledge Base Completion models have so far mostly combined distributed representations of individual entities or relations to compute truth scores of missing links. Facts can however also be represented using pairwise embeddings, i.e. embeddings for pairs of entities and relations. In this paper we explore such bigram embeddings with a flexible Factorization Machine model and several ablations from it. We investigate the relevance of various bigram types on the fb15k237 dataset and find relative improvements compared to a compositional model. "
722947442816196608,2016-04-21 00:37:56,https://t.co/e9BVH4Q0j5,Computational Drug Repositioning Using Continuous Self-controlled Case Series. (arXiv:1604.05976v1 [stat.AP]) https://t.co/e9BVH4Q0j5,0,2," Abstract: Computational Drug Repositioning (CDR) is the task of discovering potential new indications for existing drugs by mining large-scale heterogeneous drug-related data sources. Leveraging the patient-level temporal ordering information between numeric physiological measurements and various drug prescriptions provided in Electronic Health Records (EHRs), we propose a Continuous Self-controlled Case Series (CSCCS) model for CDR. As an initial evaluation, we look for drugs that can control Fasting Blood Glucose (FBG) level in our experiments. Applying CSCCS to the Marshfield Clinic EHR, well-known drugs that are indicated for controlling blood glucose level are rediscovered. Furthermore, some drugs with recent literature support for the potential effect of blood glucose level control are also identified. "
722947439779520512,2016-04-21 00:37:56,https://t.co/MPPYpACFbJ,Constructive Preference Elicitation by Setwise Max-margin Learning. (arXiv:1604.06020v1 [stat.ML]) https://t.co/MPPYpACFbJ,0,4," Abstract: In this paper we propose an approach to preference elicitation that is suitable to large configuration spaces beyond the reach of existing state-of-the-art approaches. Our setwise max-margin method can be viewed as a generalization of max-margin learning to sets, and can produce a set of ""diverse"" items that can be used to ask informative queries to the user. Moreover, the approach can encourage sparsity in the parameter space, in order to favor the assessment of utility towards combinations of weights that concentrate on just few features. We present a mixed integer linear programming formulation and show how our approach compares favourably with Bayesian preference elicitation alternatives and easily scales to realistic datasets. "
722947437766291456,2016-04-21 00:37:55,https://t.co/495seECiyl,Random Projection Estimation of Discrete-Choice Models with Large Choice Sets. (arXiv:1604.06036v1 [stat.ML]) https://t.co/495seECiyl,0,4," Abstract: We introduce sparse random projection, an important dimension-reduction tool from machine learning, for the estimation of discrete-choice models with high-dimensional choice sets. Initially, high-dimensional data are compressed into a lower-dimensional Euclidean space using random projections. Subsequently, estimation proceeds using cyclic monotonicity moment inequalities implied by the multinomial choice model; the estimation procedure is semi-parametric and does not require explicit distributional assumptions to be made regarding the random utility errors. The random projection procedure is justified via the Johnson-Lindenstrauss Lemma -- the pairwise distances between data points are preserved during data compression, which we exploit to show convergence of our estimator. The estimator works well in simulations and in an application to a supermarket scanner dataset. "
722947435765592064,2016-04-21 00:37:55,https://t.co/PMeluM7bmO,Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation. (arXiv:1604.0… https://t.co/PMeluM7bmO,2,16," Abstract: Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'. "
722947433265774593,2016-04-21 00:37:54,https://t.co/QEWmjKvxfS,Mondrian Forests for Large-Scale Regression when Uncertainty Matters. (arXiv:1506.03805v3 [stat.ML] UPDATED) https://t.co/QEWmjKvxfS,0,4," Abstract: Many real-world regression problems demand a measure of the uncertainty associated with each prediction. Standard decision forests deliver efficient state-of-the-art predictive performance, but high-quality uncertainty estimates are lacking. Gaussian processes (GPs) deliver uncertainty estimates, but scaling GPs to large-scale data sets comes at the cost of approximating the uncertainty estimates. We extend Mondrian forests, first proposed by Lakshminarayanan et al. (2014) for classification problems, to the large-scale non-parametric regression setting. Using a novel hierarchical Gaussian prior that dovetails with the Mondrian forest framework, we obtain principled uncertainty estimates, while still retaining the computational advantages of decision forests. Through a combination of illustrative examples, real-world large-scale datasets, and Bayesian optimization benchmarks, we demonstrate that Mondrian forests outperform approximate GPs on large-scale regression tasks and deliver better-calibrated uncertainty assessments than decision-forest-based methods. "
722947431034413056,2016-04-21 00:37:53,https://t.co/yTXMNOedlT,Clustering of Sparse and Approximately Sparse Graphs by Semidefinite Programming. (arXiv:1603.05296v2 [math.OC] UP… https://t.co/yTXMNOedlT,0,4," Abstract: As a model problem for clustering, we consider the densest k-disjoint-clique problem of partitioning a weighted complete graph into k disjoint subgraphs such that the sum of the densities of these subgraphs is maximized. We establish that such subgraphs can be recovered from the solution of a particular semidefinite relaxation with high probability if the input graph is sampled from a distribution of clusterable graphs. Specifically, the semidefinite relaxation is exact if the graph consists of k large disjoint subgraphs, corresponding to clusters, with weight concentrated within these subgraphs, plus a moderate number of outliers. Further, we establish that if noise is weakly obscuring these clusters, i.e, the between-cluster edges are assigned very small weights, then we can recover significantly smaller clusters. For example, we show that in approximately sparse graphs, where the between-cluster weights tend to zero as the size n of the graph tends to infinity, we can recover clusters of size polylogarithmic in n. Empirical evidence from numerical simulations is also provided to support these theoretical phase transitions to perfect recovery of the cluster structure. "
722589985204006913,2016-04-20 00:57:32,https://t.co/fT0gRAFReo,Learning Sparse Additive Models with Interactions in High Dimensions. (arXiv:1604.05307v1 [cs.LG]) https://t.co/fT0gRAFReo,0,7," Abstract: A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is referred to as a Sparse Additive Model (SPAM), if it is of the form $f(\mathbf{x}) = \sum_{l \in \mathcal{S}}\phi_{l}(x_l)$, where $\mathcal{S} \subset [d]$, $|\mathcal{S}| \ll d$. Assuming $\phi_l$'s and $\mathcal{S}$ to be unknown, the problem of estimating $f$ from its samples has been studied extensively. In this work, we consider a generalized SPAM, allowing for second order interaction terms. For some $\mathcal{S}_1 \subset [d], \mathcal{S}_2 \subset {[d] \choose 2}$, the function $f$ is assumed to be of the form: $$f(\mathbf{x}) = \sum_{p \in \mathcal{S}_1}\phi_{p} (x_p) + \sum_{(l,l^{\prime}) \in \mathcal{S}_2}\phi_{(l,l^{\prime})} (x_{l},x_{l^{\prime}}).$$ Assuming $\phi_{p},\phi_{(l,l^{\prime})}$, $\mathcal{S}_1$ and, $\mathcal{S}_2$ to be unknown, we provide a randomized algorithm that queries $f$ and exactly recovers $\mathcal{S}_1,\mathcal{S}_2$. Consequently, this also enables us to estimate the underlying $\phi_p, \phi_{(l,l^{\prime})}$. We derive sample complexity bounds for our scheme and also extend our analysis to include the situation where the queries are corrupted with noise -- either stochastic, or arbitrary but bounded. Lastly, we provide simulation results on synthetic data, that validate our theoretical findings. "
722589984201523200,2016-04-20 00:57:31,https://t.co/zGrtsi15jL,Churn analysis using deep convolutional neural networks and autoencoders. (arXiv:1604.05377v1 [stat.ML]) https://t.co/zGrtsi15jL,1,13," Abstract: Customer temporal behavioral data was represented as images in order to perform churn prediction by leveraging deep learning architectures prominent in image classification. Supervised learning was performed on labeled data of over 6 million customers using deep convolutional neural networks, which achieved an AUC of 0.743 on the test dataset using no more than 12 temporal features for each customer. Unsupervised learning was conducted using autoencoders to better understand the reasons for customer churn. Images that maximally activate the hidden units of an autoencoder trained with churned customers reveal ample opportunities for action to be taken to prevent churn among strong data, no voice users. "
722589983190736901,2016-04-20 00:57:31,https://t.co/pnhF2G9uus,Triplet Probabilistic Embedding for Face Verification and Clustering. (arXiv:1604.05417v1 [cs.CV]) https://t.co/pnhF2G9uus,0,2," Abstract: Despite significant progress made over the past twenty five years, unconstrained face verification remains a challenging problem. This paper proposes an approach that couples a deep CNN-based approach with a low-dimensional discriminative embedding learned using triplet probability constraints to solve the unconstrained face verification problem. Aside from yielding performance improvements, this embedding provides significant advantages in terms of memory and for post-processing operations like subject specific clustering. Experiments on the challenging IJB-A dataset show that the proposed algorithm performs comparably or better than the state of the art methods in verification and identification metrics, while requiring much less training data and training time. The superior performance of the proposed method on the CFP dataset shows that the representation learned by our deep CNN is robust to extreme pose variation. Furthermore, we demonstrate the robustness of the deep features to challenges including age, pose, blur and clutter by performing simple clustering experiments on both IJB-A and LFW datasets. "
722589982142111745,2016-04-20 00:57:31,https://t.co/Qv5wXW4Psw,Streaming Label Learning for Modeling Labels on the Fly. (arXiv:1604.05449v1 [stat.ML]) https://t.co/Qv5wXW4Psw,0,5," Abstract: It is challenging to handle a large volume of labels in multi-label learning. However, existing approaches explicitly or implicitly assume that all the labels in the learning process are given, which could be easily violated in changing environments. In this paper, we define and study streaming label learning (SLL), i.e., labels are arrived on the fly, to model newly arrived labels with the help of the knowledge learned from past labels. The core of SLL is to explore and exploit the relationships between new labels and past labels and then inherit the relationship into hypotheses of labels to boost the performance of new classifiers. In specific, we use the label self-representation to model the label relationship, and SLL will be divided into two steps: a regression problem and a empirical risk minimization (ERM) problem. Both problems are simple and can be efficiently solved. We further show that SLL can generate a tighter generalization error bound for new labels than the general ERM framework with trace norm or Frobenius norm regularization. Finally, we implement extensive experiments on various benchmark datasets to validate the new setting. And results show that SLL can effectively handle the constantly emerging new labels and provides excellent classification performance. "
722589981093556224,2016-04-20 00:57:31,https://t.co/4J1z1lUwjm,Scaling-up Empirical Risk Minimization: Optimization of Incomplete U-statistics. (arXiv:1501.02629v4 [stat.ML] UPD… https://t.co/4J1z1lUwjm,0,4," Abstract: In a wide range of statistical learning problems such as ranking, clustering or metric learning among others, the risk is accurately estimated by $U$-statistics of degree $d\geq 1$, i.e. functionals of the training data with low variance that take the form of averages over $k$-tuples. From a computational perspective, the calculation of such statistics is highly expensive even for a moderate sample size $n$, as it requires averaging $O(n^d)$ terms. This makes learning procedures relying on the optimization of such data functionals hardly feasible in practice. It is the major goal of this paper to show that, strikingly, such empirical risks can be replaced by drastically computationally simpler Monte-Carlo estimates based on $O(n)$ terms only, usually referred to as incomplete $U$-statistics, without damaging the $O_{\mathbb{P}}(1/\sqrt{n})$ learning rate of Empirical Risk Minimization (ERM) procedures. For this purpose, we establish uniform deviation results describing the error made when approximating a $U$-process by its incomplete version under appropriate complexity assumptions. Extensions to model selection, fast rate situations and various sampling techniques are also considered, as well as an application to stochastic gradient descent for ERM. Finally, numerical examples are displayed in order to provide strong empirical evidence that the approach we promote largely surpasses more naive subsampling techniques. "
722589979839500290,2016-04-20 00:57:30,https://t.co/6KAdzWlRVj,Fast Saddle-Point Algorithm for Generalized Dantzig Selector and FDR Control with the Ordered l1-Norm. (arXiv:1511… https://t.co/6KAdzWlRVj,0,4," Abstract: In this paper we propose a primal-dual proximal extragradient algorithm to solve the generalized Dantzig selector (GDS) estimation problem, based on a new convex-concave saddle-point (SP) reformulation. Our new formulation makes it possible to adopt recent developments in saddle-point optimization, to achieve the optimal $O(1/k)$ rate of convergence. Compared to the optimal non-SP algorithms, ours do not require specification of sensitive parameters that affect algorithm performance or solution quality. We also provide a new analysis showing a possibility of local acceleration to achieve the rate of $O(1/k^2)$ in special cases even without strong convexity or strong smoothness. As an application, we propose a GDS equipped with the ordered $\ell_1$-norm, showing its false discovery rate control properties in variable selection. Algorithm performance is compared between ours and other alternatives, including the linearized ADMM, Nesterov's smoothing, Nemirovski's mirror-prox, and the accelerated hybrid proximal extragradient techniques. "
722224751943528448,2016-04-19 00:46:13,https://t.co/PGq3c46SOh,Parallelizing Word2Vec in Shared and Distributed Memory. (arXiv:1604.04661v1 [cs.DC]) https://t.co/PGq3c46SOh,1,9," Abstract: Word2Vec is a widely used algorithm for extracting low-dimensional vector representations of words. It generated considerable excitement in the machine learning and natural language processing (NLP) communities recently due to its exceptional performance in many NLP applications such as named entity recognition, sentiment analysis, machine translation and question answering. State-of-the-art algorithms including those by Mikolov et al. have been parallelized for multi-core CPU architectures but are based on vector-vector operations that are memory-bandwidth intensive and do not efficiently use computational resources. In this paper, we improve reuse of various data structures in the algorithm through the use of minibatching, hence allowing us to express the problem using matrix multiply operations. We also explore different techniques to distribute word2vec computation across nodes in a compute cluster, and demonstrate good strong scalability up to 32 nodes. In combination, these techniques allow us to scale up the computation near linearly across cores and nodes, and process hundreds of millions of words per second, which is the fastest word2vec implementation to the best of our knowledge. "
722224750811025408,2016-04-19 00:46:13,https://t.co/dSOuVWE8mI,DS-MLR: Exploiting Double Separability for Scaling up Distributed Multinomial Logistic Regression. (arXiv:1604.047… https://t.co/dSOuVWE8mI,0,2," Abstract: Multinomial logistic regression is a popular tool in the arsenal of machine learning algorithms, yet scaling it to datasets with very large number of data points and classes has not been trivial. This is primarily because one needs to compute the log-partition function on every data point. This makes distributing the computation hard. In this paper, we present a distributed stochastic gradient descent based optimization method (DS-MLR) for scaling up multinomial logistic regression problems to very large data. Our algorithm exploits double-separability, an attractive property we observe in the objective functions of several models in machine learning, that allows us to distribute both data and model parameters simultaneously across multiple machines. In addition to being easily parallelizable, our algorithm achieves good test accuracy within a short period of time, with a low overall time and memory footprint as demonstrated by empirical results on both single and multi-machine settings. For instance, on a dataset with 93,805 training instances and 12,294 classes, we achieve close to optimal f-score in 10,000 seconds using 2 machines each having 12 cores. "
722224749573750784,2016-04-19 00:46:13,https://t.co/qehR3BmFqG,Smoothed Hierarchical Dirichlet Process: A Non-Parametric Approach to Constraint Measures. (arXiv:1604.04741v1 [st… https://t.co/qehR3BmFqG,0,7," Abstract: Time-varying mixture densities occur in many scenarios, for example, the distributions of keywords that appear in publications may evolve from year to year, video frame features associated with multiple targets may evolve in a sequence. Any models that realistically cater to this phenomenon must exhibit two important properties: the underlying mixture densities must have an unknown number of mixtures, and there must be some ""smoothness"" constraints in place for the adjacent mixture densities. The traditional Hierarchical Dirichlet Process (HDP) may be suited to the first property, but certainly not the second. This is due to how each random measure in the lower hierarchies is sampled independent of each other and hence does not facilitate any temporal correlations. To overcome such shortcomings, we proposed a new Smoothed Hierarchical Dirichlet Process (sHDP). The key novelty of this model is that we place a temporal constraint amongst the nearby discrete measures $\{G_j\}$ in the form of symmetric Kullback-Leibler (KL) Divergence with a fixed bound $B$. Although the constraint we place only involves a single scalar value, it nonetheless allows for flexibility in the corresponding successive measures. Remarkably, it also led us to infer the model within the stick-breaking process where the traditional Beta distribution used in stick-breaking is now replaced by a new constraint calculated from $B$. We present the inference algorithm and elaborate on its solutions. Our experiment using NIPS keywords has shown the desirable effect of the model. "
722224748315467777,2016-04-19 00:46:12,https://t.co/NDehV9ZpLA,"Probabilistic Receiver Architecture Combining BP, MF, and EP for Multi-Signal Detection. (arXiv:1604.04834v1 [cs.I… https://t.co/NDehV9ZpLA",0,2," Abstract: Receiver algorithms which combine belief propagation (BP) with the mean field (MF) approximation are well-suited for inference of both continuous and discrete random variables. In wireless scenarios involving detection of multiple signals, the standard construction of the combined BP-MF framework includes the equalization or multi-user detection functions within the MF subgraph. In this paper, we show that the MF approximation is not particularly effective for multi-signal detection. We develop a new factor graph construction for application of the BP-MF framework to problems involving the detection of multiple signals. We then develop a low-complexity variant to the proposed construction in which Gaussian BP is applied to the equalization factors. In this case, the factor graph of the joint probability distribution is divided into three subgraphs: (i) a MF subgraph comprised of the observation factors and channel estimation, (ii) a Gaussian BP subgraph which is applied to multi-signal detection, and (iii) a discrete BP subgraph which is applied to demodulation and decoding. Expectation propagation is used to approximate discrete distributions with a Gaussian distribution and links the discrete BP and Gaussian BP subgraphs. The result is a probabilistic receiver architecture with strong theoretical justification which can be applied to multi-signal detection. "
722224747342389249,2016-04-19 00:46:12,https://t.co/xVtM3CerR5,Regularizing Solutions to the MEG Inverse Problem Using Space-Time Separable Covariance Functions. (arXiv:1604.049… https://t.co/xVtM3CerR5,0,2," Abstract: In magnetoencephalography (MEG) the conventional approach to source reconstruction is to solve the underdetermined inverse problem independently over time and space. Here we present how the conventional approach can be extended by regularizing the solution in space and time by a Gaussian process (Gaussian random field) model. Assuming a separable covariance function in space and time, the computational complexity of the proposed model becomes (without any further assumptions or restrictions) $\mathcal{O}(t^3 + n^3 + m^2n)$, where $t$ is the number of time steps, $m$ is the number of sources, and $n$ is the number of sensors. We apply the method to both simulated and empirical data, and demonstrate the efficiency and generality of our Bayesian source reconstruction approach which subsumes various classical approaches in the literature. "
722224746105069569,2016-04-19 00:46:12,https://t.co/jICCIdz1oS,Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor Analysis. (arXiv:1604.04939v1 [stat.ML]) https://t.co/jICCIdz1oS,0,5," Abstract: Factor analysis aims to determine latent factors, or traits, which summarize a given data set. Inter-battery factor analysis extends this notion to multiple views of the data. In this paper we show how a nonlinear, nonparametric version of these models can be recovered through the Gaussian process latent variable model. This gives us a flexible formalism for multi-view learning where the latent variables can be used both for exploratory purposes and for learning representations that enable efficient inference for ambiguous estimation tasks. Learning is performed in a Bayesian manner through the formulation of a variational compression scheme which gives a rigorous lower bound on the log likelihood. Our Bayesian framework provides strong regularization during training, allowing the structure of the latent space to be determined efficiently and automatically. We demonstrate this by producing the first (to our knowledge) published results of learning from dozens of views, even when data is scarce. We further show experimental results on several different types of multi-view data sets and for different kinds of tasks, including exploratory data analysis, generation, ambiguity modelling through latent priors and classification. "
722224744972554243,2016-04-19 00:46:12,https://t.co/ompTsHgsHZ,Global optimization of factor models using alternating minimization. (arXiv:1604.04942v1 [stat.ML]) https://t.co/ompTsHgsHZ,0,2," Abstract: Learning new representations in machine learning is often tackled using a factorization of the data. For many such problems, including sparse coding and matrix completion, learning these factorizations can be difficult, in terms of efficiency and to guarantee that the solution is a global minimum. Recently, a general class of objectives have been introduced, called induced regularized factor models (RFMs), which have an induced convex form that enables global optimization. Though attractive theoretically, this induced form is impractical, particularly for large or growing datasets. In this work, we investigate the use of a practical alternating minimization algorithms for induced RFMs, that ensure convergence to global optima. We characterize the stationary points of these models, and, using these insights, highlight practical choices for the objectives. We then provide theoretical and empirical evidence that alternating minimization, from a random initialization, converges to global minima for a large subclass of induced RFMs. In particular, we prove that induced RFMs do not have degenerate saddlepoints and that local minima are actually global minima. Finally, we provide an extensive investigation into practical optimization choices for using alternating minimization for induced RFMs, for both batch and stochastic gradient descent. "
722224743924019200,2016-04-19 00:46:11,https://t.co/6goUdhJCQz,Gaussian Copula Variational Autoencoders for Mixed Data. (arXiv:1604.04960v1 [stat.ML]) https://t.co/6goUdhJCQz,2,10," Abstract: The variational autoencoder (VAE) is a generative model with continuous latent variables where a pair of probabilistic encoder (bottom-up) and decoder (top-down) is jointly learned by stochastic gradient variational Bayes. We first elaborate Gaussian VAE, approximating the local covariance matrix of the decoder as an outer product of the principal direction at a position determined by a sample drawn from Gaussian distribution. We show that this model, referred to as VAE-ROC, better captures the data manifold, compared to the standard Gaussian VAE where independent multivariate Gaussian was used to model the decoder. Then we extend the VAE-ROC to handle mixed categorical and continuous data. To this end, we employ Gaussian copula to model the local dependency in mixed categorical and continuous data, leading to {\em Gaussian copula variational autoencoder} (GCVAE). As in VAE-ROC, we use the rank-one approximation for the covariance in the Gaussian copula, to capture the local dependency structure in the mixed data. Experiments on various datasets demonstrate the useful behaviour of VAE-ROC and GCVAE, compared to the standard VAE. "
722224742892179460,2016-04-19 00:46:11,https://t.co/2mefoZ1HuG,Memory controls time perception and intertemporal choices. (arXiv:1604.05129v1 [q-bio.NC]) https://t.co/2mefoZ1HuG,0,4," Abstract: There is a consensus that human and non-human subjects experience temporal distortions in many stages of their perceptual and decision-making systems. Similarly, intertemporal choice research has shown that decision-makers undervalue future outcomes relative to immediate ones. Here we combine techniques from information theory and artificial intelligence to show how both temporal distortions and intertemporal choice preferences can be explained as a consequence of the coding efficiency of sensorimotor representation. In particular, the model implies that interactions that constrain future behavior are perceived as being both longer in duration and more valuable. Furthermore, using simulations of artificial agents, we investigate how memory constraints enforce a renormalization of the perceived timescales. Our results show that qualitatively different discount functions, such as exponential and hyperbolic discounting, arise as a consequence of an agent's probabilistic model of the world. "
722224742015623168,2016-04-19 00:46:11,https://t.co/wHlSwIBQ14,Locally Imposing Function for Generalized Constraint Neural Networks - A Study on Equality Constraints. (arXiv:160… https://t.co/wHlSwIBQ14,1,2," Abstract: This work is a further study on the Generalized Constraint Neural Network (GCNN) model [1], [2]. Two challenges are encountered in the study, that is, to embed any type of prior information and to select its imposing schemes. The work focuses on the second challenge and studies a new constraint imposing scheme for equality constraints. A new method called locally imposing function (LIF) is proposed to provide a local correction to the GCNN prediction function, which therefore falls within Locally Imposing Scheme (LIS). In comparison, the conventional Lagrange multiplier method is considered as Globally Imposing Scheme (GIS) because its added constraint term exhibits a global impact to its objective function. Two advantages are gained from LIS over GIS. First, LIS enables constraints to fire locally and explicitly in the domain only where they need on the prediction function. Second, constraints can be implemented within a network setting directly. We attempt to interpret several constraint methods graphically from a viewpoint of the locality principle. Numerical examples confirm the advantages of the proposed method. In solving boundary value problems with Dirichlet and Neumann constraints, the GCNN model with LIF is possible to achieve an exact satisfaction of the constraints. "
722224740849557505,2016-04-19 00:46:11,https://t.co/A4fILwIV9a,"Kernel Distribution Embeddings: Universal Kernels, Characteristic Kernels and Kernel Metrics on Distributions. (ar… https://t.co/A4fILwIV9a",0,8," Abstract: Kernel mean embeddings have recently attracted the attention of the machine learning community. They map measures $\mu$ from some set $M$ to functions in a reproducing kernel Hilbert space (RKHS) with kernel $k$. The RKHS distance of two mapped measures is a semi-metric $d_k$ over $M$. We study three questions. (I) For a given kernel, what sets $M$ can be embedded? (II) When is the embedding injective over $M$ (in which case $d_k$ is a metric)? (III) How does the $d_k$-induced topology compare to other topologies on $M$? The existing machine learning literature has addressed these questions in cases where $M$ is (a subset of) the finite regular Borel measures. We unify, improve and generalise those results. Our approach naturally leads to continuous and possibly even injective embeddings of (Schwartz-) distributions, i.e., generalised measures, but the reader is free to focus on measures only. In particular, we systemise and extend various (partly known) equivalences between different notions of universal, characteristic and strictly positive definite kernels, and show that on an underlying locally compact Hausdorff space, $d_k$ metrises the weak convergence of probability measures if and only if $k$ is continuous and characteristic. "
722224739775811584,2016-04-19 00:46:10,https://t.co/R26srapiEp,Chained Gaussian Processes. (arXiv:1604.05263v1 [stat.ML]) https://t.co/R26srapiEp,0,8," Abstract: Gaussian process models are flexible, Bayesian non-parametric approaches to regression. Properties of multivariate Gaussians mean that they can be combined linearly in the manner of additive models and via a link function (like in generalized linear models) to handle non-Gaussian data. However, the link function formalism is restrictive, link functions are always invertible and must convert a parameter of interest to a linear combination of the underlying processes. There are many likelihoods and models where a non-linear combination is more appropriate. We term these more general models Chained Gaussian Processes: the transformation of the GPs to the likelihood parameters will not generally be invertible, and that implies that linearisation would only be possible with multiple (localized) links, i.e. a chain. We develop an approximate inference procedure for Chained GPs that is scalable and applicable to any factorized likelihood. We demonstrate the approximation on a range of likelihood functions. "
722224738794348546,2016-04-19 00:46:10,https://t.co/jXibrgYhKg,Finding Common Characteristics Among NBA Playoff Teams: A Machine Learning Approach. (arXiv:1604.05266v1 [stat.ML]) https://t.co/jXibrgYhKg,0,3," Abstract: In this paper, we employ machine learning techniques to analyze sixteen seasons of NBA regular season data from every team to determine the common characteristics among NBA playoff teams. Each team was characterized by 42 predictor variables and one binary response variable taking on a value of ""TRUE"" if a team had made the playoffs, and value of ""FALSE"" if a team had missed the playoffs. After fitting an initial classification tree to this problem, this tree was then pruned which decreased the test error rate. Further to this, a random forest of classification trees was grown which provided a very accurate model from which a variable importance plot was generated to determine which predictor variables had the greatest influence on the response variable. The result of this work was the conclusion that the most important factors in characterizing a team's playoff eligibility are the opponent field goal percentage and the opponent points per game. This seems to suggest that \emph{defensive} factors as opposed to offensive factors are the most important characteristics shared among NBA playoff teams. We also perform a classification analysis to determine common characteristics among NBA championship teams. Using an artificial neural network structure, we show that championship teams must be able to have very strong defensive characteristics, in particular, strong perimeter defense characteristics in combination with an effective half-court offense that generates high-percentage two-point shots. A key part of this offensive strategy must also be the ability to draw fouls. This analysis will hopefully dispel the rising notion that an offense geared towards shooting many three point shots is a sufficient and necessary condition for an NBA team to be successful in qualifying for the playoffs and winning a championship. "
722224737850691584,2016-04-19 00:46:10,https://t.co/JRa9KC2tCI,Learning Sparse Low-Threshold Linear Classifiers. (arXiv:1212.3276v3 [stat.ML] UPDATED) https://t.co/JRa9KC2tCI,0,4," Abstract: We consider the problem of learning a non-negative linear classifier with a $1$-norm of at most $k$, and a fixed threshold, under the hinge-loss. This problem generalizes the problem of learning a $k$-monotone disjunction. We prove that we can learn efficiently in this setting, at a rate which is linear in both $k$ and the size of the threshold, and that this is the best possible rate. We provide an efficient online learning algorithm that achieves the optimal rate, and show that in the batch case, empirical risk minimization achieves this rate as well. The rates we show are tighter than the uniform convergence rate, which grows with $k^2$. "
722224736659456000,2016-04-19 00:46:10,https://t.co/iMVTXe7MNF,Loss minimization and parameter estimation with heavy tails. (arXiv:1307.1827v7 [cs.LG] UPDATED) https://t.co/iMVTXe7MNF,0,5," Abstract: This work studies applications and generalizations of a simple estimation technique that provides exponential concentration under heavy-tailed distributions, assuming only bounded low-order moments. We show that the technique can be used for approximate minimization of smooth and strongly convex losses, and specifically for least squares linear regression. For instance, our $d$-dimensional estimator requires just $\tilde{O}(d\log(1/\delta))$ random samples to obtain a constant factor approximation to the optimal least squares loss with probability $1-\delta$, without requiring the covariates or noise to be bounded or subgaussian. We provide further applications to sparse linear regression and low-rank covariance matrix estimation with similar allowances on the noise and covariate distributions. The core technique is a generalization of the median-of-means estimator to arbitrary metric spaces. "
722224735464120320,2016-04-19 00:46:09,https://t.co/vnXPQ4UfpK,From Denoising to Compressed Sensing. (arXiv:1406.4175v5 [cs.IT] UPDATED) https://t.co/vnXPQ4UfpK,0,5," Abstract: A denoising algorithm seeks to remove noise, errors, or perturbations from a signal. Extensive research has been devoted to this arena over the last several decades, and as a result, today's denoisers can effectively remove large amounts of additive white Gaussian noise. A compressed sensing (CS) reconstruction algorithm seeks to recover a structured signal acquired using a small number of randomized measurements. Typical CS reconstruction algorithms can be cast as iteratively estimating a signal from a perturbed observation. This paper answers a natural question: How can one effectively employ a generic denoiser in a CS reconstruction algorithm? In response, we develop an extension of the approximate message passing (AMP) framework, called Denoising-based AMP (D-AMP), that can integrate a wide class of denoisers within its iterations. We demonstrate that, when used with a high performance denoiser for natural images, D-AMP offers state-of-the-art CS recovery performance while operating tens of times faster than competing methods. We explain the exceptional performance of D-AMP by analyzing some of its theoretical features. A key element in D-AMP is the use of an appropriate Onsager correction term in its iterations, which coerces the signal perturbation at each iteration to be very close to the white Gaussian noise that denoisers are typically designed to remove. "
722224733903855622,2016-04-19 00:46:09,https://t.co/FIHaJrUmT0,Two Timescale Stochastic Approximation with Controlled Markov noise and Off-policy temporal difference learning. (… https://t.co/FIHaJrUmT0,0,3," Abstract: We present for the first time an asymptotic convergence analysis of two time-scale stochastic approximation driven by `controlled' Markov noise. In particular, both the faster and slower recursions have non-additive controlled Markov noise components in addition to martingale difference noise. We analyze the asymptotic behavior of our framework by relating it to limiting differential inclusions in both time-scales that are defined in terms of the ergodic occupation measures associated with the controlled Markov processes. Finally, we present a solution to the off-policy convergence problem for temporal difference learning with linear function approximation, using our results. "
722224732519710720,2016-04-19 00:46:09,https://t.co/h8ktrHHpEc,Optimal Estimation of Low Rank Density Matrices. (arXiv:1507.05131v4 [stat.ML] UPDATED) https://t.co/h8ktrHHpEc,0,3," Abstract: The density matrices are positively semi-definite Hermitian matrices of unit trace that describe the state of a quantum system. The goal of the paper is to develop minimax lower bounds on error rates of estimation of low rank density matrices in trace regression models used in quantum state tomography (in particular, in the case of Pauli measurements) with explicit dependence of the bounds on the rank and other complexity parameters. Such bounds are established for several statistically relevant distances, including quantum versions of Kullback-Leibler divergence (relative entropy distance) and of Hellinger distance (so called Bures distance), and Schatten $p$-norm distances. Sharp upper bounds and oracle inequalities for least squares estimator with von Neumann entropy penalization are obtained showing that minimax lower bounds are attained (up to logarithmic factors) for these distances. "
722224731328528384,2016-04-19 00:46:08,https://t.co/PnHwVmhjPe,Dropping Convexity for Faster Semi-definite Optimization. (arXiv:1509.03917v3 [stat.ML] UPDATED) https://t.co/PnHwVmhjPe,0,2," Abstract: We study the minimization of a convex function $f(X)$ over the set of $n\times n$ positive semi-definite matrices, but when the problem is recast as $\min_U g(U) := f(UU^\top)$, with $U \in \mathbb{R}^{n \times r}$ and $r \leq n$. We study the performance of gradient descent on $g$---which we refer to as Factored Gradient Descent (FGD)---under standard assumptions on the original function $f$. We provide a rule for selecting the step size and, with this choice, show that the local convergence rate of FGD mirrors that of standard gradient descent on the original $f$: i.e., after $k$ steps, the error is $O(1/k)$ for smooth $f$, and exponentially small in $k$ when $f$ is (restricted) strongly convex. In addition, we provide a procedure to initialize FGD for (restricted) strongly convex objectives and when one only has access to $f$ via a first-order oracle; for several problem instances, such proper initialization leads to global convergence guarantees. FGD and similar procedures are widely used in practice for problems that can be posed as matrix factorization. To the best of our knowledge, this is the first paper to provide precise convergence rate guarantees for general convex functions under standard convex assumptions. "
722224730242158592,2016-04-19 00:46:08,https://t.co/8yjS6cVLP3,Variance Reduction in SGD by Distributed Importance Sampling. (arXiv:1511.06481v7 [stat.ML] UPDATED) https://t.co/8yjS6cVLP3,0,8," Abstract: Humans are able to accelerate their learning by selecting training materials that are the most informative and at the appropriate level of difficulty. We propose a framework for distributing deep learning in which one set of workers search for the most informative examples in parallel while a single worker updates the model on examples selected by importance sampling. This leads the model to update using an unbiased estimate of the gradient which also has minimum variance when the sampling proposal is proportional to the L2-norm of the gradient. We show experimentally that this method reduces gradient variance even in a context where the cost of synchronization across machines cannot be ignored, and where the factors for importance sampling are not updated instantly across the training set. "
722224728472174592,2016-04-19 00:46:08,https://t.co/8UxgN3wfeO,The Variational Gaussian Process. (arXiv:1511.06499v4 [stat.ML] UPDATED) https://t.co/8UxgN3wfeO,0,8," Abstract: Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We develop the variational Gaussian process (VGP), a Bayesian nonparametric variational family, which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by auto-encoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW. "
721862376413405184,2016-04-18 00:46:16,https://t.co/QOP46pXfDS,Positive Definite Estimation of Large Covariance Matrix Using Generalized Nonconvex Penalties. (arXiv:1604.04348v1… https://t.co/QOP46pXfDS,0,4," Abstract: This work addresses the issue of large covariance matrix estimation in high-dimensional statistical analysis. Recently, improved iterative algorithms with positive-definite guarantee have been developed. However, these algorithms cannot be directly extended to use a nonconvex penalty for sparsity inducing. Generally, a nonconvex penalty has the capability of ameliorating the bias problem of the popular convex lasso penalty, and thus is more advantageous. In this work, we propose a class of positive-definite covariance estimators using generalized nonconvex penalties. We develop a first-order algorithm based on the alternating direction method framework to solve the nonconvex optimization problem efficiently. The convergence of this algorithm has been proved. Further, the statistical properties of the new estimators have been analyzed for generalized nonconvex penalties. Moreover, extension of this algorithm to covariance estimation from sketched measurements has been considered. The performances of the new estimators have been demonstrated by both a simulation study and a gene clustering example for tumor tissues. Code for the proposed estimators is available at this https URL "
721862375113146369,2016-04-18 00:46:16,https://t.co/AJNp4oUZu1,Bayesian linear regression with Student-t assumptions. (arXiv:1604.04434v1 [cs.LG]) https://t.co/AJNp4oUZu1,0,4," Abstract: As an automatic method of determining model complexity using the training data alone, Bayesian linear regression provides us a principled way to select hyperparameters. But one often needs approximation inference if distribution assumption is beyond Gaussian distribution. In this paper, we propose a Bayesian linear regression model with Student-t assumptions (BLRS), which can be inferred exactly. In this framework, both conjugate prior and expectation maximization (EM) algorithm are generalized. Meanwhile, we prove that the maximum likelihood solution is equivalent to the standard Bayesian linear regression with Gaussian assumptions (BLRG). The $q$-EM algorithm for BLRS is nearly identical to the EM algorithm for BLRG. It is showed that $q$-EM for BLRS can converge faster than EM for BLRG for the task of predicting online news popularity. "
721862373104033792,2016-04-18 00:46:15,https://t.co/4gV3BEPvug,Delta divergence: A novel decision cognizant measure of classifier incongruence. (arXiv:1604.04451v1 [cs.LG]) https://t.co/4gV3BEPvug,0,6," Abstract: Disagreement between two classifiers regarding the class membership of an observation in pattern recognition can be indicative of an anomaly and its nuance. As in general classifiers base their decision on class aposteriori probabilities, the most natural approach to detecting classifier incongruence is to use divergence. However, existing divergences are not particularly suitable to gauge classifier incongruence. In this paper, we postulate the properties that a divergence measure should satisfy and propose a novel divergence measure, referred to as Delta divergence. In contrast to existing measures, it is decision cognizant. The focus in Delta divergence on the dominant hypotheses has a clutter reducing property, the significance of which grows with increasing number of classes. The proposed measure satisfies other important properties such as symmetry, and independence of classifier confidence. The relationship of the proposed divergence to some baseline measures is demonstrated experimentally, showing its superiority. "
721862371782828032,2016-04-18 00:46:15,https://t.co/6WyGn9tDXl,A short note on extension theorems and their connection to universal consistency in machine learning. (arXiv:1604.… https://t.co/6WyGn9tDXl,0,3," Abstract: Statistical machine learning plays an important role in modern statistics and computer science. One main goal of statistical machine learning is to provide universally consistent algorithms, i.e., the estimator converges in probability or in some stronger sense to the Bayes risk or to the Bayes decision function. Kernel methods based on minimizing the regularized risk over a reproducing kernel Hilbert space (RKHS) belong to these statistical machine learning methods. It is in general unknown which kernel yields optimal results for a particular data set or for the unknown probability measure. Hence various kernel learning methods were proposed to choose the kernel and therefore also its RKHS in a data adaptive manner. Nevertheless, many practitioners often use the classical Gaussian RBF kernel or certain Sobolev kernels with good success. The goal of this short note is to offer one possible theoretical explanation for this empirical fact. "
721862370398728193,2016-04-18 00:46:15,https://t.co/DNZmWhvYIA,A Network-based End-to-End Trainable Task-oriented Dialogue System. (arXiv:1604.04562v1 [cs.CL]) https://t.co/DNZmWhvYIA,0,5," Abstract: Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring labelled datasets and solving a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable dialogue system along with a new way of collecting task-oriented dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain. "
721862369366917120,2016-04-18 00:46:15,https://t.co/cKr4fRY5Ma,Estimation of low rank density matrices: bounds in Schatten norms and other distances. (arXiv:1604.04600v1 [stat.M… https://t.co/cKr4fRY5Ma,0,4," Abstract: Let ${\mathcal S}_m$ be the set of all $m\times m$ density matrices (Hermitian positively semi-definite matrices of unit trace). Consider a problem of estimation of an unknown density matrix $\rho\in {\mathcal S}_m$ based on outcomes of $n$ measurements of observables $X_1,\dots, X_n\in {\mathbb H}_m$ (${\mathbb H}_m$ being the space of $m\times m$ Hermitian matrices) for a quantum system identically prepared $n$ times in state $\rho.$ Outcomes $Y_1,\dots, Y_n$ of such measurements could be described by a trace regression model in which ${\mathbb E}_{\rho}(Y_j|X_j)={\rm tr}(\rho X_j), j=1,\dots, n.$ The design variables $X_1,\dots, X_n$ are often sampled at random from the uniform distribution in an orthonormal basis $\{E_1,\dots, E_{m^2}\}$ of ${\mathbb H}_m$ (such as Pauli basis). The goal is to estimate the unknown density matrix $\rho$ based on the data $(X_1,Y_1), \dots, (X_n,Y_n).$ Let $$ \hat Z:=\frac{m^2}{n}\sum_{j=1}^n Y_j X_j $$ and let $\check \rho$ be the projection of $\hat Z$ onto the convex set ${\mathcal S}_m$ of density matrices. It is shown that for estimator $\check \rho$ the minimax lower bounds in classes of low rank density matrices (established earlier) are attained up logarithmic factors for all Schatten $p$-norm distances, $p\in [1,\infty]$ and for Bures version of quantum Hellinger distance. Moreover, for a slightly modified version of estimator $\check \rho$ the same property holds also for quantum relative entropy (Kullback-Leibler) distance between density matrices. "
721862368138039297,2016-04-18 00:46:14,https://t.co/a85eW6Ybvv,On deterministic conditions for subspace clustering under missing data. (arXiv:1604.04615v1 [cs.IT]) https://t.co/a85eW6Ybvv,0,3," Abstract: In this paper we present deterministic analysis of sufficient conditions for sparse subspace clustering under missing data, when data is assumed to come from a Union of Subspaces (UoS) model. In this context we consider two cases, namely Case I when all the points are sampled at the same co-ordinates, and Case II when points are sampled at different locations. We show that results for Case I directly follow from several existing results in the literature, while results for Case II are not as straightforward and we provide a set of dual conditions under which, perfect clustering holds true. We provide extensive set of simulation results for clustering as well as completion of data under missing entries, under the UoS model. Our experimental results indicate that in contrast to the full data case, accurate clustering does not imply accurate subspace identification and completion, indicating the natural order of relative hardness of these problems. "
721862366934220800,2016-04-18 00:46:14,https://t.co/HwqVYcamJL,Convex Biclustering. (arXiv:1408.0856v4 [stat.ME] UPDATED) https://t.co/HwqVYcamJL,0,4," Abstract: In the biclustering problem, we seek to simultaneously group observations and features. While biclustering has applications in a wide array of domains, ranging from text mining to collaborative filtering, the problem of identifying structure in high dimensional genomic data motivates this work. In this context, biclustering enables us to identify subsets of genes that are co-expressed only within a subset of experimental conditions. We present a convex formulation of the biclustering problem that possesses a unique global minimizer and an iterative algorithm, COBRA, that is guaranteed to identify it. Our approach generates an entire solution path of possible biclusters as a single tuning parameter is varied. We also show how to reduce the problem of selecting this tuning parameter to solving a trivial modification of the convex biclustering problem. The key contributions of our work are its simplicity, interpretability, and algorithmic guarantees - features that arguably are lacking in the current alternative algorithms. We demonstrate the advantages of our approach, which includes stably and reproducibly identifying biclusterings, on simulated and real microarray data. "
721862365600444418,2016-04-18 00:46:14,https://t.co/c36QV0kSTl,Co-Localization of Audio Sources in Images Using Binaural Features and Locally-Linear Regression. (arXiv:1408.2700… https://t.co/c36QV0kSTl,0,2," Abstract: This paper addresses the problem of localizing audio sources using binaural measurements. We propose a supervised formulation that simultaneously localizes multiple sources at different locations. The approach is intrinsically efficient because, contrary to prior work, it relies neither on source separation, nor on monaural segregation. The method starts with a training stage that establishes a locally-linear Gaussian regression model between the directional coordinates of all the sources and the auditory features extracted from binaural measurements. While fixed-length wide-spectrum sounds (white noise) are used for training to reliably estimate the model parameters, we show that the testing (localization) can be extended to variable-length sparse-spectrum sounds (such as speech), thus enabling a wide range of realistic applications. Indeed, we demonstrate that the method can be used for audio-visual fusion, namely to map speech signals onto images and hence to spatially align the audio and visual modalities, thus enabling to discriminate between speaking and non-speaking faces. We release a novel corpus of real-room recordings that allow quantitative evaluation of the co-localization method in the presence of one or two sound sources. Experiments demonstrate increased accuracy and speed relative to several state-of-the-art methods. "
721862364220551168,2016-04-18 00:46:13,https://t.co/5Co6JYLGyn,Signal Processing on Graphs: Causal Modeling of Big Data. (arXiv:1503.00173v2 [cs.IT] UPDATED) https://t.co/5Co6JYLGyn,0,9," Abstract: Many applications collect a large number of time series, for example, the financial data of companies quoted in a stock exchange, the health care data of all patients that visit the emergency room of a hospital, or the temperature sequences continuously measured by weather stations across the US. These data are often referred to as unstructured. A first task in its analytics is to derive a low dimensional representation, a graph or discrete manifold, that describes well the interrelations among the time series and their intrarelations across time. This paper presents a computationally tractable algorithm for estimating this graph that structures the data. The resulting graph is directed and weighted, possibly capturing causal relations, not just reciprocal correlations as in many existing approaches in the literature. A convergence analysis is carried out. The algorithm is demonstrated on random graph datasets and real network time series datasets, and its performance is compared to that of related methods. The adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested. "
721862362077261824,2016-04-18 00:46:13,https://t.co/99uCHaFhga,Computationally Efficient Bayesian Learning of Gaussian Process State Space Models. (arXiv:1506.02267v2 [stat.… https://t.co/99uCHaFhga,0,9," Abstract: Gaussian processes allow for flexible specification of prior assumptions of unknown dynamics in state space models. We present a procedure for efficient Bayesian learning in Gaussian process state space models, where the representation is formed by projecting the problem onto a set of approximate eigenfunctions derived from the prior covariance structure. Learning under this family of models can be conducted using a carefully crafted particle MCMC algorithm. This scheme is computationally efficient and yet allows for a fully Bayesian treatment of the problem. Compared to conventional system identification tools or existing learning methods, we show competitive performance and reliable quantification of uncertainties in the model. "
721862358960840704,2016-04-18 00:46:12,https://t.co/MZF4TAbc7D,Clustering Financial Time Series: How Long is Enough?. (arXiv:1603.04017v2 [stat.ML] UPDATED) https://t.co/MZF4TAbc7D,0,8," Abstract: Researchers have used from 30 days to several years of daily returns as source data for clustering financial time series based on their correlations. This paper sets up a statistical framework to study the validity of such practices. We first show that clustering correlated random variables from their observed values is statistically consistent. Then, we also give a first empirical answer to the much debated question: How long should the time series be? If too short, the clusters found can be spurious; if too long, dynamics can be smoothed out. "
720773920077586432,2016-04-15 00:41:08,https://t.co/s73l1bNhjR,Optimal Rates For Regularization Of Statistical Inverse Learning Problems. (arXiv:1604.04054v1 [stat.ML]) https://t.co/s73l1bNhjR,0,11," Abstract: We consider a statistical inverse learning problem, where we observe the image of a function $f$ through a linear operator $A$ at i.i.d. random design points $X_i$, superposed with an additive noise. The distribution of the design points is unknown and can be very general. We analyze simultaneously the direct (estimation of $Af$) and the inverse (estimation of $f$) learning problems. In this general framework, we obtain strong and weak minimax optimal rates of convergence (as the number of observations $n$ grows large) for a large class of spectral regularization methods over regularity classes defined through appropriate source conditions. This improves on or completes previous results obtained in related settings. The optimality of the obtained rates is shown not only in the exponent in $n$ but also in the explicit dependency of the constant factor in the variance of the noise and the radius of the source condition set. "
720773918714408960,2016-04-15 00:41:08,https://t.co/kDBKoMvxJm,Distribution-Free Predictive Inference For Regression. (arXiv:1604.04173v1 [stat.ME]) https://t.co/kDBKoMvxJm,0,6," Abstract: We develop a general framework for distribution-free predictive inference in regression, using conformal inference. The proposed methodology allows construction of prediction bands for the response variable using any estimator of the regression function. The resulting prediction band preserves the consistency properties of the original estimator under standard assumptions, while guaranteeing finite sample marginal coverage even when the assumptions do not hold. We analyze and compare, both empirically and theoretically, two major variants of our conformal procedure: the full conformal inference and split conformal inference, along with a related jackknife method. These methods offer different tradeoffs between statistical accuracy (length of resulting prediction intervals) and computational efficiency. As extensions, we develop a method for constructing valid in-sample prediction intervals called rank-one-out conformal inference, which has essentially the same computational efficiency as split conformal inference. We also describe an extension of our procedures for producing prediction bands with varying local width, in order to adapt to heteroskedascity in the data distribution. Lastly, we propose a model-free notion of variable importance, called leave-one-covariate-out or LOCO inference. Accompanying our paper is an R package conformalInference that implements all of the proposals we have introduced. In the spirit of reproducibility, all empirical results in this paper can be easily (re)generated using this package. "
720773917586100227,2016-04-15 00:41:07,https://t.co/6pEhnO2Nyy,Consistently Estimating Markov Chains with Noisy Aggregate Data. (arXiv:1604.04182v1 [cs.LG]) https://t.co/6pEhnO2Nyy,0,2," Abstract: We address the problem of estimating the parameters of a time-homogeneous Markov chain given only noisy, aggregate data. This arises when a population of individuals behave independently according to a Markov chain, but individual sample paths cannot be observed due to limitations of the observation process or the need to protect privacy. Instead, only population-level counts of the number of individuals in each state at each time step are available. When these counts are exact, a conditional least squares (CLS) estimator is known to be consistent and asymptotically normal. We initiate the study of method of moments estimators for this problem to handle the more realistic case when observations are additionally corrupted by noise. We show that CLS can be interpreted as a simple ""plug-in"" method of moments estimator. However, when observations are noisy, it is not consistent because it fails to account for additional variance introduced by the noise. We develop a new, simpler method of moments estimator that bypasses this problem and is consistent under noisy observations. "
720773916545912832,2016-04-15 00:41:07,https://t.co/SmyIV4ozYK,1-bit Matrix Completion: PAC-Bayesian Analysis of a Variational Approximation. (arXiv:1604.04191v1 [stat.ML]) https://t.co/SmyIV4ozYK,0,6," Abstract: Due to challenging applications such as collaborative filtering, the matrix completion problem has been widely studied in the past few years. Different approaches rely on different structure assumptions on the matrix in hand. Here, we focus on the completion of a (possibly) low-rank matrix with binary entries, the so-called 1-bit matrix completion problem. Our approach relies on tools from machine learning theory: empirical risk minimization and its convex relaxations. We propose an algorithm to compute a variational approximation of the pseudo-posterior. Thanks to the convex relaxation, the corresponding minimization problem is bi-convex, and thus the method behaves well in practice. We also study the performance of this variational approximation through PAC-Bayesian learning bounds. On the contrary to previous works that focused on upper bounds on the estimation error of M with various matrix norms, we are able to derive from this analysis a PAC bound on the prediction error of our algorithm. We focus essentially on convex relaxation through the hinge loss, for which we present the complete analysis, a complete simulation study and a test on the MovieLens data set. However, we also discuss a variational approximation to deal with the logistic loss. "
720773915279286272,2016-04-15 00:41:07,https://t.co/GfW53Zf6Ev,Estimation of nonlinear system parameters using the elitist particle filter based on evolutionary strategies. (arX… https://t.co/GfW53Zf6Ev,0,2," Abstract: In this article, we present the elitist particle filter based on evolutionary strategies (EPFES) as an efficient approach for nonlinear system identification. The EPFES is derived from the frequently-employed state-space model, where the relevant information of the nonlinear system is captured by an unknown state vector. Similar to classical particle filtering, the EPFES consists of a set of particles and respective weights which represent different realizations of the latent state vector and their likelihood of being the solution of the optimization problem. As main innovation, the EPFES includes an evolutionary elitist-particle selection which combines long-term information with instantaneous sampling from an approximated continuous posterior distribution. In this article, we propose two advancements of the previously-published elitist-particle selection process. Further, the EPFES is shown to be a generalization of the widely-used Gaussian particle filter and thus evaluated with respect to the latter for two completely different scenarios: First, we consider the so-called univariate nonstationary growth model with time-variant latent state variable, where the evolutionary selection of elitist particles is evaluated for non-recursively calculated particle weights. Second, the problem of nonlinear acoustic echo cancellation is addressed in a simulated scenario with speech as input signal: By using long-term fitness measures, we highlight the efficacy of the well-generalizing EPFES in estimating the nonlinear system even for large search spaces. Finally, we illustrate similarities between the EPFES and evolutionary algorithms to outline future improvements by fusing the achievements of both fields of research. "
720773914247475201,2016-04-15 00:41:07,https://t.co/k5IiOIfAcV,"Variational inference for rare variant detection in deep, heterogeneous next-generation sequencing data. (arXiv:16… https://t.co/k5IiOIfAcV",1,5," Abstract: The detection of rare variants is important for understanding the genetic heterogeneity in mixed samples. Recently, next-generation sequencing (NGS) technologies have enabled the identification of single nucleotide variants (SNVs) in mixed samples with high resolution. Yet, the noise inherent in the biological processes involved in next-generation sequencing necessitates the use of statistical methods to identify true rare variants. We propose a novel Bayesian statistical model and a variational expectation-maximization (EM) algorithm to estimate non-reference allele frequency (NRAF) and identify SNVs in heterogeneous cell populations. We demonstrate that our variational EM algorithm has comparable sensitivity and specificity compared with a Markov Chain Monte Carlo (MCMC) sampling inference algorithm, and is more computationally efficient on tests of low coverage ($27\times$ and $298\times$) data. Furthermore, we show that our model with a variational EM inference algorithm has higher specificity than many state-of-the-art algorithms. In an analysis of a directed evolution longitudinal yeast data set, we are able to identify a time-series trend in non-reference allele frequency and detect novel variants that have not yet been reported. Our model also detects the emergence of a beneficial variant earlier than was previously shown, and a pair of concomitant variants. "
720773912712318981,2016-04-15 00:41:06,https://t.co/F0NFgA2g5B,A Linearly-Convergent Stochastic L-BFGS Algorithm. (arXiv:1508.02087v2 [math.OC] UPDATED) https://t.co/F0NFgA2g5B,0,3," Abstract: We propose a new stochastic L-BFGS algorithm and prove a linear convergence rate for strongly convex and smooth functions. Our algorithm draws heavily from a recent stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well as a recent approach to variance reduction for stochastic gradient descent from Johnson and Zhang (2013). We demonstrate experimentally that our algorithm performs well on large-scale convex and non-convex optimization problems, exhibiting linear convergence and rapidly solving the optimization problems to high levels of precision. Furthermore, we show that our algorithm performs well for a wide-range of step sizes, often differing by several orders of magnitude. "
720773911558926336,2016-04-15 00:41:06,https://t.co/Uzaro5NxDm,Stochastic Neural Networks with Monotonic Activation Functions. (arXiv:1601.00034v2 [stat.ML] UPDATED) https://t.co/Uzaro5NxDm,0,4," Abstract: We propose a Laplace approximation that creates a stochastic unit from any smooth monotonic activation function, using only Gaussian noise. This paper investigates the application of this stochastic approximation in training a family of Restricted Boltzmann Machines (RBM) that are closely linked to Bregman divergences. This family, that we call exponential family RBM (Exp-RBM), is a subset of the exponential family Harmoniums that expresses family members through a choice of smooth monotonic non-linearity for each neuron. Using contrastive divergence along with our Gaussian approximation, we show that Exp-RBM can learn useful representations using novel stochastic units. "
720773910367748096,2016-04-15 00:41:06,https://t.co/26lKMXYbgK,Synthesis of Gaussian Trees with Correlation Sign Ambiguity: An Information Theoretic Approach. (arXiv:1601.06403v… https://t.co/26lKMXYbgK,0,6," Abstract: In latent Gaussian trees the pairwise correlation signs between the variables are intrinsically unrecoverable. Such information is vital since it completely determines the direction in which two variables are associated. In this work, we resort to information theoretical approaches to achieve two fundamental goals: First, we quantify the amount of information loss due to unrecoverable sign information. Second, we show the importance of such information in determining the maximum achievable rate region, in which the observed output vector can be synthesized, given its probability density function. In particular, we model the graphical model as a communication channel and propose a new layered encoding framework to synthesize observed data using upper layer Gaussian inputs and independent Bernoulli correlation sign inputs from each layer. We find the achievable rate region for the rate tuples of multi-layer latent Gaussian messages to synthesize the desired observables. "
720773907544989696,2016-04-15 00:41:05,https://t.co/5ULitOxokk,Harnessing Deep Neural Networks with Logic Rules. (arXiv:1603.06318v2 [cs.LG] UPDATED) https://t.co/5ULitOxokk,0,7," Abstract: Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems. "
720773905858826240,2016-04-15 00:41:05,https://t.co/r7vLzxBcqk,Learning to Generate Posters of Scientific Papers. (arXiv:1604.01219v1 [cs.AI] CROSS LISTED) https://t.co/r7vLzxBcqk,5,7," Abstract: Researchers often summarize their work in the form of posters. Posters provide a coherent and efficient way to convey core ideas from scientific papers. Generating a good scientific poster, however, is a complex and time consuming cognitive task, since such posters need to be readable, informative, and visually aesthetic. In this paper, for the first time, we study the challenging problem of learning to generate posters from scientific papers. To this end, a data-driven framework, that utilizes graphical models, is proposed. Specifically, given content to display, the key elements of a good poster, including panel layout and attributes of each panel, are learned and inferred from data. Then, given inferred layout and attributes, composition of graphical elements within each panel is synthesized. To learn and validate our model, we collect and make public a Poster-Paper dataset, which consists of scientific papers and corresponding posters with exhaustively labelled panels and attributes. Qualitative and quantitative results indicate the effectiveness of our approach. "
720413515740286976,2016-04-14 00:49:01,https://t.co/eUjqPJwGw1,Community Detection with Node Attributes and its Generalization. (arXiv:1604.03601v1 [cs.SI]) https://t.co/eUjqPJwGw1,0,6," Abstract: Community detection algorithms are fundamental tools to understand organizational principles in social networks. With the increasing power of social media platforms, when detecting communities there are two possi- ble sources of information one can use: the structure of social network and node attributes. However structure of social networks and node attributes are often interpreted separately in the research of community detection. When these two sources are interpreted simultaneously, one common as- sumption shared by previous studies is that nodes attributes are correlated with communities. In this paper, we present a model that is capable of combining topology information and nodes attributes information with- out assuming correlation. This new model can recover communities with higher accuracy even when node attributes and communities are uncorre- lated. We derive the detectability threshold for this model and use Belief Propagation (BP) to make inference. This algorithm is optimal in the sense that it can recover community all the way down to the threshold. This new model is also with the potential to handle edge content and dynamic settings. "
720413513777164288,2016-04-14 00:49:00,https://t.co/rx18Z9fexW,A Differentiable Transition Between Additive and Multiplicative Neurons. (arXiv:1604.03736v1 [cs.LG]) https://t.co/rx18Z9fexW,0,4," Abstract: Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. However, this leads to an extensive increase in the computational complexity of the training procedure. We present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure. "
720413510925049857,2016-04-14 00:49:00,https://t.co/OHDRyPpiGE,Variational Bayesian Inference of Line Spectra. (arXiv:1604.03744v1 [cs.IT]) https://t.co/OHDRyPpiGE,0,3," Abstract: In this paper, we address the fundamental problem of line spectral estimation in a Bayesian framework. We target model order and parameter estimation via variational inference in a probabilistic model in which the frequencies are continuous-valued, i.e., not restricted to a grid; and the coefficients are governed by a Bernoulli-Gaussian prior model turning model order selection into binary sequence detection. Unlike earlier works which retain only point estimates of the frequencies, we undertake a more complete Bayesian treatment by estimating the posterior probability density functions (pdfs) of the frequencies and computing expectations over them. Thus, we additionally capture and operate with the uncertainty of the frequency estimates. Aiming to maximize the model evidence, variational optimization provides analytic approximations of the posterior pdfs and also gives estimates of the additional parameters. We propose an accurate representation of the pdfs of the frequencies by mixtures of von Mises pdfs, which yields closed-form expectations. We define the algorithm VALSE in which the estimates of the pdfs and parameters are iteratively updated. VALSE is a gridless, convergent method, does not require parameter tuning, can easily include prior knowledge about the frequencies and provides approximate posterior pdfs based on which the uncertainty in line spectral estimation can be quantified. Simulation results show that accounting for the uncertainty of frequency estimates, rather than computing just point estimates, significantly improves the performance. The performance of VALSE is superior to that of state-of-the-art methods and closely approaches the Cram\'er-Rao bound computed for the true model order. "
720413508203114496,2016-04-14 00:48:59,https://t.co/PRtgklUYXD,Hierarchical Compound Poisson Factorization. (arXiv:1604.03853v1 [cs.LG]) https://t.co/PRtgklUYXD,0,2," Abstract: Non-negative matrix factorization models based on a hierarchical Gamma-Poisson structure capture user and item behavior effectively in extremely sparse data sets, making them the ideal choice for collaborative filtering applications. Hierarchical Poisson factorization (HPF) in particular has proved successful for scalable recommendation systems with extreme sparsity. HPF, however, suffers from a tight coupling of sparsity model (absence of a rating) and response model (the value of the rating), which limits the expressiveness of the latter. Here, we introduce hierarchical compound Poisson factorization (HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF to high-dimensional extremely sparse matrices. More importantly, HCPF decouples the sparsity model from the response model, allowing us to choose the most suitable distribution for the response. HCPF can capture binary, non-negative discrete, non-negative continuous, and zero-inflated continuous responses. We compare HCPF with HPF on nine discrete and three continuous data sets and conclude that HCPF captures the relationship between sparsity and response better than HPF. "
720413505900425216,2016-04-14 00:48:59,https://t.co/ScoVIVE5KI,Algorithms for stochastic optimization with expectation constraints. (arXiv:1604.03887v1 [math.OC]) https://t.co/ScoVIVE5KI,2,7," Abstract: This paper considers the problem of minimizing an expectation function over a closed convex set, coupled with an expectation constraint on either decision variables or problem parameters. We first present a new stochastic approximation (SA) type algorithm, namely the corporative SA (CSA), to handle problems with the expectation constraint on devision variables. We show that this algorithm exhibits the optimal ${\cal O}(1/\sqrt{N})$ rate of convergence, in terms of both optimality gap and constraint violation, when the objective and constraint functions are generally convex, where $N$ denotes the number of iterations. Moreover, we show that this rate of convergence can be improved to ${\cal O}(1/N)$ if the objective and constraint functions are strongly convex. We then present a variant of CSA, namely the corporative stochastic parameter approximation (CSPA) algorithm, to deal with the situation when the expectation constraint is defined over problem parameters and show that it exhibits similar optimal rate of convergence to CSA. It is worth noting that CSA and CSPA are primal methods which do not require the iterations on the dual space and/or the estimation on the size of the dual variables. To the best of our knowledge, this is the first time that such optimal SA methods for solving expectation constrained stochastic optimization are presented in the literature. "
720413504423989248,2016-04-14 00:48:58,https://t.co/yuVQgNLdnb,Inverse Reinforcement Learning with Simultaneous Estimation of Rewards and Dynamics. (arXiv:1604.03912v1 [cs.AI]) https://t.co/yuVQgNLdnb,0,7," Abstract: Inverse Reinforcement Learning (IRL) describes the problem of learning an unknown reward function of a Markov Decision Process (MDP) from observed behavior of an agent. Since the agent's behavior originates in its policy and MDP policies depend on both the stochastic system dynamics as well as the reward function, the solution of the inverse problem is significantly influenced by both. Current IRL approaches assume that if the transition model is unknown, additional samples from the system's dynamics are accessible, or the observed behavior provides enough samples of the system's dynamics to solve the inverse problem accurately. These assumptions are often not satisfied. To overcome this, we present a gradient-based IRL approach that simultaneously estimates the system's dynamics. By solving the combined optimization problem, our approach takes into account the bias of the demonstrations, which stems from the generating policy. The evaluation on a synthetic MDP and a transfer learning task shows improvements regarding the sample efficiency as well as the accuracy of the estimated reward functions and transition models. "
720413502066794496,2016-04-14 00:48:58,https://t.co/ljvzBOiwID,Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis. (arXi… https://t.co/ljvzBOiwID,0,3," Abstract: This paper considers the problem of canonical-correlation analysis (CCA) (Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a pair of symmetric matrices. These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009). We provide simple iterative algorithms, with improved runtimes, for solving these problems that are globally linearly convergent with moderate dependencies on the condition numbers and eigenvalue gaps of the matrices involved. We obtain our results by reducing CCA to the top-$k$ generalized eigenvector problem. We solve this problem through a general framework that simply requires black box access to an approximate linear system solver. Instantiating this framework with accelerated gradient descent we obtain a running time of $O(\frac{z k \sqrt{\kappa}}{\rho} \log(1/\epsilon) \log \left(k\kappa/\rho\right))$ where $z$ is the total number of nonzero entries, $\kappa$ is the condition number and $\rho$ is the relative eigenvalue gap of the appropriate matrices. Our algorithm is linear in the input size and the number of components $k$ up to a $\log(k)$ factor. This is essential for handling large-scale matrices that appear in practice. To the best of our knowledge this is the first such algorithm with global linear convergence. We hope that our results prompt further research and ultimately improve the practical running time for performing these important data analysis procedures on large data sets. "
720413500225392640,2016-04-14 00:48:57,https://t.co/9iABTSO39n,Simple one-pass algorithm for penalized linear regression with cross-validation on MapReduce. (arXiv:1307.0048v2 [… https://t.co/9iABTSO39n,0,5," Abstract: In this paper, we propose a one-pass algorithm on MapReduce for penalized linear regression \[f_\lambda(\alpha, \beta) = \|Y - \alpha\mathbf{1} - X\beta\|_2^2 + p_{\lambda}(\beta)\] where $\alpha$ is the intercept which can be omitted depending on application; $\beta$ is the coefficients and $p_{\lambda}$ is the penalized function with penalizing parameter $\lambda$. $f_\lambda(\alpha, \beta)$ includes interesting classes such as Lasso, Ridge regression and Elastic-net. Compared to latest iterative distributed algorithms requiring multiple MapReduce jobs, our algorithm achieves huge performance improvement; moreover, our algorithm is exact compared to the approximate algorithms such as parallel stochastic gradient decent. Moreover, what our algorithm distinguishes with others is that it trains the model with cross validation to choose optimal $\lambda$ instead of user specified one. Key words: penalized linear regression, lasso, elastic-net, ridge, MapReduce "
720413497813807104,2016-04-14 00:48:57,https://t.co/7OAwvb3Lst,Quantifying uncertainties on excursion sets under a Gaussian random field prior. (arXiv:1501.03659v2 [math.ST] UPD… https://t.co/7OAwvb3Lst,1,3," Abstract: We focus on the problem of estimating and quantifying uncertainties on the excursion set of a function under a limited evaluation budget. We adopt a Bayesian approach where the objective function is assumed to be a realization of a Gaussian random field. In this setting, the posterior distribution on the objective function gives rise to a posterior distribution on excursion sets. Several approaches exist to summarize the distribution of such sets based on random closed set theory. While the recently proposed Vorob'ev approach exploits analytical formulae, further notions of variability require Monte Carlo estimators relying on Gaussian random field conditional simulations. In the present work we propose a method to choose Monte Carlo simulation points and obtain quasi-realizations of the conditional field at fine designs through affine predictors. The points are chosen optimally in the sense that they minimize the posterior expected distance in measure between the excursion set and its reconstruction. The proposed method reduces the computational costs due to Monte Carlo simulations and enables the computation of quasi-realizations on fine designs in large dimensions. We apply this reconstruction approach to obtain realizations of an excursion set on a fine grid which allow us to give a new measure of uncertainty based on the distance transform of the excursion set. Finally we present a safety engineering test case where the simulation method is employed to compute a Monte Carlo estimate of a contour line. "
720413495750238208,2016-04-14 00:48:56,https://t.co/LWA7lrRcgg,Spectral Clustering over the Logistic Random Dot Product Graph. (arXiv:1510.00850v2 [stat.ML] UPDATED) https://t.co/LWA7lrRcgg,0,2," Abstract: Inference of clusters over networks is a central problem in machine learning. Commonly, it is formulated as a discrete optimization, and a continuous relaxation is used to obtain a spectral algorithm. An alternative problem formulation arises by considering a latent space model, in which edge probabilities are determined by continuous latent positions. A model of particular interest is the Random Dot Product Graph (RDPG), which can be fit using an efficient spectral method; however, this method is based on a heuristic that can fail, even in simple cases. In this paper, we consider a closely related latent space model, the Logistic RDPG, which uses a logistic link function to map from latent position inner products to edge likelihoods. Over this model, we show that asymptotically exact maximum likelihood inference of the latent position vectors can be achieved using a spectral method. Our method involves computing the top eigenvectors of a normalized adjacency matrix and scaling the eigenvectors using a regression step. Through simulations, we show that this method is more accurate and more robust than existing spectral and semidefinite network clustering methods. In particular, the novel regression scaling step is essential to the performance gain of the proposed method. "
720413493862735872,2016-04-14 00:48:56,https://t.co/mXkKeaGmCF,Loss Functions for Top-k Error: Analysis and Insights. (arXiv:1512.00486v2 [stat.ML] UPDATED) https://t.co/mXkKeaGmCF,0,5," Abstract: In order to push the performance on realistic computer vision tasks, the number of classes in modern benchmark datasets has significantly increased in recent years. This increase in the number of classes comes along with increased ambiguity between the class labels, raising the question if top-1 error is the right performance measure. In this paper, we provide an extensive comparison and evaluation of established multiclass methods comparing their top-k performance both from a practical as well as from a theoretical perspective. Moreover, we introduce novel top-k loss functions as modifications of the softmax and the multiclass SVM losses and provide efficient optimization schemes for them. In the experiments, we compare on various datasets all of the proposed and established methods for top-k error optimization. An interesting insight of this paper is that the softmax loss yields competitive top-k performance for all k simultaneously. For a specific top-k error, our new top-k losses lead typically to further improvements while being faster to train than the softmax. "
720413484790382592,2016-04-14 00:48:54,https://t.co/4TMB2pBWmh,Partition Functions from Rao-Blackwellized Tempered Sampling. (arXiv:1603.01912v2 [stat.ML] UPDATED) https://t.co/4TMB2pBWmh,1,2," Abstract: Partition functions of probability distributions are important quantities for model evaluation and comparisons. We present a new method to compute partition functions of complex and multimodal distributions. Such distributions are often sampled using simulated tempering, which augments the target space with an auxiliary inverse temperature variable. Our method exploits the multinomial probability law of the inverse temperatures, and provides estimates of the partition function in terms of a simple quotient of Rao-Blackwellized marginal inverse temperature probability estimates, which are updated while sampling. We show that the method has interesting connections with several alternative popular methods, and offers some significant advantages. In particular, we empirically find that the new method provides more accurate estimates than Annealed Importance Sampling when calculating partition functions of large Restricted Boltzmann Machines (RBM); moreover, the method is sufficiently accurate to track training and validation log-likelihoods during learning of RBMs, at minimal computational cost. "
720047539144290304,2016-04-13 00:34:45,https://t.co/U7T1hwE4QG,Conversational flow in Oxford-style debates. (arXiv:1604.03114v1 [cs.CL]) https://t.co/U7T1hwE4QG,0,3," Abstract: Public debates are a common platform for presenting and juxtaposing diverging views on important issues. In this work we propose a methodology for tracking how ideas flow between participants throughout a debate. We use this approach in a case study of Oxford-style debates---a competitive format where the winner is determined by audience votes---and show how the outcome of a debate depends on aspects of conversational flow. In particular, we find that winners tend to make better use of a debate's interactive component than losers, by actively pursuing their opponents' points rather than promoting their own ideas over the course of the conversation. "
720047537277779968,2016-04-13 00:34:45,https://t.co/tZ3Zhi7dei,Phase Transitions and a Model Order Selection Criterion for Spectral Graph Clustering. (arXiv:1604.03159v1 [cs.SI]) https://t.co/tZ3Zhi7dei,1,7," Abstract: One of the longstanding open problems in spectral graph clustering (SGC) is the so-called model order selection problem: automated selection of the correct number of clusters. This is equivalent to the problem of finding the number of connected components or communities in an undirected graph. We propose a solution to the SGC model selection problem under a random interconnection model (RIM) using a novel selection criterion that is based on an asymptotic phase transition analysis. Our solution can more generally be applied to discovering hidden block diagonal structure in symmetric non-negative matrices. Numerical experiments on simulated graphs validate the phase transition analysis, and real-world network data is used to validate the performance of the proposed model selection procedure. "
720047535130284032,2016-04-13 00:34:44,https://t.co/j6ovZKZj2Y,Recurrent Attentional Networks for Saliency Detection. (arXiv:1604.03227v1 [cs.CV]) https://t.co/j6ovZKZj2Y,0,3," Abstract: Convolutional-deconvolution networks can be adopted to perform end-to-end saliency detection. But, they do not work well with objects of multiple scales. To overcome such a limitation, in this work, we propose a recurrent attentional convolutional-deconvolution network (RACDNN). Using spatial transformer and recurrent network units, RACDNN is able to iteratively attend to selected image sub-regions to perform saliency refinement progressively. Besides tackling the scale problem, RACDNN can also learn context-aware features from past iterations to enhance saliency refinement in future iterations. Experiments on several challenging saliency detection datasets validate the effectiveness of RACDNN, and show that RACDNN outperforms state-of-the-art saliency detection methods. "
720047532706033664,2016-04-13 00:34:44,https://t.co/4WXjFMqmRm,Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization. (arXiv:1604.03… https://t.co/4WXjFMqmRm,0,3," Abstract: Recently, {\it stochastic momentum} methods have been widely adopted in training deep neural networks. However, their convergence analysis is still underexplored at the moment, in particular for non-convex optimization. This paper fills the gap between practice and theory by developing a basic convergence analysis of two stochastic momentum methods, namely stochastic heavy-ball method and the stochastic variant of Nesterov's accelerated gradient method. We hope that the basic convergence results developed in this paper can serve the reference to the convergence of stochastic momentum methods and also serve the baselines for comparison in future development of stochastic momentum methods. The novelty of convergence analysis presented in this paper is a unified framework, revealing more insights about the similarities and differences between different stochastic momentum methods and stochastic gradient method. The unified framework exhibits a continuous change from the gradient method to Nesterov's accelerated gradient method and finally the heavy-ball method incurred by a free parameter, which can help explain a similar change observed in the testing error convergence behavior for deep learning. Furthermore, our empirical results for optimizing deep neural networks demonstrate that the stochastic variant of Nesterov's accelerated gradient method achieves a good tradeoff (between speed of convergence in training error and robustness of convergence in testing error) among the three stochastic methods. "
720047530629799938,2016-04-13 00:34:43,https://t.co/ZlmEuA3yVy,Confidence Decision Trees via Online and Active Learning for Streaming (BIG) Data. (arXiv:1604.03278v1 [stat.ML]) https://t.co/ZlmEuA3yVy,1,5," Abstract: Decision tree classifiers are a widely used tool in data stream mining. The use of confidence intervals to estimate the gain associated with each split leads to very effective methods, like the popular Hoeffding tree algorithm. From a statistical viewpoint, the analysis of decision tree classifiers in a streaming setting requires knowing when enough new information has been collected to justify splitting a leaf. Although some of the issues in the statistical analysis of Hoeffding trees have been already clarified, a general and rigorous study of confidence intervals for splitting criteria is missing. We fill this gap by deriving accurate confidence intervals to estimate the splitting gain in decision tree learning with respect to three criteria: entropy, Gini index, and a third index proposed by Kearns and Mansour. Our confidence intervals depend in a more detailed way on the tree parameters. We also extend our confidence analysis to a selective sampling setting, in which the decision tree learner adaptively decides which labels to query in the stream. We furnish theoretical guarantee bounding the probability that the classification is non-optimal learning the decision tree via our selective sampling strategy. Experiments on real and synthetic data in a streaming setting show that our trees are indeed more accurate than trees with the same number of leaves generated by other techniques and our active learning module permits to save labeling cost. In addition, comparing our labeling strategy with recent methods, we show that our approach is more robust and consistent respect all the other techniques applied to incremental decision trees. "
720047528507531264,2016-04-13 00:34:43,https://t.co/2mbwFTyzoS,A Convex Surrogate Operator for General Non-Modular Loss Functions. (arXiv:1604.03373v1 [stat.ML]) https://t.co/2mbwFTyzoS,0,2," Abstract: Empirical risk minimization frequently employs convex surrogates to underlying discrete loss functions in order to achieve computational tractability during optimization. However, classical convex surrogates can only tightly bound modular loss functions, sub-modular functions or supermodular functions separately while maintaining polynomial time computation. In this work, a novel generic convex surrogate for general non-modular loss functions is introduced, which provides for the first time a tractable solution for loss functions that are neither super-modular nor submodular. This convex surro-gate is based on a submodular-supermodular decomposition for which the existence and uniqueness is proven in this paper. It takes the sum of two convex surrogates that separately bound the supermodular component and the submodular component using slack-rescaling and the Lov{\'a}sz hinge, respectively. It is further proven that this surrogate is convex , piecewise linear, an extension of the loss function, and for which subgradient computation is polynomial time. Empirical results are reported on a non-submodular loss based on the S{{\o}}rensen-Dice difference function, and a real-world face track dataset with tens of thousands of frames, demonstrating the improved performance, efficiency, and scalabil-ity of the novel convex surrogate. "
720047526565519360,2016-04-13 00:34:42,https://t.co/pF7WD4B22s,A statistical learning strategy for closed-loop control of fluid flows. (arXiv:1604.03392v1 [stat.ML]) https://t.co/pF7WD4B22s,0,5," Abstract: This work discusses a closed-loop control strategy for complex systems utilizing scarce and streaming data. A discrete embedding space is first built using hash functions applied to the sensor measurements from which a Markov process model is derived, approximating the complex system's dynamics. A control strategy is then learned using reinforcement learning once rewards relevant with respect to the control objective are identified. This method is designed for experimental configurations, requiring no computations nor prior knowledge of the system, and enjoys intrinsic robustness. It is illustrated on two systems: the control of the transitions of a Lorenz 63 dynamical system, and the control of the drag of a cylinder flow. The method is shown to perform well. "
720047524682276864,2016-04-13 00:34:42,https://t.co/04CjN165gL,In the mood: the dynamics of collective sentiments on Twitter. (arXiv:1604.03427v1 [cs.SI]) https://t.co/04CjN165gL,0,4," Abstract: We study the relationship between the sentiment levels of Twitter users and the evolving network structure that the users created by @-mentioning each other. We use a large dataset of tweets to which we apply three sentiment scoring algorithms, including the open source SentiStrength program. Specifically we make three contributions. Firstly we find that people who have potentially the largest communication reach (according to a dynamic centrality measure) use sentiment differently than the average user: for example they use positive sentiment more often and negative sentiment less often. Secondly we find that when we follow structurally stable Twitter communities over a period of months, their sentiment levels are also stable, and sudden changes in community sentiment from one day to the next can in most cases be traced to external events affecting the community. Thirdly, based on our findings, we create and calibrate a simple agent-based model that is capable of reproducing measures of emotive response comparable to those obtained from our empirical dataset. "
720047522622922757,2016-04-13 00:34:41,https://t.co/UhwcvvQtH0,The Matrix Generalized Inverse Gaussian Distribution: Properties and Applications. (arXiv:1604.03463v1 [stat.ML]) https://t.co/UhwcvvQtH0,0,5," Abstract: While the Matrix Generalized Inverse Gaussian ($\mathcal{MGIG}$) distribution arises naturally in some settings as a distribution over symmetric positive semi-definite matrices, certain key properties of the distribution and effective ways of sampling from the distribution have not been carefully studied. In this paper, we show that the $\mathcal{MGIG}$ is unimodal, and the mode can be obtained by solving an Algebraic Riccati Equation (ARE) equation [7]. Based on the property, we propose an importance sampling method for the $\mathcal{MGIG}$ where the mode of the proposal distribution matches that of the target. The proposed sampling method is more efficient than existing approaches [32, 33], which use proposal distributions that may have the mode far from the $\mathcal{MGIG}$'s mode. Further, we illustrate that the the posterior distribution in latent factor models, such as probabilistic matrix factorization (PMF) [25], when marginalized over one latent factor has the $\mathcal{MGIG}$ distribution. The characterization leads to a novel Collapsed Monte Carlo (CMC) inference algorithm for such latent factor models. We illustrate that CMC has a lower log loss or perplexity than MCMC, and needs fewer samples. "
720047520454471680,2016-04-13 00:34:41,https://t.co/qctl8hEpHm,Structured Matrix Recovery via the Generalized Dantzig Selector. (arXiv:1604.03492v1 [stat.ML]) https://t.co/qctl8hEpHm,0,3," Abstract: In recent years, structured matrix recovery problems have gained considerable attention for its real world applications, such as recommender systems and computer vision. Much of the existing work has focused on matrices with low-rank structure, and limited progress has been made matrices with other types of structure. In this paper we present non-asymptotic analysis for estimation of generally structured matrices via the generalized Dantzig selector under generic sub-Gaussian measurements. We show that the estimation error can always be succinctly expressed in terms of a few geometric measures of suitable sets which only depend on the structure of the underlying true matrix. In addition, we derive the general bounds on these geometric measures for structures characterized by unitarily invariant norms, which is a large family covering most matrix norms of practical interest. Examples are provided to illustrate the utility of our theoretical development. "
719686585214767104,2016-04-12 00:40:27,https://t.co/yFU2OHpKb5,Challenges in Bayesian Adaptive Data Analysis. (arXiv:1604.02492v1 [cs.LG]) https://t.co/yFU2OHpKb5,0,4," Abstract: Traditional statistical analysis requires that the analysis process and data are independent. By contrast, the new field of adaptive data analysis hopes to understand and provide algorithms and accuracy guarantees for research as it is commonly performed in practice, as an iterative process of interacting repeatedly with the same data set, such as repeated tests against a holdout set. Previous work has defined a model with a rather strong lower bound on sample complexity in terms of the number of queries, $n\sim\sqrt q$, arguing that adaptive data analysis is much harder than static data analysis, where $n\sim\log q$ is possible. Instead, we argue that those strong lower bounds point to a limitation of the previous model in that it must consider wildly asymmetric scenarios which do not hold in typical applications. To better understand other difficulties of adaptivity, we propose a new Bayesian version of the problem that mandates symmetry. Since the other lower bound techniques are ruled out, we can more effectively see difficulties that might otherwise be overshadowed. As a first contribution to this model, we produce a new problem using error-correcting codes on which a large family of methods, including all previously proposed algorithms, require roughly $n\sim\sqrt[4]q$. These early results illustrate new difficulties in adaptive data analysis regarding slightly correlated queries on problems with concentrated uncertainty. "
719686583797051394,2016-04-12 00:40:27,https://t.co/MXFbi1d9FH,A General Retraining Framework for Scalable Adversarial Classification. (arXiv:1604.02606v1 [cs.GT]) https://t.co/MXFbi1d9FH,0,3," Abstract: Traditional classification algorithms assume that training and test data come from similar distributions. This assumption is violated in adversarial settings, where malicious actors modify instances to evade detection. A number of custom methods have been developed for both adversarial evasion attacks and robust learning. We propose the first systematic and general-purpose retraining framework which can: a) boost robustness of an \emph{arbitrary} learning algorithm, in the face of b) a broader class of adversarial models than any prior methods. We show that, under natural conditions, the retraining framework minimizes an upper bound on optimal adversarial risk, and show how to extend this result to account for approximations of evasion attacks. Extensive experimental evaluation demonstrates that our retraining methods are nearly indistinguishable from state-of-the-art algorithms for optimizing adversarial risk, but are more general and far more scalable. The experiments also confirm that without retraining, our adversarial framework dramatically reduces the effectiveness of learning. In contrast, retraining significantly boosts robustness to evasion attacks without significantly compromising overall accuracy. "
719686582039683072,2016-04-12 00:40:26,https://t.co/oIRaYoR501,Grid Based Nonlinear Filtering Revisited: Recursive Estimation &amp; Asymptotic Optimality. (arXiv:1604.02631v1 [math.… https://t.co/oIRaYoR501,0,3," Abstract: We revisit the development of grid based recursive approximate filtering of general Markov processes in discrete time, partially observed in conditionally Gaussian noise. The grid based filters considered rely on two types of state quantization: The \textit{Markovian} type and the \textit{marginal} type. We propose a set of novel, relaxed sufficient conditions, ensuring strong and fully characterized pathwise convergence of these filters to the respective MMSE state estimator. In particular, for marginal state quantizations, we introduce the notion of \textit{conditional regularity of stochastic kernels}, which, to the best of our knowledge, constitutes the most relaxed condition proposed, under which asymptotic optimality of the respective grid based filters is guaranteed. Further, we extend our convergence results, including filtering of bounded and continuous functionals of the state, as well as recursive approximate state prediction. For both Markovian and marginal quantizations, the whole development of the respective grid based filters relies more on linear-algebraic techniques and less on measure theoretic arguments, making the presentation considerably shorter and technically simpler. "
719686580244508672,2016-04-12 00:40:26,https://t.co/CsPBnybCxm,Online Nonnegative Matrix Factorization with Outliers. (arXiv:1604.02634v1 [stat.ML]) https://t.co/CsPBnybCxm,0,5," Abstract: We propose a unified and systematic framework for performing online nonnegative matrix factorization in the presence of outliers. Our framework is particularly suited to large-scale data. We propose two solvers based on projected gradient descent and the alternating direction method of multipliers. We prove that the sequence of objective values converges almost surely by appealing to the quasi-martingale convergence theorem. We also show the sequence of learned dictionaries converges to the set of stationary points of the expected loss function almost surely. In addition, we extend our basic problem formulation to various settings with different constraints and regularizers. We also adapt the solvers and analyses to each setting. We perform extensive experiments on both synthetic and real datasets. These experiments demonstrate the computational efficiency and efficacy of our algorithms on tasks such as (parts-based) basis learning, image denoising, shadow removal and foreground-background separation. "
719686578981965828,2016-04-12 00:40:26,https://t.co/bsDBXKnWpO,Distance for Functional Data Clustering Based on Smoothing Parameter Commutation. (arXiv:1604.02668v1 [stat.ME]) https://t.co/bsDBXKnWpO,0,3," Abstract: We propose a novel method to determine the dissimilarity between subjects for functional data clustering. Spline smoothing or interpolation is common to deal with data of such type. Instead of estimating the best-representing curve for each subject as fixed during clustering, we measure the dissimilarity between subjects based on varying curve estimates with commutation of smoothing parameters pair-by-pair (of subjects). The intuitions are that smoothing parameters of smoothing splines reflect inverse signal-to-noise ratios and that applying an identical smoothing parameter the smoothed curves for two similar subjects are expected to be close. The effectiveness of our proposal is shown through simulations comparing to other dissimilarity measures. It also has several pragmatic advantages. First, missing values or irregular time points can be handled directly, thanks to the nature of smoothing splines. Second, conventional clustering method based on dissimilarity can be employed straightforward, and the dissimilarity also serves as a useful tool for outlier detection. Third, the implementation is almost handy since subroutines for smoothing splines and numerical integration are widely available. Fourth, the computational complexity does not increase and is parallel with that in calculating Euclidean distance between curves estimated by smoothing splines. "
719686577740455936,2016-04-12 00:40:25,https://t.co/Wa5woCQRi3,Correlated Equilibria for Approximate Variational Inference in MRFs. (arXiv:1604.02737v1 [cs.AI]) https://t.co/Wa5woCQRi3,0,6," Abstract: Almost all of the work in graphical models for game theory has mirrored previous work in probabilistic graphical models. Our work considers the opposite direction: Taking advantage of recent advances in equilibrium computation for belief inference. In particular, we present formulations of inference problems in Markov random fields (MRFs) as computation of equilibria in a certain class of game-theoretic graphical models. While some previous work explores this direction, none of that work concretely establishes the precise connection between variational probabilistic inference in MRFs and correlated equilibria. There is no work that exploits recent theoretical and empirical results from the literature on algorithmic and computational game theory on the tractable, polynomial-time computation of exact or approximate correlated equilibria in graphical games with arbitrary, loopy graph structure. Our work discusses how to design new algorithms with equally tractable guarantees for the computation of approximate variational inference in MRFs. In addition, inspired by a previously stated game-theoretic view of state-of-the-art tree-reweighed (TRW) message-passing techniques for belief inference as zero-sum game, we propose a different, general-sum potential game to design approximate fictitious-play techniques. We perform synthetic experiments evaluating our proposed approximation algorithms with standard methods and TRW on several classes of classical Ising models. Our experiments show that our global approach is competitive, particularly shinning in a class of Ising models with constant, ""highly attractive"" edge-weights, in which it is often better than all other alternatives we evaluated. While our local approach was not as effective as our global approach or TRW, almost all of the alternatives are often no better than a simple baseline: estimate the marginal probability to be 0.5. "
719686576570245120,2016-04-12 00:40:25,https://t.co/wgc7BZMAeB,Active Learning for Online Recognition of Human Activities from Streaming Videos. (arXiv:1604.02855v1 [stat.ML]) https://t.co/wgc7BZMAeB,1,6," Abstract: Recognising human activities from streaming videos poses unique challenges to learning algorithms: predictive models need to be scalable, incrementally trainable, and must remain bounded in size even when the data stream is arbitrarily long. Furthermore, as parameter tuning is problematic in a streaming setting, suitable approaches should be parameterless, and make no assumptions on what class labels may occur in the stream. We present here an approach to the recognition of human actions from streaming data which meets all these requirements by: (1) incrementally learning a model which adaptively covers the feature space with simple local classifiers; (2) employing an active learning strategy to reduce annotation requests; (3) achieving promising accuracy within a fixed model size. Extensive experiments on standard benchmarks show that our approach is competitive with state-of-the-art non-incremental methods, and outperforms the existing active incremental baselines. "
719686575467143169,2016-04-12 00:40:25,https://t.co/tnl0wmznpq,Gaussian Process Domain Experts for Model Adaptation in Facial Behavior Analysis. (arXiv:1604.02917v1 [stat.ML]) https://t.co/tnl0wmznpq,0,2," Abstract: We present a novel approach for supervised domain adaptation that is based upon the probabilistic framework of Gaussian processes (GPs). Specifically, we introduce domain-specific GPs as local experts for facial expression classification from face images. The adaptation of the classifier is facilitated in probabilistic fashion by conditioning the target expert on multiple source experts. Furthermore, in contrast to existing adaptation approaches, we also learn a target expert from available target data solely. Then, a single and confident classifier is obtained by combining the predictions from multiple experts based on their confidence. Learning of the model is efficient and requires no retraining/reweighting of the source classifiers. We evaluate the proposed approach on two publicly available datasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression classification. To this end, we perform adaptation of two contextual factors: 'where' (view) and 'who' (subject). We show in our experiments that the proposed approach consistently outperforms both source and target classifiers, while using as few as 30 target examples. It also outperforms the state-of-the-art approaches for supervised domain adaptation. "
719686574007578624,2016-04-12 00:40:25,https://t.co/HKvXlNqlac,Demystifying Fixed k-Nearest Neighbor Information Estimators. (arXiv:1604.03006v1 [cs.LG]) https://t.co/HKvXlNqlac,0,3," Abstract: Estimating mutual information from i.i.d. samples drawn from an unknown joint density function is a basic statistical problem of broad interest with multitudinous applications. The most popular estimator is one proposed by Kraskov and St\""ogbauer and Grassberger (KSG) in 2004, and is nonparametric and based on the distances of each sample to its $k^{\rm th}$ nearest neighboring sample, where $k$ is a fixed small integer. Despite its widespread use (part of scientific software packages), theoretical properties of this estimator have been largely unexplored. In this paper we demonstrate that the estimator is consistent and also identify an upper bound on the rate of convergence of the bias as a function of number of samples. We argue that the superior performance benefits of the KSG estimator stems from a curious ""correlation boosting"" effect and build on this intuition to modify the KSG estimator in novel ways to construct a superior estimator. As a byproduct of our investigations, we obtain nearly tight rates of convergence of the $\ell_2$ error of the well known fixed $k$ nearest neighbor estimator of differential entropy by Kozachenko and Leonenko. "
719686572195639297,2016-04-12 00:40:24,https://t.co/JzUbao1tWb,Variational Latent Gaussian Process for Recovering Single-Trial Dynamics from Population Spike Trains. (arXiv:1604… https://t.co/JzUbao1tWb,0,5," Abstract: When governed by underlying low-dimensional dynamics, the interdependence of simultaneously recorded population of neurons can be explained by a small number of shared factors, or a low-dimensional trajectory. Recovering these latent trajectories, particularly from single-trial population recordings, may help us understand the dynamics that drive neural computation. However, due to the biophysical constraints and noise in the spike trains, inferring trajectories from data is a challenging statistical problem in general. Here, we propose a practical and efficient inference method, called the variational latent Gaussian process (vLGP). The vLGP combines a generative model with a history-dependent point process observation together with a smoothness prior on the latent trajectories. The vLGP improves upon earlier methods for recovering latent trajectories, which assume either observation models inappropriate for point processes or linear dynamics. We compare and validate vLGP on both simulated datasets and population recordings from the primary visual cortex. In the V1 dataset, we find that vLGP achieves substantially higher performance than previous methods for predicting omitted spike trains, as well as capturing both the toroidal topology of visual stimuli space, and the noise-correlation. These results show that vLGP is a robust method with a potential to reveal hidden neural dynamics from large-scale neural recordings. "
719686570845028352,2016-04-12 00:40:24,https://t.co/mc6Fqgu4Ly,Manifold Gaussian Processes for Regression. (arXiv:1402.5876v4 [stat.ML] UPDATED) https://t.co/mc6Fqgu4Ly,1,11," Abstract: Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and non-differentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a GP regression from the feature space to observed space. The Manifold GP is a full GP and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth functions where standard GPs perform poorly, such as step functions and robotics tasks with contacts. "
719686569590964224,2016-04-12 00:40:23,https://t.co/MV3bGSf1IG,On the Sensitivity of the Lasso to the Number of Predictor Variables. (arXiv:1403.4544v2 [stat.ML] UPDATED) https://t.co/MV3bGSf1IG,0,6," Abstract: The Lasso is a computationally efficient regression regularization procedure that can produce sparse estimators when the number of predictors (p) is large. Oracle inequalities provide probability loss bounds for the Lasso estimator at a deterministic choice of the regularization parameter. These bounds tend to zero if p is appropriately controlled, and are thus commonly cited as theoretical justification for the Lasso and its ability to handle high-dimensional settings. Unfortunately, in practice the regularization parameter is not selected to be a deterministic quantity, but is instead chosen using a random, data-dependent procedure. To address this shortcoming of previous theoretical work, we study the loss of the Lasso estimator when tuned optimally for prediction. Assuming orthonormal predictors and a sparse true model, we prove that the probability that the best possible predictive performance of the Lasso deteriorates as p increases is positive and can be arbitrarily close to one given a sufficiently high signal to noise ratio and sufficiently large p. We further demonstrate empirically that the amount of deterioration in performance can be far worse than the oracle inequalities suggest and provide a real data example where deterioration is observed. "
719686568303292416,2016-04-12 00:40:23,https://t.co/heOeYONRtv,Dissimilarity-based Sparse Subset Selection. (arXiv:1407.6810v2 [cs.LG] UPDATED) https://t.co/heOeYONRtv,0,2," Abstract: Finding an informative subset of a large collection of data points or models is at the center of many problems in computer vision, recommender systems, bio/health informatics as well as image and natural language processing. Given pairwise dissimilarities between the elements of a `source set' and a `target set,' we consider the problem of finding a subset of the source set, called representatives or exemplars, that can efficiently describe the target set. We formulate the problem as a row-sparsity regularized trace minimization problem. Since the proposed formulation is, in general, NP-hard, we consider a convex relaxation. The solution of our optimization finds representatives and the assignment of each element of the target set to each representative, hence, obtaining a clustering. We analyze the solution of our proposed optimization as a function of the regularization parameter. We show that when the two sets jointly partition into multiple groups, our algorithm finds representatives from all groups and reveals clustering of the sets. In addition, we show that the proposed framework can effectively deal with outliers. Our algorithm works with arbitrary dissimilarities, which can be asymmetric or violate the triangle inequality. To efficiently implement our algorithm, we consider an Alternating Direction Method of Multipliers (ADMM) framework, which results in quadratic complexity in the problem size. We show that the ADMM implementation allows to parallelize the algorithm, hence further reducing the computational time. Finally, by experiments on real-world datasets, we show that our proposed algorithm improves the state of the art on the two problems of scene categorization using representative images and time-series modeling and segmentation using representative~models. "
719686566705291264,2016-04-12 00:40:23,https://t.co/hUrMrBtJV1,Validation of Matching. (arXiv:1411.0023v2 [cs.LG] UPDATED) https://t.co/hUrMrBtJV1,1,4," Abstract: We introduce a technique to compute probably approximately correct (PAC) bounds on precision and recall for matching algorithms. The bounds require some verified matches, but those matches may be used to develop the algorithms. The bounds can be applied to network reconciliation or entity resolution algorithms, which identify nodes in different networks or values in a data set that correspond to the same entity. For network reconciliation, the bounds do not require knowledge of the network generation process. "
719686564981403648,2016-04-12 00:40:22,https://t.co/mHoukg0EP1,Graph Connectivity in Noisy Sparse Subspace Clustering. (arXiv:1504.01046v2 [stat.ML] UPDATED) https://t.co/mHoukg0EP1,1,4," Abstract: Subspace clustering is the problem of clustering data points into a union of low-dimensional linear/affine subspaces. It is the mathematical abstraction of many important problems in computer vision, image processing and machine learning. A line of recent work (4, 19, 24, 20) provided strong theoretical guarantee for sparse subspace clustering (4), the state-of-the-art algorithm for subspace clustering, on both noiseless and noisy data sets. It was shown that under mild conditions, with high probability no two points from different subspaces are clustered together. Such guarantee, however, is not sufficient for the clustering to be correct, due to the notorious ""graph connectivity problem"" (15). In this paper, we investigate the graph connectivity problem for noisy sparse subspace clustering and show that a simple post-processing procedure is capable of delivering consistent clustering under certain ""general position"" or ""restricted eigenvalue"" assumptions. We also show that our condition is almost tight with adversarial noise perturbation by constructing a counter-example. These results provide the first exact clustering guarantee of noisy SSC for subspaces of dimension greater then 3. "
719686563664412673,2016-04-12 00:40:22,https://t.co/ZFkjNFqb0P,Randomized Robust Subspace Recovery for High Dimensional Data Matrices. (arXiv:1505.05901v2 [stat.ML] UPDATED) https://t.co/ZFkjNFqb0P,0,4," Abstract: This paper explores and analyzes two randomized designs for robust Principal Component Analysis (PCA) employing low-dimensional data sketching. In one design, a data sketch is constructed using random column sampling followed by low dimensional embedding, while in the other, sketching is based on random column and row sampling. Both designs are shown to bring about substantial savings in complexity and memory requirements for robust subspace learning over conventional approaches that use the full scale data. A characterization of the sample and computational complexity of both designs is derived in the context of two distinct outlier models, namely, sparse and independent outlier models. The proposed randomized approach can provably recover the correct subspace with computational and sample complexity that are almost independent of the size of the data. The results of the mathematical analysis are confirmed through numerical simulations using both synthetic and real data. "
719686562544545794,2016-04-12 00:40:22,https://t.co/9M7GvtHGFw,No Regret Bound for Extreme Bandits. (arXiv:1508.02933v3 [stat.ML] UPDATED) https://t.co/9M7GvtHGFw,0,6," Abstract: Algorithms for hyperparameter optimization abound, all of which work well under different and often unverifiable assumptions. Motivated by the general challenge of sequentially choosing which algorithm to use, we study the more specific task of choosing among distributions to use for random hyperparameter optimization. This work is naturally framed in the extreme bandit setting, which deals with sequentially choosing which distribution from a collection to sample in order to minimize (maximize) the single best cost (reward). Whereas the distributions in the standard bandit setting are primarily characterized by their means, a number of subtleties arise when we care about the minimal cost as opposed to the average cost. For example, there may not be a well-defined ""best"" distribution as there is in the standard bandit setting. The best distribution depends on the rewards that have been obtained and on the remaining time horizon. Whereas in the standard bandit setting, it is sensible to compare policies with an oracle which plays the single best arm, in the extreme bandit setting, there are multiple sensible oracle models. We define a sensible notion of ""extreme regret"" in the extreme bandit setting, which parallels the concept of regret in the standard bandit setting. We then prove that no policy can asymptotically achieve no extreme regret. "
719686561193967616,2016-04-12 00:40:21,https://t.co/ZeyVlYLJr4,Doubly Stochastic Primal-Dual Coordinate Method for Empirical Risk Minimization and Bilinear Saddle-Point Problem.… https://t.co/ZeyVlYLJr4,0,7," Abstract: We proposed a doubly stochastic primal-dual coordinate (DSPDC) optimization algorithm for empirical risk minimization, which can be formulated as a bilinear saddle-point problem. In each iteration, our method randomly samples a block of coordinates of the primal and dual solutions to update. The convergence of our method is established in both the distance from the current iterate to the optimal solution and the primal-dual objective gap. We show that the proposed method has a lower overall complexity than existing coordinate methods when either the data matrix has a factorized structure or the proximal mapping on each block is computationally expensive, e.g., involves an eigenvalue decomposition. Furthermore, we give a theoretical lower bound on the iteration complexity of a general family of primal-dual (block) coordinate methods for bilinear saddle-point problems, which also includes DSPDC. "
719686558706741249,2016-04-12 00:40:21,https://t.co/JeoNHurBoj,Generalized Statistical Tests for mRNA and Protein Subcellular Spatial Patterning against Complete Spatial Randomn… https://t.co/JeoNHurBoj,0,2," Abstract: We derive generalized estimators for a number of spatial statistics that have been used in the analysis of spatially resolved omics data, such as Ripley's K, H and L functions, clustering index, and degree of clustering, which allow these statistics to be calculated on data modelled by arbitrary random measures (RMs). Our estimators generalize those typically used to calculate these statistics on point process data, allowing them to be calculated on RMs which assign continuous values to spatial regions, for instance to model protein intensity. The clustering index (H*) compares Ripley's H function calculated empirically to its distribution under complete spatial randomness (CSR), leading us to consider CSR null hypotheses for RMs which are not point-processes when generalizing this statistic. We thus consider restricted classes of completely random measures which can be simulated directly (Gamma processes and Marked Poisson Processes), as well as the general class of all CSR RMs, for which we derive an exact permutation-based H* estimator. We establish several properties of the estimators, including bounds on the accuracy of our general Ripley K estimator, its relationship to a previous estimator for the cross-correlation measure, and the relationship of our generalized H* estimator to previous statistics. To test the ability of our approach to identify spatial patterning, we use Fluorescent In Situ Hybridization (FISH) and Immunofluorescence (IF) data to probe for mRNA and protein subcellular localization patterns respectively in polarizing mouse fibroblasts on micropattened cells. We observe correlated patterns of clustering over time for corresponding mRNAs and proteins, suggesting a deterministic effect of mRNA localization on protein localization for several pairs tested, including one case in which spatial patterning at the mRNA level has not been previously demonstrated. "
719686556894785536,2016-04-12 00:40:20,https://t.co/RtVvdv5jTm,Data Cleaning for XML Electronic Dictionaries via Statistical Anomaly Detection. (arXiv:1602.07807v2 [cs.DB] UPDAT… https://t.co/RtVvdv5jTm,0,4," Abstract: Many important forms of data are stored digitally in XML format. Errors can occur in the textual content of the data in the fields of the XML. Fixing these errors manually is time-consuming and expensive, especially for large amounts of data. There is increasing interest in the research, development, and use of automated techniques for assisting with data cleaning. Electronic dictionaries are an important form of data frequently stored in XML format that frequently have errors introduced through a mixture of manual typographical entry errors and optical character recognition errors. In this paper we describe methods for flagging statistical anomalies as likely errors in electronic dictionaries stored in XML format. We describe six systems based on different sources of information. The systems detect errors using various signals in the data including uncommon characters, text length, character-based language models, word-based language models, tied-field length ratios, and tied-field transliteration models. Four of the systems detect errors based on expectations automatically inferred from content within elements of a single field type. We call these single-field systems. Two of the systems detect errors based on correspondence expectations automatically inferred from content within elements of multiple related field types. We call these tied-field systems. For each system, we provide an intuitive analysis of the type of error that it is successful at detecting. Finally, we describe two larger-scale evaluations using crowdsourcing with Amazon's Mechanical Turk platform and using the annotations of a domain expert. The evaluations consistently show that the systems are useful for improving the efficiency with which errors in XML electronic dictionaries can be detected. "
719686555779141632,2016-04-12 00:40:20,https://t.co/uf2fhfhbU7,Stability and Structural Properties of Gene Regulation Networks with Coregulation Rules. (arXiv:1602.08753v2 [stat… https://t.co/uf2fhfhbU7,0,2," Abstract: Coregulation of the expression of groups of genes has been extensively demonstrated empirically in bacterial and eukaryotic systems. Such coregulation can arise through the use of shared regulatory motifs, which allow the coordinated expression of modules (and module groups) of functionally related genes across the genome. Coregulation can also arise through the physical association of multi-gene complexes through chromosomal looping, which are then transcribed together. We present a general formalism for modeling coregulation rules in the framework of Random Boolean Networks (RBN), and develop specific models for transcription factor networks with modular structure (including module groups, and multi-input modules (MIM) with autoregulation) and multi-gene complexes (including hierarchical differentiation between multi-gene complex members). We develop a mean-field approach to analyse the stability of large networks incorporating coregulation, and show that autoregulated MIM and hierarchical gene-complex models can achieve greater stability than networks without coregulation whose rules have matching activation frequency. We provide further analysis of the stability of small networks of both kinds through simulations. We also characterize several general properties of the transients and attractors in the hierarchical coregulation model, and show using simulations that the steady-state distribution factorizes hierarchically as a Bayesian network in a Markov Jump Process analogue of the RBN model. "
719686554495688704,2016-04-12 00:40:20,https://t.co/6ovuHTXYSC,Evaluating the Performance of Offensive Linemen in the NFL. (arXiv:1603.07593v2 [stat.ML] UPDATED) https://t.co/6ovuHTXYSC,0,3," Abstract: How does one objectively measure the performance of an individual offensive lineman in the NFL? The existing literature proposes various measures that rely on subjective assessments of game film, but has yet to develop an objective methodology to evaluate performance. Using a variety of statistics related to an offensive lineman's performance, we develop a framework to objectively analyze the overall performance of an individual offensive lineman and determine specific linemen who are overvalued or undervalued relative to their salary. We identify eight players across the 2013-2014 and 2014-2015 NFL seasons that are considered to be overvalued or undervalued and corroborate the results with existing metrics that are based on subjective evaluation. To the best of our knowledge, the techniques set forth in this work have not been utilized in previous works to evaluate the performance of NFL players at any position, including offensive linemen. "
719323410896187394,2016-04-11 00:37:20,https://t.co/hTrl616Cie,A Unified Bayesian Framework for Sparse Non-negative Matrix Factorization. (arXiv:1604.02181v1 [stat.ML]) https://t.co/hTrl616Cie,3,9," Abstract: In this work, we study the sparse non-negative matrix factorization (Sparse NMF or S-NMF) problem. NMF and S-NMF are popular machine learning tools which decompose a given non-negative dataset into a dictionary and an activation matrix, where both are constrained to be non-negative. We review how common concave sparsity measures from the compressed sensing literature can be extended to the S-NMF problem. Furthermore, we show that these sparsity measures have a Bayesian interpretation and each one corresponds to a specific prior on the activations. We present a comprehensive Sparse Bayesian Learning (SBL) framework for modeling non-negative data and provide details for Type I and Type II inference procedures. We show that efficient multiplicative update rules can be employed to solve the S-NMF problem for the penalty functions discussed and present experimental results validating our assertions. "
719323409914703872,2016-04-11 00:37:19,https://t.co/iloY5b56Q3,Online Open World Recognition. (arXiv:1604.02275v1 [cs.CV]) https://t.co/iloY5b56Q3,0,8," Abstract: As we enter into the big data age and an avalanche of images have become readily available, recognition systems face the need to move from close, lab settings where the number of classes and training data are fixed, to dynamic scenarios where the number of categories to be recognized grows continuously over time, as well as new data providing useful information to update the system. Recent attempts, like the open world recognition framework, tried to inject dynamics into the system by detecting new unknown classes and adding them incrementally, while at the same time continuously updating the models for the known classes. incrementally adding new classes and detecting instances from unknown classes, while at the same time continuously updating the models for the known classes. In this paper we argue that to properly capture the intrinsic dynamic of open world recognition, it is necessary to add to these aspects (a) the incremental learning of the underlying metric, (b) the incremental estimate of confidence thresholds for the unknown classes, and (c) the use of local learning to precisely describe the space of classes. We extend three existing metric learning algorithms towards these goals by using online metric learning. Experimentally we validate our approach on two large-scale datasets in different learning scenarios. For all these scenarios our proposed methods outperform their non-online counterparts. We conclude that local and online learning is important to capture the full dynamics of open world recognition. "
719323408870285312,2016-04-11 00:37:19,https://t.co/lSgtsXY08I,A Kernel-Based Nonparametric Test for Anomaly Detection over Line Networks. (arXiv:1404.0298v2 [cs.IT] UPDATED) https://t.co/lSgtsXY08I,0,3," Abstract: The nonparametric problem of detecting existence of an anomalous interval over a one dimensional line network is studied. Nodes corresponding to an anomalous interval (if exists) receive samples generated by a distribution q, which is different from the distribution p that generates samples for other nodes. If anomalous interval does not exist, then all nodes receive samples generated by p. It is assumed that the distributions p and q are arbitrary, and are unknown. In order to detect whether an anomalous interval exists, a test is built based on mean embeddings of distributions into a reproducing kernel Hilbert space (RKHS) and the metric of maximummean discrepancy (MMD). It is shown that as the network size n goes to infinity, if the minimum length of candidate anomalous intervals is larger than a threshold which has the order O(log n), the proposed test is asymptotically successful, i.e., the probability of detection error approaches zero asymptotically. An efficient algorithm to perform the test with substantial computational complexity reduction is proposed, and is shown to be asymptotically successful if the condition on the minimum length of candidate anomalous interval is satisfied. Numerical results are provided, which are consistent with the theoretical results. "
719323407729442819,2016-04-11 00:37:19,https://t.co/Zq1gNMKo4T,Support Consistency of Direct Sparse-Change Learning in Markov Networks. (arXiv:1407.0581v10 [stat.ML] UPDATED) https://t.co/Zq1gNMKo4T,2,3," Abstract: We study the problem of learning sparse structure changes between two Markov networks $P$ and $Q$. Rather than fitting two Markov networks separately to two sets of data and figuring out their differences, a recent work proposed to learn changes \emph{directly} via estimating the ratio between two Markov network models. In this paper, we give sufficient conditions for \emph{successful change detection} with respect to the sample size $n_p, n_q$, the dimension of data $m$, and the number of changed edges $d$. When using an unbounded density ratio model we prove that the true sparse changes can be consistently identified for $n_p = \Omega(d^2 \log \frac{m^2+m}{2})$ and $n_q = \Omega({n_p^2})$, with an exponentially decaying upper-bound on learning error. Such sample complexity can be improved to $\min(n_p, n_q) = \Omega(d^2 \log \frac{m^2+m}{2})$ when the boundedness of the density ratio model is assumed. Our theoretical guarantee can be applied to a wide range of discrete/continuous Markov networks. "
719323406668296192,2016-04-11 00:37:19,https://t.co/wqQMcUSuie,Block-diagonal covariance selection for high-dimensional Gaussian graphical models. (arXiv:1511.04033v2 [math.ST] … https://t.co/wqQMcUSuie,0,4," Abstract: Gaussian graphical models are widely utilized to infer and visualize networks of dependencies between continuous variables. However, inferring the graph is difficult when the sample size is small compared to the number of variables. To reduce the number of parameters to estimate in the model, we propose a non-asymptotic model selection procedure supported by strong theoretical guarantees based on an oracle inequality and a minimax lower bound. The covariance matrix of the model is approximated by a block-diagonal matrix. The structure of this matrix is detected by thresholding the sample covariance matrix, where the threshold is selected using the slope heuristic. Based on the block-diagonal structure of the covariance matrix, the estimation problem is divided into several independent problems: subsequently, the network of dependencies between variables is inferred using the graphical lasso algorithm in each block. The performance of the procedure is illustrated on simulated data. An application to a real gene expression dataset with a limited sample size is also presented: the dimension reduction allows attention to be objectively focused on interactions among smaller subsets of genes, leading to a more parsimonious and interpretable modular network. "
719323405468758016,2016-04-11 00:37:18,https://t.co/QJQdUailQQ,Feature Selection for Regression Problems Based on the Morisita Estimator of Intrinsic Dimension. (arXiv:1602.0021… https://t.co/QJQdUailQQ,0,6," Abstract: Data acquisition, storage and management have been improved, while the key factors of many phenomena are not well known. Consequently, irrelevant and redundant features artificially increase the size of datasets, which complicates learning tasks, such as regression. To address this problem, feature selection methods have been proposed. This paper introduces a new supervised filter based on the Morisita estimator of intrinsic dimension. It can identify relevant features and distinguish between redundant and irrelevant information. Besides, it offers a clear graphical representation of the results and it can be easily implemented in different programming languages. Comprehensive numerical experiments are conducted using simulated datasets characterized by different levels of complexity, sample size and noise. The suggested algorithm is also successfully tested on a selection of real world applications and compared with RReliefF using extreme learning machine. In addition, a new measure of feature relevance is presented and discussed. "
719323404210454528,2016-04-11 00:37:18,https://t.co/5stKncLvuT,"The ""Sprekend Nederland"" project and its application to accent location. (arXiv:1602.02499v2 [stat.ML] UPDATED) https://t.co/5stKncLvuT",0,3," Abstract: This paper describes the data collection effort that is part of the project Sprekend Nederland (The Netherlands Talking), and discusses its potential use in Automatic Accent Location. We define Automatic Accent Location as the task to describe the accent of a speaker in terms of the location of the speaker and its history. We discuss possible ways of describing accent location, the consequence these have for the task of automatic accent location, and potential evaluation metrics. "
719323402411122689,2016-04-11 00:37:18,https://t.co/qXnEij83o9,Pymanopt: A Python Toolbox for Manifold Optimization using Automatic Differentiation. (arXiv:1603.03236v2 [cs.MS] … https://t.co/qXnEij83o9,5,23," Abstract: Optimization on manifolds is a class of methods for optimization of an objective function, subject to constraints which are smooth, in the sense that the set of points which satisfy the constraints admits the structure of a differentiable manifold. While many optimization problems are of the described form, technicalities of differential geometry and the laborious calculation of derivatives pose a significant barrier for experimenting with these methods. We introduce Pymanopt (available at this https URL), a toolbox for optimization on manifolds, implemented in Python, that---similarly to the Manopt Matlab toolbox---implements several manifold geometries and optimization algorithms. Moreover, we lower the barriers to users further by using automated differentiation for calculating derivative information, saving users time and saving them from potential calculation and implementation errors. "
718237841214554117,2016-04-08 00:43:40,https://t.co/khqQe8rzzm,Building Ensembles of Adaptive Nested Dichotomies with Random-Pair Selection. (arXiv:1604.01854v1 [stat.ML]) https://t.co/khqQe8rzzm,0,1," Abstract: A system of nested dichotomies is a method of decomposing a multi-class problem into a collection of binary problems. Such a system recursively splits the set of classes into two subsets, and trains a binary classifier to distinguish between each subset. Even though ensembles of nested dichotomies with random structure have been shown to perform well in practice, using a more sophisticated class subset selection method can be used to improve classification accuracy. We investigate an approach to this problem called random-pair selection, and evaluate its effectiveness compared to other published methods of subset selection. We show that our method outperforms other methods in many cases when forming ensembles of nested dichotomies, and is at least on par in all other cases. "
718237839763369987,2016-04-08 00:43:39,https://t.co/yvx0KkXmwi,Deep Online Convex Optimization with Gated Games. (arXiv:1604.01952v1 [cs.LG]) https://t.co/yvx0KkXmwi,0,4," Abstract: Methods from convex optimization are widely used as building blocks for deep learning algorithms. However, the reasons for their empirical success are unclear, since modern convolutional networks (convnets), incorporating rectifier units and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. This paper provides the first convergence rates for gradient descent on rectifier convnets. The proof utilizes the particular structure of rectifier networks which consists in binary active/inactive gates applied on top of an underlying linear network. The approach generalizes to max-pooling, dropout and maxout. In other words, to precisely the neural networks that perform best empirically. The key step is to introduce gated games, an extension of convex games with similar convergence properties that capture the gating function of rectifiers. The main result is that rectifier convnets converge to a critical point at a rate controlled by the gated-regret of the units in the network. Corollaries of the main result include: (i) a game-theoretic description of the representations learned by a neural network; (ii) a logarithmic-regret algorithm for training neural nets; and (iii) a formal setting for analyzing conditional computation in neural nets that can be applied to recently developed models of attention. "
718237838307946496,2016-04-08 00:43:39,https://t.co/JhEzgzvMXG,Monitoring Chinese Population Migration in Consecutive Weekly Basis from Intra-city scale to Inter-province scale … https://t.co/JhEzgzvMXG,0,1," Abstract: Population migration is valuable information which leads to proper decision in urban-planning strategy, massive investment, and many other fields. For instance, inter-city migration is a posterior evidence to see if the government's constrain of population works, and inter-community immigration might be a prior evidence of real estate price hike. With timely data, it is also impossible to compare which city is more favorable for the people, suppose the cities release different new regulations, we could also compare the customers of different real estate development groups, where they come from, where they probably will go. Unfortunately these data was not available. In this paper, leveraging the data generated by positioning team in Didi, we propose a novel approach that timely monitoring population migration from community scale to provincial scale. Migration can be detected as soon as in a week. It could be faster, the setting of a week is for statistical purpose. A monitoring system is developed, then applied nation wide in China, some observations derived from the system will be presented in this paper. This new method of migration perception is origin from the insight that nowadays people mostly moving with their personal Access Point (AP), also known as WiFi hotspot. Assume that the ratio of AP moving to the migration of population is constant, analysis of comparative population migration would be feasible. More exact quantitative research would also be done with few sample research and model regression. The procedures of processing data includes many steps: eliminating the impact of pseudo-migration AP, for instance pocket WiFi, and second-hand traded router; distinguishing moving of population with moving of companies; identifying shifting of AP by the finger print clusters, etc.. "
718237835912941568,2016-04-08 00:43:38,https://t.co/tpb6FyqDiL,An Adaptive Resample-Move Algorithm for Estimating Normalizing Constants. (arXiv:1604.01972v1 [stat.ML]) https://t.co/tpb6FyqDiL,0,1," Abstract: The estimation of normalizing constants is a fundamental step in probabilistic model comparison. Sequential Monte Carlo methods may be used for this task and have the advantage of being inherently parallelizable. However, the standard choice of using a fixed number of particles at each iteration is suboptimal because some steps will contribute disproportionately to the variance of the estimate. We introduce an adaptive version of the Resample-Move algorithm, in which the particle set is adaptively expanded whenever a better approximation of an intermediate distribution is needed. The algorithm builds on the expression for the optimal number of particles and the corresponding minimum variance found under ideal conditions. Benchmark results on challenging Gaussian Process Classification and Restricted Boltzmann Machine applications show that Adaptive Resample-Move (ARM) estimates the normalizing constant with a smaller variance, using less computational resources, than either Resample-Move with a fixed number of particles or Annealed Importance Sampling. A further advantage over Annealed Importance Sampling is that ARM is easier to tune. "
718237834616905728,2016-04-08 00:43:38,https://t.co/ZtdyZdRvkl,Online Optimization of Smoothed Piecewise Constant Functions. (arXiv:1604.01999v1 [cs.LG]) https://t.co/ZtdyZdRvkl,0,1," Abstract: We study online optimization of smoothed piecewise constant functions over the domain [0, 1). This is motivated by the problem of adaptively picking parameters of learning algorithms as in the recently introduced framework by Gupta and Roughgarden (2016). Majority of the machine learning literature has focused on Lipschitz-continuous functions or functions with bounded gradients. 1 This is with good reason---any learning algorithm suffers linear regret even against piecewise constant functions that are chosen adversarially, arguably the simplest of non-Lipschitz continuous functions. The smoothed setting we consider is inspired by the seminal work of Spielman and Teng (2004) and the recent work of Gupta and Roughgarden---in this setting, the sequence of functions may be chosen by an adversary, however, with some uncertainty in the location of discontinuities. We give algorithms that achieve sublinear regret in the full information and bandit settings. "
718237833106964482,2016-04-08 00:43:38,https://t.co/i1aAWS4L5e,Combinatorial Topic Models using Small-Variance Asymptotics. (arXiv:1604.02027v1 [cs.LG]) https://t.co/i1aAWS4L5e,0,1," Abstract: Topic models have emerged as fundamental tools in unsupervised machine learning. Most modern topic modeling algorithms take a probabilistic view and derive inference algorithms based on Latent Dirichlet Allocation (LDA) or its variants. In contrast, we study topic modeling as a combinatorial optimization problem, and propose a new objective function derived from LDA by passing to the small-variance limit. We minimize the derived objective by using ideas from combinatorial optimization, which results in a new, fast, and high-quality topic modeling algorithm. In particular, we show that our results are competitive with popular LDA-based topic modeling approaches, and also discuss the (dis)similarities between our approach and its probabilistic counterparts. "
718237831752245248,2016-04-08 00:43:37,https://t.co/mExRY73UaI,Hankel Matrix Nuclear Norm Regularized Tensor Completion for N-dimensional Exponential Signals. (arXiv:1604.02100v… https://t.co/mExRY73UaI,0,2," Abstract: Signals are usually modeled as a superposition of exponential functions in spectroscopy of chemistry, biology and medical imaging. However, for fast data acquisition or other inevitable reasons, only a small amount of samples may be acquired. How to recover the full signal is then of great interest. Existing approaches can not efficiently recover N-dimensional exponential signals with N>=3. This paper studies the problem of recovering N-dimensional (particularly $N\geq 3$) exponential signals from partial observations, and we formulate this problem as a low-rank tensor completion problem with exponential factors. The full signal is reconstructed by simultaneously exploiting the CANDECOMP/PARAFAC (CP) tensor decomposition and the exponential structure of the associated factors, of which the latter is promoted by minimizing an objective function involving the nuclear norm of Hankel matrices. Experimental results on simulated and real magnetic resonance spectroscopy data show that the proposed approach can successfully recover full signals from very limited samples and is robust to the estimated tensor rank. "
718237830233899010,2016-04-08 00:43:37,https://t.co/TrvgA6Wprw,Multilevel Weighted Support Vector Machine for Classification on Healthcare Data with Missing Values. (arXiv:1604.… https://t.co/TrvgA6Wprw,0,2," Abstract: This work is motivated by the needs of predictive analytics on healthcare data as represented by Electronic Medical Records. Such data is invariably problematic: noisy, with missing entries, with imbalance in classes of interests, leading to serious bias in predictive modeling. Since standard data mining methods often produce poor performance measures, we argue for development of specialized techniques of data-preprocessing and classification. In this paper, we propose a new method to simultaneously classify large datasets and reduce the effects of missing values. It is based on a multilevel framework of the cost-sensitive SVM and the expected maximization imputation method for missing values, which relies on iterated regression analyses. We compare classification results of multilevel SVM-based algorithms on public benchmark datasets with imbalanced classes and missing values as well as real data in health applications, and show that our multilevel SVM-based method produces fast, and more accurate and robust classification results. "
718237828786884609,2016-04-08 00:43:37,https://t.co/rFtrOE9l4b,Penalty methods for a class of non-Lipschitz optimization problems. (arXiv:1409.2558v3 [math.OC] UPDATED) https://t.co/rFtrOE9l4b,0,3," Abstract: We consider a class of constrained optimization problems with a possibly nonconvex non-Lipschitz objective and a convex feasible set being the intersection of a polyhedron and a possibly degenerate ellipsoid. Such problems have a wide range of applications in data science, where the objective is used for inducing sparsity in the solutions while the constraint set models the noise tolerance and incorporates other prior information for data fitting. To solve this class of constrained optimization problems, a common approach is the penalty method. However, there is little theory on exact penalization for problems with nonconvex and non-Lipschitz objective functions. In this paper, we study the existence of exact penalty parameters regarding local minimizers, stationary points and $\epsilon$-minimizers under suitable assumptions. Moreover, we discuss a penalty method whose subproblems are solved via a nonmonotone proximal gradient method with a suitable update scheme for the penalty parameters, and prove the convergence of the algorithm to a KKT point of the constrained problem. Preliminary numerical results demonstrate the efficiency of the penalty method for finding sparse solutions of underdetermined linear systems. "
718237827432058880,2016-04-08 00:43:36,https://t.co/xi0wZrFsMj,Dimensionality Reduction with Subspace Structure Preservation. (arXiv:1412.2404v3 [cs.LG] UPDATED) https://t.co/xi0wZrFsMj,0,3," Abstract: Modeling data as being sampled from a union of independent subspaces has been widely applied to a number of real world applications. However, dimensionality reduction approaches that theoretically preserve this independence assumption have not been well studied. Our key contribution is to show that $2K$ projection vectors are sufficient for the independence preservation of any $K$ class data sampled from a union of independent subspaces. It is this non-trivial observation that we use for designing our dimensionality reduction technique. In this paper, we propose a novel dimensionality reduction algorithm that theoretically preserves this structure for a given dataset. We support our theoretical analysis with empirical results on both synthetic and real world data achieving \textit{state-of-the-art} results compared to popular dimensionality reduction techniques. "
718237825972494337,2016-04-08 00:43:36,https://t.co/bYZ0gDv1Eg,Relaxed Leverage Sampling for Low-rank Matrix Completion. (arXiv:1503.06379v3 [cs.IT] UPDATED) https://t.co/bYZ0gDv1Eg,0,2," Abstract: We consider the problem of exact recovery of any $m\times n$ matrix of rank $\varrho$ from a small number of observed entries via the standard nuclear norm minimization framework. Such low-rank matrices have degrees of freedom $(m+n)\varrho - \varrho^2$. We show that any arbitrary low-rank matrices can be recovered exactly from a $\Theta\left(((m+n)\varrho - \varrho^2)\log^2(m+n)\right)$ randomly sampled entries, thus matching the lower bound on the required number of entries (in terms of degrees of freedom), with an additional factor of $O(\log^2(m+n))$. To achieve this bound on sample size we observe each entry with probabilities proportional to the sum of corresponding row and column leverage scores, minus their product. We show that this relaxation in sampling probabilities (as opposed to sum of leverage scores in Chen et al, 2014) can give us an $O(\varrho^2\log^2(m+n))$ additive improvement on the (best known) sample size obtained by Chen et al, 2014, for the nuclear norm minimization. Experiments on real data corroborate the theoretical improvement on sample size. Further, exact recovery of $(a)$ incoherent matrices (with restricted leverage scores), and $(b)$ matrices with only one of the row or column spaces to be incoherent, can be performed using our relaxed leverage score sampling, via nuclear norm minimization, without knowing the leverage scores a priori. In such settings also we can achieve improvement on sample size. "
718237824584130564,2016-04-08 00:43:36,https://t.co/EQjKooP5mw,Active Learning Algorithms for Graphical Model Selection. (arXiv:1602.00354v2 [stat.ML] UPDATED) https://t.co/EQjKooP5mw,0,5," Abstract: The problem of learning the structure of a high dimensional graphical model from data has received considerable attention in recent years. In many applications such as sensor networks and proteomics it is often expensive to obtain samples from all the variables involved simultaneously. For instance, this might involve the synchronization of a large number of sensors or the tagging of a large number of proteins. To address this important issue, we initiate the study of a novel graphical model selection problem, where the goal is to optimize the total number of scalar samples obtained by allowing the collection of samples from only subsets of the variables. We propose a general paradigm for graphical model selection where feedback is used to guide the sampling to high degree vertices, while obtaining only few samples from the ones with the low degrees. We instantiate this framework with two specific active learning algorithms, one of which makes mild assumptions but is computationally expensive, while the other is more computationally efficient but requires stronger (nevertheless standard) assumptions. Whereas the sample complexity of passive algorithms is typically a function of the maximum degree of the graph, we show that the sample complexity of our algorithms is provable smaller and that it depends on a novel local complexity measure that is akin to the average degree of the graph. We finally demonstrate the efficacy of our framework via simulations. "
718237821740441601,2016-04-08 00:43:35,https://t.co/wYimkkhD67,Modeling and Estimation of Discrete-Time Reciprocal Processes via Probabilistic Graphical Models. (arXiv:1603.0441… https://t.co/wYimkkhD67,0,3," Abstract: Reciprocal processes are acausal generalizations of Markov processes introduced by Bernstein in 1932. In the literature, a significant amount of attention has been focused on developing dynamical models for reciprocal processes. In this paper, we provide a probabilistic graphical model for reciprocal processes. This leads to a principled solution of the smoothing problem via message passing algorithms. For the finite state space case, convergence analysis is revisited via the Hilbert metric. "
717875097080561664,2016-04-07 00:42:15,https://t.co/AOMacXZQQz,Collaborative Representation Learning. (arXiv:1604.01433v1 [cs.IT]) https://t.co/AOMacXZQQz,0,6," Abstract: This paper investigates an information-theoretic approach to the problem of collaborative representation learning: how to extract salient features of statistical relationships in order to build cooperatively meaningful representations of some relevant content. Modeling the structure of data and its hidden representations by independently identically distributed samples, our goal is to study fundamental limits of the so-called Two-way Collaborative Representation Learning (TW-CRL) and the Collaborative Distributed Representation Learning (CDRL) problems. The TW-CRL problem consists of two distant encoders that separately observe marginal (dependent) components $X_1$ and $X_2$ and can cooperate through multiple exchanges of limited information with the aim of learning hidden representations $(Y_1,Y_2)$, which can be arbitrarily dependent on $(X_1,X_2)$. On the other hand, in CDRL there are two cooperating encoders and the learner of the hidden representation $Y$ is a third node which can listen the exchanges between the two encoders. The relevance (figure-of-merit) of such learned representations is measured in terms of a normalized (per-sample) multi-letter mutual information metric. Inner and outer bounds to the complexity-relevance region of these problems are derived from which optimality is characterized for several cases of interest. Our resulting complexity-relevance regions are finally evaluated for binary symmetric and Gaussian statistical models showing how to identify comparatively random features that represent complexity-constrained statistics for the inference of the hidden representations. "
717875093314076672,2016-04-07 00:42:14,https://t.co/Ks19z8yN51,"Comments on: ""A Random Forest Guided Tour"" by G. Biau and E. Scornet. (arXiv:1604.01515v1 [math.ST]) https://t.co/Ks19z8yN51",0,7," Abstract: This paper is a comment on the survey paper by Biau and Scornet (2016) about random forests. We focus on the problem of quantifying the impact of each ingredient of random forests on their performance. We show that such a quantification is possible for a simple pure forest , leading to conclusions that could apply more generally. Then, we consider ""hold-out"" random forests, which are a good middle point between ""toy"" pure forests and Breiman's original random forests. "
717875089522487296,2016-04-07 00:42:13,https://t.co/WarYPHRp9d,Manifold unwrapping using density ridges. (arXiv:1604.01602v1 [stat.ML]) https://t.co/WarYPHRp9d,0,6," Abstract: Research on manifold learning within a density ridge estimation framework has shown great potential in recent work for both estimation and de-noising of manifolds, building on the intuitive and well-defined notion of principal curves and surfaces. However, the problem of unwrapping or unfolding manifolds has received relatively little attention within the density ridge approach, despite being an integral part of manifold learning in general. This paper proposes two novel algorithms for unwrapping manifolds based on estimated principal curves and surfaces for one- and multi-dimensional manifolds respectively. The methods of unwrapping are founded in the realization that both principal curves and principal surfaces will have inherent local maxima of the probability density function. Following this observation, coordinate systems that follow the shape of the manifold can be computed by following the integral curves of the gradient flow of a kernel density estimate on the manifold. Furthermore, since integral curves of the gradient flow of a kernel density estimate is inherently local, we propose to stitch together local coordinate systems using parallel transport along the manifold. We provide numerical experiments on both real and synthetic data that illustrates clear and intuitive unwrapping results comparable to state-of-the-art manifold learning algorithms. "
717875085764378624,2016-04-07 00:42:12,https://t.co/dgdJZa9qqY,Towards Bayesian Deep Learning: A Survey. (arXiv:1604.01662v1 [stat.ML]) https://t.co/dgdJZa9qqY,6,26," Abstract: While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a general introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this survey, we also discuss the relationship and differences between Bayesian deep learning and other related topics like Bayesian treatment of neural networks. "
717875081855307776,2016-04-07 00:42:11,https://t.co/yYNSmoLUjN,A U-statistic Approach to Hypothesis Testing for Structure Discovery in Undirected Graphical Models. (arXiv:1604.0… https://t.co/yYNSmoLUjN,0,4," Abstract: Structure discovery in graphical models is the determination of the topology of a graph that encodes conditional independence properties of the joint distribution of all variables in the model. For some class of probability distributions, an edge between two variables is present if and only if the corresponding entry in the precision matrix is non-zero. For a finite sample estimate of the precision matrix, entries close to zero may be due to low sample effects, or due to an actual association between variables; these two cases are not readily distinguishable. %Fisher provided a hypothesis test based on a parametric approximation to the distribution of an entry in the precision matrix of a Gaussian distribution, but this may not provide valid upper bounds on $p$-values for non-Gaussian distributions. Many related works on this topic consider potentially restrictive distributional or sparsity assumptions that may not apply to a data sample of interest, and direct estimation of the uncertainty of an estimate of the precision matrix for general distributions remains challenging. Consequently, we make use of results for $U$-statistics and apply them to the covariance matrix. By probabilistically bounding the distortion of the covariance matrix, we can apply Weyl's theorem to bound the distortion of the precision matrix, yielding a conservative, but sound test threshold for a much wider class of distributions than considered in previous works. The resulting test enables one to answer with statistical significance whether an edge is present in the graph, and convergence results are known for a wide range of distributions. The computational complexities is linear in the sample size enabling the application of the test to large data samples for which computation time becomes a limiting factor. We experimentally validate the correctness and scalability of the test on multivariate distributions for which the distributional assumptions of competing tests result in underestimates of the false positive ratio. By contrast, the proposed test remains sound, promising to be a useful tool for hypothesis testing for diverse real-world problems. "
717875078634057729,2016-04-07 00:42:10,https://t.co/yJpCJWGaiz,Improving Back-Propagation by Adding an Adversarial Gradient. (arXiv:1510.04189v2 [stat.ML] UPDATED) https://t.co/yJpCJWGaiz,0,12," Abstract: The back-propagation algorithm is widely used for learning in artificial neural networks. A challenge in machine learning is to create models that generalize to new data samples not seen in the training data. Recently, a common flaw in several machine learning algorithms was discovered: small perturbations added to the input data lead to consistent misclassification of data samples. Samples that easily mislead the model are called adversarial examples. Training a ""maxout"" network on adversarial examples has shown to decrease this vulnerability, but also increase classification performance. This paper shows that adversarial training has a regularizing effect also in networks with logistic, hyperbolic tangent and rectified linear units. A simple extension to the back-propagation method is proposed, that adds an adversarial gradient to the training. The extension requires an additional forward and backward pass to calculate a modified input sample, or mini batch, used as input for standard back-propagation learning. The first experimental results on MNIST show that the ""adversarial back-propagation"" method increases the resistance to adversarial examples and boosts the classification performance. The extension reduces the classification error on the permutation invariant MNIST from 1.60% to 0.95% in a logistic network, and from 1.40% to 0.78% in a network with rectified linear units. Results on CIFAR-10 indicate that the method has a regularizing effect similar to dropout in fully connected networks. Based on these promising results, adversarial back-propagation is proposed as a stand-alone regularizing method that should be further investigated. "
717875075823886336,2016-04-07 00:42:10,https://t.co/sXzWcPDzaJ,Denoising and Covariance Estimation of Single Particle Cryo-EM Images. (arXiv:1602.06632v3 [cs.CV] UPDATED) https://t.co/sXzWcPDzaJ,0,3," Abstract: The problem of image restoration in cryo-EM entails correcting for the effects of the Contrast Transfer Function (CTF) and noise. Popular methods for image restoration include `phase flipping', which corrects only for the Fourier phases but not amplitudes, and Wiener filtering, which requires the spectral signal to noise ratio. We propose a new image restoration method which we call `Covariance Wiener Filtering' (CWF). In CWF, the covariance matrix of the projection images is used within the classical Wiener filtering framework for solving the image restoration deconvolution problem. Our estimation procedure for the covariance matrix is new and successfully corrects for the CTF. We demonstrate the efficacy of CWF by applying it to restore both simulated and experimental cryo-EM images. Results with experimental datasets demonstrate that CWF provides a good way to evaluate the particle images and to see what the dataset contains even without 2D classification and averaging. "
717513245796483072,2016-04-06 00:44:23,https://t.co/eEaeRIUwfp,A Dynamic Bayesian Network Model for Inventory Level Estimation in Retail Marketing. (arXiv:1604.01075v1 [stat.ML]) https://t.co/eEaeRIUwfp,0,6," Abstract: Many retailers today employ inventory management systems based on Re-Order Point Policies, most of which rely on the assumption that all decreases in product inventory levels result from product sales. Unfortunately, it usually happens that small but random quantities of the product get lost, stolen or broken without record as time passes, e.g., as a consequence of shoplifting. This is usual for retailers handling large varieties of inexpensive products, e.g., grocery stores. In turn, over time these discrepancies lead to stock freezing problems, i.e., situations where the system believes the stock is above the re-order point but the actual stock is at zero, and so no replenishments or sales occur. Motivated by these issues, we model the interaction between sales, losses, replenishments and inventory levels as a Dynamic Bayesian Network (DBN), where the inventory levels are unobserved (i.e., hidden) variables we wish to estimate. We present an Expectation-Maximization (EM) algorithm to estimate the parameters of the sale and loss distributions, which relies on solving a one-dimensional dynamic program for the E-step and on solving two separate one-dimensional nonlinear programs for the M-step. "
717513244320141312,2016-04-06 00:44:22,https://t.co/KYlAH8Vufh,Restricted Isometry Constants for Gaussian and Rademacher matrices. (arXiv:1604.01171v1 [math.ST]) https://t.co/KYlAH8Vufh,0,4," Abstract: Restricted Isometry Constants (RICs) are a pivotal notion in Compressed Sensing (CS) as these constants finely assess how a linear operator is conditioned on the set of sparse vectors and hence how it performs in stable and robust sparse regression (SRSR). While it is an open problem to construct deterministic matrices with apposite RICs, one can prove that such matrices exist using random matrices models. One of the most popular model may be the sub-Gaussian matrices since it encompasses random matrices with Gaussian or Rademacher i.i.d. entries. In this paper, we provide a description of the phase transition on SRSR for those matrices using state-of-the-art (small) deviation estimates on their extreme eigenvalues. In particular, we show new upper bounds on RICs for Gaussian and Rademacher matrices. This allows us to derive a new lower bound on the probability of getting SRSR. One of the benefit of this novel approach is to broaden the scope of phase transition on RICs and SRSR to the quest of universality results in Random Matrix Theory. "
717513241933570048,2016-04-06 00:44:22,https://t.co/GwQpaVieQ6,Fast methods for training Gaussian processes on large data sets. (arXiv:1604.01250v1 [stat.ML]) https://t.co/GwQpaVieQ6,1,12," Abstract: Gaussian process regression (GPR) is a non-parametric Bayesian technique for interpolating or fitting data. The main barrier to further uptake of this powerful tool rests in the computational costs associated with the matrices which arise when dealing with large data sets. Here, we derive some simple results which we have found useful for speeding up the learning stage in the GPR algorithm, and especially for performing Bayesian model comparison between different covariance functions. We apply our techniques to both synthetic and real data and quantify the speed-up relative to using nested sampling to numerically evaluate model evidences. "
717513240616501248,2016-04-06 00:44:21,https://t.co/W8u4VJzkiT,Feature extraction using Latent Dirichlet Allocation and Neural Networks: A case study on movie synopses. (arXiv:1… https://t.co/W8u4VJzkiT,3,6," Abstract: Feature extraction has gained increasing attention in the field of machine learning, as in order to detect patterns, extract information, or predict future observations from big data, the urge of informative features is crucial. The process of extracting features is highly linked to dimensionality reduction as it implies the transformation of the data from a sparse high-dimensional space, to higher level meaningful abstractions. This dissertation employs Neural Networks for distributed paragraph representations, and Latent Dirichlet Allocation to capture higher level features of paragraph vectors. Although Neural Networks for distributed paragraph representations are considered the state of the art for extracting paragraph vectors, we show that a quick topic analysis model such as Latent Dirichlet Allocation can provide meaningful features too. We evaluate the two methods on the CMU Movie Summary Corpus, a collection of 25,203 movie plot summaries extracted from Wikipedia. Finally, for both approaches, we use K-Nearest Neighbors to discover similar movies, and plot the projected representations using T-Distributed Stochastic Neighbor Embedding to depict the context similarities. These similarities, expressed as movie distances, can be used for movies recommendation. The recommended movies of this approach are compared with the recommended movies from IMDB, which use a collaborative filtering recommendation approach, to show that our two models could constitute either an alternative or a supplementary recommendation approach. "
717513239052099584,2016-04-06 00:44:21,https://t.co/K0RRtAVvek,Bayesian Optimization with Exponential Convergence. (arXiv:1604.01348v1 [stat.ML]) https://t.co/K0RRtAVvek,1,10," Abstract: This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence requires access to the delta-cover sampling, which was considered to be impractical. Our approach eliminates both requirements and achieves an exponential convergence rate. "
717513237978288129,2016-04-06 00:44:21,https://t.co/USqXyRVjzS,Nonparametric Detection of Geometric Structures over Networks. (arXiv:1604.01351v1 [stat.ML]) https://t.co/USqXyRVjzS,0,4," Abstract: Nonparametric detection of existence of an anomalous structure over a network is investigated. Nodes corresponding to the anomalous structure (if one exists) receive samples generated by a distribution q, which is different from a distribution p generating samples for other nodes. If an anomalous structure does not exist, all nodes receive samples generated by p. It is assumed that the distributions p and q are arbitrary and unknown. The goal is to design statistically consistent tests with probability of errors converging to zero as the network size becomes asymptotically large. Kernel-based tests are proposed based on maximum mean discrepancy that measures the distance between mean embeddings of distributions into a reproducing kernel Hilbert space. Detection of an anomalous interval over a line network is first studied. Sufficient conditions on minimum and maximum sizes of candidate anomalous intervals are characterized in order to guarantee the proposed test to be consistent. It is also shown that certain necessary conditions must hold to guarantee any test to be universally consistent. Comparison of sufficient and necessary conditions yields that the proposed test is order-level optimal and nearly optimal respectively in terms of minimum and maximum sizes of candidate anomalous intervals. Generalization of the results to other networks is further developed. Numerical results are provided to demonstrate the performance of the proposed tests. "
717513236808122368,2016-04-06 00:44:21,https://t.co/gZVp7y2kxe,Partial Membership Latent Dirichlet Allocation. (arXiv:1511.02821v2 [stat.ML] UPDATED) https://t.co/gZVp7y2kxe,0,4," Abstract: Topic models (e.g., pLSA, LDA, SLDA) have been widely used for segmenting imagery. These models are confined to crisp segmentation. Yet, there are many images in which some regions cannot be assigned a crisp label (e.g., transition regions between a foggy sky and the ground or between sand and water at a beach). In these cases, a visual word is best represented with partial memberships across multiple topics. To address this, we present a partial membership latent Dirichlet allocation (PM-LDA) model and associated parameter estimation algorithms. Experimental results on two natural image datasets and one SONAR image dataset show that PM-LDA can produce both crisp and soft semantic image segmentations; a capability existing methods do not have. "
717513235612753924,2016-04-06 00:44:20,https://t.co/D2qMaa9Ofg,Fast Metric Learning For Deep Neural Networks. (arXiv:1511.06442v5 [cs.LG] UPDATED) https://t.co/D2qMaa9Ofg,0,5," Abstract: Similarity metrics are a core component of many information retrieval and machine learning systems. In this work we propose a method capable of learning a similarity metric from data equipped with a binary relation. By considering only the similarity constraints, and initially ignoring the features, we are able to learn target vectors for each instance using one of several appropriately designed loss functions. A regression model can then be constructed that maps novel feature vectors to the same target vector space, resulting in a feature extractor that computes vectors for which a predefined metric is a meaningful measure of similarity. We present results on both multiclass and multi-label classification datasets that demonstrate considerably faster convergence, as well as higher accuracy on the majority of the intrinsic evaluation tasks and all extrinsic evaluation tasks. "
717513234274713600,2016-04-06 00:44:20,https://t.co/5hTJzeE5L2,Probabilistic Integration: A Role for Statisticians in Numerical Analysis?. (arXiv:1512.00933v2 [stat.ML] UPDATED) https://t.co/5hTJzeE5L2,0,7," Abstract: A research frontier has emerged in scientific computation, founded on the principle that numerical error entails epistemic uncertainty that ought to be subjected to statistical analysis. This viewpoint raises several interesting challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational pipeline. This paper examines thoroughly the case for probabilistic numerical methods in statistical computation and a specific case study is presented for Markov chain and Quasi Monte Carlo methods. A probabilistic integrator is equipped with a full distribution over its output, providing a measure of epistemic uncertainty that is shown to be statistically valid at finite computational levels, as well as in asymptotic regimes. The approach is motivated by expensive integration problems, where, as in krigging, one is willing to expend, at worst, cubic computational effort in order to gain uncertainty quantification. There, probabilistic integrators enjoy the ""best of both worlds"", leveraging the sampling efficiency of Monte Carlo methods whilst providing a principled route to assessment of the impact of numerical error on scientific conclusions. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and uncertainty quantification in oil reservoir modelling. "
717513232982867968,2016-04-06 00:44:20,https://t.co/o59WJAsqpi,RSG: Beating SG without Smoothness and/or Strong Convexity. (arXiv:1512.03107v8 [math.OC] UPDATED) https://t.co/o59WJAsqpi,0,4," Abstract: In this paper, we study the efficiency of a {\bf R}estarted {\bf S}ub{\bf G}radient (RSG) method that periodically restarts the standard subgradient method (SG). We show that, when applied to a broad class of convex optimization problems, RSG method can find an $\epsilon$-optimal solution with a low complexity than SG method. In particular, we first show that RSG can reduce the dependence of SG's iteration complexity on the distance between the initial solution and the optimal set to that between the $\epsilon$-level set and the optimal set. In addition, we show the advantages of RSG over SG in solving three different families of convex optimization problems. (a) For the problems whose epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with local quadratic growth property, RSG has an $O(\frac{1}{\epsilon}\log(\frac{1}{\epsilon}))$ iteration complexity. (c) For the problems that admit a local Kurdyka-\L ojasiewicz property with a power constant of $\beta\in[0,1)$, RSG has an $O(\frac{1}{\epsilon^{2\beta}}\log(\frac{1}{\epsilon}))$ iteration complexity. On the contrary, with only the standard analysis, the iteration complexity of SG is known to be $O(\frac{1}{\epsilon^2})$ for these three classes of problems. The novelty of our analysis lies at exploiting the lower bound of the first-order optimality residual at the $\epsilon$-level set. It is this novelty that allows us to explore the local properties of functions (e.g., local quadratic growth property, local Kurdyka-\L ojasiewicz property, more generally local error bounds) to develop the improved convergence of RSG. We demonstrate the effectiveness of the proposed algorithms on several machine learning tasks including regression and classification. "
717513231691071489,2016-04-06 00:44:19,https://t.co/wzNULPvAPw,Uniform {\varepsilon}-Stability of Distributed Nonlinear Filtering over DNAs: Gaussian-Finite HMMs. (arXiv:1602.04… https://t.co/wzNULPvAPw,0,2," Abstract: In this work, we study stability of distributed filtering of Markov chains with finite state space, partially observed in conditionally Gaussian noise. We consider a nonlinear filtering scheme over a Distributed Network of Agents (DNA), which relies on the distributed evaluation of the likelihood part of the centralized nonlinear filter and is based on a particular specialization of the Alternating Direction Method of Multipliers (ADMM) for fast average consensus. Assuming the same number of consensus steps between any two consecutive noisy measurements for each sensor in the network, we fully characterize a minimal number of such steps, such that the distributed filter remains uniformly stable with a prescribed accuracy level, {\varepsilon} \in (0,1], within a finite operational horizon, T, and across all sensors. Stability is in the sense of the \ell_1-norm between the centralized and distributed versions of the posterior at each sensor, and at each time within T. Roughly speaking, our main result shows that uniform {\varepsilon}-stability of the distributed filtering process depends only loglinearly on T and (roughly) the size of the network, and only logarithmically on 1/{\varepsilon}. If this total loglinear bound is fulfilled, any additional consensus iterations will incur a fully quantified further exponential decay in the consensus error. Our bounds are universal, in the sense that they are independent of the particular structure of the Gaussian Hidden Markov Model (HMM) under consideration. "
717513230395039744,2016-04-06 00:44:19,https://t.co/pHlnNLn3tA,The Multivariate Generalised von Mises: Inference and applications. (arXiv:1602.05003v3 [stat.ML] UPDATED) https://t.co/pHlnNLn3tA,0,2," Abstract: Circular variables arise in a multitude of data-modelling contexts ranging from robotics to the social sciences, but they have been largely overlooked by the machine learning community. This paper partially redresses this imbalance by extending some standard probabilistic modelling tools to the circular domain. First we introduce a new multivariate distribution over circular variables, called the multivariate Generalised von Mises (mGvM) distribution. This distribution can be constructed by restricting and renormalising a general multivariate Gaussian distribution to the unit hyper-torus. Previously proposed multivariate circular distributions are shown to be special cases of this construction. Second, we introduce a new probabilistic model for circular regression, that is inspired by Gaussian Processes, and a method for probabilistic principal component analysis with circular hidden variables. These models can leverage standard modelling tools (e.g. covariance functions and methods for automatic relevance determination). Third, we show that the posterior distribution in these models is a mGvM distribution which enables development of an efficient variational free-energy scheme for performing approximate inference and approximate maximum-likelihood learning. "
717513228952150016,2016-04-06 00:44:19,https://t.co/zKaHNJ4WKi,Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series. (arXiv:1602.07109v3 [stat.ML]… https://t.co/zKaHNJ4WKi,0,10, Abstract: Approximate variational inference has shown to be a powerful tool for modeling unknown complex probability distributions. Recent advances in the field allow us to learn probabilistic models of sequences that actively exploit spatial and temporal structure. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line. 
717513227526094848,2016-04-06 00:44:18,https://t.co/gNegXDAby5,A Latent Variable Recurrent Neural Network for Discourse Relation Language Models. (arXiv:1603.01913v2 [cs.CL] UPD… https://t.co/gNegXDAby5,2,10," Abstract: This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences. A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations. The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task. The resulting model can therefore employ a training objective that includes not only discourse relation classification, but also word prediction. As a result, it outperforms state-of-the-art alternatives for two tasks: implicit discourse relation classification in the Penn Discourse Treebank, and dialog act classification in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline. "
717513224585879552,2016-04-06 00:44:18,https://t.co/6D8EEuvr2D,Stochastic Variance Reduction for Nonconvex Optimization. (arXiv:1603.06160v2 [math.OC] UPDATED) https://t.co/6D8EEuvr2D,0,7," Abstract: We study nonconvex finite-sum problems and analyze stochastic variance reduced gradient (SVRG) methods for them. SVRG and related methods have recently surged into prominence for convex optimization given their edge over stochastic gradient descent (SGD); but their theoretical analysis almost exclusively assumes convexity. In contrast, we prove non-asymptotic rates of convergence (to stationary points) of SVRG for nonconvex optimization, and show that it is provably faster than SGD and gradient descent. We also analyze a subclass of nonconvex problems on which SVRG attains linear convergence to the global optimum. We extend our analysis to mini-batch variants of SVRG, showing (theoretical) linear speedup due to mini-batching in parallel settings. "
717150351716196353,2016-04-05 00:42:22,https://t.co/mq0sml2flh,Multi-Relational Learning at Scale with ADMM. (arXiv:1604.00647v1 [stat.ML]) https://t.co/mq0sml2flh,0,3," Abstract: Learning from multiple-relational data which contains noise, ambiguities, or duplicate entities is essential to a wide range of applications such as statistical inference based on Web Linked Data, recommender systems, computational biology, and natural language processing. These tasks usually require working with very large and complex datasets - e.g., the Web graph - however, current approaches to multi-relational learning are not practical for such scenarios due to their high computational complexity and poor scalability on large data. In this paper, we propose a novel and scalable approach for multi-relational factorization based on consensus optimization. Our model, called ConsMRF, is based on the Alternating Direction Method of Multipliers (ADMM) framework, which enables us to optimize each target relation using a smaller set of parameters than the state-of-the-art competitors in this task. Due to ADMM's nature, ConsMRF can be easily parallelized which makes it suitable for large multi-relational data. Experiments on large Web datasets - derived from DBpedia, Wikipedia and YAGO - show the efficiency and performance improvement of ConsMRF over strong competitors. In addition, ConsMRF near-linear scalability indicates great potential to tackle Web-scale problem sizes. "
717150350730465280,2016-04-05 00:42:22,https://t.co/FrJuEvCDWA,Writer-independent Feature Learning for Offline Signature Verification using Deep Convolutional Neural Networks. (… https://t.co/FrJuEvCDWA,0,4," Abstract: Automatic Offline Handwritten Signature Verification has been researched over the last few decades from several perspectives, using insights from graphology, computer vision, signal processing, among others. In spite of the advancements on the field, building classifiers that can separate between genuine signatures and skilled forgeries (forgeries made targeting a particular signature) is still hard. We propose approaching the problem from a feature learning perspective. Our hypothesis is that, in the absence of a good model of the data generation process, it is better to learn the features from data, instead of using hand-crafted features that have no resemblance to the signature generation process. To this end, we use Deep Convolutional Neural Networks to learn features in a writer-independent format, and use this model to obtain a feature representation on another set of users, where we train writer-dependent classifiers. We tested our method in two datasets: GPDS-960 and Brazilian PUC-PR. Our experimental results show that the features learned in a subset of the users are discriminative for the other users, including across different datasets, reaching close to the state-of-the-art in the GPDS dataset, and improving the state-of-the-art in the Brazilian PUC-PR dataset. "
717150349728030721,2016-04-05 00:42:22,https://t.co/HR4QOL3Uo7,Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models. (arXiv:1412.7461v2 [st… https://t.co/HR4QOL3Uo7,0,13," Abstract: The future predictive performance of a Bayesian model can be estimated using Bayesian cross-validation. In this article, we consider Gaussian latent variable models where the integration over the latent values is approximated using the Laplace method or expectation propagation (EP). We study the properties of several Bayesian leave-one-out (LOO) cross-validation approximations that in most cases can be computed with a small additional cost after forming the posterior approximation given the full data. Our main objective is to assess the accuracy of the approximative LOO cross-validation estimators. That is, for each method (Laplace and EP) we compare the approximate fast computation with the exact brute force LOO computation. Secondarily, we evaluate the accuracy of the Laplace and EP approximations themselves against a ground truth established through extensive Markov chain Monte Carlo simulation. Our empirical results show that the approach based upon a Gaussian approximation to the LOO marginal distribution (the so-called cavity distribution) gives the most accurate and reliable results among the fast methods. "
717150348729782272,2016-04-05 00:42:21,https://t.co/SD4Q7lP79e,Jet-Images -- Deep Learning Edition. (arXiv:1511.05190v2 [hep-ph] UPDATED) https://t.co/SD4Q7lP79e,0,3," Abstract: Building on the notion of a particle physics detector as a camera and the collimated streams of high energy particles, or jets, it measures as an image, we investigate the potential of machine learning techniques based on deep learning architectures to identify highly boosted W bosons. Modern deep learning algorithms trained on jet images can out-perform standard physically-motivated feature driven approaches to jet tagging. We develop techniques for visualizing how these features are learned by the network and what additional information is used to improve performance. This interplay between physically-motivated feature driven tools and supervised learning algorithms is general and can be used to significantly increase the sensitivity to discover new particles and new forces, and gain a deeper understanding of the physics within jets. "
717150347664482304,2016-04-05 00:42:21,https://t.co/obq5ldfv0C,Deep Reinforcement Learning in Large Discrete Action Spaces. (arXiv:1512.07679v2 [cs.AI] UPDATED) https://t.co/obq5ldfv0C,1,11," Abstract: Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm's abilities on a series of tasks having up to one million actions. "
717150346641072129,2016-04-05 00:42:21,https://t.co/swfYuxZlo0,Partial Recovery Bounds for the Sparse Stochastic Block Model. (arXiv:1602.00877v2 [cs.IT] UPDATED) https://t.co/swfYuxZlo0,0,3," Abstract: In this paper, we study the information-theoretic limits of community detection in the symmetric two-community stochastic block model, with intra-community and inter-community edge probabilities $\frac{a}{n}$ and $\frac{b}{n}$ respectively. We consider the sparse setting, in which $a$ and $b$ do not scale with $n$, and provide upper and lower bounds on the proportion of community labels recovered on average. We provide a numerical example for which the bounds are near-matching for moderate values of $a - b$, and matching in the limit as $a-b$ grows large. "
717150345357627392,2016-04-05 00:42:20,https://t.co/KKmgEtXaBr,Fast k-means with accurate bounds. (arXiv:1602.02514v4 [stat.ML] UPDATED) https://t.co/KKmgEtXaBr,1,5," Abstract: We propose a novel accelerated exact k-means algorithm, which performs better than the current state-of-the-art low-dimensional algorithm in 18 of 22 experiments, running up to 3 times faster. We also propose a general improvement of existing state-of-the-art accelerated exact k-means algorithms through better estimates of the distance bounds used to reduce the number of distance calculations, and get a speedup in 36 of 44 experiments, up to 1.8 times faster. We have conducted experiments with our own implementations of existing methods to ensure homogeneous evaluation of performance, and we show that our implementations perform as well or better than existing available implementations. Finally, we propose simplified variants of standard approaches and show that they are faster than their fully-fledged counterparts in 59 of 62 experiments. "
717150344376094720,2016-04-05 00:42:20,https://t.co/7l8iQbfadX,Noisy Activation Functions. (arXiv:1603.00391v3 [cs.LG] UPDATED) https://t.co/7l8iQbfadX,1,4," Abstract: Common nonlinear activation functions used in neural networks can cause training difficulties due to the saturation behavior of the activation function, which may hide dependencies that are not visible to vanilla-SGD (using first order gradients only). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that the gradients may flow easily, even if the noiseless application of the activation function would yield zero gradient. Large noise will dominate the noise-free gradient and allow stochastic gradient descent toexplore more. By adding noise only to the problematic parts of the activation function, we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by noisy variants helps training in many contexts, yielding state-of-the-art or competitive results on different datasets and task, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results. "
717150343205953536,2016-04-05 00:42:20,https://t.co/JBfGHuym8J,Convex block-sparse linear regression with expanders -- provably. (arXiv:1603.06313v2 [cs.IT] UPDATED) https://t.co/JBfGHuym8J,0,4," Abstract: Sparse matrices are favorable objects in machine learning and optimization. When such matrices are used, in place of dense ones, the overall complexity requirements in optimization can be significantly reduced in practice, both in terms of space and run-time. Prompted by this observation, we study a convex optimization scheme for block-sparse recovery from linear measurements. To obtain linear sketches, we use expander matrices, i.e., sparse matrices containing only few non-zeros per column. Hitherto, to the best of our knowledge, such algorithmic solutions have been only studied from a non-convex perspective. Our aim here is to theoretically characterize the performance of convex approaches under such setting. Our key novelty is the expression of the recovery error in terms of the model-based norm, while assuring that solution lives in the model. To achieve this, we show that sparse model-based matrices satisfy a group version of the null-space property. Our experimental findings on synthetic and real applications support our claims for faster recovery in the convex setting -- as opposed to using dense sensing matrices, while showing a competitive recovery performance. "
716790789175701504,2016-04-04 00:53:36,https://t.co/up4CICbym6,Nonparametric Spherical Topic Modeling with Word Embeddings. (arXiv:1604.00126v1 [cs.CL]) https://t.co/up4CICbym6,1,5," Abstract: Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference. "
716790787858636800,2016-04-04 00:53:35,https://t.co/DGckHIzgzV,Gradient-based learning algorithms with constant-error estimators: stability and convergence. (arXiv:1604.00151v1 … https://t.co/DGckHIzgzV,0,9," Abstract: In this paper, we present easily verifiable, sufficient conditions for both stability and convergence (to the minimum set) of gradient descent ($GD$) algorithms with bounded, non-diminishing errors. These errors often arise from using gradient estimators or because the objective function is noisy to begin with. Our work extends the contributions of Mangasarian \& Solodov and Bertsekas \& Tsitsiklis. Our framework improves over the aforementioned ones in that both stability (almost sure boundedness) and convergence are guaranteed even in the case of $GD$ with non-diminishing errors. We present a simplified, yet effective implementation of $GD$ using $SPSA$ with constant sensitivity parameters. Further, unlike other papers no `additional' restrictions are imposed on the step-size and so also on the learning rate when used to implement machine learning algorithms. Finally, we present the results of some experiments to validate the theory. "
716790786499747841,2016-04-04 00:53:35,https://t.co/QdEWX0dTKn,Gaussian process optimization through sampling from the maximum distribution. (arXiv:1604.00169v1 [stat.ML]) https://t.co/QdEWX0dTKn,0,12," Abstract: Bayesian optimization through Gaussian process regression is an effective method of optimizing an unknown function for which every measurement is expensive. It approximates the objective function and then recommends a new measurement point to try out. This recommendation is usually selected by optimizing a given acquisition function. After a sufficient number of measurements, a recommendation about the maximum is made. However, a key realization is that the maximum of a Gaussian process is not a deterministic point, but a random variable with a distribution of its own. This distribution cannot be calculated analytically. Our main contribution is a sequential Monte Carlo approach towards approximating this maximum distribution. Subsequently, by taking samples from this distribution, we enable Thompson sampling to be applied to (armed-bandit) optimization problems with a continuous input space. All this is done without requiring the optimization of a nonlinear acquisition function. Experiments have shown that the resulting optimization method has a competitive performance at keeping the cumulative regret limited. "
716790785128144896,2016-04-04 00:53:35,https://t.co/XBF0ALF3ra,"Network structure, metadata and the prediction of missing nodes. (arXiv:1604.00255v1 [physics.soc-ph]) https://t.co/XBF0ALF3ra",2,4," Abstract: The empirical validation of community detection methods is often based on available annotations on the nodes that serve as putative indicators of the large-scale network structure. Most often, the suitability of the annotations as topological descriptors itself is not assessed, and without this it is not possible to ultimately distinguish between actual shortcomings of the community detection algorithms on one hand, and the incompleteness, inaccuracy or structured nature of the data annotations themselves on the other. In this work we present a principled method to access both aspects simultaneously. We construct a joint generative model for the data and metadata, and a nonparametric Bayesian framework to infer its parameters from annotated datasets. We assess the quality of the metadata not according to its direct alignment with the network communities, but rather in its capacity to predict the placement of edges in the network. We also show how this feature can be used to predict the connections to missing nodes when only the metadata is available, as well as missing metadata. By investigating a wide range of datasets, we show that while there are seldom exact agreements between metadata tokens and the inferred data groups, the metadata is often informative of the network structure nevertheless, and can improve the prediction of missing nodes. This shows that the method uncovers meaningful patterns in both the data and metadata, without requiring or expecting a perfect agreement between the two. "
716790783941152768,2016-04-04 00:53:34,https://t.co/ODf3srMEeF,Building Machines That Learn and Think Like People. (arXiv:1604.00289v1 [cs.AI]) https://t.co/ODf3srMEeF,4,11," Abstract: Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models. "
716790782913544193,2016-04-04 00:53:34,https://t.co/xW7wJYQlKL,Kernel Methods for the Approximation of Nonlinear Systems. (arXiv:1108.2903v3 [math.OC] UPDATED) https://t.co/xW7wJYQlKL,0,6," Abstract: We introduce a data-driven order reduction method for nonlinear control systems, drawing on recent progress in machine learning and statistical dimensionality reduction. The method rests on the assumption that the nonlinear system behaves linearly when lifted into a high (or infinite) dimensional feature space where balanced truncation may be carried out implicitly. This leads to a nonlinear reduction map which can be combined with a representation of the system belonging to a reproducing kernel Hilbert space to give a closed, reduced order dynamical system which captures the essential input-output characteristics of the original model. Empirical simulations illustrating the approach are also provided. "
716790781365915648,2016-04-04 00:53:34,https://t.co/P581lLJXil,Kernel Methods for the Approximation of Some Key Quantities of Nonlinear Systems. (arXiv:1204.0563v2 [math.OC] UPD… https://t.co/P581lLJXil,0,5," Abstract: We introduce a data-based approach to estimating key quantities which arise in the study of nonlinear control systems and random nonlinear dynamical systems. Our approach hinges on the observation that much of the existing linear theory may be readily extended to nonlinear systems - with a reasonable expectation of success - once the nonlinear system has been mapped into a high or infinite dimensional feature space. In particular, we develop computable, non-parametric estimators approximating controllability and observability energy functions for nonlinear systems, and study the ellipsoids they induce. In all cases the relevant quantities are estimated from simulated or observed data. It is then shown that the controllability energy estimator provides a key means for approximating the invariant measure of an ergodic, stochastically forced nonlinear system. "
716790780204068864,2016-04-04 00:53:33,https://t.co/KLgS6O2Dlx,COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution. (arXiv:1507.02293v2 [cs.… https://t.co/KLgS6O2Dlx,1,3," Abstract: Information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics. We propose a temporal point process model, COEVOLVE, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. This model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks. Furthermore, we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. We experimented with both synthetic data and data gathered from Twitter, and show that our model provides a good fit to the data as well as more accurate predictions than alternatives. "
716790778555723776,2016-04-04 00:53:33,https://t.co/7XVBVuxbiQ,Minimax Optimal Variable Clustering in G-models via Cord. (arXiv:1508.01939v2 [stat.ME] UPDATED) https://t.co/7XVBVuxbiQ,0,2," Abstract: The goal of variable clustering is to partition a random vector ${\bf X} \in R^p$ in sub-groups of similar probabilistic behavior. Popular methods such as hierarchical clustering or $K$-means are algorithmic procedures applied to observations on ${\bf X}$, while no population level target is defined prior to estimation. We take a different view in this paper, where we propose and investigate model based variable clustering. We consider three models, of increasing level of complexity, termed generically $G$-models, with $G$ standing for the partition to be estimated. Motivated by the potential lack of identifiability of the $G$-latent models, which are currently used in problems involving variable clustering, we introduce two new classes of models, the $G$-exchangeable and the $G$-block covariance models. We show that both classes are identifiable, for any distribution of ${\bf X}$. Our focus is on clusters that are invariant with respect to unknown monotone transformations of the data, and that can be estimated in a computationally feasible manner. Both desiderata can be met if the clusters correspond to blocks in the copula correlation matrix of ${\bf X}$, assumed to have a Gaussian copula distribution. This motivates the introduction of a new similarity metric for cluster membership, CORD, and a homonymous method for cluster estimation. Central to our work is the derivation of the minimax value of the CORD cluster separation for exact partition recovery. We obtained the surprising result that this value is of order $\sqrt{{\log (p)}/{n}}$, irrespective of the number of clusters, or of the size of the smallest cluster. Our new procedure, CORD, available on CRAN, achieves this bound, is easy to implement and has computational complexity that is polynomial in $p$. "
716790776131416065,2016-04-04 00:53:33,https://t.co/h6SMJ5wQWl,Cost-sensitive Label Embedding for Multi-label Classification. (arXiv:1603.09048v2 [cs.LG] CROSS LISTED) https://t.co/h6SMJ5wQWl,0,5," Abstract: Label embedding (LE) is an important family of multi-label classification algorithms that digest the label information jointly for better performance. Different real-world applications evaluate performance by different cost functions of interest. Current LE algorithms often aim to optimize one specific cost function, but they can suffer from bad performance with respect to other cost functions. In this paper, we resolve the performance issue by proposing a novel cost-sensitive LE algorithm that takes the cost function of interest into account. The proposed algorithm, cost-sensitive label embedding with multidimensional scaling (CLEMS), approximates the cost information with the distances of the embedded vectors using the classic multidimensional scaling approach for manifold learning. CLEMS is able to deal with both symmetric and asymmetric cost functions, and effectively makes cost-sensitive decisions by nearest-neighbor decoding within the embedded vectors. Theoretical results justify that CLEMS achieves the cost-sensitivity and extensive experimental results demonstrate that CLEMS is significantly better than a wide spectrum of existing LE algorithms and state-of-the-art cost-sensitive algorithms across different cost functions. "
715699982771290112,2016-04-01 00:39:07,https://t.co/Gk157G2QWp,A Stratified Analysis of Bayesian Optimization Methods. (arXiv:1603.09441v1 [cs.LG]) https://t.co/Gk157G2QWp,3,12," Abstract: Empirical analysis serves as an important complement to theoretical analysis for studying practical Bayesian optimization. Often empirical insights expose strengths and weaknesses inaccessible to theoretical analysis. We define two metrics for comparing the performance of Bayesian optimization methods and propose a ranking mechanism for summarizing performance within various genres or strata of test functions. These test functions serve to mimic the complexity of hyperparameter optimization problems, the most prominent application of Bayesian optimization, but with a closed form which allows for rapid evaluation and more predictable behavior. This offers a flexible and efficient way to investigate functions with specific properties of interest, such as oscillatory behavior or an optimum on the domain boundary. "
715699981819199488,2016-04-01 00:39:07,https://t.co/7TrO3AVZHf,Sparse Representation of Multivariate Extremes with Applications to Anomaly Ranking. (arXiv:1603.09584v1 [stat.ML]) https://t.co/7TrO3AVZHf,0,2," Abstract: Extremes play a special role in Anomaly Detection. Beyond inference and simulation purposes, probabilistic tools borrowed from Extreme Value Theory (EVT), such as the angular measure, can also be used to design novel statistical learning methods for Anomaly Detection/ranking. This paper proposes a new algorithm based on multivariate EVT to learn how to rank observations in a high dimensional space with respect to their degree of 'abnormality'. The procedure relies on an original dimension-reduction technique in the extreme domain that possibly produces a sparse representation of multivariate extremes and allows to gain insight into the dependence structure thereof, escaping the curse of dimensionality. The representation output by the unsupervised methodology we propose here can be combined with any Anomaly Detection technique tailored to non-extreme data. As it performs linearly with the dimension and almost linearly in the data (in O(dn log n)), it fits to large scale problems. The approach in this paper is novel in that EVT has never been used in its multivariate version in the field of Anomaly Detection. Illustrative experimental results provide strong empirical evidence of the relevance of our approach. "
715699980833566723,2016-04-01 00:39:07,https://t.co/MmhH0SIkcI,Online Optimization with Costly and Noisy Measurements using Random Fourier Expansions. (arXiv:1603.09620v1 [cs.LG… https://t.co/MmhH0SIkcI,1,5," Abstract: This paper analyzes DONE, an online optimization algorithm that iteratively minimizes an unknown function based on costly and noisy measurements. The algorithm maintains a surrogate of the unknown function in the form of a random Fourier expansion (RFE). The surrogate is updated whenever a new measurement is available, and then used to determine the next measurement point. The algorithm is comparable to Bayesian optimization algorithms, but its computational complexity per iteration does not depend on the number of measurements. We derive several theoretical results that provide insight on how the hyper-parameters of the algorithm should be chosen. The algorithm is compared to a Bayesian optimization algorithm for a benchmark problem and three applications, namely, optical coherence tomography, optical beam-forming network tuning, and robot arm control. It is found that the DONE algorithm is significantly faster than Bayesian optimization in the discussed problems, while achieving a similar or better performance. "
715699979105484800,2016-04-01 00:39:06,https://t.co/UhZdNfnFf5,Building Better Detection with Privileged Information. (arXiv:1603.09638v1 [cs.CR]) https://t.co/UhZdNfnFf5,1,2," Abstract: For over a quarter century, security-relevant detection has been driven by models learned from input features collected from real or simulated environments. An artifact (e.g., network event, potential malware sample, suspicious email) is deemed malicious or non-malicious based on its similarity to the learned model at run-time. However, the training of the models has been historically limited to only those features available at run time. In this paper, we consider an alternate model construction approach that trains models using forensic ""privileged"" information--features available at training time but not at runtime--to improve the accuracy and resilience of detection systems. In particular, we adapt and extend recent advances in knowledge transfer, model influence, and distillation to enable the use of forensic data in a range of security domains. Our empirical study shows that privileged information increases detection precision and recall over a system with no privileged information: we observe up to 7.7% relative decrease in detection error for fast-flux bot detection, 8.6% for malware traffic detection, 7.3% for malware classification, and 16.9% for face recognition. We explore the limitations and applications of different privileged information techniques in detection systems. Such techniques open the door to systems that can integrate forensic data directly into detection models, and therein provide a means to fully exploit the information available about past security-relevant events. "
715699978023337984,2016-04-01 00:39:06,https://t.co/M2BYbq5pV4,Multi-task Recurrent Model for Speech and Speaker Recognition. (arXiv:1603.09643v1 [cs.CL]) https://t.co/M2BYbq5pV4,2,6," Abstract: Although highly correlated, speech and speaker recognition have been regarded as two independent tasks and studied by two communities. This is certainly not the way that people behave: we decipher both speech content and speaker traits at the same time. This paper presents a unified model to perform speech and speaker recognition simultaneously and altogether. The model is based on a unified neural network where the output of one task is fed to the input of the other, leading to a multi-task recurrent network. Experiments show that the joint model outperforms the task-specific models on both the two tasks. "
715699977104793601,2016-04-01 00:39:06,https://t.co/3zatbA16jH,Hierarchical Quickest Change Detection via Surrogates. (arXiv:1603.09739v1 [cs.LG]) https://t.co/3zatbA16jH,0,5," Abstract: Change detection (CD) in time series data is a critical problem as it reveal changes in the underlying generative processes driving the time series. Despite having received significant attention, one important unexplored aspect is how to efficiently utilize additional correlated information to improve the detection and the understanding of changepoints. We propose hierarchical quickest change detection (HQCD), a framework that formalizes the process of incorporating additional correlated sources for early changepoint detection. The core ideas behind HQCD are rooted in the theory of quickest detection and HQCD can be regarded as its novel generalization to a hierarchical setting. The sources are classified into targets and surrogates, and HQCD leverages this structure to systematically assimilate observed data to update changepoint statistics across layers. The decision on actual changepoints are provided by minimizing the delay while still maintaining reliability bounds. In addition, HQCD also uncovers interesting relations between changes at targets from changes across surrogates. We validate HQCD for reliability and performance against several state-of-the-art methods for both synthetic dataset (known changepoints) and several real-life examples (unknown changepoints). Our experiments indicate that we gain significant robustness without loss of detection delay through HQCD. Our real-life experiments also showcase the usefulness of the hierarchical setting by connecting the surrogate sources (such as Twitter chatter) to target sources (such as Employment related protests that ultimately lead to major uprisings). "
715699975997558785,2016-04-01 00:39:05,https://t.co/YbNdV1QVxq,An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes. (arXiv:1509.01520v2 [cs.CV]… https://t.co/YbNdV1QVxq,1,2," Abstract: Object tracking is an ubiquitous problem that appears in many applications such as remote sensing, audio processing, computer vision, human-machine interfaces, human-robot interaction, etc. Although thoroughly investigated in computer vision, tracking a time-varying number of persons remains a challenging open problem. In this paper, we propose an on-line variational Bayesian model for multi-person tracking from cluttered visual observations provided by person detectors. The contributions of this paper are the followings. First, we propose a variational Bayesian framework for tracking an unknown and varying number of persons. Second, our model results in a variational expectation-maximization (VEM) algorithm with closed-form expressions for the posterior distributions of the latent variables and for the estimation of the model parameters. Third, the proposed model exploits observations from multiple detectors, and it is therefore multimodal by nature. Finally, we propose to embed both object-birth and object-visibility processes in an effort to robustly handle person appearances and disappearances over time. Evaluated on classical multiple person tracking datasets, our method shows competitive results with respect to state-of-the-art multiple-object tracking models, such as the probability hypothesis density (PHD) filter among others. "
715699974915366919,2016-04-01 00:39:05,https://t.co/MTpFB6H8Fp,Getting started with particle Metropolis-Hastings for inference in nonlinear dynamical models. (arXiv:1511.01707v4… https://t.co/MTpFB6H8Fp,2,6," Abstract: We provide a gentle introduction to the particle Metropolis-Hastings (PMH) algorithm for parameter inference in nonlinear state space models (SSMs) together with a software implementation in the statistical programming language R. Throughout this tutorial, we develop an implementation of the PMH algorithm (and the integrated particle filter), which is distributed as the package pmhtutorial available from the CRAN repository. Moreover, we provide the reader with some intuition for how the algorithm operates and discuss some solutions to numerical problems that might occur in practice. To illustrate the use of PMH, we consider parameter inference in a linear Gaussian SSM with synthetic data and a nonlinear stochastic volatility model with real-world data. We conclude the tutorial by discussing important possible improvements to the algorithm and we also list suitable references for further study. "
715699973862637568,2016-04-01 00:39:05,https://t.co/M7nCvZK8nA,BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies. (arXiv:1511.06909v7 [… https://t.co/M7nCvZK8nA,2,10," Abstract: We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. Although we describe BlackOut in the context of RNNLM training, it can be used to any networks with large softmax output layers. "
715699971559989249,2016-04-01 00:39:04,https://t.co/6iUuey6g36,An Explicit Rate Bound for the Over-Relaxed ADMM. (arXiv:1512.02063v2 [stat.ML] UPDATED) https://t.co/6iUuey6g36,0,3," Abstract: The framework of Integral Quadratic Constraints of Lessard et al. (2014) reduces the computation of upper bounds on the convergence rate of several optimization algorithms to semi-definite programming (SDP). Followup work by Nishihara et al. (2015) applies this technique to the entire family of over-relaxed Alternating Direction Method of Multipliers (ADMM). Unfortunately, they only provide an explicit error bound for sufficiently large values of some of the parameters of the problem, leaving the computation for the general case as a numerical optimization problem. In this paper we provide an exact analytical solution to this SDP and obtain a general and explicit upper bound on the convergence rate of the entire family of over-relaxed ADMM. Furthermore, we demonstrate that it is not possible to extract from this SDP a general bound better than ours. We end with a few numerical illustrations of our result and a comparison between the convergence rate we obtain for the ADMM with known convergence rates for the Gradient Descent. "
715699970553352196,2016-04-01 00:39:04,https://t.co/jjlVVptYkz,A Mathematical Formalization of Hierarchical Temporal Memory's Spatial Pooler. (arXiv:1601.06116v2 [stat.ML] UPDAT… https://t.co/jjlVVptYkz,1,3," Abstract: Hierarchical temporal memory (HTM) is an emerging machine learning algorithm, with the potential to provide a means to perform predictions on spatiotemporal data. The algorithm, inspired by the neocortex, currently does not have a comprehensive mathematical framework. This work brings together all aspects of the spatial pooler (SP), a critical learning component in HTM, under a single unifying framework. The primary learning mechanism is explored, where a maximum likelihood estimator for determining the degree of permanence update is proposed. The boosting mechanisms are studied and found to be only relevant during the initial few iterations of the network. Observations are made relating HTM to well-known algorithms such as competitive learning and attribute bagging. Methods are provided for using the SP for classification as well as dimensionality reduction. Empirical evidence verifies that given the proper parameterizations, the SP may be used for feature learning. "
715699969584402437,2016-04-01 00:39:04,https://t.co/IQyyo2N3TV,Investigating practical linear temporal difference learning. (arXiv:1602.08771v2 [cs.LG] UPDATED) https://t.co/IQyyo2N3TV,1,5," Abstract: Off-policy reinforcement learning has many applications including: learning from demonstration, learning multiple goal seeking policies in parallel, and representing predictive knowledge. Recently there has been an proliferation of new policy-evaluation algorithms that fill a longstanding algorithmic void in reinforcement learning: combining robustness to off-policy sampling, function approximation, linear complexity, and temporal difference (TD) updates. This paper contains two main contributions. First, we derive two new hybrid TD policy-evaluation algorithms, which fill a gap in this collection of algorithms. Second, we perform an empirical comparison to elicit which of these new linear TD methods should be preferred in different situations, and make concrete suggestions about practical use. "
715699968569446400,2016-04-01 00:39:04,https://t.co/AHjtXTiJHd,Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors. (arXiv:1603.04733v2 [stat.ML] … https://t.co/AHjtXTiJHd,3,18," Abstract: We introduce a variational Bayesian neural network where the parameters are governed via a probability distribution on random matrices. Specifically, we employ a matrix variate Gaussian \cite{gupta1999matrix} parameter posterior distribution where we explicitly model the covariance among the input and output dimensions of each layer. Furthermore, with approximate covariance matrices we can achieve a more efficient way to represent those correlations that is also cheaper than fully factorized parameter posteriors. We further show that with the ""local reprarametrization trick"" \cite{kingma2015variational} on this posterior distribution we arrive at a Gaussian Process \cite{rasmussen2006gaussian} interpretation of the hidden units in each layer and we, similarly with \cite{gal2015dropout}, provide connections with deep Gaussian processes. We continue in taking advantage of this duality and incorporate ""pseudo-data"" \cite{snelson2005sparse} in our model, which in turn allows for more efficient sampling while maintaining the properties of the original model. The validity of the proposed approach is verified through extensive experiments. "
715699966988197888,2016-04-01 00:39:03,https://t.co/q4DQhm1gmz,Improving and Scaling Trans-dimensional Random Field Language Models. (arXiv:1603.09170v1 [cs.CL] CROSS LISTED) https://t.co/q4DQhm1gmz,0,3," Abstract: The dominant language models (LMs) such as n-gram and neural network (NN) models represent sentence probabilities in terms of conditionals. In contrast, a new trans-dimensional random field (TRF) LM has been recently introduced to show superior performances, where the whole sentence is modeled as a random field. In this paper, we examine how the TRF models can be interpolated with the NN models, and obtain 12.1\% and 17.9\% relative error rate reductions over 6-gram LMs for English and Chinese speech recognition respectively through log-linear combination. "
715338162445410304,2016-03-31 00:41:22,https://t.co/5wWQtbuQ7T,Detecting weak changes in dynamic events over networks. (arXiv:1603.08981v1 [cs.LG]) https://t.co/5wWQtbuQ7T,0,3," Abstract: Large volume of networked streaming event data are becoming increasingly available in a wide variety of applications, such as social network analysis, Internet traffic monitoring and healthcare analytics. Streaming event data are discrete observation occurred in continuous time, and the precise time interval between two events carries a great deal of information about the dynamics of the underlying systems. How to promptly detect changes in these dynamic systems using these streaming event data? In this paper, we propose a novel change-point detection framework for multi-dimensional event data over networks. We cast the problem into sequential hypothesis test, and derive the likelihood ratios for point processes, which are computed efficiently via an EM-like algorithm that is parameter-free and can be computed in a distributed fashion. We derive a highly accurate theoretical characterization of the false-alarm-rate, and show that it can achieve weak signal detection by aggregating local statistics over time and networks. Finally, we demonstrate the good performance of our algorithm on numerical examples and real-world datasets from twitter and Memetracker. "
715338161505832960,2016-03-31 00:41:22,https://t.co/mHzhVwkyK0,Towards Practical Bayesian Parameter and State Estimation. (arXiv:1603.08988v1 [cs.AI]) https://t.co/mHzhVwkyK0,0,7," Abstract: Joint state and parameter estimation is a core problem for dynamic Bayesian networks. Although modern probabilistic inference toolkits make it relatively easy to specify large and practically relevant probabilistic models, the silver bullet---an efficient and general online inference algorithm for such problems---remains elusive, forcing users to write special-purpose code for each application. We propose a novel blackbox algorithm -- a hybrid of particle filtering for state variables and assumed density filtering for parameter variables. It has following advantages: (a) it is efficient due to its online nature, and (b) it is applicable to both discrete and continuous parameter spaces . On a variety of toy and real models, our system is able to generate more accurate results within a fixed computation budget. This preliminary evidence indicates that the proposed approach is likely to be of practical use. "
715338160541138944,2016-03-31 00:41:22,https://t.co/jAsbp5FkTq,Online Rules for Control of False Discovery Rate and False Discovery Exceedance. (arXiv:1603.09000v1 [math.ST]) https://t.co/jAsbp5FkTq,0,2," Abstract: Multiple hypothesis testing is a core problem in statistical inference and arises in almost every scientific field. Given a set of null hypotheses $\mathcal{H}(n) = (H_1,\dotsc, H_n)$, Benjamini and Hochberg introduced the false discovery rate (FDR), which is the expected proportion of false positives among rejected null hypotheses, and proposed a testing procedure that controls FDR below a pre-assigned significance level. Nowadays FDR is the criterion of choice for large scale multiple hypothesis testing. In this paper we consider the problem of controlling FDR in an ""online manner"". Concretely, we consider an ordered --possibly infinite-- sequence of null hypotheses $\mathcal{H} = (H_1,H_2,H_3,\dots )$ where, at each step $i$, the statistician must decide whether to reject hypothesis $H_i$ having access only to the previous decisions. This model was introduced by Foster and Stine. We study a class of ""generalized alpha-investing"" procedures and prove that any rule in this class controls online FDR, provided $p$-values corresponding to true nulls are independent from the other $p$-values. (Earlier work only established mFDR control.) Next, we obtain conditions under which generalized alpha-investing controls FDR in the presence of general $p$-values dependencies. Finally, we develop a modified set of procedures that also allow to control the false discovery exceedance (the tail of the proportion of false discoveries). Numerical simulations and analytical results indicate that online procedures do not incur a large loss in statistical power with respect to offline approaches, such as Benjamini-Hochberg. "
715338159463211009,2016-03-31 00:41:22,https://t.co/Fw21z2eOsK,Maximize Pointwise Cost-sensitively Submodular Functions With Budget Constraint. (arXiv:1603.09029v1 [cs.AI]) https://t.co/Fw21z2eOsK,1,2," Abstract: We study the worst-case adaptive optimization problem with budget constraint. Unlike previous works, we consider the general setting where the cost is a set function on sets of decisions. For this setting, we investigate the near-optimality of greedy policies when the utility function satisfies a novel property called pointwise cost-sensitive submodularity. This property is an extension of cost-sensitive submodularity, which in turn is a generalization of submodularity to general cost functions. We prove that two simple greedy policies for the problem are not near-optimal but the best between them is near-optimal. With this result, we propose a combined policy that is near-optimal with respect to the optimal worst-case policy that uses half of the budget. We discuss applications of our theoretical results and also report experimental results comparing the greedy policies on the active learning problem. "
715338158360100864,2016-03-31 00:41:21,https://t.co/Dpw9SeaTqR,Towards Geo-Distributed Machine Learning. (arXiv:1603.09035v1 [cs.LG]) https://t.co/Dpw9SeaTqR,1,4," Abstract: Latency to end-users and regulatory requirements push large companies to build data centers all around the world. The resulting data is ""born"" geographically distributed. On the other hand, many machine learning applications require a global view of such data in order to achieve the best results. These types of applications form a new class of learning problems, which we call Geo-Distributed Machine Learning (GDML). Such applications need to cope with: 1) scarce and expensive cross-data center bandwidth, and 2) growing privacy concerns that are pushing for stricter data sovereignty regulations. Current solutions to learning from geo-distributed data sources revolve around the idea of first centralizing the data in one data center, and then training locally. As machine learning algorithms are communication-intensive, the cost of centralizing the data is thought to be offset by the lower cost of intra-data center communication during training. In this work, we show that the current centralized practice can be far from optimal, and propose a system for doing geo-distributed training. Furthermore, we argue that the geo-distributed approach is structurally more amenable to dealing with regulatory constraints, as raw data never leaves the source data center. Our empirical evaluation on three real datasets confirms the general validity of our approach, and shows that GDML is not only possible but also advisable in many scenarios. "
715338157202481152,2016-03-31 00:41:21,https://t.co/ncezrVki31,Performance of a community detection algorithm based on semidefinite programming. (arXiv:1603.09045v1 [stat.ML]) https://t.co/ncezrVki31,0,3," Abstract: The problem of detecting communities in a graph is maybe one the most studied inference problems, given its simplicity and widespread diffusion among several disciplines. A very common benchmark for this problem is the stochastic block model or planted partition problem, where a phase transition takes place in the detection of the planted partition by changing the signal-to-noise ratio. Optimal algorithms for the detection exist which are based on spectral methods, but we show these are extremely sensible to slight modification in the generative model. Recently Javanmard, Montanari and Ricci-Tersenghi (arXiv:1511.08769) have used statistical physics arguments, and numerical simulations to show that finding communities in the stochastic block model via semidefinite programming is quasi optimal. Further, the resulting semidefinite relaxation can be solved efficiently, and is very robust with respect to changes in the generative model. In this paper we study in detail several practical aspects of this new algorithm based on semidefinite programming for the detection of the planted partition. The algorithm turns out to be very fast, allowing the solution of problems with $O(10^5)$ variables in few second on a laptop computer. "
715338155990368258,2016-03-31 00:41:21,https://t.co/mJ5sZ5r1ho,Robustness of Bayesian Pool-based Active Learning Against Prior Misspecification. (arXiv:1603.09050v1 [cs.LG]) https://t.co/mJ5sZ5r1ho,0,4," Abstract: We study the robustness of active learning (AL) algorithms against prior misspecification: whether an algorithm achieves similar performance using a perturbed prior as compared to using the true prior. In both the average and worst cases of the maximum coverage setting, we prove that all $\alpha$-approximate algorithms are robust (i.e., near $\alpha$-approximate) if the utility is Lipschitz continuous in the prior. We further show that robustness may not be achieved if the utility is non-Lipschitz. This suggests we should use a Lipschitz utility for AL if robustness is required. For the minimum cost setting, we can also obtain a robustness result for approximate AL algorithms. Our results imply that many commonly used AL algorithms are robust against perturbed priors. We then propose the use of a mixture prior to alleviate the problem of prior misspecification. We analyze the robustness of the uniform mixture prior and show experimentally that it performs reasonably well in practice. "
715338154413334528,2016-03-31 00:41:21,https://t.co/NT8FcQox8Q,Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders. (arXiv:1603.09128v1 [cs.CL]) https://t.co/NT8FcQox8Q,0,3," Abstract: We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information. Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense for a given word, and a decoder which predicts context words based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time. "
715338153486336000,2016-03-31 00:41:20,https://t.co/ahCi7fb1Ip,A latent-observed dissimilarity measure. (arXiv:1603.09254v1 [stat.ML]) https://t.co/ahCi7fb1Ip,0,2," Abstract: Quantitatively assessing relationships between latent variables and observed variables is important for understanding and developing generative models and representation learning. In this paper, we propose latent-observed dissimilarity (LOD) to evaluate the dissimilarity between the probabilistic characteristics of latent and observed variables. We also define four essential types of generative models with different independence/conditional independence configurations. Experiments using tractable real-world data show that LOD can effectively capture the differences between models and reflect the capability for higher layer learning. They also show that the conditional independence of latent variables given observed variables contributes to improving the transmission of information and characteristics from lower layers to higher layers. "
715338152613965824,2016-03-31 00:41:20,https://t.co/FiCvTcAjhw,Degrees of Freedom in Deep Neural Networks. (arXiv:1603.09260v1 [cs.LG]) https://t.co/FiCvTcAjhw,1,11," Abstract: In this paper, we explore degrees of freedom in deep sigmoidal neural networks. We show that the degrees of freedom in these models is related to the expected optimism, which is the expected difference between test error and training error. We provide an efficient Monte-Carlo method to estimate the degrees of freedom for multi-class classification methods. We show degrees of freedom are lower than the parameter count in a simple XOR network. We extend these results to neural nets trained on synthetic and real data, and investigate impact of network's architecture and different regularization choices. The degrees of freedom in deep networks are dramatically smaller than the number of parameters, in some real datasets several orders of magnitude. Further, we observe that for fixed number of parameters, deeper networks have less degrees of freedom exhibiting a regularization-by-depth. "
715338151590555649,2016-03-31 00:41:20,https://t.co/Fss2JXeBJg,On the Geometry of Message Passing Algorithms for Gaussian Reciprocal Processes. (arXiv:1603.09279v1 [stat.ML]) https://t.co/Fss2JXeBJg,0,8," Abstract: Reciprocal processes are acausal generalizations of Markov processes introduced by Bernstein in 1932. In the literature, a significant amount of attention has been focused on developing dynamical models for reciprocal processes. Recently, probabilistic graphical models for reciprocal processes have been provided. This opens the way to the application of efficient inference algorithms in the machine learning literature to solve the smoothing problem for reciprocal processes. Such algorithms are known to converge if the underlying graph is a tree. This is not the case for a reciprocal process, whose associated graphical model is a single loop network. The contribution of this paper is twofold. First, we introduce belief propagation for Gaussian reciprocal processes. Second, we establish a link between convergence analysis of belief propagation for Gaussian reciprocal processes and stability theory for differentially positive systems. "
715338150546128896,2016-03-31 00:41:20,https://t.co/KxKCdgYmrA,Estimating Treatment Effects using Multiple Surrogates: The Role of the Surrogate Score and the Surrogate Index. (… https://t.co/KxKCdgYmrA,0,5," Abstract: Estimating the long-term effects of treatments is of interest in many fields. A common challenge in estimating such treatment effects is that long-term outcomes are unobserved in the time frame needed to make policy decisions. One approach to overcome this missing data problem is to analyze treatments effects on an intermediate outcome, often called a statistical surrogate, if it satisfies the condition that treatment and outcome are independent conditional on the statistical surrogate. The validity of the surrogacy condition is often controversial. Here we exploit that fact that in modern datasets, researchers often observe a large number, possibly hundreds or thousands, of intermediate outcomes, thought to lie on or close to the causal chain between the treatment and the long-term outcome of interest. Even if none of the individual proxies satisfies the statistical surrogacy criterion by itself, using multiple proxies can be useful in causal inference. We focus primarily on a setting with two samples, an experimental sample containing data about the treatment indicator and the surrogates and an observational sample containing information about the surrogates and the primary outcome. We state assumptions under which the average treatment effect be identified and estimated with a high-dimensional vector of proxies that collectively satisfy the surrogacy assumption, and derive the bias from violations of the surrogacy assumption, and show that even if the primary outcome is also observed in the experimental sample, there is still information to be gained from using surrogates. "
715338149573083137,2016-03-31 00:41:19,https://t.co/O26Lgvovox,Nonparametric modal regression. (arXiv:1412.1716v3 [stat.ME] UPDATED) https://t.co/O26Lgvovox,0,7," Abstract: Modal regression estimates the local modes of the distribution of $Y$ given $X=x$, instead of the mean, as in the usual regression sense, and can hence reveal important structure missed by usual regression methods. We study a simple nonparametric method for modal regression, based on a kernel density estimate (KDE) of the joint distribution of $Y$ and $X$. We derive asymptotic error bounds for this method, and propose techniques for constructing confidence sets and prediction sets. The latter is used to select the smoothing bandwidth of the underlying KDE. The idea behind modal regression is connected to many others, such as mixture regression and density ridge estimation, and we discuss these ties as well. "
715338148444811264,2016-03-31 00:41:19,https://t.co/H8W785HKTi,Collaborative Filtering Bandits. (arXiv:1502.03473v5 [cs.LG] UPDATED) https://t.co/H8W785HKTi,0,7," Abstract: Classical collaborative filtering, and content-based filtering methods try to learn a static recommendation model given training data. These approaches are far from ideal in highly dynamic recommendation domains such as news recommendation and computational advertisement, where the set of items and users is very fluid. In this work, we investigate an adaptive clustering technique for content recommendation based on exploration-exploitation strategies in contextual multi-armed bandit settings. Our algorithm takes into account the collaborative effects that arise due to the interaction of the users with the items, by dynamically grouping users based on the items under consideration and, at the same time, grouping items based on the similarity of the clusterings induced over the users. The resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods. We provide an empirical analysis on medium-size real-world datasets, showing scalability and increased prediction performance (as measured by click-through rate) over state-of-the-art methods for clustering bandits. We also provide a regret analysis within a standard linear stochastic noise setting. "
715338147480133632,2016-03-31 00:41:19,https://t.co/qhD9duyX7L,Multiple Instance Dictionary Learning using Functions of Multiple Instances. (arXiv:1511.02825v2 [cs.CV] UPDATED) https://t.co/qhD9duyX7L,0,2," Abstract: A multiple instance dictionary learning method using functions of multiple instances (DL-FUMI) is proposed to address target detection and two-class classification problems with inaccurate training labels. Given inaccurate training labels, DL-FUMI learns a set of target dictionary atoms that describe the most distinctive and representative features of the true positive class as well as a set of nontarget dictionary atoms that account for the shared information found in both the positive and negative instances. Experimental results show that the estimated target dictionary atoms found by DL-FUMI are more representative prototypes and identify better discriminative features of the true positive class than existing methods in the literature. DL-FUMI is shown to have significantly better performance on several target detection and classification problems as compared to other multiple instance learning (MIL) dictionary learning algorithms on a variety of MIL problems. "
715338146410602497,2016-03-31 00:41:19,https://t.co/mXdQprU24a,On the interplay of network structure and gradient convergence in deep learning. (arXiv:1511.05297v5 [cs.LG] UPDAT… https://t.co/mXdQprU24a,1,12," Abstract: The regularization and output consistency behavior of dropout and layer-wise pretraining for learning deep networks have been fairly well studied. However, our understanding of how the asymptotic convergence of backpropagation in deep architectures is related to the structural properties of the network and other design choices (like denoising and dropout rate) is less clear at this time. An interesting question one may ask is whether the network architecture and input data statistics may guide the choices of learning parameters and vice versa. In this work, we explore the association between such structural, distributional and learnability aspects vis-\`a-vis their interaction with parameter convergence rates. We present a framework to address these questions based on convergence of backpropagation for general nonconvex objectives using first-order information. This analysis suggests an interesting relationship between feature denoising and dropout. Building upon these results, we obtain a setup that provides systematic guidance regarding the choice of learning parameters and network sizes that achieve a certain level of convergence (in the optimization sense) often mediated by statistical attributes of the inputs. Our results are supported by a set of experimental evaluations as well as independent empirical observations reported by other groups. "
715338144007249920,2016-03-31 00:41:18,https://t.co/1JjGPrtb3b,Active Task Selection for Multi-Task Learning. (arXiv:1602.06518v2 [stat.ML] UPDATED) https://t.co/1JjGPrtb3b,0,6," Abstract: In this paper we consider the problem of multi-task learning, in which a learner is given a collection of prediction tasks that need to be solved. In contrast to previous work, we give up on the assumption that labeled training data is available for all tasks. Instead, we propose an active task selection framework, where based only on the unlabeled data, the learner can choose a, typically small, subset of tasks for which he gets some labeled examples. For the remaining tasks, which have no available annotation, solutions are found by transferring information from the selected tasks. We analyze two transfer strategies and develop generalization bounds for each of them. Based on this theoretical analysis we propose two algorithms for making the choice of labeled tasks in a principled way and show their effectiveness on synthetic and real data. "
714976479516471297,2016-03-30 00:44:10,https://t.co/zI1F4bPujA,Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image Segmentation. (arXiv:1603.08564v1 [cs.CV]) https://t.co/zI1F4bPujA,1,2," Abstract: The paper proposes a novel Kernelized image segmentation scheme for noisy images that utilizes the concept of Smallest Univalue Segment Assimilating Nucleus (SUSAN) and incorporates spatial constraints by computing circular colour map induced weights. Fuzzy damping coefficients are obtained for each nucleus or center pixel on the basis of the corresponding weighted SUSAN area values, the weights being equal to the inverse of the number of horizontal and vertical moves required to reach a neighborhood pixel from the center pixel. These weights are used to vary the contributions of the different nuclei in the Kernel based framework. The paper also presents an edge quality metric obtained by fuzzy decision based edge candidate selection and final computation of the blurriness of the edges after their selection. The inability of existing algorithms to preserve edge information and structural details in their segmented maps necessitates the computation of the edge quality factor (EQF) for all the competing algorithms. Qualitative and quantitative analysis have been rendered with respect to state-of-the-art algorithms and for images ridden with varying types of noises. Speckle noise ridden SAR images and Rician noise ridden Magnetic Resonance Images have also been considered for evaluating the effectiveness of the proposed algorithm in extracting important segmentation information. "
714976478245597184,2016-03-30 00:44:10,https://t.co/FxEUcFoz2u,Analysis of k-Nearest Neighbor Distances with Application to Entropy Estimation. (arXiv:1603.08578v1 [math.ST]) https://t.co/FxEUcFoz2u,2,7," Abstract: Estimating entropy and mutual information consistently is important for many machine learning applications. The Kozachenko-Leonenko (KL) estimator (Kozachenko & Leonenko, 1987) is a widely used nonparametric estimator for the entropy of multivariate continuous random variables, as well as the basis of the mutual information estimator of Kraskov et al. (2004), perhaps the most widely used estimator of mutual information in this setting. Despite the practical importance of these estimators, major theoretical questions regarding their finite-sample behavior remain open. This paper proves finite-sample bounds on the bias and variance of the KL estimator, showing that it achieves the minimax convergence rate for certain classes of smooth functions. In proving these bounds, we analyze finite-sample behavior of k-nearest neighbors (k-NN) distance statistics (on which the KL estimator is based). We derive concentration inequalities for k-NN distances and a general expectation bound for statistics of k-NN distances, which may be useful for other analyses of k-NN methods. "
714976477100523520,2016-03-30 00:44:10,https://t.co/hzmKpsBoiK,Exponential Concentration of a Density Functional Estimator. (arXiv:1603.08584v1 [math.ST]) https://t.co/hzmKpsBoiK,0,4," Abstract: We analyze a plug-in estimator for a large class of integral functionals of one or more continuous probability densities. This class includes important families of entropy, divergence, mutual information, and their conditional versions. For densities on the $d$-dimensional unit cube $[0,1]^d$ that lie in a $\beta$-H\""older smoothness class, we prove our estimator converges at the rate $O \left( n^{-\frac{\beta}{\beta + d}} \right)$. Furthermore, we prove the estimator is exponentially concentrated about its mean, whereas most previous related results have proven only expected error bounds on estimators. "
714976475951325185,2016-03-30 00:44:10,https://t.co/dMsggtjo7H,Generalized Exponential Concentration Inequality for R\'enyi Divergence Estimation. (arXiv:1603.08589v1 [cs.IT]) https://t.co/dMsggtjo7H,0,5," Abstract: Estimating divergences in a consistent way is of great importance in many machine learning tasks. Although this is a fundamental problem in nonparametric statistics, to the best of our knowledge there has been no finite sample exponential inequality convergence bound derived for any divergence estimators. The main contribution of our work is to provide such a bound for an estimator of R\'enyi-$\alpha$ divergence for a smooth H\""older class of densities on the $d$-dimensional unit cube $[0, 1]^d$. We also illustrate our theoretical results with a numerical experiment. "
714976474944643072,2016-03-30 00:44:09,https://t.co/9Cox5zFUXR,Submodular Variational Inference for Network Reconstruction. (arXiv:1603.08616v1 [cs.LG]) https://t.co/9Cox5zFUXR,0,3," Abstract: In real-world and online social networks, individuals receive and transmit information in real time. Cascading information transmissions --- phone calls, text messages, social media posts --- may be understood as a realization of a diffusion process operating on the network, and its branching path can be represented by a directed tree. One important feature of dynamic real-world diffusion processes is that the process may not traverse every edge in the network on which it operates. When the network itself is unknown, the path of the diffusion process may reveal some, but not all, of the edges connecting nodes that have received the diffusing information. The network reconstruction/inference problem is to estimate connections that are not revealed by the diffusion processes. This problem naturally arises in a many disciplines. Most of existing works on network reconstruction study this problem by deriving a likelihood function for the realized diffusion process given full knowledge of the network on which it operates, and attempting to find the network topology that maximizes this likelihood. The major challenge in this work is the intractability of the optimization problem. In this paper, we focus on the network reconstruction problem for a broad class of real-world diffusion processes, exemplified by a network diffusion scheme called respondent-driven sampling (RDS) that is widely used in epidemiology. We prove that under a reasonable model of network diffusion, the likelihood of an observed RDS realization is a Bayesian log-submodular model. We propose a novel, accurate, and computationally efficient variational inference algorithm for the network reconstruction problem under this model. In this algorithm, we allow for more flexibility for the possible deviation of the subjects' reported total degrees in the underlying graphical structure from the true ones. "
714976473879339009,2016-03-30 00:44:09,https://t.co/giyIySQPCs,Regret Analysis of the Anytime Optimally Confident UCB Algorithm. (arXiv:1603.08661v1 [cs.LG]) https://t.co/giyIySQPCs,1,3," Abstract: I introduce and analyse an anytime version of the Optimally Confident UCB (OCUCB) algorithm designed for minimising the cumulative regret in finite-armed stochastic bandits with subgaussian noise. The new algorithm is simple, intuitive (in hindsight) and comes with the strongest finite-time regret guarantees for a horizon-free algorithm so far. I also show a finite-time lower bound that nearly matches the upper bound. "
714976472876904449,2016-03-30 00:44:09,https://t.co/0dvVSSvRr4,Interpretability of Multivariate Brain Maps in Brain Decoding: Definition and Quantification. (arXiv:1603.08704v1 … https://t.co/0dvVSSvRr4,0,2," Abstract: Brain decoding is a popular multivariate approach for hypothesis testing in neuroimaging. It is well known that the brain maps derived from weights of linear classifiers are hard to interpret because of high correlations between predictors, low signal to noise ratios, and the high dimensionality of neuroimaging data. Therefore, improving the interpretability of brain decoding approaches is of primary interest in many neuroimaging studies. Despite extensive studies of this type, at present, there is no formal definition for interpretability of multivariate brain maps. As a consequence, there is no quantitative measure for evaluating the interpretability of different brain decoding methods. In this paper, first, we present a theoretical definition of interpretability in brain decoding; we show that the interpretability of multivariate brain maps can be decomposed into their reproducibility and representativeness. Second, as an application of the proposed theoretical definition, we formalize a heuristic method for approximating the interpretability of multivariate brain maps in a binary magnetoencephalography (MEG) decoding scenario. Third, we propose to combine the approximated interpretability and the performance of the brain decoding model into a new multi-objective criterion for model selection. Our results for the MEG data show that optimizing the hyper-parameters of the regularized linear classifier based on the proposed criterion results in more informative multivariate brain maps. More importantly, the presented definition provides the theoretical background for quantitative evaluation of interpretability, and hence, facilitates the development of more effective brain decoding algorithms in the future. "
714976471786373121,2016-03-30 00:44:09,https://t.co/AHZtIgXN1y,Unified View of Matrix Completion under General Structural Constraints. (arXiv:1603.08708v1 [stat.ML]) https://t.co/AHZtIgXN1y,1,6," Abstract: In this paper, we present a unified analysis of matrix completion under general low-dimensional structural constraints induced by {\em any} norm regularization. We consider two estimators for the general problem of structured matrix completion, and provide unified upper bounds on the sample complexity and the estimation error. Our analysis relies on results from generic chaining, and we establish two intermediate results of independent interest: (a) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space, a certain partial complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure of Gaussian widths, and (b) it is shown that a form of restricted strong convexity holds for matrix completion problems under general norm regularization. Further, we provide several non-trivial examples of structures included in our framework, notably the recently proposed spectral $k$-support norm. "
714976470754529281,2016-03-30 00:44:08,https://t.co/vhPZCGBSWw,COCO: A Platform for Comparing Continuous Optimizers in a Black-Box Setting. (arXiv:1603.08785v1 [cs.AI]) https://t.co/vhPZCGBSWw,1,3," Abstract: COCO is a platform for Comparing Continuous Optimizers in a black-box setting. It aims at automatizing the tedious and repetitive task of benchmarking numerical optimization algorithms to the greatest possible extent. We present the rationals behind the development of the platform as a general proposition for a guideline towards better benchmarking. We detail underlying fundamental concepts of COCO such as its definition of a problem, the idea of instances, the relevance of target values, and runtime as central performance measure. Finally, we give a quick overview of the basic code structure and the available test suites. "
714976469634719744,2016-03-30 00:44:08,https://t.co/zcF1N9Qwet,Locally Epistatic Models for Genome-wide Prediction and Association by Importance Sampling. (arXiv:1603.08813v1 [s… https://t.co/zcF1N9Qwet,0,2," Abstract: In statistical genetics an important task involves building predictive models for the genotype-phenotype relationships and thus attribute a proportion of the total phenotypic variance to the variation in genotypes. Numerous models have been proposed to incorporate additive genetic effects into models for prediction or association. However, there is a scarcity of models that can adequately account for gene by gene or other forms of genetical interactions. In addition, there is an increased interest in using marker annotations in genome-wide prediction and association. In this paper, we discuss an hybrid modeling methodology which combines the parametric mixed modeling approach and the non-parametric rule ensembles. This approach gives us a flexible class of models that can be used to capture additive, locally epistatic genetic effects, gene x background interactions and allows us to incorporate one or more annotations into the genomic selection or association models. We use benchmark data sets covering a range of organisms and traits in addition to simulated data sets to illustrate the strengths of this approach. The improvement of model accuracies and association results suggest that a part of the ""missing heritability"" in complex traits can be captured by modeling local epistasis. "
714976468590268416,2016-03-30 00:44:08,https://t.co/erH6dS8txH,Zero Shot Recognition with Unreliable Attributes. (arXiv:1409.4327v2 [cs.CV] UPDATED) https://t.co/erH6dS8txH,1,6," Abstract: In principle, zero-shot learning makes it possible to train a recognition model simply by specifying the category's attributes. For example, with classifiers for generic attributes like \emph{striped} and \emph{four-legged}, one can construct a classifier for the zebra category by enumerating which properties it possesses---even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute's error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly. "
714976467508150272,2016-03-30 00:44:08,https://t.co/AjiCrCQ0gT,Some Insights About the Small Ball Probability Factorization for Hilbert Random Elements. (arXiv:1501.04308v2 [mat… https://t.co/AjiCrCQ0gT,0,2," Abstract: Asymptotic factorizations for the small-ball probability (SmBP) of a Hilbert valued random element $X$ are rigorously established and discussed. In particular, given the first $d$ principal components (PCs) and as the radius $\varepsilon$ of the ball tends to zero, the SmBP is asymptotically proportional to (a) the joint density of the first $d$ PCs, (b) the volume of the $d$-dimensional ball with radius $\varepsilon$, and (c) a correction factor weighting the use of a truncated version of the process expansion. Moreover, under suitable assumptions on the spectrum of the covariance operator of $X$ and as $d$ diverges to infinity when $\varepsilon$ vanishes, some simplifications occur. In particular, the SmBP factorizes asymptotically as the product of the joint density of the first $d$ PCs and a pure volume parameter. All the provided factorizations allow to define a surrogate intensity of the SmBP that, in some cases, leads to a genuine intensity. To operationalize the stated results, a non-parametric estimator for the surrogate intensity is introduced and it is proved that the use of estimated PCs, instead of the true ones, does not affect the rate of convergence. Finally, as an illustration, simulations in controlled frameworks are provided. "
714976466170195968,2016-03-30 00:44:07,https://t.co/NOo51wnCiQ,Analysis of classifiers' robustness to adversarial perturbations. (arXiv:1502.02590v4 [cs.LG] UPDATED) https://t.co/NOo51wnCiQ,1,3," Abstract: The goal of this paper is to analyze an intriguing phenomenon recently discovered in deep networks, namely their instability to adversarial perturbations (Szegedy et. al., 2014). We provide a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and show fundamental upper bounds on the robustness of classifiers. Specifically, we establish a general upper bound on the robustness of classifiers to adversarial perturbations, and then illustrate the obtained upper bound on the families of linear and quadratic classifiers. In both cases, our upper bound depends on a distinguishability measure that captures the notion of difficulty of the classification task. Our results for both classes imply that in tasks involving small distinguishability, no classifier in the considered set will be robust to adversarial perturbations, even if a good accuracy is achieved. Our theoretical framework moreover suggests that the phenomenon of adversarial instability is due to the low flexibility of classifiers, compared to the difficulty of the classification task (captured by the distinguishability). Moreover, we show the existence of a clear distinction between the robustness of a classifier to random noise and its robustness to adversarial perturbations. Specifically, the former is shown to be larger than the latter by a factor that is proportional to \sqrt{d} (with d being the signal dimension) for linear classifiers. This result gives a theoretical explanation for the discrepancy between the two robustness properties in high dimensional problems, which was empirically observed in the context of neural networks. To the best of our knowledge, our results provide the first theoretical work that addresses the phenomenon of adversarial instability recently observed for deep networks. Our analysis is complemented by experimental results on controlled and real-world data. "
714976465180352512,2016-03-30 00:44:07,https://t.co/bqfCBK4EIH,A Neural Transfer Function for a Smooth and Differentiable Transition Between Additive and Multiplicative Interact… https://t.co/bqfCBK4EIH,1,6," Abstract: Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. This leads either to an inefficient distribution of computational resources or an extensive increase in the computational complexity of the training procedure. We present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure. "
714976464064671744,2016-03-30 00:44:07,https://t.co/oLCZUIRUBW,Learning image representations tied to ego-motion. (arXiv:1505.02206v2 [cs.CV] UPDATED) https://t.co/oLCZUIRUBW,0,4," Abstract: Understanding how images of objects and scenes behave in response to specific ego-motions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video. Specifically, we enforce that our learned features exhibit equivariance i.e. they respond predictably to transformations associated with distinct ego-motions. With three datasets, we show that our unsupervised feature learning approach significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in static images from a disjoint domain. "
714976463003512832,2016-03-30 00:44:07,https://t.co/DxGWT3teHR,On the Influence of Momentum Acceleration on Online Learning. (arXiv:1603.04136v2 [math.OC] UPDATED) https://t.co/DxGWT3teHR,0,7," Abstract: The article examines in some detail the convergence rate and mean-square-error performance of momentum stochastic gradient methods in the constant step-size and slow adaptation regime. The results establish that momentum methods are equivalent to the standard stochastic gradient method with a re-scaled (larger) step-size value. The size of the re-scaling is determined by the value of the momentum parameter. The equivalence result is established for all time instants and not only in steady-state. The analysis is carried out for general strongly convex and smooth risk functions, and is not limited to quadratic risks. One notable conclusion is that the well-known bene ts of momentum constructions for deterministic optimization problems do not necessarily carry over to the adaptive online setting when small constant step-sizes are used to enable continuous adaptation and learn- ing in the presence of persistent gradient noise. From simulations, the equivalence between momentum and standard stochastic gradient methods is also observed for non-differentiable and non-convex problems. "
714976461837508608,2016-03-30 00:44:06,https://t.co/hMwhVxPJbT,"A universal tradeoff between power, precision and speed in physical communication. (arXiv:1603.07758v1 [cond-mat.s… https://t.co/hMwhVxPJbT",1,2," Abstract: Maximizing the speed and precision of communication while minimizing power dissipation is a fundamental engineering design goal. Also, biological systems achieve remarkable speed, precision and power efficiency using poorly understood physical design principles. Powerful theories like information theory and thermodynamics do not provide general limits on power, precision and speed. Here we go beyond these classical theories to prove that the product of precision and speed is universally bounded by power dissipation in any physical communication channel whose dynamics is faster than that of the signal. Moreover, our derivation involves a novel connection between friction and information geometry. These results may yield insight into both the engineering design of communication devices and the structure and function of biological signaling systems. "
714612782218522628,2016-03-29 00:38:58,https://t.co/pgN2rDxTCf,Resnet in Resnet: Generalizing Residual Architectures. (arXiv:1603.08029v1 [cs.LG]) https://t.co/pgN2rDxTCf,0,8," Abstract: Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dual-stream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100. "
714612781132214272,2016-03-29 00:38:58,https://t.co/gmYsHpuSby,Universality of Mallows' and degeneracy of Kendall's kernels for rankings. (arXiv:1603.08035v1 [stat.ML]) https://t.co/gmYsHpuSby,0,2," Abstract: Kernel methods provide an attractive framework for aggregating and learning from ranking data, and so understanding the fundamental properties of kernels over permutations is a question of broad interest. We provide a detailed analysis of the Fourier spectra of the standard Kendall and Mallows kernels, and a new class of polynomial-type kernels. We prove that the Kendall kernel has exactly two irreducible representations at which the Fourier transform is non-zero, and moreover, the associated matrices are rank one. This implies that the Kendall kernel is nearly degenerate, with limited expressive and discriminative power. In sharp contrast, we prove that the Fourier transform of the Mallows kernel is a strictly positive definite matrix at all irreducible representations. This property guarantees that the Mallows kernel is both characteristic and universal. We introduce a family of normalized polynomial kernels of degree p that interpolates between the Kendall (degree one) and Mallows (infinite degree) kernels, and show that for d-dimensional permutations, the p-th degree kernel is characteristic when p is greater or equal than d - 1, unlike the Euclidean case in which no finite-degree polynomial kernel is characteristic. "
714612779991306244,2016-03-29 00:38:58,https://t.co/M5AR9sEJwE,"""Did I Say Something Wrong?"" A Word-Level Analysis of Wikipedia Articles for Deletion Discussions. (arXiv:1603.080… https://t.co/M5AR9sEJwE",0,2," Abstract: This thesis focuses on gaining linguistic insights into textual discussions on a word level. It was of special interest to distinguish messages that constructively contribute to a discussion from those that are detrimental to them. Thereby, we wanted to determine whether ""I""- and ""You""-messages are indicators for either of the two discussion styles. These messages are nowadays often used in guidelines for successful communication. Although their effects have been successfully evaluated multiple times, a large-scale analysis has never been conducted. Thus, we used Wikipedia Articles for Deletion (short: AfD) discussions together with the records of blocked users and developed a fully automated creation of an annotated data set. In this data set, messages were labelled either constructive or disruptive. We applied binary classifiers to the data to determine characteristic words for both discussion styles. Thereby, we also investigated whether function words like pronouns and conjunctions play an important role in distinguishing the two. We found that ""You""-messages were a strong indicator for disruptive messages which matches their attributed effects on communication. However, we found ""I""-messages to be indicative for disruptive messages as well which is contrary to their attributed effects. The importance of function words could neither be confirmed nor refuted. Other characteristic words for either communication style were not found. Yet, the results suggest that a different model might represent disruptive and constructive messages in textual discussions better. "
714612778917634049,2016-03-29 00:38:58,https://t.co/toR7bocy5i,Reconstructing undirected graphs from eigenspaces. (arXiv:1603.08113v1 [math.ST]) https://t.co/toR7bocy5i,0,3," Abstract: In this paper, we aim at recovering an undirected weighted graph of N vertices from the knowledge of a perturbed version of the eigenspaces of its adjacency matrix W. Our approach is based on minimizing a cost function given by the Frobenius norm of the commutator AB-BA between symmetric matrices A and B. In the Erd\H{o}s-R\'enyi model with no self-loops, we show that identifiability (i.e., the ability to reconstruct W from the knowledge of its eigenspaces) follows a sharp phase transition on the expected number of edges with threshold function N\log N/2. Given an estimation of the eigenspaces based on a n-sample, we provide support selection procedures from theoretical and practical point of views. In particular, when deleting an edge from the active support, our study unveils that our test statistic is the order of O(1/n) when we overestimate the true support and lower bounded by a positive constant when the estimated support is smaller than the true support. This feature leads to a powerful practical support estimation procedure when properly thresholding. Simulated and real life numerical experiments assert our new methodology. "
714612777390841856,2016-03-29 00:38:57,https://t.co/wTbqUy2H1a,Data-Driven Dynamic Decision Models. (arXiv:1603.08150v1 [stat.ML]) https://t.co/wTbqUy2H1a,0,2," Abstract: This article outlines a method for automatically generating models of dynamic decision-making that both have strong predictive power and are interpretable in human terms. This is useful for designing empirically grounded agent-based simulations and for gaining direct insight into observed dynamic processes. We use an efficient model representation and a genetic algorithm-based estimation process to generate simple approximations that explain most of the structure of complex stochastic processes. This method, implemented in C++ and R, scales well to large data sets. We apply our methods to empirical data from human subjects game experiments and international relations. We also demonstrate the method's ability to recover known data-generating processes by simulating data with agent-based models and correctly deriving the underlying decision models for multiple agent models and degrees of stochasticity. "
714612776245862400,2016-03-29 00:38:57,https://t.co/asLv28cPja,Regularization Parameter Selection for a Bayesian Multi-Level Group Lasso Regression Model with Application to Ima… https://t.co/asLv28cPja,0,2," Abstract: We investigate the choice of tuning parameters for a Bayesian multi-level group lasso model developed for the joint analysis of neuroimaging and genetic data. The regression model we consider relates multivariate phenotypes consisting of brain summary measures (volumetric and cortical thickness values) to single nucleotide polymorphism (SNPs) data and imposes penalization at two nested levels, the first corresponding to genes and the second corresponding to SNPs. Associated with each level in the penalty is a tuning parameter which corresponds to a hyperparameter in the hierarchical Bayesian formulation. Following previous work on Bayesian lassos we consider the estimation of tuning parameters through either hierarchical Bayes based on hyperpriors and Gibbs sampling or through empirical Bayes based on maximizing the marginal likelihood using a Monte Carlo EM algorithm. For the specific model under consideration we find that these approaches can lead to severe overshrinkage of the regression parameter estimates in the high-dimensional setting or when the genetic effects are weak. We demonstrate these problems through simulation examples and study an approximation to the marginal likelihood which sheds light on the cause of this problem. We then suggest an alternative approach based on the widely applicable information criterion (WAIC), an asymptotic approximation to leave-one-out cross-validation that can be computed conveniently within an MCMC framework. "
714612775113400320,2016-03-29 00:38:57,https://t.co/uFsa4flLGi]),Exact MCMC Subsampling. (arXiv:1603.08232v1 [https://t.co/uFsa4flLGi]) https://t.co/jvSjfVAITn,3,8,INDEXERROR
714612774106742784,2016-03-29 00:38:56,https://t.co/SlFpNmnQJL,Estimating Mixture Models via Mixtures of Polynomials. (arXiv:1603.08482v1 [stat.ML]) https://t.co/SlFpNmnQJL,0,2," Abstract: Mixture modeling is a general technique for making any simple model more expressive through weighted combination. This generality and simplicity in part explains the success of the Expectation Maximization (EM) algorithm, in which updates are easy to derive for a wide class of mixture models. However, the likelihood of a mixture model is non-convex, so EM has no known global convergence guarantees. Recently, method of moments approaches offer global guarantees for some mixture models, but they do not extend easily to the range of mixture models that exist. In this work, we present Polymom, an unifying framework based on method of moments in which estimation procedures are easily derivable, just as in EM. Polymom is applicable when the moments of a single mixture component are polynomials of the parameters. Our key observation is that the moments of the mixture model are a mixture of these polynomials, which allows us to cast estimation as a Generalized Moment Problem. We solve its relaxations using semidefinite optimization, and then extract parameters using ideas from computer algebra. This framework allows us to draw insights and apply tools from convex optimization, computer algebra and the theory of moments to study problems in statistical estimation. "
714612773053927424,2016-03-29 00:38:56,https://t.co/WJsyE1NhwN,Kernel Nonnegative Matrix Factorization Without the Curse of the Pre-image - Application to Unmixing Hyperspectral… https://t.co/WJsyE1NhwN,0,2," Abstract: The nonnegative matrix factorization (NMF) is widely used in signal and image processing, including bio-informatics, blind source separation and hyperspectral image analysis in remote sensing. A great challenge arises when dealing with a nonlinear formulation of the NMF. Within the framework of kernel machines, the models suggested in the literature do not allow the representation of the factorization matrices, which is a fallout of the curse of the pre-image. In this paper, we propose a novel kernel-based model for the NMF that does not suffer from the pre-image problem, by investigating the estimation of the factorization matrices directly in the input space. For different kernel functions, we describe two schemes for iterative algorithms: an additive update rule based on a gradient descent scheme and a multiplicative update rule in the same spirit as in the Lee and Seung algorithm. Within the proposed framework, we develop several extensions to incorporate constraints, including sparseness, smoothness, and spatial regularization with a total-variation-like penalty. The effectiveness of the proposed method is demonstrated with the problem of unmixing hyperspectral images, using well-known real images and results with state-of-the-art techniques. "
714612772017979392,2016-03-29 00:38:56,https://t.co/uFsa4flLGi],Hamiltonian Monte Carlo Without Detailed Balance. (arXiv:1409.5191v5 [https://t.co/uFsa4flLGi] UPDATED) https://t.co/JbK92ZXSdp,0,6,INDEXERROR
714612771036520449,2016-03-29 00:38:56,https://t.co/jACqVIYurz,Perturbed Iterate Analysis for Asynchronous Stochastic Optimization. (arXiv:1507.06970v2 [stat.ML] UPDATED) https://t.co/jACqVIYurz,0,3," Abstract: We introduce and analyze stochastic optimization methods where the input to each gradient update is perturbed by bounded noise. We show that this framework forms the basis of a unified approach to analyze asynchronous implementations of stochastic optimization algorithms.In this framework, asynchronous stochastic optimization algorithms can be thought of as serial methods operating on noisy inputs. Using our perturbed iterate framework, we provide new analyses of the Hogwild! algorithm and asynchronous stochastic coordinate descent, that are simpler than earlier analyses, remove many assumptions of previous models, and in some cases yield improved upper bounds on the convergence rates. We proceed to apply our framework to develop and analyze KroMagnon: a novel, parallel, sparse stochastic variance-reduced gradient (SVRG) algorithm. We demonstrate experimentally on a 16-core machine that the sparse and parallel version of SVRG is in some cases more than four orders of magnitude faster than the standard SVRG algorithm. "
714612769987948544,2016-03-29 00:38:55,https://t.co/dxgN77AegE,Perceptron like Algorithms for Online Learning to Rank. (arXiv:1508.00842v3 [cs.LG] UPDATED) https://t.co/dxgN77AegE,1,6," Abstract: Perceptron is a classic online algorithm for learning a classification function. In this paper, we provide a novel extension of the perceptron algorithm to the learning to rank problem in information retrieval. We consider popular listwise performance measures such as Normalized Discounted Cumulative Gain (NDCG) and Average Precision (AP). A modern perspective on perceptron for classification is that it is simply an instance of online gradient descent (OGD), during mistake rounds, using the hinge loss function. Motivated by this interpretation, we propose a novel family of listwise, large margin ranking surrogates. Members of this family can be thought of as analogs of the hinge loss. Exploiting a certain self-bounding property of the proposed family, we provide a guarantee on the cumulative NDCG (or AP) induced loss incurred by our perceptron-like algorithm. We show that, if there exists a perfect oracle ranker which can correctly rank each instance in an online sequence of ranking data, with some margin, the cumulative loss of perceptron algorithm on that sequence is bounded by a constant, irrespective of the length of the sequence. This result is reminiscent of Novikoff's convergence theorem for the classification perceptron. Moreover, we prove a lower bound on the cumulative loss achievable by any deterministic algorithm, under the assumption of existence of perfect oracle ranker. The lower bound shows that our perceptron bound is not tight, and we propose another, \emph{purely online}, algorithm which achieves the lower bound. We provide empirical results on simulated and large commercial datasets to corroborate our theoretical results. "
714612768868065280,2016-03-29 00:38:55,https://t.co/qTA6XeHuL6,A Randomized Rounding Algorithm for Sparse PCA. (arXiv:1508.03337v4 [cs.DS] UPDATED) https://t.co/qTA6XeHuL6,0,5," Abstract: We present and analyze a simple, two-step algorithm to approximate the optimal solution of the sparse PCA problem. Our approach first solves a L1 penalized version of the NP-hard sparse PCA optimization problem and then uses a randomized rounding strategy to sparsify the resulting dense solution. Our main theoretical result guarantees an additive error approximation and provides a tradeoff between sparsity and accuracy. Our experimental evaluation indicates that our approach is competitive in practice, even compared to state-of-the-art toolboxes such as Spasm. "
714612767781728257,2016-03-29 00:38:55,https://t.co/2ZfHkdxEA0,Provable approximation properties for deep neural networks. (arXiv:1509.07385v3 [stat.ML] UPDATED) https://t.co/2ZfHkdxEA0,1,7," Abstract: We discuss approximation of functions using deep neural nets. Given a function $f$ on a $d$-dimensional manifold $\Gamma \subset \mathbb{R}^m$, we construct a sparsely-connected depth-4 neural network and bound its error in approximating $f$. The size of the network depends on dimension and curvature of the manifold $\Gamma$, the complexity of $f$, in terms of its wavelet description, and only weakly on the ambient dimension $m$. Essentially, our network computes wavelet functions, which are computed from Rectified Linear Units (ReLU) "
714612766674448384,2016-03-29 00:38:55,https://t.co/HcGWb69Rjb,Learning-based Compressive Subsampling. (arXiv:1510.06188v3 [cs.IT] UPDATED) https://t.co/HcGWb69Rjb,0,2," Abstract: The problem of recovering a structured signal $\mathbf{x} \in \mathbb{C}^p$ from a set of dimensionality-reduced linear measurements $\mathbf{b} = \mathbf {A}\mathbf {x}$ arises in a variety of applications, such as medical imaging, spectroscopy, Fourier optics, and computerized tomography. Due to computational and storage complexity or physical constraints imposed by the problem, the measurement matrix $\mathbf{A} \in \mathbb{C}^{n \times p}$ is often of the form $\mathbf{A} = \mathbf{P}_{\Omega}\boldsymbol{\Psi}$ for some orthonormal basis matrix $\boldsymbol{\Psi}\in \mathbb{C}^{p \times p}$ and subsampling operator $\mathbf{P}_{\Omega}: \mathbb{C}^{p} \rightarrow \mathbb{C}^{n}$ that selects the rows indexed by $\Omega$. This raises the fundamental question of how best to choose the index set $\Omega$ in order to optimize the recovery performance. Previous approaches to addressing this question rely on non-uniform \emph{random} subsampling using application-specific knowledge of the structure of $\mathbf{x}$. In this paper, we instead take a principled learning-based approach in which a \emph{fixed} index set is chosen based on a set of training signals $\mathbf{x}_1,\dotsc,\mathbf{x}_m$. We formulate combinatorial optimization problems seeking to maximize the energy captured in these signals in an average-case or worst-case sense, and we show that these can be efficiently solved either exactly or approximately via the identification of modularity and submodularity structures. We provide both deterministic and statistical theoretical guarantees showing how the resulting measurement matrices perform on signals differing from the training signals, and we provide numerical examples showing our approach to be effective on a variety of data sets. "
714612765445500931,2016-03-29 00:38:54,https://t.co/AVsZ5kTYq1,Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja's Algorithm. (arXiv:160… https://t.co/AVsZ5kTYq1,2,12," Abstract: This work provides improved guarantees for streaming principle component analysis (PCA). Given $A_1, \ldots, A_n\in \mathbb{R}^{d\times d}$ sampled independently from distributions satisfying $\mathbb{E}[A_i] = \Sigma$ for $\Sigma \succeq \mathbf{0}$, this work provides an $O(d)$-space linear-time single-pass streaming algorithm for estimating the top eigenvector of $\Sigma$. The algorithm nearly matches (and in certain cases improves upon) the accuracy obtained by the standard batch method that computes top eigenvector of the empirical covariance $\frac{1}{n} \sum_{i \in [n]} A_i$ as analyzed by the matrix Bernstein inequality. Moreover, to achieve constant accuracy, our algorithm improves upon the best previous known sample complexities of streaming algorithms by either a multiplicative factor of $O(d)$ or $1/\mathrm{gap}$ where $\mathrm{gap}$ is the relative distance between the top two eigenvalues of $\Sigma$. These results are achieved through a novel analysis of the classic Oja's algorithm, one of the oldest and most popular algorithms for streaming PCA. In particular, this work shows that simply picking a random initial point $w_0$ and applying the update rule $w_{i + 1} = w_i + \eta_i A_i w_i$ suffices to accurately estimate the top eigenvector, with a suitable choice of $\eta_i$. We believe our result sheds light on how to efficiently perform streaming PCA both in theory and in practice and we hope that our analysis may serve as the basis for analyzing many variants and extensions of streaming PCA. "
714249053064790016,2016-03-28 00:33:39,https://t.co/FY9J14q4Xz,Skill-Based Differences in Spatio-Temporal Team Behavior in Defence of The Ancients 2. (arXiv:1603.07738v1 [stat.M… https://t.co/FY9J14q4Xz,0,2," Abstract: Multiplayer Online Battle Arena (MOBA) games are among the most played digital games in the world. In these games, teams of players fight against each other in arena environments, and the gameplay is focused on tactical combat. Mastering MOBAs requires extensive practice, as is exemplified in the popular MOBA Defence of the Ancients 2 (DotA 2). In this paper, we present three data-driven measures of spatio-temporal behavior in DotA 2: 1) Zone changes; 2) Distribution of team members and: 3) Time series clustering via a fuzzy approach. We present a method for obtaining accurate positional data from DotA 2. We investigate how behavior varies across these measures as a function of the skill level of teams, using four tiers from novice to professional players. Results indicate that spatio-temporal behavior of MOBA teams is related to team skill, with professional teams having smaller within-team distances and conducting more zone changes than amateur teams. The temporal distribution of the within-team distances of professional and high-skilled teams also generally follows patterns distinct from lower skill ranks. "
714249050804105216,2016-03-28 00:33:38,https://t.co/lBZfqqTtyW,Pathway Lasso: Estimate and Select Sparse Mediation Pathways with High Dimensional Mediators. (arXiv:1603.07749v1 … https://t.co/lBZfqqTtyW,0,3," Abstract: In many scientific studies, it becomes increasingly important to delineate the causal pathways through a large number of mediators, such as genetic and brain mediators. Structural equation modeling (SEM) is a popular technique to estimate the pathway effects, commonly expressed as products of coefficients. However, it becomes unstable to fit such models with high dimensional mediators, especially for a general setting where all the mediators are causally dependent but the exact causal relationships between them are unknown. This paper proposes a sparse mediation model using a regularized SEM approach, where sparsity here means that a small number of mediators have nonzero mediation effects between a treatment and an outcome. To address the model selection challenge, we innovate by introducing a new penalty called Pathway Lasso. This penalty function is a convex relaxation of the non-convex product function, and it enables a computationally tractable optimization criterion to estimate and select many pathway effects simultaneously. We develop a fast ADMM-type algorithm to compute the model parameters, and we show that the iterative updates can be expressed in closed form. On both simulated data and a real fMRI dataset, the proposed approach yields higher pathway selection accuracy and lower estimation bias than other competing methods. "
714249049344487424,2016-03-28 00:33:38,https://t.co/Ff5pxYU2r0,An end-to-end convolutional selective autoencoder approach to Soybean Cyst Nematode eggs detection. (arXiv:1603.07… https://t.co/Ff5pxYU2r0,1,3," Abstract: This paper proposes a novel selective autoencoder approach within the framework of deep convolutional networks. The crux of the idea is to train a deep convolutional autoencoder to suppress undesired parts of an image frame while allowing the desired parts resulting in efficient object detection. The efficacy of the framework is demonstrated on a critical plant science problem. In the United States, approximately $1 billion is lost per annum due to a nematode infection on soybean plants. Currently, plant-pathologists rely on labor-intensive and time-consuming identification of Soybean Cyst Nematode (SCN) eggs in soil samples via manual microscopy. The proposed framework attempts to significantly expedite the process by using a series of manually labeled microscopic images for training followed by automated high-throughput egg detection. The problem is particularly difficult due to the presence of a large population of non-egg particles (disturbances) in the image frames that are very similar to SCN eggs in shape, pose and illumination. Therefore, the selective autoencoder is trained to learn unique features related to the invariant shapes and sizes of the SCN eggs without handcrafting. After that, a composite non-maximum suppression and differencing is applied at the post-processing stage. "
714249047872241664,2016-03-28 00:33:37,https://t.co/yo7mha3SIk,Markov substitute processes : a new model for linguistics and beyond. (arXiv:1603.07850v1 [stat.ML]) https://t.co/yo7mha3SIk,0,2," Abstract: We introduce Markov substitute processes, a new model at the crossroad of statistics and formal grammars, and prove its main property : Markov substitute processes with a given support form an exponential family. "
714249046290993152,2016-03-28 00:33:37,https://t.co/ApJ2CslQPA,Exact Bayesian inference for off-line change-point detection in tree-structured graphical models. (arXiv:1603.0787… https://t.co/ApJ2CslQPA,0,3," Abstract: We consider the problem of change-point detection in multivariate time-series. The multivariate distribution of the observations is supposed to follow a graphical model, whose graph and parameters are affected by abrupt changes throughout time. We demonstrate that it is possible to perform exact Bayesian inference whenever one considers a simple class of undirected graphs called spanning trees as possible structures. We are then able to integrate on the graph and segmentation spaces at the same time by combining classical dynamic programming with algebraic results pertaining to spanning trees. In particular, we show that quantities such as posterior distributions for change-points or posterior edge probabilities over time can efficiently be obtained. We illustrate our results on both synthetic and experimental data arising from biology and neuroscience. "
714249044722376705,2016-03-28 00:33:37,https://t.co/EQVl3syI9E,Hybridization of Expectation-Maximization and K-Means Algorithms for Better Clustering Performance. (arXiv:1603.07… https://t.co/EQVl3syI9E,2,7," Abstract: The present work proposes hybridization of Expectation-Maximization (EM) and K-Means techniques as an attempt to speed-up the clustering process. Though both K-Means and EM techniques look into different areas, K-means can be viewed as an approximate way to obtain maximum likelihood estimates for the means. Along with the proposed algorithm for hybridization, the present work also experiments with the Standard EM algorithm. Six different datasets are used for the experiments of which three are synthetic datasets. Clustering fitness and Sum of Squared Errors (SSE) are computed for measuring the clustering performance. In all the experiments it is observed that the proposed algorithm for hybridization of EM and K-Means techniques is consistently taking less execution time with acceptable Clustering Fitness value and less SSE than the standard EM algorithm. It is also observed that the proposed algorithm is producing better clustering results than the Cluster package of Purdue University. "
714249043057254400,2016-03-28 00:33:36,https://t.co/kKagqZhjHz,Generalized system identification with stable spline kernels. (arXiv:1309.7857v3 [stat.ML] UPDATED) https://t.co/kKagqZhjHz,0,5," Abstract: Regularized least-squares approaches have been successfully applied to linear system identification. Recent approaches use quadratic penalty terms on the unknown impulse response defined by stable spline kernels, which control model space complexity by leveraging regularity and bounded-input bounded-output stability. This paper extends linear system identification to a wide class of nonsmooth stable spline estimators, where regularization functionals and data misfits can be selected from a rich set of piecewise linear quadratic penalties. This class encompasses the 1-norm, huber, and vapnik, in addition to the least-squares penalty, and the approach allows linear inequality constraints on the unknown impulse response. We develop a customized interior point solver for the entire class of proposed formulations. By representing penalties through their conjugates, we allow a simple interface that enables the user to specify any piecewise linear quadratic penalty for misfit and regularizer, together with inequality constraints on the response. The solver is locally quadratically convergent, with O(n2(m+n)) arithmetic operations per iteration, for n impulse response coefficients and m output measurements. In the system identification context, where n << m, IPsolve is competitive with available alternatives, illustrated by a comparison with TFOCS and libSVM. The modeling framework is illustrated with a range of numerical experiments, featuring robust formulations for contaminated data, relaxation systems, and nonnegativity and unimodality constraints on the impulse response. Incorporating constraints yields significant improvements in system identification. The solver used to obtain the results is distributed via an open source code repository. "
714249041455013889,2016-03-28 00:33:36,https://t.co/CkIVIBfPYv,Equitability of Dependence Measure. (arXiv:1501.02102v2 [stat.ML] UPDATED) https://t.co/CkIVIBfPYv,0,2," Abstract: A measure of dependence is said to be equitable if it gives similar scores to equally noisy relationship of different types. In practice, we do not know what kind of functional relationship is underlying two given observations, Hence the equitability of dependence measure is critical in analysis and by scoring relationships according to an equitable measure one hopes to find important patterns of any type of further examination. In this paper, we introduce our definition of equitability of a dependence measure, which is naturally from this initial description, and Further more power-equitable(weak-equitable) is introduced which is of the most practical meaning in evaluating the equitablity of a dependence measure. "
714249039831822336,2016-03-28 00:33:35,https://t.co/5j4kpa51Hw,The Benefit of Multitask Representation Learning. (arXiv:1505.06279v2 [stat.ML] UPDATED) https://t.co/5j4kpa51Hw,1,5," Abstract: We discuss a general method to learn data representations from multiple tasks. We provide a justification for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by multitask representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneficial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks. "
714249038040801280,2016-03-28 00:33:35,https://t.co/thOcWDNLWA,Information-based inference in sloppy and singular models. (arXiv:1506.05855v3 [stat.ML] UPDATED) https://t.co/thOcWDNLWA,0,4," Abstract: A central problem in statistics is model selection: the choice between competing models of a stochastic process whose observables are corrupted by noise. In information-based inference, model selection is performed by maximizing the estimated predictive performance. We propose a frequen- tist information criterion (FIC) which extends the applicability of information-based inference to the analysis of singular and sloppy models. In these scenarios, the Akaike information criterion (AIC) can result in significant under or over-estimates of the predictive complexity. Two important mechanisms for this failure are examined: an implicit multiple testing problem and the presence of unidentifiable parameters. FIC rectifies this failure by applying a frequentist approximation to compute the com- plexity. For regular models in the large-sample-size limit, AIC and FIC are equal, but in general the complexity exhibits a sample-size dependent scaling. In the context of singular models, FIC can ex- hibit Bayesian information criterion-like or Hannan-Quinn-like scalings with sample size. FIC does not depend on ad hoc prior distributions or exogenous regularization and can be applied when struc- tured data complicates the use of cross-validatation. "
714249036195360769,2016-03-28 00:33:34,https://t.co/8aEgsHSeCs,Nonlinear variable selection with continuous outcome: a nonparametric incremental forward stagewise approach. (arX… https://t.co/8aEgsHSeCs,0,3," Abstract: We present a method of variable selection for the situation where some predictors are nonlinearly associated with a continuous outcome variable. The method doesn't assume any specific functional form, and can select from a large number of candidates. It takes the form of incremental forward stagewise regression, in which very small steps are taken to select the variables. Given no functional form is assumed, we devised an approach termed roughening to adjust the residuals in the iterations. In simulations, we show the new method is competitive against popular machine learning approaches. We also demonstrate its performance using some real datasets. "
714249034265976832,2016-03-28 00:33:34,https://t.co/JKhUc4wYcN,Unsupervised Transductive Domain Adaptation. (arXiv:1602.03534v3 [stat.ML] UPDATED) https://t.co/JKhUc4wYcN,0,5," Abstract: Supervised learning with large scale labeled datasets and deep layered models has made a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers generalization issues under the presence of a domain shift between the training and the test data distribution. In this regard, unsupervised domain adaptation algorithms have been proposed to directly address the domain shift problem. In this paper, we approach the problem from a transductive perspective. We incorporate the domain shift and the transductive target inference into our framework by jointly solving for an asymmetric similarity metric and the optimal transductive target label assignment. We also show that our model can easily be extended for deep feature learning in order to learn features which are discriminative in the target domain. Our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin. "
714249032567271424,2016-03-28 00:33:34,https://t.co/IckJnzzgZK,A Minimalistic Approach to Sum-Product Network Learning for Real Applications. (arXiv:1602.04259v2 [cs.AI] UPDATED) https://t.co/IckJnzzgZK,0,4," Abstract: Sum-Product Networks (SPNs) are a class of expressive yet tractable hierarchical graphical models. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identifying similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete and there is no missing data. We introduce a practical, simplified version of LearnSPN, MiniSPN, that runs faster and can handle missing data and heterogeneous features common in real applications. We demonstrate the performance of MiniSPN on standard benchmark datasets and on two datasets from Google's Knowledge Graph exhibiting high missingness rates and a mix of discrete and continuous features. "
713161551088390144,2016-03-25 00:32:18,https://t.co/VnGF61St5m,Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices. (arXiv:1603.07341v1 [cs.LG]) https://t.co/VnGF61St5m,1,8," Abstract: In recent years, deep neural networks (DNN) have demonstrated significant business impact in large scale analysis and classification tasks such as speech recognition, visual object detection, pattern extraction, etc. Training of large DNNs, however, is universally considered as time consuming and computationally intensive task that demands datacenter-scale computational resources recruited for many days. Here we propose a concept of resistive processing unit (RPU) devices that can potentially accelerate DNN training by orders of magnitude while using much less power. The proposed RPU device can store and update the weight values locally thus minimizing data movement during training and allowing to fully exploit the locality and the parallelism of the training algorithm. We identify the RPU device and system specifications for implementation of an accelerator chip for DNN training in a realistic CMOS-compatible technology. For large DNNs with about 1 billion weights this massively parallel RPU architecture can achieve acceleration factors of 30,000X compared to state-of-the-art microprocessors while providing power efficiency of 84,000 GigaOps/s/W. Problems that currently require days of training on a datacenter-size cluster with thousands of machines can be addressed within hours on a single RPU accelerator. A system consisted of a cluster of RPU accelerators will be able to tackle Big Data problems with trillions of parameters that is impossible to address today like, for example, natural speech recognition and translation between all world languages, real-time analytics on large streams of business and scientific data, integration and analysis of multimodal sensory data flows from massive number of IoT (Internet of Things) sensors. "
713161550127828993,2016-03-25 00:32:18,https://t.co/P66dJf6Ns4,Predicting litigation likelihood and time to litigation for patents. (arXiv:1603.07394v1 [stat.ML]) https://t.co/P66dJf6Ns4,0,3," Abstract: Patent lawsuits are costly and time-consuming. An ability to forecast a patent litigation and time to litigation allows companies to better allocate budget and time in managing their patent portfolios. We develop predictive models for estimating the likelihood of litigation for patents and the expected time to litigation based on both textual and non-textual features. Our work focuses on improving the state-of-the-art by relying on a different set of features and employing more sophisticated algorithms with more realistic data. The rate of patent litigations is very low, which consequently makes the problem difficult. The initial model for predicting the likelihood is further modified to capture a time-to-litigation perspective. "
713161549297414144,2016-03-25 00:32:17,https://t.co/6ovuHTXYSC,Evaluating the Performance of Offensive Linemen in the NFL. (arXiv:1603.07593v1 [stat.ML]) https://t.co/6ovuHTXYSC,1,3," Abstract: How does one objectively measure the performance of an individual offensive lineman in the NFL? The existing literature proposes various measures that rely on subjective assessments of game film, but has yet to develop an objective methodology to evaluate performance. Using a variety of statistics related to an offensive lineman's performance, we develop a framework to objectively analyze the overall performance of an individual offensive lineman and determine specific linemen who are overvalued or undervalued relative to their salary. We identify eight players across the 2013-2014 and 2014-2015 NFL seasons that are considered to be overvalued or undervalued and corroborate the results with existing metrics that are based on subjective evaluation. To the best of our knowledge, the techniques set forth in this work have not been utilized in previous works to evaluate the performance of NFL players at any position, including offensive linemen. "
713161548366278656,2016-03-25 00:32:17,https://t.co/uFDQ3vLKI1,Clustering Time-Series Energy Data from Smart Meters. (arXiv:1603.07602v1 [stat.ML]) https://t.co/uFDQ3vLKI1,0,3," Abstract: Investigations have been performed into using clustering methods in data mining time-series data from smart meters. The problem is to identify patterns and trends in energy usage profiles of commercial and industrial customers over 24-hour periods, and group similar profiles. We tested our method on energy usage data provided by several U.S. power utilities. The results show accurate grouping of accounts similar in their energy usage patterns, and potential for the method to be utilized in energy efficiency programs. "
713161546923446272,2016-03-25 00:32:17,https://t.co/c2VhZ5qOgX,Going Out of Business: Auction House Behavior in the Massively Multi-Player Online Game. (arXiv:1603.07610v1 [cs.C… https://t.co/c2VhZ5qOgX,0,2," Abstract: The in-game economies of massively multi-player online games (MMOGs) are complex systems that have to be carefully designed and managed. This paper presents the results of an analysis of auction house data from the MMOG Glitch, across a 14 month time period, the entire lifetime of the game. The data comprise almost 3 million data points, over 20,000 unique players and more than 650 products. Furthermore, an interactive visualization, based on Sankey flow diagrams, is presented which shows the proportion of the different clusters across each time bin, as well as the flow of players between clusters. The diagram allows evaluation of migration of players between clusters as a function of time, as well as churn analysis. The presented work provides a template analysis and visualization model for progression-based or temporal-based analysis of player behavior broadly applicable to games. "
713161546017476608,2016-03-25 00:32:17,https://t.co/I34t90XWhG,Semantic Properties of Customer Sentiment in Tweets. (arXiv:1603.07624v1 [cs.CL]) https://t.co/I34t90XWhG,0,4," Abstract: An increasing number of people are using online social networking services (SNSs), and a significant amount of information related to experiences in consumption is shared in this new media form. Text mining is an emerging technique for mining useful information from the web. We aim at discovering in particular tweets semantic patterns in consumers' discussions on social media. Specifically, the purposes of this study are twofold: 1) finding similarity and dissimilarity between two sets of textual documents that include consumers' sentiment polarities, two forms of positive vs. negative opinions and 2) driving actual content from the textual data that has a semantic trend. The considered tweets include consumers opinions on US retail companies (e.g., Amazon, Walmart). Cosine similarity and K-means clustering methods are used to achieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topic modeling algorithm, is used for the latter purpose. This is the first study which discover semantic properties of textual data in consumption context beyond sentiment analysis. In addition to major findings, we apply LDA (Latent Dirichlet Allocations) to the same data and drew latent topics that represent consumers' positive opinions and negative opinions on social media. "
713161544738148352,2016-03-25 00:32:16,https://t.co/RSvByf5Xpo,Predictive Analytics Using Smartphone Sensors for Depressive Episodes. (arXiv:1603.07692v1 [cs.CY]) https://t.co/RSvByf5Xpo,0,2," Abstract: The behaviors of patients with depression are usually difficult to predict because the patients demonstrate the symptoms of a depressive episode without a warning at unexpected times. The goal of this research is to build algorithms that detect signals of such unusual moments so that doctors can be proactive in approaching already diagnosed patients before they fall in depression. Each patient is equipped with a smartphone with the capability to track its sensors. We first find the home location of a patient, which is then augmented with other sensor data to identify sleep patterns and select communication patterns. The algorithms require two to three weeks of training data to build standard patterns, which are considered normal behaviors; and then, the methods identify any anomalies in day-to-day data readings of sensors. Four smartphone sensors, including the accelerometer, the gyroscope, the location probe and the communication log probe are used for anomaly detection in sleeping and communication patterns. "
713161543714779136,2016-03-25 00:32:16,https://t.co/vXtbhH4C9Z,A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Meas… https://t.co/vXtbhH4C9Z,0,5," Abstract: We propose a simple, scalable, and fast gradient descent algorithm to optimize a nonconvex objective for the rank minimization problem and a closely related family of semidefinite programs. With $O(r^3 \kappa^2 n \log n)$ random measurements of a positive semidefinite $n \times n$ matrix of rank $r$ and condition number $\kappa$, our method is guaranteed to converge linearly to the global optimum. "
713161542695575552,2016-03-25 00:32:16,https://t.co/8UxgN3wfeO,The Variational Gaussian Process. (arXiv:1511.06499v3 [stat.ML] UPDATED) https://t.co/8UxgN3wfeO,1,15," Abstract: Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We develop the variational Gaussian process (VGP), a Bayesian nonparametric variational family, which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by auto-encoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW. "
712800433169960961,2016-03-24 00:37:21,https://t.co/kIsIhbwDxS,Predicting Glaucoma Visual Field Loss by Hierarchically Aggregating Clustering-based Predictors. (arXiv:1603.07094… https://t.co/kIsIhbwDxS,2,7," Abstract: This study addresses the issue of predicting the glaucomatous visual field loss from patient disease datasets. Our goal is to accurately predict the progress of the disease in individual patients. As very few measurements are available for each patient, it is difficult to produce good predictors for individuals. A recently proposed clustering-based method enhances the power of prediction using patient data with similar spatiotemporal patterns. Each patient is categorized into a cluster of patients, and a predictive model is constructed using all of the data in the class. Predictions are highly dependent on the quality of clustering, but it is difficult to identify the best clustering method. Thus, we propose a method for aggregating cluster-based predictors to obtain better prediction accuracy than from a single cluster-based prediction. Further, the method shows very high performances by hierarchically aggregating experts generated from several cluster-based methods. We use real datasets to demonstrate that our method performs significantly better than conventional clustering-based and patient-wise regression methods, because the hierarchical aggregating strategy has a mechanism whereby good predictors in a small community can thrive. "
712800431936823296,2016-03-24 00:37:20,https://t.co/aEdNTsZ1al,A guide to convolution arithmetic for deep learning. (arXiv:1603.07285v1 [stat.ML]) https://t.co/aEdNTsZ1al,5,23," Abstract: We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive. "
712800430674296832,2016-03-24 00:37:20,https://t.co/T5AmA3xiXt,Debugging Machine Learning Tasks. (arXiv:1603.07292v1 [cs.LG]) https://t.co/T5AmA3xiXt,3,15," Abstract: Unlike traditional programs (such as operating systems or word processors) which have large amounts of code, machine learning tasks use programs with relatively small amounts of code (written in machine learning libraries), but voluminous amounts of data. Just like developers of traditional programs debug errors in their code, developers of machine learning tasks debug and fix errors in their data. However, algorithms and tools for debugging and fixing errors in data are less common, when compared to their counterparts for detecting and fixing errors in code. In this paper, we consider classification tasks where errors in training data lead to misclassifications in test points, and propose an automated method to find the root causes of such misclassifications. Our root cause analysis is based on Pearl's theory of causation, and uses Pearl's PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi, encodes the computation of PS as a probabilistic program, and uses recent work on probabilistic programs and transformations on probabilistic programs (along with gray-box models of machine learning algorithms) to efficiently compute PS. Psi is able to identify root causes of data errors in interesting data sets. "
712800429617324033,2016-03-24 00:37:20,https://t.co/5RmzfF0U5v,On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis. (arXiv:1603.07294v1 [cs.LG]) https://t.co/5RmzfF0U5v,0,11," Abstract: Bayesian inference has great promise for the privacy-preserving analysis of sensitive data, as posterior sampling automatically preserves differential privacy, an algorithmic notion of data privacy, under certain conditions (Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample (OPS) approach elegantly provides privacy ""for free,"" it is data inefficient in the sense of asymptotic relative efficiency (ARE). We show that a simple alternative based on the Laplace mechanism, the workhorse of differential privacy, is as asymptotically efficient as non-private posterior inference, under general assumptions. This technique also has practical advantages including efficient use of the privacy budget for MCMC. We demonstrate the practicality of our approach on a time-series analysis of sensitive military records from the Afghanistan and Iraq wars disclosed by the Wikileaks organization. "
712800428560412672,2016-03-24 00:37:20,https://t.co/ovAsC7EG3v,Nuclear norm penalization and optimal rates for noisy low rank matrix completion. (arXiv:1011.6256v4 [math.ST] UPD… https://t.co/ovAsC7EG3v,0,5," Abstract: This paper deals with the trace regression model where $n$ entries or linear combinations of entries of an unknown $m_1\times m_2$ matrix $A_0$ corrupted by noise are observed. We propose a new nuclear norm penalized estimator of $A_0$ and establish a general sharp oracle inequality for this estimator for arbitrary values of $n,m_1,m_2$ under the condition of isometry in expectation. Then this method is applied to the matrix completion problem. In this case, the estimator admits a simple explicit form and we prove that it satisfies oracle inequalities with faster rates of convergence than in the previous works. They are valid, in particular, in the high-dimensional setting $m_1m_2\gg n$. We show that the obtained rates are optimal up to logarithmic factors in a minimax sense and also derive, for any fixed matrix $A_0$, a non-minimax lower bound on the rate of convergence of our estimator, which coincides with the upper bound up to a constant factor. Finally, we show that our procedure provides an exact recovery of the rank of $A_0$ with probability close to 1. We also discuss the statistical learning setting where there is no underlying model determined by $A_0$ and the aim is to find the best trace regression model approximating the data. "
712800427117518849,2016-03-24 00:37:19,https://t.co/3xlFsjakMt,BLC: Private Matrix Factorization Recommenders via Automatic Group Learning. (arXiv:1509.05789v2 [cs.LG] UPDATED) https://t.co/3xlFsjakMt,1,3," Abstract: We propose a privacy-enhanced matrix factorization recommender that exploits the fact that users can often be grouped together by interest. This allows a form of ""hiding in the crowd"" privacy. We introduce a novel matrix factorization approach suited to making recommendations in a shared group (or nym) setting and the BLC algorithm for carrying out this matrix factorization in a privacy-enhanced manner. We demonstrate that the increased privacy does not come at the cost of reduced recommendation accuracy. "
712800425758601216,2016-03-24 00:37:19,https://t.co/qI3waC9vKM,A Kernel Test of Goodness of Fit. (arXiv:1602.02964v2 [stat.ML] UPDATED) https://t.co/qI3waC9vKM,0,8," Abstract: We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein's method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation. "
712800424449982464,2016-03-24 00:37:19,https://t.co/zKaHNJ4WKi,Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series. (arXiv:1602.07109v2 [stat.ML]… https://t.co/zKaHNJ4WKi,0,13, Abstract: Approximate variational inference has shown to be a powerful tool for modeling unknown complex probability distributions. Recent advances in the field allow us to learn probabilistic models of sequences that actively exploit spatial and temporal structure. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line. 
712439572098113536,2016-03-23 00:43:25,https://t.co/kAXxv3GD9e,Variational Autoencoders for Feature Detection of Magnetic Resonance Imaging Data. (arXiv:1603.06624v1 [cs.LG]) https://t.co/kAXxv3GD9e,0,10," Abstract: Independent component analysis (ICA), as an approach to the blind source-separation (BSS) problem, has become the de-facto standard in many medical imaging settings. Despite successes and a large ongoing research effort, the limitation of ICA to square linear transformations have not been overcome, so that general INFOMAX is still far from being realized. As an alternative, we present feature analysis in medical imaging as a problem solved by Helmholtz machines, which include dimensionality reduction and reconstruction of the raw data under the same objective, and which recently have overcome major difficulties in inference and learning with deep and nonlinear configurations. We demonstrate one approach to training Helmholtz machines, variational auto-encoders (VAE), as a viable approach toward feature extraction with magnetic resonance imaging (MRI) data. "
712439570747559936,2016-03-23 00:43:24,https://t.co/mjXzEoR6p3,Sparse Network Lasso for Local High-dimensional Regression. (arXiv:1603.06743v1 [stat.ML]) https://t.co/mjXzEoR6p3,0,11," Abstract: We introduce the localized Lasso, which is suited for learning models that are both interpretable and have a high predictive power in problems with high dimensionality $d$ and small sample size $n$. More specifically, we consider a function defined by local sparse models, one at each data point. We introduce sample-wise network regularization to borrow strength across the models, and sample-wise exclusive group sparsity (a.k.a., $\ell_{1,2}$ norm) to introduce diversity into the choice of feature sets in the local models. The local models are interpretable in terms of similarity of their sparsity patterns. The cost function is convex, and thus has a globally optimal solution. Moreover, we propose a simple yet efficient iterative least-squares based optimization procedure for the localized Lasso, which does not need a tuning parameter, and is guaranteed to converge to a globally optimal solution. The solution is empirically shown to outperform alternatives for both simulated and genomic personalized medicine data. "
712439569417949184,2016-03-23 00:43:24,https://t.co/bRQtaHJT0o,Multi-domain machine translation enhancements by parallel data extraction from comparable corpora. (arXiv:1603.067… https://t.co/bRQtaHJT0o,0,2," Abstract: Parallel texts are a relatively rare language resource, however, they constitute a very useful research material with a wide range of applications. This study presents and analyses new methodologies we developed for obtaining such data from previously built comparable corpora. The methodologies are automatic and unsupervised which makes them good for large scale research. The task is highly practical as non-parallel multilingual data occur much more frequently than parallel corpora and accessing them is easy, although parallel sentences are a considerably more useful resource. In this study, we propose a method of automatic web crawling in order to build topic-aligned comparable corpora, e.g. based on the Wikipedia or Euronews.com. We also developed new methods of obtaining parallel sentences from comparable data and proposed methods of filtration of corpora capable of selecting inconsistent or only partially equivalent translations. Our methods are easily scalable to other languages. Evaluation of the quality of the created corpora was performed by analysing the impact of their use on statistical machine translation systems. Experiments were presented on the basis of the Polish-English language pair for texts from different domains, i.e. lectures, phrasebooks, film dialogues, European Parliament proceedings and texts contained medicines leaflets. We also tested a second method of creating parallel corpora based on data from comparable corpora which allows for automatically expanding the existing corpus of sentences about a given domain on the basis of analogies found between them. It does not require, therefore, having past parallel resources in order to train a classifier. "
712439568369377285,2016-03-23 00:43:24,https://t.co/S3DtaLQ8vf,"New metrics for learning and inference on sets, ontologies, andfunctions. (arXiv:1603.06846v1 [stat.ML]) https://t.co/S3DtaLQ8vf",0,3," Abstract: We propose new metrics on sets, ontologies, and functions that can be used in various stages of probabilistic modeling, including exploratory data analysis, learning, inference, and result interpretation. These new functions unify and generalize some of the popular metrics on sets and functions, such as the Jaccard and bag distances on sets and Marczewski-Steinhaus distance on functions. We then introduce information-theoretic metrics on directed acyclic graphs drawn independently according to a fixed probability distribution and show how they can be used to calculate similarity between class labels for the objects with hierarchical output spaces (e.g., protein function). Finally, we provide evidence that the proposed metrics are useful by clustering species based solely on functional annotations available for subsets of their genes. The functional trees resemble evolutionary trees obtained by the phylogenetic analysis of their genomes. "
712439567366889473,2016-03-23 00:43:24,https://t.co/ukuyKBb13F,Enhanced perceptrons using contrastive biclusters. (arXiv:1603.06859v1 [cs.NE]) https://t.co/ukuyKBb13F,0,3," Abstract: Perceptrons are neuronal devices capable of fully discriminating linearly separable classes. Although straightforward to implement and train, their applicability is usually hindered by non-trivial requirements imposed by real-world classification problems. Therefore, several approaches, such as kernel perceptrons, have been conceived to counteract such difficulties. In this paper, we investigate an enhanced perceptron model based on the notion of contrastive biclusters. From this perspective, a good discriminative bicluster comprises a subset of data instances belonging to one class that show high coherence across a subset of features and high differentiation from nearest instances of the other class under the same features (referred to as its contrastive bicluster). Upon each local subspace associated with a pair of contrastive biclusters a perceptron is trained and the model with highest area under the receiver operating characteristic curve (AUC) value is selected as the final classifier. Experiments conducted on a range of data sets, including those related to a difficult biosignal classification problem, show that the proposed variant can be indeed very useful, prevailing in most of the cases upon standard and kernel perceptrons in terms of accuracy and AUC measures. "
712439566368677888,2016-03-23 00:43:23,https://t.co/v768DQygYp,Trading-off variance and complexity in stochastic gradient descent. (arXiv:1603.06861v1 [stat.ML]) https://t.co/v768DQygYp,0,7," Abstract: Stochastic gradient descent is the method of choice for large-scale machine learning problems, by virtue of its light complexity per iteration. However, it lags behind its non-stochastic counterparts with respect to the convergence rate, due to high variance introduced by the stochastic updates. The popular Stochastic Variance-Reduced Gradient (SVRG) method mitigates this shortcoming, introducing a new update rule which requires infrequent passes over the entire input dataset to compute the full-gradient. In this work, we propose CheapSVRG, a stochastic variance-reduction optimization scheme. Our algorithm is similar to SVRG but instead of the full gradient, it uses a surrogate which can be efficiently computed on a small subset of the input data. It achieves a linear convergence rate ---up to some error level, depending on the nature of the optimization problem---and features a trade-off between the computational complexity and the convergence rate. Empirical evaluation shows that CheapSVRG performs at least competitively compared to the state of the art. "
712439565240442881,2016-03-23 00:43:23,https://t.co/OjLs8b9hUZ,Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of Pairwise Comparisons. (arXiv:1603.06881v1 [cs… https://t.co/OjLs8b9hUZ,1,6," Abstract: We study methods for aggregating pairwise comparison data in order to estimate outcome probabilities for future comparisons among a collection of n items. Working within a flexible framework that imposes only a form of strong stochastic transitivity (SST), we introduce an adaptivity index defined by the indifference sets of the pairwise comparison probabilities. In addition to measuring the usual worst-case risk of an estimator, this adaptivity index also captures the extent to which the estimator adapts to instance-specific difficulty relative to an oracle estimator. We prove three main results that involve this adaptivity index and different algorithms. First, we propose a three-step estimator termed Count-Randomize-Least squares (CRL), and show that it has adaptivity index upper bounded as $\sqrt{n}$ up to logarithmic factors. We then show that that conditional on the hardness of planted clique, no computationally efficient estimator can achieve an adaptivity index smaller than $\sqrt{n}$. Second, we show that a regularized least squares estimator can achieve a poly-logarithmic adaptivity index, thereby demonstrating a $\sqrt{n}$-gap between optimal and computationally achievable adaptivity. Finally, we prove that the standard least squares estimator, which is known to be optimally adaptive in several closely related problems, fails to adapt in the context of estimating pairwise probabilities. "
712439564120551425,2016-03-23 00:43:23,https://t.co/hS5zvv5Jte,A Selection of Giant Radio Sources from NVSS. (arXiv:1603.06895v1 [astro-ph.GA]) https://t.co/hS5zvv5Jte,0,2," Abstract: Results of the application of pattern recognition techniques to the problem of identifying Giant Radio Sources (GRS) from the data in the NVSS catalog are presented and issues affecting the process are explored. Decision-tree pattern recognition software was applied to training set source pairs developed from known NVSS large angular size radio galaxies. The full training set consisted of 51,195 source pairs, 48 of which were known GRS for which each lobe was primarily represented by a single catalog component. The source pairs had a maximum separation of 20 arc minutes and a minimum component area of 1.87 square arc minutes at the 1.4 mJy level. The importance of comparing resulting probability distributions of the training and application sets for cases of unknown class ratio is demonstrated. The probability of correctly ranking a randomly selected (GRS, non-GRS) pair from the best of the tested classifiers was determined to be 97.8 +/- 1.5%. The best classifiers were applied to the over 870,000 candidate pairs from the entire catalog. Images of higher ranked sources were visually screened and a table of over sixteen hundred candidates, including morphological annotation, is presented. These systems include doubles and triples, Wide-Angle Tail (WAT) and Narrow-Angle Tail (NAT), S- or Z-shaped systems, and core-jets and resolved cores. While some resolved lobe systems are recovered with this technique, generally it is expected that such systems would require a different approach. "
712439562971258883,2016-03-23 00:43:22,https://t.co/Pe1aG8w3KE,Edge-exchangeable graphs and sparsity. (arXiv:1603.06898v1 [math.ST]) https://t.co/Pe1aG8w3KE,0,6," Abstract: A known failing of many popular random graph models is that the Aldous-Hoover Theorem guarantees these graphs are dense with probability one; that is, the number of edges grows quadratically with the number of nodes. This behavior is considered unrealistic in observed graphs. We define a notion of edge exchangeability for random graphs in contrast to the established notion of infinite exchangeability for random graphs --- which has traditionally relied on exchangeability of nodes (rather than edges) in a graph. We show that, unlike node exchangeability, edge exchangeability encompasses models that are known to provide a projective sequence of random graphs that circumvent the Aldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of the number of edges with the number of nodes. We show how edge-exchangeability of graphs relates naturally to existing notions of exchangeability from clustering (a.k.a. partitions) and other familiar combinatorial structures. "
712439561520074752,2016-03-23 00:43:22,https://t.co/MC2uRtQN1Q,Completely random measures for modeling power laws in sparse graphs. (arXiv:1603.06915v1 [stat.ML]) https://t.co/MC2uRtQN1Q,0,5," Abstract: Network data appear in a number of applications, such as online social networks and biological networks, and there is growing interest in both developing models for networks as well as studying the properties of such data. Since individual network datasets continue to grow in size, it is necessary to develop models that accurately represent the real-life scaling properties of networks. One behavior of interest is having a power law in the degree distribution. However, other types of power laws that have been observed empirically and considered for applications such as clustering and feature allocation models have not been studied as frequently in models for graph data. In this paper, we enumerate desirable asymptotic behavior that may be of interest for modeling graph data, including sparsity and several types of power laws. We outline a general framework for graph generative models using completely random measures; by contrast to the pioneering work of Caron and Fox (2015), we consider instantiating more of the existing atoms of the random measure as the dataset size increases rather than adding new atoms to the measure. We see that these two models can be complementary; they respectively yield interpretations as (1) time passing among existing members of a network and (2) new individuals joining a network. We detail a particular instance of this framework and show simulated results that suggest this model exhibits some desirable asymptotic power-law behavior. "
712439560337227776,2016-03-23 00:43:22,https://t.co/M1gHAzqgtn,Inference via Message Passing on Partially Labeled Stochastic Block Models. (arXiv:1603.06923v1 [math.ST]) https://t.co/M1gHAzqgtn,0,2," Abstract: We study the community detection and recovery problem in partially-labeled stochastic block models (SBM). We develop a fast linearized message-passing algorithm to reconstruct labels for SBM (with $n$ nodes, $k$ blocks, $p,q$ intra and inter block connectivity) when $\delta$ proportion of node labels are revealed. The signal-to-noise ratio ${\sf SNR}(n,k,p,q,\delta)$ is shown to characterize the fundamental limitations of inference via local algorithms. On the one hand, when ${\sf SNR}>1$, the linearized message-passing algorithm provides the statistical inference guarantee with mis-classification rate at most $\exp(-({\sf SNR}-1)/2)$, thus interpolating smoothly between strong and weak consistency. This exponential dependence improves upon the known error rate $({\sf SNR}-1)^{-1}$ in the literature on weak recovery. On the other hand, when ${\sf SNR}<1$ (for $k=2$) and ${\sf SNR}<1/4$ (for general growing $k$), we prove that local algorithms suffer an error rate at least $\frac{1}{2} - \sqrt{\delta \cdot {\sf SNR}}$, which is only slightly better than random guess for small $\delta$. "
712439559326408704,2016-03-23 00:43:22,https://t.co/aXMWo2c2Ah,Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems. (arXiv:1505.05114v2 [cs… https://t.co/aXMWo2c2Ah,0,2," Abstract: We consider the fundamental problem of solving quadratic systems of equations in $n$ variables, where $y_i = |\langle \boldsymbol{a}_i, \boldsymbol{x} \rangle|^2$, $i = 1, \ldots, m$ and $\boldsymbol{x} \in \mathbb{R}^n$ is unknown. We propose a novel method, which starting with an initial guess computed by means of a spectral method, proceeds by minimizing a nonconvex functional as in the Wirtinger flow approach. There are several key distinguishing features, most notably, a distinct objective functional and novel update rules, which operate in an adaptive fashion and drop terms bearing too much influence on the search direction. These careful selection rules provide a tighter initial guess, better descent directions, and thus enhanced practical performance. On the theoretical side, we prove that for certain unstructured models of quadratic systems, our algorithms return the correct solution in linear time, i.e. in time proportional to reading the data $\{\boldsymbol{a}_i\}$ and $\{y_i\}$ as soon as the ratio $m/n$ between the number of equations and unknowns exceeds a fixed numerical constant. We extend the theory to deal with noisy systems in which we only have $y_i \approx |\langle \boldsymbol{a}_i, \boldsymbol{x} \rangle|^2$ and prove that our algorithms achieve a statistical accuracy, which is nearly un-improvable. We complement our theoretical study with numerical examples showing that solving random quadratic systems is both computationally and statistically not much harder than solving linear systems of the same size---hence the title of this paper. For instance, we demonstrate empirically that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size. "
712439558290411520,2016-03-23 00:43:21,https://t.co/UyddqDfcn6,How Robust are Reconstruction Thresholds for Community Detection?. (arXiv:1511.01473v2 [cs.DS] UPDATED) https://t.co/UyddqDfcn6,0,2," Abstract: The stochastic block model is one of the oldest and most ubiquitous models for studying clustering and community detection. In an exciting sequence of developments, motivated by deep but non-rigorous ideas from statistical physics, Decelle et al. conjectured a sharp threshold for when community detection is possible in the sparse regime. Mossel, Neeman and Sly and Massoulie proved the conjecture and gave matching algorithms and lower bounds. Here we revisit the stochastic block model from the perspective of semirandom models where we allow an adversary to make `helpful' changes that strengthen ties within each community and break ties between them. We show a surprising result that these `helpful' changes can shift the information-theoretic threshold, making the community detection problem strictly harder. We complement this by showing that an algorithm based on semidefinite programming (which was known to get close to the threshold) continues to work in the semirandom model (even for partial recovery). This suggests that algorithms based on semidefinite programming are robust in ways that any algorithm meeting the information-theoretic threshold cannot be. These results point to an interesting new direction: Can we find robust, semirandom analogues to some of the classical, average-case thresholds in statistics? We also explore this question in the broadcast tree model, and we show that the viewpoint of semirandom models can help explain why some algorithms are preferred to others in practice, in spite of the gaps in their statistical performance on random models. "
712439557254455296,2016-03-23 00:43:21,https://t.co/pHlnNLn3tA,The Multivariate Generalised von Mises: Inference and applications. (arXiv:1602.05003v2 [stat.ML] UPDATED) https://t.co/pHlnNLn3tA,0,5," Abstract: Circular variables arise in a multitude of data-modelling contexts ranging from robotics to the social sciences, but they have been largely overlooked by the machine learning community. This paper partially redresses this imbalance by extending some standard probabilistic modelling tools to the circular domain. First we introduce a new multivariate distribution over circular variables, called the multivariate Generalised von Mises (mGvM) distribution. This distribution can be constructed by restricting and renormalising a general multivariate Gaussian distribution to the unit hyper-torus. Previously proposed multivariate circular distributions are shown to be special cases of this construction. Second, we introduce a new probabilistic model for circular regression, that is inspired by Gaussian Processes, and a method for probabilistic principal component analysis with circular hidden variables. These models can leverage standard modelling tools (e.g. covariance functions and methods for automatic relevance determination). Third, we show that the posterior distribution in these models is a mGvM distribution which enables development of an efficient variational free-energy scheme for performing approximate inference and approximate maximum-likelihood learning. "
712439555920674817,2016-03-23 00:43:21,https://t.co/qKJTEshrEL,Patterns of Scalable Bayesian Inference. (arXiv:1602.05221v2 [stat.ML] UPDATED) https://t.co/qKJTEshrEL,0,9," Abstract: Datasets are growing not just in size but in complexity, creating a demand for rich models and quantification of uncertainty. Bayesian methods are an excellent fit for this demand, but scaling Bayesian inference is a challenge. In response to this challenge, there has been considerable recent work based on varying assumptions about model structure, underlying computational resources, and the importance of asymptotic correctness. As a result, there is a zoo of ideas with few clear overarching principles. In this paper, we seek to identify unifying principles, patterns, and intuitions for scaling Bayesian inference. We review existing work on utilizing modern computing resources with both MCMC and variational approximation techniques. From this taxonomy of ideas, we characterize the general principles that have proven successful for designing scalable inference procedures and comment on the path forward. "
712439554184183808,2016-03-23 00:43:20,https://t.co/Ize2EaRLyF,Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits. (arXiv:1603.06560v1 [cs.LG] CROSS LISTED) https://t.co/Ize2EaRLyF,2,10," Abstract: Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While current methods offer efficiencies by adaptively choosing new configurations to train, an alternative strategy is to adaptively allocate resources across the selected configurations. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinitely many armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce Hyperband for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with state-of-the-art methods on a suite of hyperparameter optimization problems. We observe that Hyperband provides five times to thirty times speedup over state-of-the-art Bayesian optimization algorithms on a variety of deep-learning and kernel-based learning problems. "
712076765720666113,2016-03-22 00:41:45,https://t.co/OC3fYdYIJ8,A Message Passing Algorithm for the Problem of Path Packing in Graphs. (arXiv:1603.06002v1 [cs.DS]) https://t.co/OC3fYdYIJ8,0,3," Abstract: We consider the problem of packing node-disjoint directed paths in a directed graph. We consider a variant of this problem where each path starts within a fixed subset of root nodes, subject to a given bound on the length of paths. This problem is motivated by the so-called kidney exchange problem, but has potential other applications and is interesting in its own right. We propose a new algorithm for this problem based on the message passing/belief propagation technique. A priori this problem does not have an associated graphical model, so in order to apply a belief propagation algorithm we provide a novel representation of the problem as a graphical model. Standard belief propagation on this model has poor scaling behavior, so we provide an efficient implementation that significantly decreases the complexity. We provide numerical results comparing the performance of our algorithm on both artificially created graphs and real world networks to several alternative algorithms, including algorithms based on integer programming (IP) techniques. These comparisons show that our algorithm scales better to large instances than IP-based algorithms and often finds better solutions than a simple algorithm that greedily selects the longest path from each root node. In some cases it also finds better solutions than the ones found by IP-based algorithms even when the latter are allowed to run significantly longer than our algorithm. "
712076764604973057,2016-03-22 00:41:45,https://t.co/TftKx6bthU,L0-norm Sparse Graph-regularized SVD for Biclustering. (arXiv:1603.06035v1 [cs.LG]) https://t.co/TftKx6bthU,1,11," Abstract: Learning the ""blocking"" structure is a central challenge for high dimensional data (e.g., gene expression data). Recently, a sparse singular value decomposition (SVD) has been used as a biclustering tool to achieve this goal. However, this model ignores the structural information between variables (e.g., gene interaction graph). Although typical graph-regularized norm can incorporate such prior graph information to get accurate discovery and better interpretability, it fails to consider the opposite effect of variables with different signs. Motivated by the development of sparse coding and graph-regularized norm, we propose a novel sparse graph-regularized SVD as a powerful biclustering tool for analyzing high-dimensional data. The key of this method is to impose two penalties including a novel graph-regularized norm ($|\pmb{u}|\pmb{L}|\pmb{u}|$) and $L_0$-norm ($\|\pmb{u}\|_0$) on singular vectors to induce structural sparsity and enhance interpretability. We design an efficient Alternating Iterative Sparse Projection (AISP) algorithm to solve it. Finally, we apply our method and related ones to simulated and real data to show its efficiency in capturing natural blocking structures. "
712076761413107712,2016-03-22 00:41:44,https://t.co/HQI3ws9WTj,Tensor Methods and Recommender Systems. (arXiv:1603.06038v1 [cs.LG]) https://t.co/HQI3ws9WTj,0,5," Abstract: A substantial progress in development of new and efficient tensor factorization techniques has led to an extensive research of their applicability in recommender systems field. Tensor-based recommender models push the boundaries of traditional collaborative filtering techniques by taking into account a multifaceted nature of real environments, which allows to produce more accurate, situational (e.g. context-aware, criteria-driven) recommendations. Despite the promising results, tensor-based methods are poorly covered in existing recommender systems surveys. This survey aims to complement previous works and provide a comprehensive overview on the subject. To the best of our knowledge, this is the first attempt to consolidate studies from various application domains in an easily readable, digestible format, which helps to get a notion of the current state of the field. We also provide a high level discussion of the future perspectives and directions for further improvement of tensor-based recommendation systems. "
712076759940857856,2016-03-22 00:41:43,https://t.co/DKgexyJvdS,The Computational Power of Dynamic Bayesian Networks. (arXiv:1603.06125v1 [cs.AI]) https://t.co/DKgexyJvdS,0,4," Abstract: This paper considers the computational power of constant size, dynamic Bayesian networks. Although discrete dynamic Bayesian networks are no more powerful than hidden Markov models, dynamic Bayesian networks with continuous random variables and discrete children of continuous parents are capable of performing Turing-complete computation. With modified versions of existing algorithms for belief propagation, such a simulation can be carried out in real time. This result suggests that dynamic Bayesian networks may be more powerful than previously considered. Relationships to causal models and recurrent neural networks are also discussed. "
712076758921682944,2016-03-22 00:41:43,https://t.co/wf3NKCfDF7,Fast Incremental Method for Nonconvex Optimization. (arXiv:1603.06159v1 [math.OC]) https://t.co/wf3NKCfDF7,1,7," Abstract: We analyze a fast incremental aggregated gradient method for optimizing nonconvex problems of the form $\min_x \sum_i f_i(x)$. Specifically, we analyze the SAGA algorithm within an Incremental First-order Oracle framework, and show that it converges to a stationary point provably faster than both gradient descent and stochastic gradient descent. We also discuss a Polyak's special class of nonconvex problems for which SAGA converges at a linear rate to the global optimum. Finally, we analyze the practically valuable regularized and minibatch variants of SAGA. To our knowledge, this paper presents the first analysis of fast convergence for an incremental aggregated gradient method for nonconvex problems. "
712076757764063232,2016-03-22 00:41:43,https://t.co/6D8EEuvr2D,Stochastic Variance Reduction for Nonconvex Optimization. (arXiv:1603.06160v1 [math.OC]) https://t.co/6D8EEuvr2D,1,7," Abstract: We study nonconvex finite-sum problems and analyze stochastic variance reduced gradient (SVRG) methods for them. SVRG and related methods have recently surged into prominence for convex optimization given their edge over stochastic gradient descent (SGD); but their theoretical analysis almost exclusively assumes convexity. In contrast, we prove non-asymptotic rates of convergence (to stationary points) of SVRG for nonconvex optimization, and show that it is provably faster than SGD and gradient descent. We also analyze a subclass of nonconvex problems on which SVRG attains linear convergence to the global optimum. We extend our analysis to mini-batch variants of SVRG, showing (theoretical) linear speedup due to mini-batching in parallel settings. "
712076756799332352,2016-03-22 00:41:43,https://t.co/rFVdwD5Wup,Joint Stochastic Approximation learning of Helmholtz Machines. (arXiv:1603.06170v1 [cs.LG]) https://t.co/rFVdwD5Wup,0,2," Abstract: Though with progress, model learning and performing posterior inference still remains a common challenge for using deep generative models, especially for handling discrete hidden variables. This paper is mainly concerned with algorithms for learning Helmholz machines, which is characterized by pairing the generative model with an auxiliary inference model. A common drawback of previous learning algorithms is that they indirectly optimize some bounds of the targeted marginal log-likelihood. In contrast, we successfully develop a new class of algorithms, based on stochastic approximation (SA) theory of the Robbins-Monro type, to directly optimize the marginal log-likelihood and simultaneously minimize the inclusive KL-divergence. The resulting learning algorithm is thus called joint SA (JSA). Moreover, we construct an effective MCMC operator for JSA. Our results on the MNIST datasets demonstrate that the JSA's performance is consistently superior to that of competing algorithms like RWS, for learning a range of difficult models. "
712076755591405568,2016-03-22 00:41:42,https://t.co/sGdpQuPYqM,The Multiscale Laplacian Graph Kernel. (arXiv:1603.06186v1 [stat.ML]) https://t.co/sGdpQuPYqM,0,4," Abstract: Many real world graphs, such as the graphs of molecules, exhibit structure at multiple different scales, but most existing kernels between graphs are either purely local or purely global in character. In contrast, by building a hierarchy of nested subgraphs, the Multiscale Laplacian Graph kernels (MLG kernels) that we define in this paper can account for structure at a range of different scales. At the heart of the MLG construction is another new graph kernel, called the Feature Space Laplacian Graph kernel (FLG kernel), which has the property that it can lift a base kernel defined on the vertices of two graphs to a kernel between the graphs. The MLG kernel applies such FLG kernels to subgraphs recursively. To make the MLG kernel computationally feasible, we also introduce a randomized projection procedure, similar to the Nystr\""om method, but for RKHS operators. "
712076754295365633,2016-03-22 00:41:42,https://t.co/Tod1gF4LLw,Extracting Predictive Information from Heterogeneous Data Streams using Gaussian Processes. (arXiv:1603.06202v1 [q… https://t.co/Tod1gF4LLw,1,8," Abstract: Financial markets are notoriously complex environments, presenting vast amounts of noisy, yet potentially informative data. We consider the problem of forecasting financial time series from a wide range of information sources using online Gaussian Processes with Automatic Relevance Determination (ARD) kernels. We measure the performance gain, quantified in terms of Normalised Root Mean Square Error (NRMSE), Median Absolute Deviation (MAD) and Pearson correlation, from fusing each of four separate data domains: time series technicals, sentiment analysis, options market data and broker recommendations. We show evidence that ARD kernels produce meaningful feature rankings that help retain salient inputs and reduce input dimensionality, providing a framework for sifting through financial complexity. We measure the performance gain from fusing each domain's heterogeneous data streams into a single probabilistic model. In particular our findings highlight the critical value of options data in mapping out the curvature of price space and inspire an intuitive, novel direction for research in financial prediction. "
712076753213194242,2016-03-22 00:41:42,https://t.co/MBNDvdeXAx,Structured VAEs: Composing Probabilistic Graphical Models and Variational Autoencoders. (arXiv:1603.06277v1 [stat.… https://t.co/MBNDvdeXAx,1,14," Abstract: We propose a general modeling and inference framework that composes probabilistic graphical models with deep learning methods and combines their respective strengths. Our model family augments graphical structure in latent variables with neural network observation models. For inference, we extend variational autoencoders to use graphical model approximating distributions with recognition networks that output conjugate potentials. All components of these models are learned simultaneously with a single objective, giving a scalable algorithm that leverages stochastic variational inference, natural gradients, graphical model message passing, and the reparameterization trick. We illustrate this framework with several example models and an application to mouse behavioral phenotyping. "
712076752139513856,2016-03-22 00:41:42,https://t.co/6LXDqYtQe3,Multi-fidelity Gaussian Process Bandit Optimisation. (arXiv:1603.06288v1 [stat.ML]) https://t.co/6LXDqYtQe3,0,4," Abstract: In many scientific and engineering applications, we are tasked with the optimisation of an expensive to evaluate black box function $f$. Traditional methods for this problem assume just the availability of this single function. However, in many cases, cheap approximations to $f$ may be obtainable. For example, the expensive real world behaviour of a robot can be approximated by a cheap computer simulation. We can use these approximations to eliminate low function value regions and use the expensive evaluations to $f$ in a small promising region and speedily identify the optimum. We formalise this task as a \emph{multi-fidelity} bandit problem where the target function and its approximations are sampled from a Gaussian process. We develop a method based on upper confidence bound techniques and prove that it exhibits precisely the above behaviour, hence achieving better regret than strategies which ignore multi-fidelity information. Our method outperforms such naive strategies on several synthetic and real experiments. "
712076750541475840,2016-03-22 00:41:41,https://t.co/JBfGHuym8J,Convex block-sparse linear regression with expanders -- provably. (arXiv:1603.06313v1 [cs.IT]) https://t.co/JBfGHuym8J,0,4," Abstract: Sparse matrices are favorable objects in machine learning and optimization. When such matrices are used, in place of dense ones, the overall complexity requirements in optimization can be significantly reduced in practice, both in terms of space and run-time. Prompted by this observation, we study a convex optimization scheme for block-sparse recovery from linear measurements. To obtain linear sketches, we use expander matrices, i.e., sparse matrices containing only few non-zeros per column. Hitherto, to the best of our knowledge, such algorithmic solutions have been only studied from a non-convex perspective. Our aim here is to theoretically characterize the performance of convex approaches under such setting. Our key novelty is the expression of the recovery error in terms of the model-based norm, while assuring that solution lives in the model. To achieve this, we show that sparse model-based matrices satisfy a group version of the null-space property. Our experimental findings on synthetic and real applications support our claims for faster recovery in the convex setting -- as opposed to using dense sensing matrices, while showing a competitive recovery performance. "
712076749270548480,2016-03-22 00:41:41,https://t.co/5ULitOxokk,Harnessing Deep Neural Networks with Logic Rules. (arXiv:1603.06318v1 [cs.LG]) https://t.co/5ULitOxokk,1,9," Abstract: Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems. "
712076748125544448,2016-03-22 00:41:41,https://t.co/VZh1ftdpDj,Data Augmentation via Levy Processes. (arXiv:1603.06340v1 [stat.ML]) https://t.co/VZh1ftdpDj,1,7," Abstract: If a document is about travel, we may expect that short snippets of the document should also be about travel. We introduce a general framework for incorporating these types of invariances into a discriminative classifier. The framework imagines data as being drawn from a slice of a Levy process. If we slice the Levy process at an earlier point in time, we obtain additional pseudo-examples, which can be used to train the classifier. We show that this scheme has two desirable properties: it preserves the Bayes decision boundary, and it is equivalent to fitting a generative model in the limit where we rewind time back to 0. Our construction captures popular schemes such as Gaussian feature noising and dropout training, as well as admitting new generalizations. "
712076746779185152,2016-03-22 00:41:40,https://t.co/3ZoNNslv19,A Comparison Study of Nonlinear Kernels. (arXiv:1603.06541v1 [stat.ML]) https://t.co/3ZoNNslv19,1,6," Abstract: In this paper, we compare 5 different nonlinear kernels: min-max, RBF, fRBF (folded RBF), acos, and acos-$\chi^2$, on a wide range of publicly available datasets. The proposed fRBF kernel performs very similarly to the RBF kernel. Both RBF and fRBF kernels require an important tuning parameter ($\gamma$). Interestingly, for a significant portion of the datasets, the min-max kernel outperforms the best-tuned RBF/fRBF kernels. The acos kernel and acos-$\chi^2$ kernel also perform well in general and in some datasets achieve the best accuracies. One crucial issue with the use of nonlinear kernels is the excessive computational and memory cost. These days, one increasingly popular strategy is to linearize the kernels through various randomization algorithms. In our study, the randomization method for the min-max kernel demonstrates excellent performance compared to the randomization methods for other types of nonlinear kernels, measured in terms of the number of nonzero terms in the transformed dataset. Our study provides evidence for supporting the use of the min-max kernel and the corresponding randomized linearization method (i.e., the so-called ""0-bit CWS""). Furthermore, the results motivate at least two directions for future research: (i) To develop new (and linearizable) nonlinear kernels for better accuracies; and (ii) To develop better linearization algorithms for improving the current linearization methods for the RBF kernel, the acos kernel, and the acos-$\chi^2$ kernel. One attempt is to combine the min-max kernel with the acos kernel or the acos-$\chi^2$ kernel. The advantages of these two new and tuning-free nonlinear kernels are demonstrated vias our extensive experiments. "
712076745566978048,2016-03-22 00:41:40,https://t.co/L2th0bRuSs,Phase transitions and sample complexity in Bayes-optimal matrix factorization. (arXiv:1402.1298v3 [cs.NA] UPDATED) https://t.co/L2th0bRuSs,4,9," Abstract: We analyse the matrix factorization problem. Given a noisy measurement of a product of two matrices, the problem is to estimate back the original matrices. It arises in many applications such as dictionary learning, blind matrix calibration, sparse principal component analysis, blind source separation, low rank matrix completion, robust principal component analysis or factor analysis. It is also important in machine learning: unsupervised representation learning can often be studied through matrix factorization. We use the tools of statistical mechanics - the cavity and replica methods - to analyze the achievability and computational tractability of the inference problems in the setting of Bayes-optimal inference, which amounts to assuming that the two matrices have random independent elements generated from some known distribution, and this information is available to the inference algorithm. In this setting, we compute the minimal mean-squared-error achievable in principle in any computational time, and the error that can be achieved by an efficient approximate message passing algorithm. The computation is based on the asymptotic state-evolution analysis of the algorithm. The performance that our analysis predicts, both in terms of the achieved mean-squared-error, and in terms of sample complexity, is extremely promising and motivating for a further development of the algorithm. "
712076744472313856,2016-03-22 00:41:40,https://t.co/QdBe89fPjd,Predictive Interval Models for Non-parametric Regression. (arXiv:1402.5874v2 [cs.LG] UPDATED) https://t.co/QdBe89fPjd,1,4," Abstract: Having a regression model, we are interested in finding two-sided intervals that are guaranteed to contain at least a desired proportion of the conditional distribution of the response variable given a specific combination of predictors. We name such intervals predictive intervals. This work presents a new method to find two-sided predictive intervals for non-parametric least squares regression without the homoscedasticity assumption. Our predictive intervals are built by using tolerance intervals on prediction errors in the query point's neighborhood. We proposed a predictive interval model test and we also used it as a constraint in our hyper-parameter tuning algorithm. This gives an algorithm that finds the smallest reliable predictive intervals for a given dataset. We also introduce a measure for comparing different interval prediction methods yielding intervals having different size and coverage. These experiments show that our methods are more reliable, effective and precise than other interval prediction methods. "
712076743285350401,2016-03-22 00:41:40,https://t.co/nAGqtZJT9H,"Linear Dimensionality Reduction: Survey, Insights, and Generalizations. (arXiv:1406.0873v2 [stat.ML] UPDATED) https://t.co/nAGqtZJT9H",1,12," Abstract: Linear dimensionality reduction methods are a cornerstone of analyzing high dimensional data, due to their simple geometric interpretations and typically attractive computational properties. These methods capture many data features of interest, such as covariance, dynamical structure, correlation between data sets, input-output relationships, and margin between data classes. Methods have been developed with a variety of names and motivations in many fields, and perhaps as a result the connections between all these methods have not been highlighted. Here we survey methods from this disparate literature as optimization programs over matrix manifolds. We discuss principal component analysis, factor analysis, linear multidimensional scaling, Fisher's linear discriminant analysis, canonical correlations analysis, maximum autocorrelation factors, slow feature analysis, sufficient dimensionality reduction, undercomplete independent component analysis, linear regression, distance metric learning, and more. This optimization framework gives insight to some rarely discussed shortcomings of well-known methods, such as the suboptimality of certain eigenvector solutions. Modern techniques for optimization over matrix manifolds enable a generic linear dimensionality reduction solver, which accepts as input data and an objective to be optimized, and returns, as output, an optimal low-dimensional projection of the data. This simple optimization framework further allows straightforward generalizations and novel variants of classical methods, which we demonstrate here by creating an orthogonal-projection canonical correlations analysis. More broadly, this survey and generic solver suggest that linear dimensionality reduction can move toward becoming a blackbox, objective-agnostic numerical technology. "
712076742182248453,2016-03-22 00:41:39,https://t.co/grUlZHbgAu,Bayesian multi-tensor factorization. (arXiv:1412.4679v3 [stat.ML] UPDATED) https://t.co/grUlZHbgAu,0,8," Abstract: We introduce Bayesian multi-tensor factorization, a model that is the first Bayesian formulation for joint factorization of multiple matrices and tensors. The research problem generalizes the joint matrix-tensor factorization problem to arbitrary sets of tensors of any depth, including matrices, can be interpreted as unsupervised multi-view learning from multiple data tensors, and can be generalized to relax the usual trilinear tensor factorization assumptions. The result is a factorization of the set of tensors into factors shared by any subsets of the tensors, and factors private to individual tensors. We demonstrate the performance against existing baselines in multiple tensor factorization tasks in structural toxicogenomics and functional neuroimaging. "
712076740856778752,2016-03-22 00:41:39,https://t.co/GO6u4SoHMc,Analysis of Crowdsourced Sampling Strategies for HodgeRank with Sparse Random Graphs. (arXiv:1503.00164v2 [stat.ML… https://t.co/GO6u4SoHMc,0,3," Abstract: Crowdsourcing platforms are now extensively used for conducting subjective pairwise comparison studies. In this setting, a pairwise comparison dataset is typically gathered via random sampling, either \emph{with} or \emph{without} replacement. In this paper, we use tools from random graph theory to analyze these two random sampling methods for the HodgeRank estimator. Using the Fiedler value of the graph as a measurement for estimator stability (informativeness), we provide a new estimate of the Fiedler value for these two random graph models. In the asymptotic limit as the number of vertices tends to infinity, we prove the validity of the estimate. Based on our findings, for a small number of items to be compared, we recommend a two-stage sampling strategy where a greedy sampling method is used initially and random sampling \emph{without} replacement is used in the second stage. When a large number of items is to be compared, we recommend random sampling with replacement as this is computationally inexpensive and trivially parallelizable. Experiments on synthetic and real-world datasets support our analysis. "
712076739522990080,2016-03-22 00:41:39,https://t.co/FIHaJrUmT0,Two Timescale Stochastic Approximation with Controlled Markov noise and Off-policy temporal difference learning. (… https://t.co/FIHaJrUmT0,1,3," Abstract: We present for the first time an asymptotic convergence analysis of two time-scale stochastic approximation driven by `controlled' Markov noise. In particular, both the faster and slower recursions have non-additive controlled Markov noise components in addition to martingale difference noise. We analyze the asymptotic behavior of our framework by relating it to limiting differential inclusions in both time-scales that are defined in terms of the ergodic occupation measures associated with the controlled Markov processes. Finally, we present a solution to the off-policy convergence problem for temporal difference learning with linear function approximation, using our results. "
712076738239598593,2016-03-22 00:41:38,https://t.co/KNaXLvBKZe,New Optimisation Methods for Machine Learning. (arXiv:1510.02533v2 [cs.LG] UPDATED) https://t.co/KNaXLvBKZe,0,14," Abstract: A thesis submitted for the degree of Doctor of Philosophy of The Australian National University. In this work we introduce several new optimisation methods for problems in machine learning. Our algorithms broadly fall into two categories: optimisation of finite sums and of graph structured objectives. The finite sum problem is simply the minimisation of objective functions that are naturally expressed as a summation over a large number of terms, where each term has a similar or identical weight. Such objectives most often appear in machine learning in the empirical risk minimisation framework in the non-online learning setting. The second category, that of graph structured objectives, consists of objectives that result from applying maximum likelihood to Markov random field models. Unlike the finite sum case, all the non-linearity is contained within a partition function term, which does not readily decompose into a summation. For the finite sum problem, we introduce the Finito and SAGA algorithms, as well as variants of each. For graph-structured problems, we take three complementary approaches. We look at learning the parameters for a fixed structure, learning the structure independently, and learning both simultaneously. Specifically, for the combined approach, we introduce a new method for encouraging graph structures with the ""scale-free"" property. For the structure learning problem, we establish SHORTCUT, a O(n^{2.5}) expected time approximate structure learning method for Gaussian graphical models. For problems where the structure is known but the parameters unknown, we introduce an approximate maximum likelihood learning algorithm that is capable of learning a useful subclass of Gaussian graphical models. "
712076736914137092,2016-03-22 00:41:38,https://t.co/BzdHA2r3Wd,Additive Approximations in High Dimensional Nonparametric Regression via the SALSA. (arXiv:1602.00287v2 [stat.ML] … https://t.co/BzdHA2r3Wd,1,4," Abstract: High dimensional nonparametric regression is an inherently difficult problem with known lower bounds depending exponentially in dimension. A popular strategy to alleviate this curse of dimensionality has been to use additive models of \emph{first order}, which model the regression function as a sum of independent functions on each dimension. Though useful in controlling the variance of the estimate, such models are often too restrictive in practical settings. Between non-additive models which often have large variance and first order additive models which have large bias, there has been little work to exploit the trade-off in the middle via additive models of intermediate order. In this work, we propose SALSA, which bridges this gap by allowing interactions between variables, but controls model capacity by limiting the order of interactions. SALSA minimises the residual sum of squares with squared RKHS norm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression with an additive kernel. When the regression function is additive, the excess risk is only polynomial in dimension. Using the Girard-Newton formulae, we efficiently sum over a combinatorial number of terms in the additive expansion. Via a comparison on $15$ real datasets, we show that our method is competitive against $21$ other alternatives. "
712076735299362816,2016-03-22 00:41:38,https://t.co/OzzdZalIpm,Statistical Foundation of Spectral Graph Theory. (arXiv:1602.03861v3 [math.ST] UPDATED) https://t.co/OzzdZalIpm,1,13," Abstract: The goal of this paper is to show that there exists a simple, yet universal statistical logic of spectral graph analysis by recasting it into a nonparametric function estimation problem. The prescribed viewpoint appears to be good enough to accommodate most of the existing spectral graph techniques as a consequence of just one single formalism and algorithm. "
711716337236774912,2016-03-21 00:49:32,https://t.co/6q8nAfiLPS,Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?. (arXiv:1603.05691v1 [stat.ML]) https://t.co/6q8nAfiLPS,6,18," Abstract: Yes, they do. This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained. Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution. Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher. "
711716335005335552,2016-03-21 00:49:31,https://t.co/2sYUuL1VIz,Convergence of Contrastive Divergence Algorithm in Exponential Family. (arXiv:1603.05729v1 [stat.ML]) https://t.co/2sYUuL1VIz,2,7," Abstract: This paper studies the convergence properties of contrastive divergence algorithm for parameter inference in exponential family, by relating it to Markov chain theory and stochastic stability literature. We prove that, under mild conditions and given a finite data sample $X_1,\dots,X_n \sim p_{\theta^*}$ i.i.d. in an event with probability approaching to 1, the sequence $\{\theta_t\}_{t \ge 0}$ generated by CD algorithm is a positive Harris recurrent chain, and thus processes an unique invariant distribution $\pi_n$. The invariant distribution concentrates around the Maximum Likelihood Estimate at a speed arbitrarily slower than $\sqrt{n}$, and the number of steps in Markov Chain Monte Carlo only affects the coefficient factor of the concentration rate. Finally we conclude that as $n \to \infty$, $$\limsup_{t \to \infty} \left\Vert \frac{1}{t} \sum_{s=1}^t \theta_s - \theta^*\right\Vert \overset{p}{\to} 0.$$ "
711716333562503169,2016-03-21 00:49:31,https://t.co/Zta6LbEaFC,A Probabilistic Machine Learning Approach to Detect Industrial Plant Faults. (arXiv:1603.05770v1 [stat.ML]) https://t.co/Zta6LbEaFC,0,4," Abstract: Fault detection in industrial plants is a hot research area as more and more sensor data are being collected throughout the industrial process. Automatic data-driven approaches are widely needed and seen as a promising area of investment. This paper proposes an effective machine learning algorithm to predict industrial plant faults based on classification methods such as penalized logistic regression, random forest and gradient boosted tree. A fault's start time and end time are predicted sequentially in two steps by formulating the original prediction problems as classification problems. The algorithms described in this paper won first place in the Prognostics and Health Management Society 2015 Data Challenge. "
711716332308406272,2016-03-21 00:49:31,https://t.co/JhpuWOEun3,A Comparison between Deep Neural Nets and Kernel Acoustic Models for Speech Recognition. (arXiv:1603.05800v1 [cs.L… https://t.co/JhpuWOEun3,4,8," Abstract: We study large-scale kernel methods for acoustic modeling and compare to DNNs on performance metrics related to both acoustic modeling and recognition. Measuring perplexity and frame-level classification accuracy, kernel-based acoustic models are as effective as their DNN counterparts. However, on token-error-rates DNN models can be significantly better. We have discovered that this might be attributed to DNN's unique strength in reducing both the perplexity and the entropy of the predicted posterior probabilities. Motivated by our findings, we propose a new technique, entropy regularized perplexity, for model selection. This technique can noticeably improve the recognition performance of both types of models, and reduces the gap between them. While effective on Broadcast News, this technique could be also applicable to other tasks. "
711716330802696192,2016-03-21 00:49:30,https://t.co/Hq1Bfuu5Hv,Generalized support vector regression: duality and tensor-kernel representation. (arXiv:1603.05876v1 [math.OC]) https://t.co/Hq1Bfuu5Hv,1,4," Abstract: In this paper we study the variational problem associated to support vector regression in Banach function spaces. Using the Fenchel-Rockafellar duality theory, we give explicit formulation of the dual problem as well as of the related optimality conditions. Moreover, we provide a new computational framework for solving the problem which relies on a tensor-kernel representation. This analysis overcomes the typical difficulties connected to learning in Banach spaces. We finally present a large class of tensor-kernels to which our theory fully applies: power series tensor kernels. This type of kernels describe Banach spaces of analytic functions and include generalizations of the exponential and polynomial kernels as well as, in the complex case, generalizations of the Szeg\""o and Bergman kernels. "
711716328630054912,2016-03-21 00:49:30,https://t.co/URHhK9guUt,Katyusha: Accelerated Variance Reduction for Faster SGD. (arXiv:1603.05953v1 [math.OC]) https://t.co/URHhK9guUt,1,7," Abstract: We introduce $\mathtt{Katyusha}$, the first direct, primal-only stochastic gradient method that has a provably accelerated convergence rate in convex optimization. In contrast, previous methods are based on dual coordinate descent which are more restrictive, or based on outer-inner loops which make them ""blind"" to the underlying stochastic nature of the optimization process. $\mathtt{Katyusha}$ is the first algorithm that incorporates acceleration directly into stochastic gradient updates. Unlike previous results, $\mathtt{Katyusha}$ obtains an optimal convergence rate. It also supports proximal updates, non-Euclidean norm smoothness, non-uniform sampling, and mini-batch sampling. When applied to interesting classes of convex objectives, including smooth objectives (e.g., Lasso, Logistic Regression), strongly-convex objectives (e.g., SVM), and non-smooth objectives (e.g., L1SVM), $\mathtt{Katyusha}$ improves the best known convergence rates. The main ingredient behind our result is $\textit{Katyusha momentum}$, a novel ""negative momentum on top of momentum"" that can be incorporated into a variance-reduction based algorithm and speed it up. As a result, since variance reduction has been successfully applied to a fast growing list of practical problems, our paper suggests that in each of such cases, one had better hurry up and give Katyusha a hug. "
711716327107469312,2016-03-21 00:49:30,https://t.co/MzYryKuaTb,Program Evaluation and Causal Inference with High-Dimensional Data. (arXiv:1311.2645v6 [math.ST] UPDATED) https://t.co/MzYryKuaTb,1,10," Abstract: In this paper, we provide efficient estimators and honest confidence bands for a variety of treatment effects including local average (LATE) and local quantile treatment effects (LQTE) in data-rich environments. We can handle very many control variables, endogenous receipt of treatment, heterogeneous treatment effects, and function-valued outcomes. Our framework covers the special case of exogenous receipt of treatment, either conditional on controls or unconditionally as in randomized control trials. In the latter case, our approach produces efficient estimators and honest bands for (functional) average treatment effects (ATE) and quantile treatment effects (QTE). To make informative inference possible, we assume that key reduced form predictive relationships are approximately sparse. This assumption allows the use of regularization and selection methods to estimate those relations, and we provide methods for post-regularization and post-selection inference that are uniformly valid (honest) across a wide-range of models. We show that a key ingredient enabling honest inference is the use of orthogonal or doubly robust moment conditions in estimating certain reduced form functional parameters. We illustrate the use of the proposed methods with an application to estimating the effect of 401(k) eligibility and participation on accumulated assets. "
711716325723410433,2016-03-21 00:49:29,https://t.co/p6AOGrsMUR,Approximating Likelihood Ratios with Calibrated Discriminative Classifiers. (arXiv:1506.02169v2 [stat.AP] UPDATED) https://t.co/p6AOGrsMUR,5,12," Abstract: In many fields of science, generalized likelihood ratio tests are established tools for statistical inference. At the same time, it has become increasingly common that a simulator (or generative model) is used to describe complex processes that tie parameters $\theta$ of an underlying theory and measurement apparatus to high-dimensional observations $\mathbf{x}\in \mathbb{R}^p$. However, simulator often do not provide a way to evaluate the likelihood function for a given observation $\mathbf{x}$, which motivates a new class of likelihood-free inference algorithms. In this paper, we show that likelihood ratios are invariant under a specific class of dimensionality reduction maps $\mathbb{R}^p \mapsto \mathbb{R}$. As a direct consequence, we show that discriminative classifiers can be used to approximate the generalized likelihood ratio statistic when only a generative model for the data is available. This leads to a new machine learning-based approach to likelihood-free inference that is complementary to Approximate Bayesian Computation, and which does not require a prior on the model parameters. Experimental results on artificial problems with known exact likelihoods illustrate the potential of the proposed method. "
711716324267986944,2016-03-21 00:49:29,https://t.co/9lGLmfR5R1,Learning Fair Classifiers. (arXiv:1507.05259v3 [stat.ML] UPDATED) https://t.co/9lGLmfR5R1,0,7," Abstract: Automated data-driven decision systems are ubiquitous across a wide variety of online services, from online social networking and e-commerce to e-government. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes have a disproportionally large adverse impact on particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers in a principled manner, by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism on two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control of the level of fairness, often at a minimal cost in terms of accuracy. "
711716322602835968,2016-03-21 00:49:29,https://t.co/Z72rmfoeQR,Mean-Field Inference in Gaussian Restricted Boltzmann Machine. (arXiv:1512.00927v2 [stat.ML] UPDATED) https://t.co/Z72rmfoeQR,1,10," Abstract: A Gaussian restricted Boltzmann machine (GRBM) is a Boltzmann machine defined on a bipartite graph and is an extension of usual restricted Boltzmann machines. A GRBM consists of two different layers: a visible layer composed of continuous visible variables and a hidden layer composed of discrete hidden variables. In this paper, we derive two different inference algorithms for GRBMs based on the naive mean-field approximation (NMFA). One is an inference algorithm for whole variables in a GRBM, and the other is an inference algorithm for partial variables in a GBRBM. We compare the two methods analytically and numerically and show that the latter method is better. "
710642781404979202,2016-03-18 01:43:36,https://t.co/yTXMNOedlT,Clustering of Sparse and Approximately Sparse Graphs by Semidefinite Programming. (arXiv:1603.05296v1 [math.OC]) https://t.co/yTXMNOedlT,1,4," Abstract: As a model problem for clustering, we consider the densest k-disjoint-clique problem of partitioning a weighted complete graph into k disjoint subgraphs such that the sum of the densities of these subgraphs is maximized. We establish that such subgraphs can be recovered from the solution of a particular semidefinite relaxation with high probability if the input graph is sampled from a distribution of clusterable graphs. Specifically, the semidefinite relaxation is exact if the graph consists of k large disjoint subgraphs, corresponding to clusters, with weight concentrated within these subgraphs, plus a moderate number of outliers. Further, we establish that if noise is weakly obscuring these clusters, i.e, the between-cluster edges are assigned very small weights, then we can recover significantly smaller clusters. For example, we show that in approximately sparse graphs, where the between-cluster weights tend to zero as the size n of the graph tends to infinity, we can recover clusters of size polylogarithmic in n. Empirical evidence from numerical simulations is also provided to support these theoretical phase transitions to perfect recovery of the cluster structure. "
710642780205404160,2016-03-18 01:43:36,https://t.co/eQTyUz3hkb,Near-Optimal Stochastic Approximation for Online Principal Component Estimation. (arXiv:1603.05305v1 [math.OC]) https://t.co/eQTyUz3hkb,2,9," Abstract: Principal component analysis (PCA) has been a prominent tool for high-dimensional data analysis. Online algorithms that estimate the principal component by processing streaming data are of tremendous practical and theoretical interests. Despite its rich applications, theoretical convergence analysis remains largely open. In this paper, we cast online PCA into a stochastic nonconvex optimization problem, and we analyze the online PCA algorithm as a stochastic approximation iteration. The stochastic approximation iteration processes data points incrementally and maintains a running estimate of the principal component. We prove for the first time a nearly optimal convergence rate result for the online PCA algorithm. We show that the finite-sample error closely matches the minimax information lower bound. In addition, we characterize the convergence process using ordinary and stochastic differential equation approximations. "
710642779030945792,2016-03-18 01:43:36,https://t.co/6L4kghGm6N,Cascading Bandits for Large-Scale Recommendation Problems. (arXiv:1603.05359v1 [cs.LG]) https://t.co/6L4kghGm6N,1,4," Abstract: Most recommender systems recommend a list of items. The user examines the list, from the first item to the last, and often chooses the first attractive item and does not examine the rest. This type of user behavior can be modeled by the cascade model. In this work, we study cascading bandits, an online learning variant of the cascade model where the goal is to recommend $K$ most attractive items from a large set of $L$ candidate items. We propose two algorithms for solving this problem, which are based on the idea of linear generalization. The key idea in our solutions is that we learn a predictor of the attraction probabilities of items from their features, as opposing to learning the attraction probability of each item independently as in the existing work. This results in practical learning algorithms whose regret does not depend on the number of items $L$. We bound the regret of one algorithm and comprehensively evaluate the other on a range of recommendation problems. The algorithm performs well and outperforms all baselines. "
710642777848201220,2016-03-18 01:43:36,https://t.co/Vq4lwSU54H,Online semi-parametric learning for inverse dynamics modeling. (arXiv:1603.05412v1 [math.OC]) https://t.co/Vq4lwSU54H,0,3," Abstract: This paper presents a semi-parametric algorithm for online learning of a robot inverse dynamics model. It combines the strength of the parametric and non-parametric modeling. The former exploits the rigid body dynamics equa- tion, while the latter exploits a suitable kernel function. We provide an extensive comparison with other methods from the literature using real data from the iCub humanoid robot. In doing so we also compare two different techniques, namely cross validation and marginal likelihood optimization, for estimating the hyperparameters of the kernel function. "
710627587144417280,2016-03-18 00:43:14,https://t.co/LCvyaKMnIY,"Mapping Temporal Variables into the NeuCube for Improved Pattern Recognition, Predictive Modelling and Understandi… https://t.co/LCvyaKMnIY",0,5," Abstract: This paper proposes a new method for an optimized mapping of temporal variables, describing a temporal stream data, into the recently proposed NeuCube spiking neural network architecture. This optimized mapping extends the use of the NeuCube, which was initially designed for spatiotemporal brain data, to work on arbitrary stream data and to achieve a better accuracy of temporal pattern recognition, a better and earlier event prediction and a better understanding of complex temporal stream data through visualization of the NeuCube connectivity. The effect of the new mapping is demonstrated on three bench mark problems. The first one is early prediction of patient sleep stage event from temporal physiological data. The second one is pattern recognition of dynamic temporal patterns of traffic in the Bay Area of California and the last one is the Challenge 2012 contest data set. In all cases the use of the proposed mapping leads to an improved accuracy of pattern recognition and event prediction and a better understanding of the data when compared to traditional machine learning techniques or spiking neural network reservoirs with arbitrary mapping of the variables. "
710627586100088832,2016-03-18 00:43:14,https://t.co/tQfUclwpBX,Optimal Black-Box Reductions Between Optimization Objectives. (arXiv:1603.05642v1 [math.OC]) https://t.co/tQfUclwpBX,0,6," Abstract: The diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods, finely tuned to the specific regression or classification task at hand. We reduce the complexity of algorithm design for machine learning by reductions: we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity in applications. Furthermore, unlike existing results, our new reductions are OPTIMAL and more PRACTICAL. We show how these new reductions give rise to new and faster running times on training linear classifiers for various families of loss functions, and conclude with experiments showing their successes also in practice. "
710627584829210624,2016-03-18 00:43:13,https://t.co/KlUo1nnw7v,Variance Reduction for Faster Non-Convex Optimization. (arXiv:1603.05643v1 [math.OC]) https://t.co/KlUo1nnw7v,1,8," Abstract: We consider the fundamental problem in non-convex optimization of efficiently reaching a stationary point. In contrast to the convex case, in the long history of this basic problem, the only known theoretical results on first-order non-convex optimization remain to be full gradient descent that converges in $O(1/\varepsilon)$ iterations for smooth objectives, and stochastic gradient descent that converges in $O(1/\varepsilon^2)$ iterations for objectives that are sum of smooth functions. We provide the first improvement in this line of research. Our result is based on the variance reduction trick recently introduced to convex optimization, as well as a brand new analysis of variance reduction that is suitable for non-convex optimization. For objectives that are sum of smooth functions, our first-order minibatch stochastic method converges with an $O(1/\varepsilon)$ rate, and is faster than full gradient descent by $\Omega(n^{1/3})$. We demonstrate the effectiveness of our methods on empirical risk minimizations with non-convex loss functions and training neural nets. "
710627583461867520,2016-03-18 00:43:13,https://t.co/ICeGNpOtQA,Feature Selection with Annealing for Computer Vision and Big Data Learning. (arXiv:1310.2880v7 [stat.ML] UPDATED) https://t.co/ICeGNpOtQA,2,6," Abstract: Many computer vision and medical imaging problems are faced with learning from large-scale datasets, with millions of observations and features. In this paper we propose a novel efficient learning scheme that tightens a sparsity constraint by gradually removing variables based on a criterion and a schedule. The attractive fact that the problem size keeps dropping throughout the iterations makes it particularly suitable for big data learning. Our approach applies generically to the optimization of any differentiable loss function, and finds applications in regression, classification and ranking. The resultant algorithms build variable screening into estimation and are extremely simple to implement. We provide theoretical guarantees of convergence and selection consistency. In addition, one dimensional piecewise linear response functions are used to account for nonlinearity and a second order prior is imposed on these functions to avoid overfitting. Experiments on real and synthetic data show that the proposed method compares very well with other state of the art methods in regression, classification and ranking while being computationally very efficient and scalable. "
710627581909983234,2016-03-18 00:43:13,https://t.co/yFcPvqrg17,Validation of k-Nearest Neighbor Classifiers Using Inclusion and Exclusion. (arXiv:1410.2500v3 [cs.LG] UPDATED) https://t.co/yFcPvqrg17,1,6," Abstract: This paper presents a series of PAC exponential error bounds for $k$-nearest neighbors classifiers, with O($n^{-\frac{r}{2r+1}}\sqrt{k \ln n}$) error bound range for each integer $r>0$, where $n$ is the number of in-sample examples. This shows that $k$-nn classifiers, in spite of their famously fractured decision boundaries, come close to having Gaussian-style exponential error bounds with O($n^{-\frac{1}{2}}$) bound ranges. "
710627580467138560,2016-03-18 00:43:12,https://t.co/RdlqcB1QuA,Classification and Reconstruction of High-Dimensional Signals from Low-Dimensional Features in the Presence of Sid… https://t.co/RdlqcB1QuA,1,7," Abstract: This paper offers a characterization of fundamental limits on the classification and reconstruction of high-dimensional signals from low-dimensional features, in the presence of side information. We consider a scenario where a decoder has access both to linear features of the signal of interest and to linear features of the side information signal; while the side information may be in a compressed form, the objective is recovery or classification of the primary signal, not the side information. The signal of interest and the side information are each assumed to have (distinct) latent discrete labels; conditioned on these two labels, the signal of interest and side information are drawn from a multivariate Gaussian distribution. With joint probabilities on the latent labels, the overall signal-(side information) representation is defined by a Gaussian mixture model. We then provide sharp sufficient and/or necessary conditions for these quantities to approach zero when the covariance matrices of the Gaussians are nearly low-rank. These conditions, which are reminiscent of the well-known Slepian-Wolf and Wyner-Ziv conditions, are a function of the number of linear features extracted from the signal of interest, the number of linear features extracted from the side information signal, and the geometry of these signals and their interplay. Moreover, on assuming that the signal of interest and the side information obey such an approximately low-rank model, we derive expansions of the reconstruction error as a function of the deviation from an exactly low-rank model; such expansions also allow identification of operational regimes where the impact of side information on signal reconstruction is most relevant. Our framework, which offers a principled mechanism to integrate side information in high-dimensional data problems, is also tested in the context of imaging applications. "
710627579254992896,2016-03-18 00:43:12,https://t.co/VdZjETiKui,"Less is More: Nystr\""om Computational Regularization. (arXiv:1507.04717v6 [stat.ML] UPDATED) https://t.co/VdZjETiKui",1,4," Abstract: We study Nystr\""om type subsampling approaches to large scale kernel methods, and prove learning bounds in the statistical learning setting, where random sampling and high probability estimates are considered. In particular, we prove that these approaches can achieve optimal learning bounds, provided the subsampling level is suitably chosen. These results suggest a simple incremental variant of Nystr\""om Kernel Regularized Least Squares, where the subsampling level implements a form of computational regularization, in the sense that it controls at the same time regularization and computations. Extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets. "
710627577401090048,2016-03-18 00:43:11,https://t.co/JEwS5uSmAf,Deep Manifold Traversal: Changing Labels with Convolutional Features. (arXiv:1511.06421v3 [cs.LG] UPDATED) https://t.co/JEwS5uSmAf,2,17," Abstract: Many tasks in computer vision can be cast as a ""label changing"" problem, where the goal is to make a semantic change to the appearance of an image or some subject in an image in order to alter the class membership. Although successful task-specific methods have been developed for some label changing applications, to date no general purpose method exists. Motivated by this we propose deep manifold traversal, a method that addresses the problem in its most general form: it first approximates the manifold of natural images then morphs a test image along a traversal path away from a source class and towards a target class while staying near the manifold throughout. The resulting algorithm is surprisingly effective and versatile. It is completely data driven, requiring only an example set of images from the desired source and target domains. We demonstrate deep manifold traversal on highly diverse label changing tasks: changing an individual's appearance (age and hair color), changing the season of an outdoor image, and transforming a city skyline towards nighttime. "
710262658075914241,2016-03-17 00:33:08,https://t.co/sYbIf7teK9,Bias Correction for Regularized Regression and its Application in Learning with Streaming Data. (arXiv:1603.04882v… https://t.co/sYbIf7teK9,0,6, Abstract: We propose an approach to reduce the bias of ridge regression and regularization kernel network. When applied to a single data set the new algorithms have comparable learning performance with the original ones. When applied to incremental learning with block wise streaming data the new algorithms are more efficient due to bias reduction. Both theoretical characterizations and simulation studies are used to verify the effectiveness of these new algorithms. 
710262653738942464,2016-03-17 00:33:07,https://t.co/9mqC5qcKNR,Turing learning: a metric-free approach to inferring behavior and its application to swarms. (arXiv:1603.04904v1 [… https://t.co/9mqC5qcKNR,1,5," Abstract: We propose Turing Learning, a novel system identification method for inferring the behavior of natural or artificial systems. Turing Learning simultaneously optimizes two populations of computer programs, one representing models of the behavior of the system under investigation, and the other representing classifiers. By observing the behavior of the system as well as the behaviors produced by the models, two sets of data samples are obtained. The classifiers are rewarded for discriminating between these two sets, that is, for correctly categorizing data samples as either genuine or counterfeit. Conversely, the models are rewarded for 'tricking' the classifiers into categorizing their data samples as genuine. Unlike other methods for system identification, Turing Learning does not require predefined metrics to quantify the difference between the system and its models. We present two case studies with swarms of simulated robots and prove that the underlying behaviors cannot be inferred by a metric-based system identification method. By contrast, Turing Learning infers the behaviors with high accuracy. It also produces a useful by-product - the classifiers - that can be used to detect abnormal behavior in the swarm. Moreover, we show that Turing Learning also successfully infers the behavior of physical robot swarms. The results show that collective behaviors can be directly inferred from motion trajectories of individuals in the swarm, which may have significant implications for the study of animal collectives. Furthermore, Turing Learning could prove useful whenever a behavior is not easily characterizable using metrics, making it suitable for a wide range of applications. "
710262651935440896,2016-03-17 00:33:06,https://t.co/Ad0EM9z2Ti,Data Clustering and Graph Partitioning via Simulated Mixing. (arXiv:1603.04918v1 [cs.LG]) https://t.co/Ad0EM9z2Ti,1,3," Abstract: Spectral clustering approaches have led to well-accepted algorithms for finding accurate clusters in a given dataset. However, their application to large-scale datasets has been hindered by computational complexity of eigenvalue decompositions. Several algorithms have been proposed in the recent past to accelerate spectral clustering, however they compromise on the accuracy of the spectral clustering to achieve faster speed. In this paper, we propose a novel spectral clustering algorithm based on a mixing process on a graph. Unlike the existing spectral clustering algorithms, our algorithm does not require computing eigenvectors. Specifically, it finds the equivalent of a linear combination of eigenvectors of the normalized similarity matrix weighted with corresponding eigenvalues. This linear combination is then used to partition the dataset into meaningful clusters. Simulations on real datasets show that partitioning datasets based on such linear combinations of eigenvectors achieves better accuracy than standard spectral clustering methods as the number of clusters increase. Our algorithm can easily be implemented in a distributed setting. "
710262650320642048,2016-03-17 00:33:06,https://t.co/lBCP3RlTFX,Regret-optimal Strategies for Playing Repeated Games with Discounted Losses. (arXiv:1603.04981v1 [cs.GT]) https://t.co/lBCP3RlTFX,0,4," Abstract: The regret-minimization paradigm has emerged as a powerful technique for designing algorithms for online decision-making in adversarial environments. But so far, designing exact minmax-optimal algorithms for minimizing the worst-case regret has proven to be a difficult task in general, with only a few known results in specific settings. In this paper, we present a novel set-valued dynamic programming approach for designing such exact regret-optimal policies for playing repeated games with discounted losses. Our approach first draws the connection between regret minimization, and determining minimal achievable guarantees in repeated games with vector-valued losses. We then characterize the set of these minimal guarantees as the fixed point of a dynamic programming operator defined on the space of Pareto frontiers of convex and compact sets. This approach simultaneously results in the characterization of the optimal strategies that achieve these minimal guarantees, and hence of regret-optimal strategies in the original repeated game. As an illustration of our approach, we design a simple near-optimal strategy for prediction using expert advice for the case of 2 experts. "
710262648705785856,2016-03-17 00:33:06,https://t.co/u2PwOt2td9,Short-term time series prediction using Hilbert space embeddings of autoregressive processes. (arXiv:1603.05060v1 … https://t.co/u2PwOt2td9,0,8," Abstract: Linear autoregressive models serve as basic representations of discrete time stochastic processes. Different attempts have been made to provide non-linear versions of the basic autoregressive process, including different versions based on kernel methods. Motivated by the powerful framework of Hilbert space embeddings of distributions, in this paper we apply this methodology for the kernel embedding of an autoregressive process of order $p$. By doing so, we provide a non-linear version of an autoregressive process, that shows increased performance over the linear model in highly complex time series. We use the method proposed for one-step ahead forecasting of different time-series, and compare its performance against other non-linear methods. "
710262647023935488,2016-03-17 00:33:05,https://t.co/U0tlcwR7wL,One-Shot Generalization in Deep Generative Models. (arXiv:1603.05106v1 [stat.ML]) https://t.co/U0tlcwR7wL,7,35," Abstract: Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples---having seen new examples just once---providing an important class of general-purpose models for one-shot machine learning. "
710262645211996165,2016-03-17 00:33:05,https://t.co/jVgYsMJMSo,Feature Selection as a Multiagent Coordination Problem. (arXiv:1603.05152v1 [cs.LG]) https://t.co/jVgYsMJMSo,2,4," Abstract: Datasets with hundreds to tens of thousands features is the new norm. Feature selection constitutes a central problem in machine learning, where the aim is to derive a representative set of features from which to construct a classification (or prediction) model for a specific task. Our experimental study involves microarray gene expression datasets, these are high-dimensional and noisy datasets that contain genetic data typically used for distinguishing between benign or malicious tissues or classifying different types of cancer. In this paper, we formulate feature selection as a multiagent coordination problem and propose a novel feature selection method using multiagent reinforcement learning. The central idea of the proposed approach is to ""assign"" a reinforcement learning agent to each feature where each agent learns to control a single feature, we refer to this approach as MARL. Applying this to microarray datasets creates an enormous multiagent coordination problem between thousands of learning agents. To address the scalability challenge we apply a form of reward shaping called CLEAN rewards. We compare in total nine feature selection methods, including state-of-the-art methods, and show that the proposed method using CLEAN rewards can significantly scale-up, thus outperforming the rest of learning-based methods. We further show that a hybrid variant of MARL achieves the best overall performance. "
710262643571953664,2016-03-17 00:33:04,https://t.co/1fQOV8Ok5R,On semidefinite relaxations for the block model. (arXiv:1406.5647v3 [cs.LG] UPDATED) https://t.co/1fQOV8Ok5R,0,4," Abstract: The stochastic block model (SBM) is a popular tool for community detection in networks, but fitting it by maximum likelihood (MLE) involves a computationally infeasible optimization problem. We propose a new semidefinite programming (SDP) solution to the problem of fitting the SBM, derived as a relaxation of the MLE. We put ours and previously proposed SDPs in a unified framework, as relaxations of the MLE over various sub-classes of the SBM, revealing a connection to sparse PCA. Our main relaxation, which we call SDP-1, is tighter than other recently proposed SDP relaxations, and thus previously established theoretical guarantees carry over. However, we show that SDP-1 exactly recovers true communities over a wider class of SBMs than those covered by current results. In particular, the assumption of strong assortativity of the SBM, implicit in consistency conditions for previously proposed SDPs, can be relaxed to weak assortativity for our approach, thus significantly broadening the class of SBMs covered by the consistency results. We also show that strong assortativity is indeed a necessary condition for exact recovery for previously proposed SDP approaches and not an artifact of the proofs. Our analysis of SDPs is based on primal-dual witness constructions, which provides some insight into the nature of the solutions of various SDPs. We show how to combine features from SDP-1 and already available SDPs to achieve the most flexibility in terms of both assortativity and block-size constraints, as our relaxation has the tendency to produce communities of similar sizes. This tendency makes it the ideal tool for fitting network histograms, a method gaining popularity in the graphon estimation literature, as we illustrate on an example of a social networks of dolphins. We also provide empirical evidence that SDPs outperform spectral methods for fitting SBMs with a large number of blocks. "
710262640774352898,2016-03-17 00:33:04,https://t.co/vzAutbH1gQ,Variational Gaussian Copula Inference. (arXiv:1506.05860v2 [stat.ML] UPDATED) https://t.co/vzAutbH1gQ,0,5," Abstract: We utilize copulas to constitute a unified framework for constructing and optimizing variational proposals in hierarchical Bayesian models. For models with continuous and non-Gaussian hidden variables, we propose a semiparametric and automated variational Gaussian copula approach, in which the parametric Gaussian copula family is able to preserve multivariate posterior dependence, and the nonparametric transformations based on Bernstein polynomials provide ample flexibility in characterizing the univariate marginal posteriors. "
709903130155212800,2016-03-16 00:44:30,https://t.co/Kiqg7ngk7o,Know Your Customer: Multi-armed Bandits with Capacity Constraints. (arXiv:1603.04549v1 [cs.LG]) https://t.co/Kiqg7ngk7o,1,6," Abstract: A wide range of resource allocation and platform operation settings exhibit the following two simultaneous challenges: (1) service resources are capacity constrained; and (2) clients' preferences are not perfectly known. To study this pair of challenges, we consider a service system with heterogeneous servers and clients. Server types are known and there is fixed capacity of servers of each type. Clients arrive over time, with types initially unknown and drawn from some distribution. Each client sequentially brings $N$ jobs before leaving. The system operator assigns each job to some server type, resulting in a payoff whose distribution depends on the client and server types. Our main contribution is a complete characterization of the structure of the optimal policy for maximization of the rate of payoff accumulation. Such a policy must balance three goals: (i) earning immediate payoffs; (ii) learning client types to increase future payoffs; and (iii) satisfying the capacity constraints. We construct a policy that has provably optimal regret (to leading order as $N$ grows large). Our policy has an appealingly simple three-phase structure: a short type-""guessing"" phase, a type-""confirmation"" phase that balances payoffs with learning, and finally an ""exploitation"" phase that focuses on payoffs. Crucially, our approach employs the shadow prices of the capacity constraints in the assignment problem with known types as ""externality prices"" on the servers' capacity. "
709903129173757952,2016-03-16 00:44:30,https://t.co/6Bj0d5JEis,On the exact recovery of sparse signals via conic relaxations. (arXiv:1603.04572v1 [stat.ML]) https://t.co/6Bj0d5JEis,1,6," Abstract: In this note we compare two recently proposed semidefinite relaxations for the sparse linear regression problem by Pilanci, Wainwright and El Ghaoui (Sparse learning via boolean relaxations, 2015) and Dong, Chen and Linderoth (Relaxation vs. Regularization A conic optimization perspective of statistical variable selection, 2015). We focus on the cardinality constrained formulation, and prove that the relaxation proposed by Dong, etc. is theoretically no weaker than the one proposed by Pilanci, etc. Therefore any sufficient condition of exact recovery derived by Pilanci can be readily applied to the other relaxation, including their results on high probability recovery for Gaussian ensemble. Finally we provide empirical evidence that the relaxation by Dong, etc. requires much fewer observations to guarantee the recovery of true support. "
709903128146083840,2016-03-16 00:44:29,https://t.co/p65IqPGEMU,Accelerating a hybrid continuum-atomistic fluidic model with on-the-fly machine learning. (arXiv:1603.04628v1 [phy… https://t.co/p65IqPGEMU,1,3," Abstract: We present a hybrid continuum-atomistic scheme which combines molecular dynamics (MD) simulations with on-the-fly machine learning techniques for the accurate and efficient prediction of multiscale fluidic systems. By using a Gaussian process as a surrogate model for the computationally expensive MD simulations, we use Bayesian inference to predict the system behaviour at the atomistic scale, purely by consideration of the macroscopic inputs and outputs. Whenever the uncertainty of this prediction is greater than a predetermined acceptable threshold, a new MD simulation is performed to continually augment the database, which is never required to be complete. This provides a substantial enhancement to the current generation of hybrid methods, which often require many similar atomistic simulations to be performed, discarding information after it is used once. We apply our hybrid scheme to nano-confined unsteady flow through a high-aspect-ratio converging-diverging channel, and make comparisons between the new scheme and full MD simulations for a range of uncertainty thresholds and initial databases. For low thresholds, our hybrid solution is highly accurate\,---\,within the thermal noise of a full MD simulation. As the uncertainty threshold is raised, the accuracy of our scheme decreases and the computational speed-up increases (relative to a full MD simulation), enabling the compromise between precision and efficiency to be tuned. The speed-up of our hybrid solution ranges from an order of magnitude, with no initial database, to cases where an extensive initial database ensures no new MD simulations are required. "
709903127143718912,2016-03-16 00:44:29,https://t.co/AHjtXTiJHd,Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors. (arXiv:1603.04733v1 [stat.ML]) https://t.co/AHjtXTiJHd,3,12," Abstract: We introduce a variational Bayesian neural network where the parameters are governed via a probability distribution on random matrices. Specifically, we employ a matrix variate Gaussian \cite{gupta1999matrix} parameter posterior distribution where we explicitly model the covariance among the input and output dimensions of each layer. Furthermore, with approximate covariance matrices we can achieve a more efficient way to represent those correlations that is also cheaper than fully factorized parameter posteriors. We further show that with the ""local reprarametrization trick"" \cite{kingma2015variational} on this posterior distribution we arrive at a Gaussian Process \cite{rasmussen2006gaussian} interpretation of the hidden units in each layer and we, similarly with \cite{gal2015dropout}, provide connections with deep Gaussian processes. We continue in taking advantage of this duality and incorporate ""pseudo-data"" \cite{snelson2005sparse} in our model, which in turn allows for more efficient sampling while maintaining the properties of the original model. The validity of the proposed approach is verified through extensive experiments. "
709903125981831168,2016-03-16 00:44:29,https://t.co/5Vbl3pkxVS,Ensemble of Deep Convolutional Neural Networks for Learning to Detect Retinal Vessels in Fundus Images. (arXiv:160… https://t.co/5Vbl3pkxVS,1,3," Abstract: Vision impairment due to pathological damage of the retina can largely be prevented through periodic screening using fundus color imaging. However the challenge with large scale screening is the inability to exhaustively detect fine blood vessels crucial to disease diagnosis. In this work we present a computational imaging framework using deep and ensemble learning for reliable detection of blood vessels in fundus color images. An ensemble of deep convolutional neural networks is trained to segment vessel and non-vessel areas of a color fundus image. During inference, the responses of the individual ConvNets of the ensemble are averaged to form the final segmentation. In experimental evaluation with the DRIVE database, we achieve the objective of vessel detection with maximum average accuracy of 94.7\% and area under ROC curve of 0.9283. "
709903124723539972,2016-03-16 00:44:28,https://t.co/xTXGS9m7Bw,Estimation of Large Covariance and Precision Matrices from Temporally Dependent Observations. (arXiv:1412.5059v4 [… https://t.co/xTXGS9m7Bw,2,5," Abstract: We consider the estimation of large covariance and precision matrices from high-dimensional sub-Gaussian observations with slowly decaying temporal dependence that is bounded by certain polynomial decay rate. The temporal dependence is allowed to be long-range so with longer memory than those considered in the current literature. The rates of convergence are obtained for the generalized thresholding estimation of covariance and correlation matrices, and for the constrained $\ell_1$ minimization and the $\ell_1$ penalized likelihood estimation of precision matrix. Properties of sparsistency and sign-consistency are also established. A gap-block cross-validation method is proposed for the tuning parameter selection, which performs well in simulations. As our motivating example, we study the brain functional connectivity using resting-state fMRI time series data with long-range temporal dependence. "
709903123964362753,2016-03-16 00:44:28,https://t.co/pJZBYBoCk2,Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?. (arXiv:1504.08291v5 [cs.N… https://t.co/pJZBYBoCk2,3,19," Abstract: Three important properties of a classification machinery are: (i) the system preserves the core information of the input data; (ii) the training examples convey information about unseen data; and (iii) the system is able to treat differently points from different classes. In this work we show that these fundamental properties are satisfied by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks. "
709903122941026304,2016-03-16 00:44:28,https://t.co/z2mmJWPUXl,Online Learning to Sample. (arXiv:1506.09016v2 [cs.LG] UPDATED) https://t.co/z2mmJWPUXl,4,6," Abstract: Stochastic Gradient Descent (SGD) is one of the most widely used techniques for online optimization in machine learning. In this work, we accelerate SGD by adaptively learning how to sample the most useful training examples at each time step. First, we show that SGD can be used to learn the best possible sampling distribution of an importance sampling estimator. Second, we show that the sampling distribution of an SGD algorithm can be estimated online by incrementally minimizing the variance of the gradient. The resulting algorithm - called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to optimize, as well as a set of parameters to sample learning examples. We show that AWSGD yields faster convergence in three different applications: (i) image classification with deep features, where the sampling of images depends on their labels, (ii) matrix factorization, where rows and columns are not sampled uniformly, and (iii) reinforcement learning, where the optimized and exploration policies are estimated at the same time, where our approach corresponds to an off-policy gradient algorithm. "
709903121988898816,2016-03-16 00:44:28,https://t.co/sdKZVyeIRg,Second Order Stochastic Optimization in Linear Time. (arXiv:1602.03943v3 [stat.ML] UPDATED) https://t.co/sdKZVyeIRg,4,10," Abstract: First-order stochastic methods are the state-of-the-art in large-scale machine learning optimization owing to efficient per-iteration complexity. Second-order methods, while able to provide faster convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop second-order stochastic methods for optimization problems in machine learning that match the per-iteration cost of gradient based methods, and in certain settings improves upon the overall running time upon the state-of-the-art. Furthermore, our algorithm has the desirable property of being implementable in time linear in the sparsity of the input data. "
709903119795294209,2016-03-16 00:44:27,https://t.co/b8qP6QTFx6,Regularization vs. Relaxation: A conic optimization perspective of statistical variable selection. (arXiv:1510.060… https://t.co/b8qP6QTFx6,2,4," Abstract: Variable selection is a fundamental task in statistical data analysis. Sparsity-inducing regularization methods are a popular class of methods that simultaneously perform variable selection and model estimation. The central problem is a quadratic optimization problem with an l0-norm penalty. Exactly enforcing the l0-norm penalty is computationally intractable for larger scale problems, so dif- ferent sparsity-inducing penalty functions that approximate the l0-norm have been introduced. In this paper, we show that viewing the problem from a convex relaxation perspective offers new insights. In particular, we show that a popular sparsity-inducing concave penalty function known as the Minimax Concave Penalty (MCP), and the reverse Huber penalty derived in a recent work by Pilanci, Wainwright and Ghaoui, can both be derived as special cases of a lifted convex relaxation called the perspective relaxation. The optimal perspective relaxation is a related minimax problem that balances the overall convexity and tightness of approximation to the l0 norm. We show it can be solved by a semidefinite relaxation. Moreover, a probabilistic interpretation of the semidefinite relaxation reveals connections with the boolean quadric polytope in combinatorial optimization. Finally by reformulating the l0-norm pe- nalized problem as a two-level problem, with the inner level being a Max-Cut problem, our proposed semidefinite relaxation can be realized by replacing the inner level problem with its semidefinite relaxation studied by Goemans and Williamson. This interpretation suggests using the Goemans-Williamson rounding procedure to find approximate solutions to the l0-norm penalized problem. Numerical experiments demonstrate the tightness of our proposed semidefinite relaxation, and the effectiveness of finding approximate solutions by Goemans-Williamson rounding. "
709540479558356994,2016-03-15 00:43:27,https://t.co/KRbixjmQuY,A Primer on the Signature Method in Machine Learning. (arXiv:1603.03788v1 [stat.ML]) https://t.co/KRbixjmQuY,2,5," Abstract: In these notes, we wish to provide an introduction to the signature method, focusing on its basic theoretical properties and recent numerical applications. The notes are split into two parts. The first part focuses on the definition and fundamental properties of the signature of a path, or the path signature. We have aimed for a minimalistic approach, assuming only familiarity with classical real analysis and integration theory, and supplementing theory with straightforward examples. We have chosen to focus in detail on the principle properties of the signature which we believe are fundamental to understanding its role in applications. We also present an informal discussion on some of its deeper properties and briefly mention the role of the signature in rough paths theory, which we hope could serve as a light introduction to rough paths for the interested reader. The second part of these notes discusses practical applications of the path signature to the area of machine learning. The signature approach represents a non-parametric way for extraction of characteristic features from data. The data are converted into a multi-dimensional path by means of various embedding algorithms and then processed for computation of individual terms of the signature which summarise certain information contained in the data. The signature thus transforms raw data into a set of features which are used in machine learning tasks. We will review current progress in applications of signatures to machine learning problems. "
709540477729677312,2016-03-15 00:43:27,https://t.co/Fhjtao4fkK,Provable Non-convex Phase Retrieval with Outliers: Median Truncated Wirtinger Flow. (arXiv:1603.03805v1 [stat.ML]) https://t.co/Fhjtao4fkK,0,4," Abstract: Solving systems of quadratic equations is a central problem in machine learning and signal processing. One important example is phase retrieval, which aims to recover a signal from only magnitudes of its linear measurements. This paper focuses on the situation when the measurements are corrupted by arbitrary outliers, for which the recently developed non-convex gradient descent Wirtinger flow (WF) and truncated Wirtinger flow (TWF) algorithms likely fail. We develop a novel median-TWF algorithm that exploits robustness of sample median to resist arbitrary outliers in the initialization and the gradient update in each iteration. We show that such a non-convex algorithm provably recovers the signal from a near-optimal number of measurements composed of i.i.d. Gaussian entries, up to a logarithmic factor, even when a constant portion of the measurements are corrupted by arbitrary outliers. We further show that median-TWF is also robust when measurements are corrupted by both arbitrary outliers and bounded noise. Our analysis of performance guarantee is accomplished by development of non-trivial concentration measures of median-related quantities, which may be of independent interest. We further provide numerical experiments to demonstrate the effectiveness of the approach. "
709540475733217280,2016-03-15 00:43:26,https://t.co/KNPIStJmpt,Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks. (arXiv:1603.03827v1 [cs.CL]) https://t.co/KNPIStJmpt,1,8," Abstract: Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction. "
709540474357489664,2016-03-15 00:43:26,https://t.co/6DutUE1aU0,"Laplacian Eigenmaps from Sparse, Noisy Similarity Measurements. (arXiv:1603.03972v1 [stat.ML]) https://t.co/6DutUE1aU0",1,3," Abstract: Manifold learning and dimensionality reduction techniques are ubiquitous in science and engineering, but can be computationally expensive procedures when applied to large data sets or when similarities are expensive to compute. To date, little work has been done to investigate the tradeoff between computational resources and the quality of learned representations. We present both theoretical and experimental explorations of this question. In particular, we consider Laplacian eigenmaps embeddings based on a kernel matrix, and explore how the embeddings behave when this kernel matrix is corrupted by occlusion and noise. Our main theoretical result shows that under modest noise and occlusion assumptions, we can (with high probability) recover a good approximation to the Laplacian eigenmaps embedding based on the uncorrupted kernel matrix. Our results also show how regularization can aid this approximation. Experimentally, we explore the effects of noise and occlusion on Laplacian eigenmaps embeddings of two real-world data sets, one from speech processing and one from neuroscience, as well as a synthetic data set. "
709540472516157440,2016-03-15 00:43:25,https://t.co/F5Kh7g8WjH,Privacy-preserving Analysis of Correlated Data. (arXiv:1603.03977v1 [cs.LG]) https://t.co/F5Kh7g8WjH,1,3," Abstract: Many modern machine learning applications involve sensitive correlated data, such as private information on users connected together in a social network, and measurements of physical activity of a single user across time. However, the current standard of privacy in machine learning, differential privacy, does not adequately address privacy issues in this kind of data. This work looks at a recent generalization of differential privacy, called Pufferfish, that can be used to address privacy in correlated data. The main challenge in applying Pufferfish is a lack of suitable mechanisms. In this paper, we provide a general mechanism, called the Wasserstein Mechanism, which applies to any Pufferfish framework. Since the Wasserstein Mechanism may be computationally inefficient, we provide an additional mechanism that applies to some practical cases such as physical activity measurements across time, is computationally efficient, and provides both privacy and utility. "
709540470704234497,2016-03-15 00:43:25,https://t.co/1Y84dmg6JH,On Learning High Dimensional Structured Single Index Models. (arXiv:1603.03980v1 [stat.ML]) https://t.co/1Y84dmg6JH,1,3," Abstract: Single Index Models (SIMs) are simple yet flexible semi-parametric models for classification and regression, where response variables are modeled as a nonlinear, monotonic function of a linear combination of features. Estimation in this context requires learning both the feature weights and the nonlinear function that relates features to observations. While methods have been described to learn SIMs in the low dimensional regime, a method that can efficiently learn SIMs in high dimensions, and under general structural assumptions, has not been forthcoming. In this paper, we propose computationally efficient algorithms for SIM inference in high dimensions using atomic norm regularization. This general approach to imposing structure in high-dimensional modeling specializes to sparsity, group sparsity, and low-rank assumptions among others. We also provide a scalable, stochastic version of the method. Experiments show that the method we propose enjoys superior predictive performance when compared to generalized linear models such as logistic regression, on several real-world datasets. "
709540468661604353,2016-03-15 00:43:25,https://t.co/MZF4TAbc7D,Clustering Financial Time Series: How Long is Enough?. (arXiv:1603.04017v1 [stat.ML]) https://t.co/MZF4TAbc7D,3,5," Abstract: Researchers have used from 30 days to several years of daily returns as source data for clustering financial time series based on their correlations. This paper sets up a statistical framework to study the validity of such practices. We first show that clustering correlated random variables from their observed values is statistically consistent. Then, we also give a first empirical answer to the much debated question: How long should the time series be? If too short, the clusters found can be spurious; if too long, dynamics can be smoothed out. "
709540467034222592,2016-03-15 00:43:24,https://t.co/n0sIo2zjOP,A Grothendieck-type inequality for local maxima. (arXiv:1603.04064v1 [math.OC]) https://t.co/n0sIo2zjOP,1,3," Abstract: A large number of problems in optimization, machine learning, signal processing can be effectively addressed by suitable semidefinite programming (SDP) relaxations. Unfortunately, generic SDP solvers hardly scale beyond instances with a few hundreds variables (in the underlying combinatorial problem). On the other hand, it has been observed empirically that an effective strategy amounts to introducing a (non-convex) rank constraint, and solving the resulting smooth optimization problem by ascent methods. This non-convex problem has --generically-- a large number of local maxima, and the reason for this success is therefore unclear. This paper provides rigorous support for this approach. For the problem of maximizing a linear functional over the elliptope, we prove that all local maxima are within a small gap from the SDP optimum. In several problems of interest, arbitrarily small relative error can be achieved by taking the rank constraint $k$ to be of order one, independently of the problem size. "
709540465486471168,2016-03-15 00:43:24,https://t.co/TC4jo7vH96,Bandit Approaches to Preference Learning Problems with Multiple Populations. (arXiv:1603.04118v1 [stat.ML]) https://t.co/TC4jo7vH96,0,3," Abstract: In this paper we model the problem of learning preferences of a population as an active learning problem. We propose an algorithm can adaptively choose pairs of items to show to users coming from a heterogeneous population, and use the obtained reward to decide which pair of items to show next. We provide computationally efficient algorithms with provable sample complexity guarantees for this problem in both the noiseless and noisy cases. In the process of establishing sample complexity guarantees for our algorithms, we establish new results using a Nystr{\""o}m-like method which can be of independent interest. We supplement our theoretical results with experimental comparisons. "
709540463779442688,2016-03-15 00:43:23,https://t.co/m7z33pwvvo,Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains. (arXiv:1603.04119v1 [cs.AI]) https://t.co/m7z33pwvvo,0,4," Abstract: High-dimensional observations and complex real-world dynamics present major challenges in reinforcement learning for both function approximation and exploration. We address both of these challenges with two complementary techniques: First, we develop a gradient-boosting style, non-parametric function approximator for learning on $Q$-function residuals. And second, we propose an exploration strategy inspired by the principles of state abstraction and information acquisition under uncertainty. We demonstrate the empirical effectiveness of these techniques, first, as a preliminary check, on two standard tasks (Blackjack and $n$-Chain), and then on two much larger and more realistic tasks with high-dimensional observation spaces. Specifically, we introduce two benchmarks built within the game Minecraft where the observations are pixel arrays of the agent's visual field. A combination of our two algorithmic techniques performs competitively on the standard reinforcement-learning tasks while consistently and substantially outperforming baselines on the two tasks with high-dimensional observation spaces. The new function approximator, exploration strategy, and evaluation benchmarks are each of independent interest in the pursuit of reinforcement-learning methods that scale to real-world domains. "
709540461866778626,2016-03-15 00:43:23,https://t.co/DxGWT3teHR,On the Influence of Momentum Acceleration on Online Learning. (arXiv:1603.04136v1 [math.OC]) https://t.co/DxGWT3teHR,0,3," Abstract: The article examines in some detail the convergence rate and mean-square-error performance of momentum stochastic gradient methods in the constant step-size and slow adaptation regime. The results establish that momentum methods are equivalent to the standard stochastic gradient method with a re-scaled (larger) step-size value. The size of the re-scaling is determined by the value of the momentum parameter. The equivalence result is established for all time instants and not only in steady-state. The analysis is carried out for general strongly convex and smooth risk functions, and is not limited to quadratic risks. One notable conclusion is that the well-known bene ts of momentum constructions for deterministic optimization problems do not necessarily carry over to the adaptive online setting when small constant step-sizes are used to enable continuous adaptation and learn- ing in the presence of persistent gradient noise. From simulations, the equivalence between momentum and standard stochastic gradient methods is also observed for non-differentiable and non-convex problems. "
709540460012969984,2016-03-15 00:43:22,https://t.co/gN81weSTa6,Top-$K$ Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal. (arXiv:1603.04153v1 [cs.LG]) https://t.co/gN81weSTa6,0,3," Abstract: We explore the top-$K$ rank aggregation problem. Suppose a collection of items is compared in pairs repeatedly, and we aim to recover a consistent ordering that focuses on the top-$K$ ranked items based on partially revealed preference information. We investigate the Bradley-Terry-Luce model in which one ranks items according to their perceived utilities modeled as noisy observations of their underlying true utilities. Our main contributions are two-fold. First, in a general comparison model where item pairs to compare are given a priori, we attain an upper and lower bound on the sample size for reliable recovery of the top-$K$ ranked items. Second, more importantly, extending the result to a random comparison model where item pairs to compare are chosen independently with some probability, we show that in slightly restricted regimes, the gap between the derived bounds reduces to a constant factor, hence reveals that a spectral method can achieve the minimax optimality on the (order-wise) sample size required for top-$K$ ranking. That is to say, we demonstrate a spectral method alone to be sufficient to achieve the optimality and advantageous in terms of computational complexity, as it does not require an additional stage of maximum likelihood estimation that a state-of-the-art scheme employs to achieve the optimality. We corroborate our main results by numerical experiments. "
709540458385506304,2016-03-15 00:43:22,https://t.co/1wdQ8uAi1Q,Online Isotonic Regression. (arXiv:1603.04190v1 [cs.LG]) https://t.co/1wdQ8uAi1Q,0,1," Abstract: We consider the online version of the isotonic regression problem. Given a set of linearly ordered points (e.g., on the real line), the learner must predict labels sequentially at adversarially chosen positions and is evaluated by her total squared loss compared against the best isotonic (non-decreasing) function in hindsight. We survey several standard online learning algorithms and show that none of them achieve the optimal regret exponent; in fact, most of them (including Online Gradient Descent, Follow the Leader and Exponential Weights) incur linear regret. We then prove that the Exponential Weights algorithm played over a covering net of isotonic functions has a regret bounded by $O\big(T^{1/3} \log^{2/3}(T)\big)$ and present a matching $\Omega(T^{1/3})$ lower bound on regret. We provide a computationally efficient version of this algorithm. We also analyze the noise-free case, in which the revealed labels are isotonic, and show that the bound can be improved to $O(\log T)$ or even to $O(1)$ (when the labels are revealed in isotonic order). Finally, we extend the analysis beyond squared loss and give bounds for entropic loss and absolute loss. "
709540456514920448,2016-03-15 00:43:22,https://t.co/4EAWiLQMYG,A Variational Perspective on Accelerated Methods in Optimization. (arXiv:1603.04245v1 [math.OC]) https://t.co/4EAWiLQMYG,1,10," Abstract: Accelerated gradient methods play a central role in optimization, achieving optimal rates in many settings. While many generalizations and extensions of Nesterov's original acceleration method have been proposed, it is not yet clear what is the natural scope of the acceleration concept. In this paper, we study accelerated methods from a continuous-time perspective. We show that there is a Lagrangian functional that we call the \emph{Bregman Lagrangian} which generates a large class of accelerated methods in continuous time, including (but not limited to) accelerated gradient descent, its non-Euclidean extension, and accelerated higher-order gradient methods. We show that the continuous-time limit of all of these methods correspond to traveling the same curve in spacetime at different speeds. From this perspective, Nesterov's technique and many of its generalizations can be viewed as a systematic way to go from the continuous-time curves generated by the Bregman Lagrangian to a family of discrete-time accelerated algorithms. "
709540454447112192,2016-03-15 00:43:21,https://t.co/wPMfdeiDP8,Learning Network of Multivariate Hawkes Processes: A Time Series Approach. (arXiv:1603.04319v1 [cs.LG]) https://t.co/wPMfdeiDP8,0,3," Abstract: Learning the influence structure of multiple time series data is of great interest to many disciplines. This paper studies the problem of recovering the causal structure in network of multivariate linear Hawkes processes. In such processes, the occurrence of an event in one process affects the probability of occurrence of new events in some other processes. Thus, a natural notion of causality exists between such processes captured by the support of the excitation matrix. We show that the resulting causal influence network is equivalent to the Directed Information graph (DIG) of the processes, which encodes the causal factorization of the joint distribution of the processes. Furthermore, we present an algorithm for learning the support of excitation matrix (or equivalently the DIG). The performance of the algorithm is evaluated on synthesized multivariate Hawkes networks as well as a stock market and MemeTracker real-world dataset. "
709540451913744384,2016-03-15 00:43:21,https://t.co/jgAaawl5Ak,A Ranking Approach to Global Optimization. (arXiv:1603.04381v1 [stat.ML]) https://t.co/jgAaawl5Ak,0,3," Abstract: In this paper, we consider the problem of maximizing an unknown function f over a compact and convex set using as few observations f(x) as possible. We observe that the optimization of the function f essentially relies on learning the induced bipartite ranking rule of f. Based on this idea, we relate global optimization to bipartite ranking which allows to address problems with high dimensional input space, as well as cases of functions with weak regularity properties. The paper introduces novel meta-algorithms for global optimization which rely on the choice of any bipartite ranking method. Theoretical properties are provided as well as convergence guarantees and equivalences between various optimization methods are obtained as a by-product. Eventually, numerical evidence is given to show that the main algorithm of the paper which adapts empirically to the underlying ranking structure essentially outperforms existing state-of-the-art global optimization algorithms in typical benchmarks. "
709540450147942402,2016-03-15 00:43:20,https://t.co/wYimkkhD67,Modeling and Estimation of Discrete-Time Reciprocal Processes via Probabilistic Graphical Models. (arXiv:1603.0441… https://t.co/wYimkkhD67,0,2," Abstract: Reciprocal processes are acausal generalizations of Markov processes introduced by Bernstein in 1932. In the literature, a significant amount of attention has been focused on developing dynamical models for reciprocal processes. In this paper, we provide a probabilistic graphical model for reciprocal processes. This leads to a principled solution of the smoothing problem via message passing algorithms. For the finite state space case, convergence analysis is revisited via the Hilbert metric. "
709540448843505666,2016-03-15 00:43:20,https://t.co/LTqDlOf6PT,Two New Approaches to Compressed Sensing Exhibiting Both Robust Sparse Recovery and the Grouping Effect. (arXiv:14… https://t.co/LTqDlOf6PT,0,3," Abstract: In this paper we introduce a new optimization formulation for sparse regression and compressed sensing, called CLOT (Combined L-One and Two), wherein the regularizer is a convex combination of the $\ell_1$- and $\ell_2$-norms. This formulation differs from the Elastic Net (EN) formulation, in which the regularizer is a convex combination of the $\ell_1$- and $\ell_2$-norm squared. This seemingly simple modification has fairly significant consequences. In particular, it is shown in this paper that the EN formulation \textit{does not achieve} robust recovery of sparse vectors in the context of compressed sensing, whereas the new CLOT formulation does so. Also, like EN but unlike LASSO, the CLOT formulation achieves the grouping effect, wherein coefficients of highly correlated columns of the measurement (or design) matrix are assigned roughly comparable values. It is noteworthy that LASSO does not have the grouping effect and EN (as shown here) does not achieve robust sparse recovery. Therefore the CLOT formulation combines the best features of both LASSO (robust sparse recovery) and EN (grouping effect). The CLOT formulation is a special case of another one called SGL (Sparse Group LASSO) which was introduced into the literature previously, but without any analysis of either the grouping effect or robust sparse recovery. It is shown here that SGL achieves robust sparse recovery, and also achieves a version of the grouping effect in that coefficients of highly correlated columns of the measurement (or design) matrix are assigned roughly comparable values, if the columns belong to the same group. oughly comparable values, if the columns belong to the same group. "
709540447211950080,2016-03-15 00:43:19,https://t.co/Hh16MH5iv4,Correlations between the Hurst exponent and the maximal Lyapunov exponent for some low-dimensional discrete conser… https://t.co/Hh16MH5iv4,0,2," Abstract: The Chirikov standard map and the 2D Froeschl\'e map are investigated. A few thousand values of the Hurst exponent (HE) and the maximal Lyapunov exponent (mLE) are plotted in a mixed space of the nonlinear parameter versus the initial condition. Both characteristic exponents reveal remarkably similar structures in this mixed space. Moreover, a tight correlation between the HEs and mLEs for the two maps was found: $\rho=0.83$ and $\rho=0.75$ for the Chirikov and Froeschl\'e maps, respectively, where $\rho$ is the Spearman rank. Based on this relation, a machine learning (ML) procedure, using the nearest neighbour algorithm, was performed to reproduce the HE distributions based on the mLE distributions. A few thousand HE and mLE values from the mixed spaces were used for training, and then using $2-2.4\times 10^5$ mLEs, the HEs were retrieved. The ML procedure allowed to reproduce the structure of the mixed spaces in great detail. The HE is proposed as an informative parameter in the area of chaotic control, as it provides expectations about the general trend in a time series. "
709540445517389825,2016-03-15 00:43:19,https://t.co/G4qaNfJAp9,Sparse Coding with Earth Mover's Distance for Multi-Instance Histogram Representation. (arXiv:1502.02377v2 [cs.LG]… https://t.co/G4qaNfJAp9,0,6," Abstract: Sparse coding (Sc) has been studied very well as a powerful data representation method. It attempts to represent the feature vector of a data sample by reconstructing it as the sparse linear combination of some basic elements, and a $L_2$ norm distance function is usually used as the loss function for the reconstruction error. In this paper, we investigate using Sc as the representation method within multi-instance learning framework, where a sample is given as a bag of instances, and further represented as a histogram of the quantized instances. We argue that for the data type of histogram, using $L_2$ norm distance is not suitable, and propose to use the earth mover's distance (EMD) instead of $L_2$ norm distance as a measure of the reconstruction error. By minimizing the EMD between the histogram of a sample and the its reconstruction from some basic histograms, a novel sparse coding method is developed, which is refereed as SC-EMD. We evaluate its performances as a histogram representation method in tow multi-instance learning problems --- abnormal image detection in wireless capsule endoscopy videos, and protein binding site retrieval. The encouraging results demonstrate the advantages of the new method over the traditional method using $L_2$ norm distance. "
709540443957161985,2016-03-15 00:43:19,https://t.co/YW93HwRe3g,Recurrent Neural Network Training with Dark Knowledge Transfer. (arXiv:1505.04630v4 [stat.ML] UPDATED) https://t.co/YW93HwRe3g,1,13," Abstract: Recurrent neural networks (RNNs), particularly long short-term memory (LSTM), have gained much attention in automatic speech recognition (ASR). Although some successful stories have been reported, training RNNs remains highly challenging, especially with limited training data. Recent research found that a well-trained model can be used as a teacher to train other child models, by using the predictions generated by the teacher model as supervision. This knowledge transfer learning has been employed to train simple neural nets with a complex one, so that the final performance can reach a level that is infeasible to obtain by regular training. In this paper, we employ the knowledge transfer learning approach to train RNNs (precisely LSTM) using a deep neural network (DNN) model as the teacher. This is different from most of the existing research on knowledge transfer learning, since the teacher (DNN) is assumed to be weaker than the child (RNN); however, our experiments on an ASR task showed that it works fairly well: without applying any tricks on the learning scheme, this approach can train RNNs successfully even with limited training data. "
709540442082316288,2016-03-15 00:43:18,https://t.co/HkE7NpCnCR,A New PAC-Bayesian Perspective on Domain Adaptation. (arXiv:1506.04573v3 [stat.ML] UPDATED) https://t.co/HkE7NpCnCR,0,4," Abstract: We study the issue of PAC-Bayesian domain adaptation: We want to learn, from a source domain, a majority vote model dedicated to a target one. Our theoretical contribution brings a new perspective by deriving an upper-bound on the target risk where the distributions' divergence---expressed as a ratio---controls the trade-off between a source error measure and the target voters' disagreement. Our bound suggests that one has to focus on regions where the source data is informative.From this result, we derive a PAC-Bayesian generalization bound, and specialize it to linear classifiers. Then, we infer a learning algorithmand perform experiments on real data. "
709540440316485633,2016-03-15 00:43:18,https://t.co/qTJTY0XmEf,Sparsity in Multivariate Extremes with Applications to Anomaly Detection. (arXiv:1507.05899v2 [stat.ML] UPDATED) https://t.co/qTJTY0XmEf,0,3," Abstract: Capturing the dependence structure of multivariate extreme events is a major concern in many fields involving the management of risks stemming from multiple sources, e.g. portfolio monitoring, insurance, environmental risk management and anomaly detection. One convenient (non-parametric) characterization of extremal dependence in the framework of multivariate Extreme Value Theory (EVT) is the angular measure, which provides direct information about the probable 'directions' of extremes, that is, the relative contribution of each feature/coordinate of the 'largest' observations. Modeling the angular measure in high dimensional problems is a major challenge for the multivariate analysis of rare events. The present paper proposes a novel methodology aiming at exhibiting a sparsity pattern within the dependence structure of extremes. This is done by estimating the amount of mass spread by the angular measure on representative sets of directions, corresponding to specific sub-cones of $R^d\_+$. This dimension reduction technique paves the way towards scaling up existing multivariate EVT methods. Beyond a non-asymptotic study providing a theoretical validity framework for our method, we propose as a direct application a --first-- anomaly detection algorithm based on multivariate EVT. This algorithm builds a sparse 'normal profile' of extreme behaviours, to be confronted with new (possibly abnormal) extreme observations. Illustrative experimental results provide strong empirical evidence of the relevance of our approach. "
709540438395506688,2016-03-15 00:43:17,https://t.co/nC5UJeUyUo,Conditional Risk Minimization for Stochastic Processes. (arXiv:1510.02706v2 [stat.ML] UPDATED) https://t.co/nC5UJeUyUo,1,5," Abstract: We study the task of learning from non-i.i.d. data. In particular, we aim at learning predictors that minimize the conditional risk for a stochastic process, i.e. the expected loss of the predictor on the next point conditioned on the set of training samples observed so far. For non-i.i.d. data, the training set contains information about the upcoming samples, so learning with respect to the conditional distribution can be expected to yield better predictors than one obtains from the classical setting of minimizing the marginal risk. Our main contribution is a practical estimator for the conditional risk based on the theory of non-parametric time-series prediction, and a finite sample concentration bound that establishes uniform convergence of the estimator to the true conditional risk under certain regularity assumptions on the process. "
709540436428398592,2016-03-15 00:43:17,https://t.co/jr7jM4EP6o,Neuroprosthetic decoder training as imitation learning. (arXiv:1511.04156v2 [stat.ML] UPDATED) https://t.co/jr7jM4EP6o,0,1," Abstract: Neuroprosthetic brain-computer interfaces function via an algorithm which decodes neural activity of the user into movements of an end effector, such as a cursor or robotic arm. In practice, the decoder is often learned by updating its parameters while the user performs a task. When the user's intention is not directly observable, recent methods have demonstrated value in training the decoder against a surrogate for the user's intended movement. We describe how training a decoder in this way is a novel variant of an imitation learning problem, where an oracle or expert is employed for supervised training in lieu of direct observations, which are not available. Specifically, we describe how a generic imitation learning meta-algorithm, dataset aggregation (DAgger, [1]), can be adapted to train a generic brain-computer interface. By deriving existing learning algorithms for brain-computer interfaces in this framework, we provide a novel analysis of regret (an important metric of learning efficacy) for brain-computer interfaces. This analysis allows us to characterize the space of algorithmic variants and bounds on their regret rates. Existing approaches for decoder learning have been performed in the cursor control setting, but the available design principles for these decoders are such that it has been impossible to scale them to naturalistic settings. Leveraging our findings, we then offer an algorithm that combines imitation learning with optimal control, which should allow for training of arbitrary effectors for which optimal control can generate goal-oriented control. We demonstrate this novel and general BCI algorithm with simulated neuroprosthetic control of a 26 degree-of-freedom model of an arm, a sophisticated and realistic end effector. "
709540434595471361,2016-03-15 00:43:16,https://t.co/HXpweHShki,Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks. (arXiv:1511.04508v2 [cs.CR] U… https://t.co/HXpweHShki,1,4," Abstract: Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10^30. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested. "
709540431877505024,2016-03-15 00:43:16,https://t.co/19CzEF1er1,Iterative Refinement of Approximate Posterior for Training Directed Belief Networks. (arXiv:1511.06382v4 [cs.LG] U… https://t.co/19CzEF1er1,0,1," Abstract: Variational methods that rely on a recognition network to approximate the posterior of directed graphical models offer better inference and learning than previous methods. Recent advances that exploit the capacity and flexibility in this approach have expanded what kinds of models can be trained. However, as a proposal for the posterior, the capacity of the recognition network is limited, which can constrain the representational power of the generative model and increase the variance of Monte Carlo estimates. To address these issues, we introduce an iterative refinement procedure for improving the approximate posterior of the recognition network and show that training with the refined posterior is competitive with state-of-the-art methods. The advantages of refinement are further evident in an increased effective sample size, which implies a lower variance of gradient estimates. "
709179773357039617,2016-03-14 00:50:08,https://t.co/9rW21AerQT,Square Root Graphical Models: Multivariate Generalizations of Univariate Exponential Families that Permit Positive… https://t.co/9rW21AerQT,1,3," Abstract: We develop Square Root Graphical Models (SQR), a novel class of parametric graphical models that provides multivariate generalizations of univariate exponential family distributions. Previous multivariate graphical models [Yang et al. 2015] did not allow positive dependencies for the exponential and Poisson generalizations. However, in many real-world datasets, variables clearly have positive dependencies. For example, the airport delay time in New York---modeled as an exponential distribution---is positively related to the delay time in Boston. With this motivation, we give an example of our model class derived from the univariate exponential distribution that allows for almost arbitrary positive and negative dependencies with only a mild condition on the parameter matrix---a condition akin to the positive definiteness of the Gaussian covariance matrix. Our Poisson generalization allows for both positive and negative dependencies without any constraints on the parameter values. We also develop parameter estimation methods using node-wise regressions with $\ell_1$ regularization and likelihood approximation methods using sampling. Finally, we demonstrate our exponential generalization on a synthetic dataset and a real-world dataset of airport delay times. "
709179771977125888,2016-03-14 00:50:08,https://t.co/Smad3ck9mj,Distance Metric Tracking. (arXiv:1603.03678v1 [stat.ML]) https://t.co/Smad3ck9mj,1,8," Abstract: Recent work in distance metric learning has focused on learning transformations of data that best align with provided sets of pairwise similarity and dissimilarity constraints. The learned transformations lead to improved retrieval, classification, and clustering algorithms due to the better adapted distance or similarity measures. Here, we introduce the problem of learning these transformations when the underlying constraint generation process is nonstationary. This nonstationarity can be due to changes in either the ground-truth clustering used to generate constraints or changes to the feature subspaces in which the class structure is apparent. We propose and evaluate COMID-SADL, an adaptive, online approach for learning and tracking optimal metrics as they change over time that is highly robust to a variety of nonstationary behaviors in the changing metric. We demonstrate COMID-SADL on both real and synthetic data sets and show significant performance improvements relative to previously proposed batch and online distance metric learning algorithms. "
709179770257408000,2016-03-14 00:50:07,https://t.co/cJP5q6tBOM,Efficient Clustering of Correlated Variables and Variable Selection in High-Dimensional Linear Models. (arXiv:1603… https://t.co/cJP5q6tBOM,2,8," Abstract: In this paper, we introduce Adaptive Cluster Lasso(ACL) method for variable selection in high dimensional sparse regression models with strongly correlated variables. To handle correlated variables, the concept of clustering or grouping variables and then pursuing model fitting is widely accepted. When the dimension is very high, finding an appropriate group structure is as difficult as the original problem. The ACL is a three-stage procedure where, at the first stage, we use the Lasso(or its adaptive or thresholded version) to do initial selection, then we also include those variables which are not selected by the Lasso but are strongly correlated with the variables selected by the Lasso. At the second stage we cluster the variables based on the reduced set of predictors and in the third stage we perform sparse estimation such as Lasso on cluster representatives or the group Lasso based on the structures generated by clustering procedure. We show that our procedure is consistent and efficient in finding true underlying population group structure(under assumption of irrepresentable and beta-min conditions). We also study the group selection consistency of our method and we support the theory using simulated and pseudo-real dataset examples. "
709179768923688960,2016-03-14 00:50:07,https://t.co/xUdedtR5pF,Designing labeled graph classifiers by exploiting the R\'enyi entropy of the dissimilarity representation. (arXiv:… https://t.co/xUdedtR5pF,1,5," Abstract: Representing patterns by complex relational structures, such as labeled graphs, is becoming an increasingly common practice in the broad field of computational intelligence. Accordingly, a wide repertoire of pattern recognition tools, such as classifiers and knowledge discovery procedures, are nowadays available and tested for various labeled graph data types. However, the design of effective learning and mining procedures operating in the space of labeled graphs is still a challenging problem, especially from the computational complexity viewpoint. In this paper, we present a major improvement of a general-purpose graph classification system, which is conceived on an interplay among dissimilarity representation, clustering, information-theoretic techniques, and evolutionary optimization. The improvement focuses on a specific key subroutine of the system that performs the compression of the input data. We prove different theorems which are fundamental to the setting of such a compression operation. We demonstrate the effectiveness of the resulting classifier by benchmarking the developed variants on well-known datasets of labeled graphs, considering as distinct performance indicators the classification accuracy, the computing time, and the parsimony in terms of structural complexity of the synthesized classification model. Overall, the results show state-of-the-art standards in terms of test set accuracy, while achieving considerable reductions for what concerns both the effective computing time and model complexity. "
709179767833153536,2016-03-14 00:50:07,https://t.co/BMst1V90Wj,Fast Gaussian Process Regression for Big Data. (arXiv:1509.05142v3 [cs.LG] UPDATED) https://t.co/BMst1V90Wj,2,12," Abstract: Gaussian Processes are widely used for regression tasks. A known limitation in the application of Gaussian Processes to regression tasks is that the computation of the solution requires performing a matrix inversion. The solution also requires the storage of a large matrix in memory. These factors restrict the application of Gaussian Process regression to small and moderate size data sets. We present an algorithm based on empirically determined subset selection. The algorithm is based on applying model averaging to Gaussian Process estimators developed on bootstrapped datasets. We compare the performance of this algorithm with two other methods that are used to apply Gaussian Processes regression to large datasets. In the proposed method, hyper-parameter learning is performed over small datasets and requires very little tuning effort. Methods currently used to apply Gaussian Process regression to large datasets are typically associated with more hyper-parameters than the proposed method and can require a significant tuning effort. The results of the experiments reported in this work are consistent with results from Mini-max theory for non-parametric regression. The key benefit of this algorithm is the simplicity associated with implementation.. "
709179766256115713,2016-03-14 00:50:06,https://t.co/x8EgL4as5T,Group Equivariant Convolutional Networks. (arXiv:1602.07576v2 [cs.LG] UPDATED) https://t.co/x8EgL4as5T,0,6," Abstract: We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST. "
709179765203312640,2016-03-14 00:50:06,https://t.co/gNegXDAby5,A Latent Variable Recurrent Neural Network for Discourse Relation Language Models. (arXiv:1603.01913v1 [cs.CL] CRO… https://t.co/gNegXDAby5,1,8," Abstract: This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences. A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations. The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task. The resulting model can therefore employ a training objective that includes not only discourse relation classification, but also word prediction. As a result, it outperforms state-of-the-art alternatives for two tasks: implicit discourse relation classification in the Penn Discourse Treebank, and dialog act classification in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline. "
708106492822556672,2016-03-11 01:45:18,https://t.co/xHuHxxAFMM,Global and Local Uncertainty Principles for Signals on Graphs. (arXiv:1603.03030v1 [stat.ML]) https://t.co/xHuHxxAFMM,1,4," Abstract: Uncertainty principles such as Heisenberg's provide limits on the time-frequency concentration of a signal, and constitute an important theoretical tool for designing and evaluating linear signal transforms. Generalizations of such principles to the graph setting can inform dictionary design for graph signals, lead to algorithms for reconstructing missing information from graph signals via sparse representations, and yield new graph analysis tools. While previous work has focused on generalizing notions of spreads of a graph signal in the vertex and graph spectral domains, our approach is to generalize the methods of Lieb in order to develop uncertainty principles that provide limits on the concentration of the analysis coefficients of any graph signal under a dictionary transform whose atoms are jointly localized in the vertex and graph spectral domains. One challenge we highlight is that due to the inhomogeneity of the underlying graph data domain, the local structure in a single small region of the graph can drastically affect the uncertainty bounds for signals concentrated in different regions of the graph, limiting the information provided by global uncertainty principles. Accordingly, we suggest a new way to incorporate a notion of locality, and develop local uncertainty principles that bound the concentration of the analysis coefficients of each atom of a localized graph spectral filter frame in terms of quantities that depend on the local structure of the graph around the center vertex of the given atom. Finally, we demonstrate how our proposed local uncertainty measures can improve the random sampling of graph signals. "
708106491300089858,2016-03-11 01:45:18,https://t.co/3uIEJIfB33,Blind Source Separation: Fundamentals and Recent Advances (A Tutorial Overview Presented at SBrT-2001). (arXiv:160… https://t.co/3uIEJIfB33,1,7," Abstract: Blind source separation (BSS), i.e., the decoupling of unknown signals that have been mixed in an unknown way, has been a topic of great interest in the signal processing community for the last decade, covering a wide range of applications in such diverse fields as digital communications, pattern recognition, biomedical engineering, and financial data analysis, among others. This course aims at an introduction to the BSS problem via an exposition of well-known and established as well as some more recent approaches to its solution. A unified way is followed in presenting the various results so as to more easily bring out their similarities/differences and emphasize their relative advantages/disadvantages. Only a representative sample of the existing knowledge on BSS will be included in this course. The interested readers are encouraged to consult the list of bibliographical references for more details on this exciting and always active research topic. "
708106489613979648,2016-03-11 01:45:17,https://t.co/iYImAYz4JD,"Theoretical Comparisons of Learning from Positive-Negative, Positive-Unlabeled, and Negative-Unlabeled Data. (arXi… https://t.co/iYImAYz4JD",1,4," Abstract: In PU learning, a binary classifier is trained from positive (P) and unlabeled (U) data without negative (N) data. Although N data is missing, it sometimes outperforms PN learning (i.e., ordinary supervised learning). Hitherto, neither theoretical nor experimental analysis has been given to explain this phenomenon. In this paper, we theoretically compare PU (and NU) learning against PN learning based on the upper bounds on estimation errors. We find simple conditions when PU and NU learning are likely to outperform PN learning, and we prove that, in terms of the upper bounds, either PU or NU learning (depending on the class-prior probability and the sizes of P and N data) given infinite U data will improve on PN learning. Our theoretical findings well agree with the experimental results on artificial and benchmark data even when the experimental setup does not match the theoretical assumptions exactly. "
708106487894253568,2016-03-11 01:45:17,https://t.co/qXnEij83o9,Pymanopt: A Python Toolbox for Manifold Optimization using Automatic Differentiation. (arXiv:1603.03236v1 [cs.MS]) https://t.co/qXnEij83o9,1,11," Abstract: Optimization on manifolds is a class of methods for optimization of an objective function, subject to constraints which are smooth, in the sense that the set of points which satisfy the constraints admits the structure of a differentiable manifold. While many optimization problems are of the described form, technicalities of differential geometry and the laborious calculation of derivatives pose a significant barrier for experimenting with these methods. We introduce Pymanopt (available at this https URL), a toolbox for optimization on manifolds, implemented in Python, that---similarly to the Manopt Matlab toolbox---implements several manifold geometries and optimization algorithms. Moreover, we lower the barriers to users further by using automated differentiation for calculating derivative information, saving users time and saving them from potential calculation and implementation errors. "
708106486598266881,2016-03-11 01:45:17,https://t.co/sAR8WfMNF3,Spectral Ranking using Seriation. (arXiv:1406.5370v4 [cs.LG] UPDATED) https://t.co/sAR8WfMNF3,0,1," Abstract: We describe a seriation algorithm for ranking a set of items given pairwise comparisons between these items. Intuitively, the algorithm assigns similar rankings to items that compare similarly with all others. It does so by constructing a similarity matrix from pairwise comparisons, using seriation methods to reorder this matrix and construct a ranking. We first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order. We then show that ranking reconstruction is still exact when some pairwise comparisons are corrupted or missing, and that seriation based spectral ranking is more robust to noise than classical scoring methods. Finally, we bound the ranking error when only a random subset of the comparions are observed. An additional benefit of the seriation formulation is that it allows us to solve semi-supervised ranking problems. Experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods. "
708106485147045888,2016-03-11 01:45:16,https://t.co/uFsa4flLGi],Sequential Monte Carlo Methods for System Identification. (arXiv:1503.06058v3 [https://t.co/uFsa4flLGi] UPDATED) https://t.co/aeFjjvGEGB,0,6,INDEXERROR
708106483536404480,2016-03-11 01:45:16,https://t.co/qdkZ9LeWct,Efficient Bayesian experimentation using an expected information gain lower bound. (arXiv:1506.00053v2 [stat.ML] U… https://t.co/qdkZ9LeWct,0,2," Abstract: Experimental design is crucial for inference where limitations in the data collection procedure are present due to cost or other restrictions. Optimal experimental designs determine parameters that in some appropriate sense make the data the most informative possible. In a Bayesian setting this is translated to updating to the best possible posterior. Information theoretic arguments have led to the formation of the expected information gain as a design criterion. This can be evaluated mainly by Monte Carlo sampling and maximized by using stochastic approximation methods, both known for being computationally expensive tasks. We propose a framework where a lower bound of the expected information gain is used as an alternative design criterion. In addition to alleviating the computational burden, this also addresses issues concerning estimation bias. The problem of permeability inference in a large contaminated area is used to demonstrate the validity of our approach where we employ the massively parallel version of the multiphase multicomponent simulator TOUGH2 to simulate contaminant transport and a Polynomial Chaos approximation of the forward model that further accelerates the objective function evaluations. The proposed methodology is demonstrated to a setting where field measurements are available. "
708106481900658688,2016-03-11 01:45:15,https://t.co/NiPu2DJdUw,Multi-centrality Graph Spectral Decompositions and their Application to Cyber Intrusion Detection. (arXiv:1512.073… https://t.co/NiPu2DJdUw,0,2," Abstract: Many modern datasets can be represented as graphs and hence spectral decompositions such as graph principal component analysis (PCA) can be useful. Distinct from previous graph decomposition approaches based on subspace projection of a single topological feature, e.g., the Fiedler vector of centered graph adjacency matrix (graph Laplacian), we propose spectral decomposition approaches to graph PCA and graph dictionary learning that integrate multiple features, including graph walk statistics, centrality measures and graph distances to reference nodes. In this paper we propose a new PCA method for single graph analysis, called multi-centrality graph PCA (MC-GPCA), and a new dictionary learning method for ensembles of graphs, called multi-centrality graph dictionary learning (MC-GDL), both based on spectral decomposition of multi-centrality matrices. As an application to cyber intrusion detection, MC-GPCA can be an effective indicator of anomalous connectivity pattern and MC-GDL can provide discriminative basis for attack classification. "
708106480084455424,2016-03-11 01:45:15,https://t.co/AEPXm3ln8O,Optimality of Belief Propagation for Crowdsourced Classification. (arXiv:1602.03619v2 [cs.LG] UPDATED) https://t.co/AEPXm3ln8O,0,1," Abstract: Crowdsourcing systems are popular for solving large-scale labelling tasks with low-paid (or even non-paid) workers. We study the problem of recovering the true labels from the possibly erroneous crowdsourced labels under the popular Dawid-Skene model. To address this inference problem, several algorithms have recently been proposed, but the best known guarantee is still significantly larger than the fundamental limit. In our previous work, we closed this gap under a canonical assumption where each worker is assigned only two tasks, i.e., $r=2$, and each task is assigned to sufficiently but constantly many workers, $\ell \ge C_r$. In this work, we further remove the condition on r and show that for all $r \ge 1$, Belief Propagation exactly matches a lower bound on the fundamental limit if $\ell \ge C_r$. The guaranteed optimality of BP is the strongest in the sense that it is information-theoretically impossible for any other algorithm to correctly label a larger fraction of the tasks. In the general setting, regardless of the number of workers assigned to a task, we establish the dominance result on BP that it outperforms all existing algorithms with provable guarantees. Experimental results suggest that BP is close to optimal for all regimes considered, while all other algorithms show suboptimal performances in certain regimes. "
707742384159903744,2016-03-10 01:38:28,https://t.co/7oFYRoYVw0,Discriminative models for robust image classification. (arXiv:1603.02736v1 [stat.ML]) https://t.co/7oFYRoYVw0,1,4," Abstract: A variety of real-world tasks involve the classification of images into pre-determined categories. Designing image classification algorithms that exhibit robustness to acquisition noise and image distortions, particularly when the available training data are insufficient to learn accurate models, is a significant challenge. This dissertation explores the development of discriminative models for robust image classification that exploit underlying signal structure, via probabilistic graphical models and sparse signal representations. Probabilistic graphical models are widely used in many applications to approximate high-dimensional data in a reduced complexity set-up. Learning graphical structures to approximate probability distributions is an area of active research. Recent work has focused on learning graphs in a discriminative manner with the goal of minimizing classification error. In the first part of the dissertation, we develop a discriminative learning framework that exploits the complementary yet correlated information offered by multiple representations (or projections) of a given signal/image. Specifically, we propose a discriminative tree-based scheme for feature fusion by explicitly learning the conditional correlations among such multiple projections in an iterative manner. Experiments reveal the robustness of the resulting graphical model classifier to training insufficiency. "
707742383220367360,2016-03-10 01:38:28,https://t.co/P4H0TA6rY2,Pairwise Choice Markov Chains. (arXiv:1603.02740v1 [stat.ML]) https://t.co/P4H0TA6rY2,2,3," Abstract: As datasets capturing human choices grow in richness and scale---particularly in online domains---there is an increasing need for choice models that escape traditional choice-theoretic axioms such as regularity, stochastic transitivity, and Luce's choice axiom. In this work we introduce the Pairwise Choice Markov Chain (PCMC) model of discrete choice, an inferentially tractable model that does not assume any of the above axioms while still satisfying the foundational axiom of uniform expansion, a considerably weaker assumption than Luce's choice axiom. We show that the PCMC model significantly outperforms the Multinomial Logit (MNL) model in prediction tasks on both synthetic and empirical datasets known to exhibit violations of Luce's axiom. Our analysis also synthesizes several recent observations connecting the Multinomial Logit model and Markov chains; the PCMC model retains the Multinomial Logit model as a special case. "
707742382264033280,2016-03-10 01:38:27,https://t.co/2gM5Znf7mN,Computing AIC for black-box models using Generalised Degrees of Freedom: a comparison with cross-validation. (arXi… https://t.co/2gM5Znf7mN,0,3," Abstract: Generalised Degrees of Freedom (GDF), as defined by Ye (1998 JASA 93:120-131), represent the sensitivity of model fits to perturbations of the data. As such they can be computed for any statistical model, making it possible, in principle, to derive the number of parameters in machine-learning approaches. Defined originally for normally distributed data only, we here investigate the potential of this approach for Bernoulli-data. GDF-values for models of simulated and real data are compared to model complexity-estimates from cross-validation. Similarly, we computed GDF-based AICc for randomForest, neural networks and boosted regression trees and demonstrated its similarity to cross-validation. GDF-estimates for binary data were unstable and inconsistently sensitive to the number of data points perturbed simultaneously, while at the same time being extremely computer-intensive in their calculation. Repeated 10-fold cross-validation was more robust, based on fewer assumptions and faster to compute. Our findings suggest that the GDF-approach does not readily transfer to Bernoulli data and a wider range of regression approaches. "
707742381337145344,2016-03-10 01:38:27,https://t.co/xungXTAytH,Best-of-K Bandits. (arXiv:1603.02752v1 [cs.LG]) https://t.co/xungXTAytH,1,3," Abstract: This paper studies the Best-of-K Bandit game: At each time the player chooses a subset S among all N-choose-K possible options and observes reward max(X(i) : i in S) where X is a random vector drawn from a joint distribution. The objective is to identify the subset that achieves the highest expected reward with high probability using as few queries as possible. We present distribution-dependent lower bounds based on a particular construction which force a learner to consider all N-choose-K subsets, and match naive extensions of known upper bounds in the bandit setting obtained by treating each subset as a separate arm. Nevertheless, we present evidence that exhaustive search may be avoided for certain, favorable distributions because the influence of high-order order correlations may be dominated by lower order statistics. Finally, we present an algorithm and analysis for independent arms, which mitigates the surprising non-trivial information occlusion that occurs due to only observing the max in the subset. This may inform strategies for more general dependent measures, and we complement these result with independent-arm lower bounds. "
707742380389228545,2016-03-10 01:38:27,https://t.co/dd8GR5kTCa,megaman: Manifold Learning with Millions of points. (arXiv:1603.02763v1 [cs.LG]) https://t.co/dd8GR5kTCa,0,3," Abstract: Manifold Learning is a class of algorithms seeking a low-dimensional non-linear representation of high-dimensional data. Thus manifold learning algorithms are, at least in theory, most applicable to high-dimensional data and sample sizes to enable accurate estimation of the manifold. Despite this, most existing manifold learning implementations are not particularly scalable. Here we present a Python package that implements a variety of manifold learning algorithms in a modular and scalable fashion, using fast approximate neighbors searches and fast sparse eigendecompositions. The package incorporates theoretical advances in manifold learning, such as the unbiased Laplacian estimator and the estimation of the embedding distortion by the Riemannian metric method. In benchmarks, even on a single-core desktop computer, our code embeds millions of data points in minutes, and takes just 200 minutes to embed the main sample of galaxy spectra from the Sloan Digital Sky Survey --- consisting of 0.6 million samples in 3750-dimensions --- a task which has not previously been possible. "
707742379445460993,2016-03-10 01:38:27,https://t.co/XRTweU6tJH,Bipartite Correlation Clustering -- Maximizing Agreements. (arXiv:1603.02782v1 [cs.DS]) https://t.co/XRTweU6tJH,0,3," Abstract: In Bipartite Correlation Clustering (BCC) we are given a complete bipartite graph $G$ with `+' and `-' edges, and we seek a vertex clustering that maximizes the number of agreements: the number of all `+' edges within clusters plus all `-' edges cut across clusters. BCC is known to be NP-hard. We present a novel approximation algorithm for $k$-BCC, a variant of BCC with an upper bound $k$ on the number of clusters. Our algorithm outputs a $k$-clustering that provably achieves a number of agreements within a multiplicative ${(1-\delta)}$-factor from the optimal, for any desired accuracy $\delta$. It relies on solving a combinatorially constrained bilinear maximization on the bi-adjacency matrix of $G$. It runs in time exponential in $k$ and $\delta^{-1}$, but linear in the size of the input. Further, we show that, in the (unconstrained) BCC setting, an ${(1-\delta)}$-approximation can be achieved by $O(\delta^{-1})$ clusters regardless of the size of the graph. In turn, our $k$-BCC algorithm implies an Efficient PTAS for the BCC objective of maximizing agreements. "
707742378480762880,2016-03-10 01:38:26,https://t.co/E27kwtY2LB,Optimized Kernel Entropy Components. (arXiv:1603.02806v1 [stat.ML]) https://t.co/E27kwtY2LB,1,3," Abstract: This work addresses two main issues of the standard Kernel Entropy Component Analysis (KECA) algorithm: the optimization of the kernel decomposition and the optimization of the Gaussian kernel parameter. KECA roughly reduces to a sorting of the importance of kernel eigenvectors by entropy instead of by variance as in Kernel Principal Components Analysis. In this work, we propose an extension of the KECA method, named Optimized KECA (OKECA), that directly extracts the optimal features retaining most of the data entropy by means of compacting the information in very few features (often in just one or two). The proposed method produces features which have higher expressive power. In particular, it is based on the Independent Component Analysis (ICA) framework, and introduces an extra rotation to the eigen-decomposition, which is optimized via gradient ascent search. This maximum entropy preservation suggests that OKECA features are more efficient than KECA features for density estimation. In addition, a critical issue in both methods is the selection of the kernel parameter since it critically affects the resulting performance. Here we analyze the most common kernel length-scale selection criteria. Results of both methods are illustrated in different synthetic and real problems. Results show that 1) OKECA returns projections with more expressive power than KECA, 2) the most successful rule for estimating the kernel parameter is based on maximum likelihood, and 3) OKECA is more robust to the selection of the length-scale parameter in kernel density estimation. "
707742377528709121,2016-03-10 01:38:26,https://t.co/1VQjIjYvXE,Frequency estimation in three-phase power systems with harmonic contamination: A multistage quaternion Kalman filt… https://t.co/1VQjIjYvXE,0,1," Abstract: Motivated by the need for accurate frequency information, a novel algorithm for estimating the fundamental frequency and its rate of change in three-phase power systems is developed. This is achieved through two stages of Kalman filtering. In the first stage a quaternion extended Kalman filter, which provides a unified framework for joint modeling of voltage measurements from all the phases, is used to estimate the instantaneous phase increment of the three-phase voltages. The phase increment estimates are then used as observations of the extended Kalman filter in the second stage that accounts for the dynamic behavior of the system frequency and simultaneously estimates the fundamental frequency and its rate of change. The framework is then extended to account for the presence of harmonics. Finally, the concept is validated through simulation on both synthetic and real-world data. "
707742376589172736,2016-03-10 01:38:26,https://t.co/7BXFHM2gNa,Nonparametric Bayesian Double Articulation Analyzer for Direct Language Acquisition from Continuous Speech Signals… https://t.co/7BXFHM2gNa,0,1," Abstract: Human infants can discover words directly from unsegmented speech signals without any explicitly labeled data. In this paper, we develop a novel machine learning method called nonparametric Bayesian double articulation analyzer (NPB-DAA) that can directly acquire language and acoustic models from observed continuous speech signals. For this purpose, we propose an integrative generative model that combines a language model and an acoustic model into a single generative model called the ""hierarchical Dirichlet process hidden language model"" (HDP-HLM). The HDP-HLM is obtained by extending the hierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by Johnson et al. An inference procedure for the HDP-HLM is derived using the blocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure enables the simultaneous and direct inference of language and acoustic models from continuous speech signals. Based on the HDP-HLM and its inference procedure, we developed a novel double articulation analyzer. By assuming HDP-HLM as a generative model of observed time series data, and by inferring latent variables of the model, the method can analyze latent double articulation structure, i.e., hierarchically organized latent words and phonemes, of the data in an unsupervised manner. The novel unsupervised double articulation analyzer is called NPB-DAA. The NPB-DAA can automatically estimate double articulation structure embedded in speech signals. We also carried out two evaluation experiments using synthetic data and actual human continuous speech signals representing Japanese vowel sequences. In the word acquisition and phoneme categorization tasks, the NPB-DAA outperformed a conventional double articulation analyzer (DAA) and baseline automatic speech recognition system whose acoustic model was trained in a supervised manner. "
707742375628693505,2016-03-10 01:38:26,https://t.co/ZrjQj3vGQh,Minimum Density Hyperplanes. (arXiv:1507.04201v2 [stat.ML] UPDATED) https://t.co/ZrjQj3vGQh,1,1," Abstract: Associating distinct groups of objects (clusters) with contiguous regions of high probability density (high-density clusters), is central to many statistical and machine learning approaches to the classification of unlabelled data. We propose a novel hyperplane classifier for clustering and semi-supervised classification which is motivated by this objective. The proposed minimum density hyperplane minimises the integral of the empirical probability density function along it, thereby avoiding intersection with high density clusters. We show that the minimum density and the maximum margin hyperplanes are asymptotically equivalent, thus linking this approach to maximum margin clustering and semi-supervised support vector classifiers. We propose a projection pursuit formulation of the associated optimisation problem which allows us to find minimum density hyperplanes efficiently in practice, and evaluate its performance on a range of benchmark datasets. The proposed approach is found to be very competitive with state of the art methods for clustering and semi-supervised classification. "
707742374370406400,2016-03-10 01:38:25,https://t.co/lx8LvC33yi,Selective Inference Approach for Statistically Sound Predictive Pattern Mining. (arXiv:1602.04601v2 [stat.ML] UPDA… https://t.co/lx8LvC33yi,0,0," Abstract: Discovering statistically significant patterns from databases is an important challenging problem. The main obstacle of this problem is in the difficulty of taking into account the selection bias, i.e., the bias arising from the fact that patterns are selected from extremely large number of candidates in databases. In this paper, we introduce a new approach for predictive pattern mining problems that can address the selection bias issue. Our approach is built on a recently popularized statistical inference framework called selective inference. In selective inference, statistical inferences (such as statistical hypothesis testing) are conducted based on sampling distributions conditional on a selection event. If the selection event is characterized in a tractable way, statistical inferences can be made without minding selection bias issue. However, in pattern mining problems, it is difficult to characterize the entire selection process of mining algorithms. Our main contribution in this paper is to solve this challenging problem for a class of predictive pattern mining problems by introducing a novel algorithmic framework. We demonstrate that our approach is useful for finding statistically significant patterns from databases. "
707742373334360065,2016-03-10 01:38:25,https://t.co/FpwbwiV5FA,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF. (arXiv:1603.01354v3 [cs.LG] CROSS LISTED) https://t.co/FpwbwiV5FA,3,9," Abstract: State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\% accuracy for POS tagging and 91.21\% F1 for NER. "
707742371954499584,2016-03-10 01:38:25,https://t.co/eKZTbEA7aC,"A single-phase, proximal path-following framework. (arXiv:1603.01681v1 [math.OC] CROSS LISTED) https://t.co/eKZTbEA7aC",0,1," Abstract: We propose a new proximal, path-following framework for a class of---possibly non-smooth---constrained convex problems. We consider settings where the non-smooth part is endowed with a proximity operator, and the constraint set is equipped with a self-concordant barrier. Our main contribution is a new re-parametrization of the optimality condition of the barrier problem, that allows us to process the objective function with its proximal operator within a new path following scheme. In particular, our approach relies on the following two main ideas. First, we re-parameterize the optimality condition as an auxiliary problem, such that a ""good"" initial point is available. Second, we combine the proximal operator of the objective and path-following ideas to design a single phase, proximal, path-following algorithm. Our method has several advantages. First, it allows handling non-smooth objectives via proximal operators, this avoids lifting the problem dimension via slack variables and additional constraints. Second, it consists of only a \emph{single phase} as compared to a two-phase algorithm in [43] In this work, we show how to overcome this difficulty in the proximal setting and prove that our scheme has the same $\mathcal{O}(\sqrt{\nu}\log(1/\varepsilon))$ worst-case iteration-complexity with standard approaches [30, 33], but our method can handle nonsmooth objectives, where $\nu$ is the barrier parameter and $\varepsilon$ is a desired accuracy. Finally, our framework allows errors in the calculation of proximal-Newton search directions, without sacrificing the worst-case iteration complexity. We demonstrate the merits of our algorithm via three numerical examples, where proximal operators play a key role to improve the performance over off-the-shelf interior-point solvers. "
707738685689696257,2016-03-10 01:23:46,https://t.co/e6yZXNq628,Multigrid with rough coefficients and Multiresolution operator decomposition from Hierarchical Information Games. … https://t.co/e6yZXNq628,1,0," Abstract: We introduce a near-linear complexity (geometric and meshless/algebraic) multigrid/multiresolution method for PDEs with rough ($L^\infty$) coefficients with rigorous a-priori accuracy and performance estimates. The method is discovered through a decision/game theory formulation of the problems of (1) identifying restriction and interpolation operators (2) recovering a signal from incomplete measurements based on norm constraints on its image under a linear operator (3) gambling on the value of the solution of the PDE based on a hierarchy of nested measurements of its solution or source term. The resulting elementary gambles form a hierarchy of (deterministic) basis functions of $H^1_0(\Omega)$ (gamblets) that (1) are orthogonal across subscales/subbands with respect to the scalar product induced by the energy norm of the PDE (2) enable sparse compression of the solution space in $H^1_0(\Omega)$ (3) induce an orthogonal multiresolution operator decomposition. The operating diagram of the multigrid method is that of an inverted pyramid in which gamblets are computed locally (by virtue of their exponential decay), hierarchically (from fine to coarse scales) and the PDE is decomposed into a hierarchy of independent linear systems with uniformly bounded condition numbers. The resulting algorithm is parallelizable both in space (via localization) and in bandwith/subscale (subscales can be computed independently from each other). Although the method is deterministic it has a natural Bayesian interpretation under the measure of probability emerging (as a mixed strategy) from the information game formulation and multiresolution approximations form a martingale with respect to the filtration induced by the hierarchy of nested measurements. "
707383645099712512,2016-03-09 01:52:58,https://t.co/8dUfC6YohJ,Stochastic dual averaging methods using variance reduction techniques for regularized empirical risk minimization … https://t.co/8dUfC6YohJ,2,5," Abstract: We consider a composite convex minimization problem associated with regularized empirical risk minimization, which often arises in machine learning. We propose two new stochastic gradient methods that are based on stochastic dual averaging method with variance reduction. Our methods generate a sparser solution than the existing methods because we do not need to take the average of the history of the solutions. This is favorable in terms of both interpretability and generalization. Moreover, our methods have theoretical support for both a strongly and a non-strongly convex regularizer and achieve the best known convergence rates among existing nonaccelerated stochastic gradient methods. "
707383644277641216,2016-03-09 01:52:58,https://t.co/cKJEKIsl62,Effective Mean-Field Inference Method for Nonnegative Boltzmann Machines. (arXiv:1603.02434v1 [stat.ML]) https://t.co/cKJEKIsl62,2,3," Abstract: Nonnegative Boltzmann machines (NNBMs) are recurrent probabilistic neural network models that can describe multi-modal nonnegative data. NNBMs form rectified Gaussian distributions that appear in biological neural network models, positive matrix factorization, nonnegative matrix factorization, and so on. In this paper, an effective inference method for NNBMs is proposed that uses the mean-field method, referred to as the Thouless--Anderson--Palmer equation, and the diagonal consistency method, which was recently proposed. "
707383643132583936,2016-03-09 01:52:57,https://t.co/WOxFxbr5c9,Note on the equivalence of hierarchical variational models and auxiliary deep generative models. (arXiv:1603.02443… https://t.co/WOxFxbr5c9,1,6," Abstract: This note compares two recently published machine learning methods for constructing flexible, but tractable families of variational hidden-variable posteriors. The first method, called ""hierarchical variational models"" enriches the inference model with an extra variable, while the other, called ""auxiliary deep generative models"", enriches the generative model instead. We conclude that the two methods are mathematically equivalent. "
707383642021093377,2016-03-09 01:52:57,https://t.co/y1XOju60VX,A Bayesian non-parametric method for clustering high-dimensional binary data. (arXiv:1603.02494v1 [stat.AP]) https://t.co/y1XOju60VX,3,13," Abstract: In many real life problems, objects are described by large number of binary features. For instance, documents are characterized by presence or absence of certain keywords; cancer patients are characterized by presence or absence of certain mutations etc. In such cases, grouping together similar objects/profiles based on such high dimensional binary features is desirable, but challenging. Here, I present a Bayesian non parametric algorithm for clustering high dimensional binary data. It uses a Dirichlet Process (DP) mixture model and simulated annealing to not only cluster binary data, but also find optimal number of clusters in the data. The performance of the algorithm was evaluated and compared with other algorithms using simulated datasets. It outperformed all other clustering methods that were tested in the simulation studies. It was also used to cluster real datasets arising from document analysis, handwritten image analysis and cancer research. It successfully divided a set of documents based on their topics, hand written images based on different styles of writing digits and identified tissue and mutation specificity of chemotherapy treatments. "
707383640464818177,2016-03-09 01:52:57,https://t.co/GjVql1tAnY,Mixture Proportion Estimation via Kernel Embedding of Distributions. (arXiv:1603.02501v1 [cs.LG]) https://t.co/GjVql1tAnY,0,1," Abstract: Mixture proportion estimation (MPE) is the problem of estimating the weight of a component distribution in a mixture, given samples from the mixture and component. This problem constitutes a key part in many ""weakly supervised learning"" problems like learning with positive and unlabelled samples, learning with label noise, anomaly detection and crowdsourcing. While there have been several methods proposed to solve this problem, to the best of our knowledge no efficient algorithm with a proven convergence rate towards the true proportion exists for this problem. We fill this gap by constructing a provably correct algorithm for MPE, and derive convergence rates under certain assumptions on the distribution. Our method is based on embedding distributions onto an RKHS, and implementing it only requires solving a simple convex quadratic programming problem a few times. We run our algorithm on several standard classification datasets, and demonstrate that it performs comparably to or better than other algorithms on most datasets. "
707383639382876160,2016-03-09 01:52:56,https://t.co/rldvJp0Oql,On the inconsistency of $\ell_1$-penalised sparse precision matrix estimation. (arXiv:1603.02532v1 [cs.LG]) https://t.co/rldvJp0Oql,1,3," Abstract: Various $\ell_1$-penalised estimation methods such as graphical lasso and CLIME are widely used for sparse precision matrix estimation. Many of these methods have been shown to be consistent under various quantitative assumptions about the underlying true covariance matrix. Intuitively, these conditions are related to situations where the penalty term will dominate the optimisation. In this paper, we explore the consistency of $\ell_1$-based methods for a class of sparse latent variable -like models, which are strongly motivated by several types of applications. We show that all $\ell_1$-based methods fail dramatically for models with nearly linear dependencies between the variables. We also study the consistency on models derived from real gene expression data and note that the assumptions needed for consistency never hold even for modest sized gene networks and $\ell_1$-based methods also become unreliable in practice for larger networks. "
707383638439170049,2016-03-09 01:52:56,https://t.co/Maa0Eni6jn,Small ensembles of kriging models for optimization. (arXiv:1603.02638v1 [math.OC]) https://t.co/Maa0Eni6jn,0,3," Abstract: The Efficient Global Optimization (EGO) algorithm uses a conditional Gaus-sian Process (GP) to approximate an objective function known at a finite number of observation points and sequentially adds new points which maximize the Expected Improvement criterion according to the GP. The important factor that controls the efficiency of EGO is the GP covariance function (or kernel) which should be chosen according to the objective function. Traditionally, a pa-rameterized family of covariance functions is considered whose parameters are learned through statistical procedures such as maximum likelihood or cross-validation. However, it may be questioned whether statistical procedures for learning covariance functions are the most efficient for optimization as they target a global agreement between the GP and the observations which is not the ultimate goal of optimization. Furthermore, statistical learning procedures are computationally expensive. The main alternative to the statistical learning of the GP is self-adaptation, where the algorithm tunes the kernel parameters based on their contribution to objective function improvement. After questioning the possibility of self-adaptation for kriging based optimizers, this paper proposes a novel approach for tuning the length-scale of the GP in EGO: At each iteration, a small ensemble of kriging models structured by their length-scales is created. All of the models contribute to an iterate in an EGO-like fashion. Then, the set of models is densified around the model whose length-scale yielded the best iterate and further points are produced. Numerical experiments are provided which motivate the use of many length-scales. The tested implementation does not perform better than the classical EGO algorithm in a sequential context but show the potential of the approach for parallel implementations. "
707383637352824832,2016-03-09 01:52:56,https://t.co/PUQE1cTDGr,Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling. (arXiv:1603.02644v1 [cs.LG]) https://t.co/PUQE1cTDGr,2,5," Abstract: We study parameter inference in large-scale latent variable models. We first propose an unified treatment of online inference for latent variable models from a non-canonical exponential family, and draw explicit links between several previously proposed frequentist or Bayesian methods. We then propose a novel inference method for the frequentist estimation of parameters, that adapts MCMC methods to online inference of latent variable models with the proper use of local Gibbs sampling. Then, for latent Dirich-let allocation,we provide an extensive set of experiments and comparisons with existing work, where our new approach outperforms all previously proposed methods. In particular, using Gibbs sampling for latent variable inference is superior to variational inference in terms of test log-likelihoods. Moreover, Bayesian inference through variational methods perform poorly, sometimes leading to worse fits with latent variables of higher dimensionality. "
707383636220370945,2016-03-09 01:52:56,https://t.co/IcPDzE978M,Doubly Decomposing Nonparametric Tensor Regression. (arXiv:1506.05967v3 [stat.ML] UPDATED) https://t.co/IcPDzE978M,1,1," Abstract: Nonparametric extension of tensor regression is proposed. Nonlinearity in a high-dimensional tensor space is broken into simple local functions by incorporating low-rank tensor decomposition. Compared to naive nonparametric approaches, our formulation considerably improves the convergence rate of estimation while maintaining consistency with the same function class under specific conditions. To estimate local functions, we develop a Bayesian estimator with the Gaussian process prior. Experimental results show its theoretical properties and high performance in terms of predicting a summary statistic of a real complex network. "
707383635171786755,2016-03-09 01:52:55,https://t.co/12jT9H5I08,A Scalable and Extensible Framework for Superposition-Structured Models. (arXiv:1509.02314v2 [cs.NA] UPDATED) https://t.co/12jT9H5I08,1,2," Abstract: In many learning tasks, structural models usually lead to better interpretability and higher generalization performance. In recent years, however, the simple structural models such as lasso are frequently proved to be insufficient. Accordingly, there has been a lot of work on ""superposition-structured"" models where multiple structural constraints are imposed. To efficiently solve these ""superposition-structured"" statistical models, we develop a framework based on a proximal Newton-type method. Employing the smoothed conic dual approach with the LBFGS updating formula, we propose a scalable and extensible proximal quasi-Newton (SEP-QN) framework. Empirical analysis on various datasets shows that our framework is potentially powerful, and achieves super-linear convergence rate for optimizing some popular ""superposition-structured"" statistical models such as the fused sparse group lasso. "
707383634110648320,2016-03-09 01:52:55,https://t.co/oGhAmN2LdO,"Coordinate Friendly Structures, Algorithms and Applications. (arXiv:1601.00863v2 [math.OC] UPDATED) https://t.co/oGhAmN2LdO",1,2," Abstract: This paper focuses on coordinate update methods, which are useful for solving problems involving large or high-dimensional datasets. They decompose a problem into simple subproblems, where each updates one, or a small block of, variables while fixing others. These methods can deal with linear and nonlinear mappings, smooth and nonsmooth functions, as well as convex and nonconvex problems. In addition, they are easy to parallelize. The great performance of coordinate update methods depends on solving simple sub-problems. To derive simple subproblems for several new classes of applications, this paper systematically studies coordinate-friendly operators that perform low-cost coordinate updates. Based on the discovered coordinate friendly operators, as well as operator splitting techniques, we obtain new coordinate update algorithms for a variety of problems in machine learning, image processing, as well as sub-areas of optimization. Several problems are treated with coordinate update for the first time in history. The obtained algorithms are scalable to large instances through parallel and even asynchronous computing. We present numerical examples to illustrate how effective these algorithms are. "
707383633049489412,2016-03-09 01:52:55,https://t.co/5mJhj6IKd6,Rectified Gaussian Scale Mixtures and the Sparse Non-Negative Least Squares Problem. (arXiv:1601.06207v2 [cs.LG] U… https://t.co/5mJhj6IKd6,1,3," Abstract: In this paper we introduce a hierarchical Bayesian framework to obtain sparse and non-negative solutions to the sparse non-negative least squares problem (S-NNLS). We introduce a new family of scale mixtures, the Rectified Gaussian Scale Mixture (R-GSM), to model the sparsity enforcing prior distribution for the signal of interest. One advantage of the R-GSM prior is that through proper choice of the mixing density it encompasses a wide variety of heavy tailed distributions, such as the rectified Laplacian and rectified Student's t distributions. Similar to the Gaussian Scale Mixture (GSM) approach, a Type II Expectation-Maximization framework is developed to estimate the hyper-parameters and obtain a point estimate of the parameter of interest. In the proposed method, called rectified Sparse Bayesian Learning (R-SBL), we provide two ways to perform the Expectation step; Markov-Chain Monte-Carlo (MCMC) simulations and a simple yet effective diagonal approximation approach (DA). Through numerical experiments we show that R-SBL outperforms existing S-NNLS solvers in terms of both signal and support recovery and that the proposed DA approach admits both computational efficiency and numerical accuracy. "
707383631463972865,2016-03-09 01:52:54,https://t.co/8fM6kpdzNo,Censoring Representations with an Adversary. (arXiv:1511.05897v3 [cs.LG] CROSS LISTED) https://t.co/8fM6kpdzNo,2,2," Abstract: In practice, there are often explicit constraints on what representations or decisions are acceptable in an application of machine learning. For example it may be a legal requirement that a decision must not favour a particular group. Alternatively it can be that that representation of data must not have identifying information. We address these two related issues by learning flexible representations that minimize the capability of an adversarial critic. This adversary is trying to predict the relevant sensitive variable from the representation, and so minimizing the performance of the adversary ensures there is little or no information in the representation about the sensitive variable. We demonstrate this adversarial approach on two problems: making decisions free from discrimination and removing private information from images. We formulate the adversarial model as a minimax problem, and optimize that minimax objective using a stochastic gradient alternate min-max optimizer. We demonstrate the ability to provide discriminant free representations for standard test problems, and compare with previous state of the art methods for fairness, showing statistically significant improvement across most cases. The flexibility of this method is shown via a novel problem: removing annotations from images, from unaligned training examples of annotated and unannotated images, and with no a priori knowledge of the form of annotation provided to the model. "
707026388289724417,2016-03-08 02:13:21,https://t.co/h3jFV7eP5U,High-Dimensional Metrics in R. (arXiv:1603.01700v1 [stat.ML]) https://t.co/h3jFV7eP5U,0,4," Abstract: The package High-dimensional Metrics (\Rpackage{hdm}) is an evolving collection of statistical methods for estimation and quantification of uncertainty in high-dimensional approximately sparse models. It focuses on providing confidence intervals and significance testing for (possibly many) low-dimensional subcomponents of the high-dimensional parameter vector. Efficient estimators and uniformly valid confidence intervals for regression coefficients on target variables (e.g., treatment or policy variable) in a high-dimensional approximately sparse regression model, for average treatment effect (ATE) and average treatment effect for the treated (ATET), as well for extensions of these parameters to the endogenous setting are provided. Theory grounded, data-driven methods for selecting the penalization parameter in Lasso regressions under heteroscedastic and non-Gaussian errors are implemented. Moreover, joint/ simultaneous confidence intervals for regression coefficients of a high-dimensional sparse regression are implemented, including a joint significance test for Lasso regression. Data sets which have been used in the literature and might be useful for classroom demonstration and for testing new estimators are included. \R and the package \Rpackage{hdm} are open-source software projects and can be freely downloaded from CRAN: \texttt{this http URL}. "
707026386104532992,2016-03-08 02:13:21,https://t.co/BovXsygzLm,Classical Statistics and Statistical Learning in Imaging Neuroscience. (arXiv:1603.01857v1 [stat.ML]) https://t.co/BovXsygzLm,0,5," Abstract: Neuroimaging research has predominantly drawn conclusions based on classical statistics, including null-hypothesis testing, t-tests, and ANOVA. Throughout recent years, statistical learning methods enjoy increasing popularity, including cross-validation, pattern classification, and sparsity-inducing regression. These two methodological families used for neuroimaging data analysis can be viewed as two extremes of a continuum. Yet, they originated from different historical contexts, build on different theories, rest on different assumptions, evaluate different outcome metrics, and permit different conclusions. This paper portrays commonalities and differences between classical statistics and statistical learning with their relation to neuroimaging research. The conceptual implications are illustrated in three common analysis scenarios. It is thus tried to resolve possible confusion between classical hypothesis testing and data-guided model estimation by discussing their ramifications for the neuroimaging access to neurobiology. "
707026384091226112,2016-03-08 02:13:20,https://t.co/tGaUFN4NCS,Composing inference algorithms as program transformations. (arXiv:1603.01882v1 [stat.ML]) https://t.co/tGaUFN4NCS,0,4," Abstract: Probabilistic inference procedures are usually coded painstakingly from scratch, for each target model and each inference algorithm. We reduce this coding effort by generating inference procedures from models automatically. We make this code generation modular by decomposing inference algorithms into reusable program transformations. These source-to-source transformations perform exact inference as well as generate probabilistic programs that compute expectations, densities, and MCMC samples. The resulting inference procedures run in time comparable to that of handwritten procedures. "
707026382153453568,2016-03-08 02:13:20,https://t.co/4TMB2pTxKR,Partition Functions from Rao-Blackwellized Tempered Sampling. (arXiv:1603.01912v1 [stat.ML]) https://t.co/4TMB2pTxKR,0,4," Abstract: Partition functions of probability distributions are important quantities for model evaluation and comparisons. We present a new method to compute partition functions of complex and multimodal distributions. Such distributions are often sampled using simulated tempering, which augments the target space with an auxiliary inverse temperature variable. Our method exploits the multinomial probability law of the inverse temperatures, and provides estimates of the partition function in terms of a simple quotient of Rao-Blackwellized marginal inverse temperature probability estimates, which are updated while sampling. We show that the method has interesting connections with several alternative popular methods, and offers some significant advantages. In particular, we empirically find that the new method provides more accurate estimates than Annealed Importance Sampling when calculating partition functions of large Restricted Boltzmann Machines (RBM); moreover, the method is sufficiently accurate to track training and validation log-likelihoods during learning of RBMs, at minimal computational cost. "
707026380270280704,2016-03-08 02:13:19,https://t.co/SD2RzDrhRU,Differentially Private Policy Evaluation. (arXiv:1603.02010v1 [cs.LG]) https://t.co/SD2RzDrhRU,0,3," Abstract: We present the first differentially private algorithms for reinforcement learning, which apply to the task of evaluating a fixed policy. We establish two approaches for achieving differential privacy, provide a theoretical analysis of the privacy and utility of the two algorithms, and show promising results on simple empirical examples. "
707026378445742080,2016-03-08 02:13:19,https://t.co/i00RnBdKt5,Optimal dictionary for least squares representation. (arXiv:1603.02074v1 [cs.LG]) https://t.co/i00RnBdKt5,0,5," Abstract: Dictionaries are collections of vectors used for representations of random vectors in Euclidean spaces. Recent research on optimal dictionaries is focused on constructing dictionaries that offer sparse representations, i.e., $\ell_0$-optimal representations. Here we consider the problem of finding optimal dictionaries with which representations of samples of a random vector are optimal in an $\ell_2$-sense: optimality of representation is defined as attaining the minimal average $\ell_2$-norm of the coefficients used to represent the random vector. With the help of recent results on rank-$1$ decompositions of symmetric positive semidefinite matrices, we provide an explicit description of $\ell_2$-optimal dictionaries as well as their algorithmic constructions in polynomial time. "
707026376344379392,2016-03-08 02:13:18,https://t.co/HUxIHA1S4V,Bayesian Learning of Kernel Embeddings. (arXiv:1603.02160v1 [stat.ML]) https://t.co/HUxIHA1S4V,0,3," Abstract: Kernel methods are one of the mainstays of machine learning, but the problem of kernel learning remains challenging, with only a few heuristics and very little theory. This is of particular importance in methods based on estimation of kernel mean embeddings of probability measures. For characteristic kernels, which include most commonly used ones, the kernel mean embedding uniquely determines its probability measure, so it can be used to design a powerful statistical testing framework, which includes nonparametric two-sample and independence tests. In practice, however, the performance of these tests can be very sensitive to the choice of kernel and its lengthscale parameters. To address this central issue, we propose a new probabilistic model for kernel mean embeddings, the Bayesian Kernel Embedding model, combining a Gaussian process prior over the Reproducing Kernel Hilbert Space containing the mean embedding with a conjugate likelihood function, thus yielding a closed form posterior over the mean embedding. The posterior mean of our model is closely related to recently proposed shrinkage estimators for kernel mean embeddings, while the posterior uncertainty is a new, interesting feature with various possible applications. Critically for the purposes of kernel learning, our model gives a simple, closed form marginal pseudolikelihood of the observed data given the kernel hyperparameters. This marginal pseudolikelihood can either be optimized to inform the hyperparameter choice or fully Bayesian inference can be used. "
707026374540836864,2016-03-08 02:13:18,https://t.co/vO3rCBRqe3,Distributed Multi-Task Learning with Shared Representation. (arXiv:1603.02185v1 [cs.LG]) https://t.co/vO3rCBRqe3,1,6," Abstract: We study the problem of distributed multi-task learning with shared representation, where each machine aims to learn a separate, but related, task in an unknown shared low-dimensional subspaces, i.e. when the predictor matrix has low rank. We consider a setting where each task is handled by a different machine, with samples for the task available locally on the machine, and study communication-efficient methods for exploiting the shared structure. "
707026372766638080,2016-03-08 02:13:17,https://t.co/kKagqZhjHz,Generalized system identification with stable spline kernels. (arXiv:1309.7857v2 [stat.ML] UPDATED) https://t.co/kKagqZhjHz,0,2," Abstract: Regularized least-squares approaches have been successfully applied to linear system identification. Recent approaches use quadratic penalty terms on the unknown impulse response defined by stable spline kernels, which control model space complexity by leveraging regularity and bounded-input bounded-output stability. This paper extends linear system identification to a wide class of nonsmooth stable spline estimators, where regularization functionals and data misfits can be selected from a rich set of piecewise linear quadratic penalties. This class encompasses the 1-norm, huber, and vapnik, in addition to the least-squares penalty, and the approach allows linear inequality constraints on the unknown impulse response. We develop a customized interior point solver for the entire class of proposed formulations. By representing penalties through their conjugates, we allow a simple interface that enables the user to specify any piecewise linear quadratic penalty for misfit and regularizer, together with inequality constraints on the response. The solver is locally quadratically convergent, with O(n2(m+n)) arithmetic operations per iteration, for n impulse response coefficients and m output measurements. In the system identification context, where n << m, IPsolve is competitive with available alternatives, illustrated by a comparison with TFOCS and libSVM. The modeling framework is illustrated with a range of numerical experiments, featuring robust formulations for contaminated data, relaxation systems, and nonnegativity and unimodality constraints on the impulse response. Incorporating constraints yields significant improvements in system identification. The solver used to obtain the results is distributed via an open source code repository. "
707026370984071169,2016-03-08 02:13:17,https://t.co/5kIsVrELHL,Manifold Matching using Shortest-Path Distance and Joint Neighborhood Selection. (arXiv:1412.4098v3 [stat.ML] UPDA… https://t.co/5kIsVrELHL,0,4," Abstract: We propose a nonlinear manifold matching algorithm to match multiple data sets using shortest-path distance and joint neighborhood selection. Based on the correspondence information, a neighborhood graph is jointly constructed; then the shortest-path distance within each data set is computed from the joint neighborhood graph, followed by embedding into and matching in a common low-dimensional Euclidean space. Our approach exhibits superior and robust performance for matching data from disparate sources, compared to algorithms that do not use shortest-path distance or joint neighborhood selection. "
707026369226645504,2016-03-08 02:13:16,https://t.co/VdZjETiKui,"Less is More: Nystr\""om Computational Regularization. (arXiv:1507.04717v5 [stat.ML] UPDATED) https://t.co/VdZjETiKui",0,4," Abstract: We study Nystr\""om type subsampling approaches to large scale kernel methods, and prove learning bounds in the statistical learning setting, where random sampling and high probability estimates are considered. In particular, we prove that these approaches can achieve optimal learning bounds, provided the subsampling level is suitably chosen. These results suggest a simple incremental variant of Nystr\""om Kernel Regularized Least Squares, where the subsampling level implements a form of computational regularization, in the sense that it controls at the same time regularization and computations. Extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets. "
707026367154622464,2016-03-08 02:13:16,https://t.co/dxgN77AegE,Perceptron like Algorithms for Online Learning to Rank. (arXiv:1508.00842v2 [cs.LG] UPDATED) https://t.co/dxgN77AegE,0,3," Abstract: Perceptron is a classic online algorithm for learning a classification function. In this paper, we provide a novel extension of the perceptron algorithm to the learning to rank problem in information retrieval. We consider popular listwise performance measures such as Normalized Discounted Cumulative Gain (NDCG) and Average Precision (AP). A modern perspective on perceptron for classification is that it is simply an instance of online gradient descent (OGD), during mistake rounds, using the hinge loss function. Motivated by this interpretation, we propose a novel family of listwise, large margin ranking surrogates. Members of this family can be thought of as analogs of the hinge loss. Exploiting a certain self-bounding property of the proposed family, we provide a guarantee on the cumulative NDCG (or AP) induced loss incurred by our perceptron-like algorithm. We show that, if there exists a perfect oracle ranker which can correctly rank each instance in an online sequence of ranking data, with some margin, the cumulative loss of perceptron algorithm on that sequence is bounded by a constant, irrespective of the length of the sequence. This result is reminiscent of Novikoff's convergence theorem for the classification perceptron. Moreover, we prove a lower bound on the cumulative loss achievable by any deterministic algorithm, under the assumption of existence of perfect oracle ranker. The lower bound shows that our perceptron bound is not tight, and we propose another, \emph{purely online}, algorithm which achieves the lower bound. We provide empirical results on simulated and large commercial datasets to corroborate our theoretical results. "
707026364885549057,2016-03-08 02:13:15,https://t.co/pIaoVAogPf,Feature Selection via Binary Simultaneous Perturbation Stochastic Approximation. (arXiv:1508.07630v3 [stat.ML] UPD… https://t.co/pIaoVAogPf,0,7," Abstract: Feature selection (FS) has become an indispensable task in dealing with today's highly complex pattern recognition problems with massive number of features. In this study, we propose a new wrapper approach for FS based on binary simultaneous perturbation stochastic approximation (BSPSA). This pseudo-gradient descent stochastic algorithm starts with an initial feature vector and moves toward the optimal feature vector via successive iterations. In each iteration, the current feature vector's individual components are perturbed simultaneously by random offsets from a qualified probability distribution. We present computational experiments on datasets with numbers of features ranging from a few dozens to thousands using three widely-used classifiers as wrappers: nearest neighbor, decision tree, and linear support vector machine. We compare our methodology against the full set of features as well as a binary genetic algorithm and sequential FS methods using cross-validated classification error rate and AUC as the performance criteria. Our results indicate that features selected by BSPSA compare favorably to alternative methods in general and BSPSA can yield superior feature sets for datasets with tens of thousands of features by examining an extremely small fraction of the solution space. We are not aware of any other wrapper FS methods that are computationally feasible with good convergence properties for such large datasets. "
707026362691928068,2016-03-08 02:13:15,https://t.co/2Iqr1Wzf7h,Quantifying the information of the prior and likelihood in parametric Bayesian modeling. (arXiv:1511.01214v7 [stat… https://t.co/2Iqr1Wzf7h,1,5," Abstract: I suggest using a pair of metrics to quantify the information of the prior and likelihood functions within a parametric Bayesian model, one of which is closely related to the reference priors of Berger and Bernardo (Bernardo 1979, Berger and Bernardo 2009) and information measure introduced by Lindley (Lindley 1956). A Monte Carlo algorithm to estimate these metrics is developed and their properties are explored via a combination of theoretical results, simulations, and applications to public medical data sets. This combination of theoretical, empirical, and computational support provides evidence that these metrics may be useful diagnostic tools when performing a Bayesian analysis. "
707026360481488896,2016-03-08 02:13:14,https://t.co/k67DXRECwa,Efficient Multiscale Gaussian Process Regression using Hierarchical Clustering. (arXiv:1511.02258v2 [cs.LG] UPDATE… https://t.co/k67DXRECwa,1,5," Abstract: Standard Gaussian Process (GP) regression, a powerful machine learning tool, is computationally expensive when it is applied to large datasets, and potentially inaccurate when data points are sparsely distributed in a high-dimensional feature space. To address these challenges, a new multiscale, sparsified GP algorithm is formulated, with the goal of application to large scientific computing datasets. In this approach, the data is partitioned into clusters and the cluster centers are used to define a reduced training set, resulting in an improvement over standard GPs in terms of training and evaluation costs. Further, a hierarchical technique is used to adaptively map the local covariance representation to the underlying sparsity of the feature space, leading to improved prediction accuracy when the data distribution is highly non-uniform. A theoretical investigation of the computational complexity of the algorithm is presented. The efficacy of this method is then demonstrated on smooth and discontinuous analytical functions and on data from a direct numerical simulation of turbulent combustion. "
707026358472450048,2016-03-08 02:13:14,https://t.co/Sn7GhthFWV,A Latent-Variable Grid Model. (arXiv:1512.07587v4 [cs.LG] UPDATED) https://t.co/Sn7GhthFWV,0,4," Abstract: Markov random field (MRF) learning is intractable, and its approximation algorithms are computationally expensive. We target a small subset of MRF that is used frequently in computer vision. We characterize this subset with three concepts: Lattice, Homogeneity, and Inertia; and design a non-markov model as an alternative. Our goal is robust learning from small datasets. Our learning algorithm uses vector quantization and, at time complexity O(U log U) for a dataset of U pixels, is much faster than that of general-purpose MRF. "
707026356543086592,2016-03-08 02:13:13,https://t.co/QJQdUailQQ,Feature Selection for Regression Problems Based on the Morisita Estimator of Intrinsic Dimension. (arXiv:1602.0021… https://t.co/QJQdUailQQ,0,3," Abstract: Data acquisition, storage and management have been improved, while the key factors of many phenomena are not well known. Consequently, irrelevant and redundant features artificially increase the size of datasets, which complicates learning tasks, such as regression. To address this problem, feature selection methods have been proposed. This paper introduces a new supervised filter based on the Morisita estimator of intrinsic dimension. It can identify relevant features and distinguish between redundant and irrelevant information. Besides, it offers a clear graphical representation of the results and it can be easily implemented in different programming languages. Comprehensive numerical experiments are conducted using simulated datasets characterized by different levels of complexity, sample size and noise. The suggested algorithm is also successfully tested on a selection of real world applications and compared with RReliefF using extreme learning machine. In addition, a new measure of feature relevance is presented and discussed. "
707026354324316160,2016-03-08 02:13:13,https://t.co/F9mRZgmY3t,Fundamental differences between Dropout and Weight Decay in Deep Networks. (arXiv:1602.04484v2 [cs.LG] UPDATED) https://t.co/F9mRZgmY3t,0,13," Abstract: We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear, and that dropout is insensitive to various re-scalings of the input features, outputs, and network weights. This last insensitivity implies that there are no isolated local minima of the dropout training criterion. Our work uncovers new properties of dropout, extends our understanding of why dropout succeeds, and lays the foundation for further progress. "
707026351992250368,2016-03-08 02:13:12,https://t.co/DhhfSN00eV,Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling Techniques. (arXiv:1602.06516v2 [cs.LG] UPDA… https://t.co/DhhfSN00eV,0,5," Abstract: Graph partitioning plays a central role in machine learning, and the development of graph partitioning algorithms is still an active area of research. The immense demand for such algorithms arises due to the abundance of applications that involve pairwise interactions or similarities among entities. Recent studies in computer vision and databases systems have emphasized on the necessity of considering multi-way interactions, and has led to the study of a more general problem in the form of hypergraph partitioning. This paper focuses on the problem of partitioning uniform hypergraphs, which arises in applications such as subspace clustering, motion segmentation etc. We show that uniform hypergraph partitioning is equivalent to a tensor trace maximization problem, and hence, a tensor based method is a natural answer to this problem. We also propose a tensor spectral method that extends the widely known spectral clustering algorithm to the case of uniform hypergraphs. While the theoretical guarantees of spectral clustering have been extensively studied, very little is known about the statistical properties of tensor based methods. To this end, we prove the consistency of the proposed algorithm under a planted partition model. The computational complexity of tensorial approaches has resulted in the use of various tensor sampling strategies. We present the first theoretical study on the effect of sampling in tensor based hypergraph partitioning. Our result justifies the empirical success of iterative sampling techniques often used in practice. We also present an iteratively sampled variant of the proposed algorithm for the purpose of subspace clustering, and demonstrate the performance of this method on a benchmark problem. "
706655887822364673,2016-03-07 01:41:07,https://t.co/S4q6q1lgnI,Learning deep representation of multityped objects and tasks. (arXiv:1603.01359v1 [stat.ML]) https://t.co/S4q6q1lgnI,3,4," Abstract: We introduce a deep multitask architecture to integrate multityped representations of multimodal objects. This multitype exposition is less abstract than the multimodal characterization, but more machine-friendly, and thus is more precise to model. For example, an image can be described by multiple visual views, which can be in the forms of bag-of-words (counts) or color/texture histograms (real-valued). At the same time, the image may have several social tags, which are best described using a sparse binary vector. Our deep model takes as input multiple type-specific features, narrows the cross-modality semantic gaps, learns cross-type correlation, and produces a high-level homogeneous representation. At the same time, the model supports heterogeneously typed tasks. We demonstrate the capacity of the model on two applications: social image retrieval and multiple concept prediction. The deep architecture produces more compact representation, naturally integrates multiviews and multimodalities, exploits better side information, and most importantly, performs competitively against baselines. "
706655886803148800,2016-03-07 01:41:07,https://t.co/d0TbDpt9hd,A Unified View of Localized Kernel Learning. (arXiv:1603.01374v1 [cs.LG]) https://t.co/d0TbDpt9hd,2,4," Abstract: Multiple Kernel Learning, or MKL, extends (kernelized) SVM by attempting to learn not only a classifier/regressor but also the best kernel for the training task, usually from a combination of existing kernel functions. Most MKL methods seek the combined kernel that performs best over every training example, sacrificing performance in some areas to seek a global optimum. Localized kernel learning (LKL) overcomes this limitation by allowing the training algorithm to match a component kernel to the examples that can exploit it best. Several approaches to the localized kernel learning problem have been explored in the last several years. We unify many of these approaches under one simple system and design a new algorithm with improved performance. We also develop enhanced versions of existing algorithms, with an eye on scalability and performance. "
706655885528006656,2016-03-07 01:41:06,https://t.co/x9YAXENubB,Lasso estimation for GEFCom2014 probabilistic electric load forecasting. (arXiv:1603.01376v1 [stat.AP]) https://t.co/x9YAXENubB,0,1," Abstract: We present a methodology for probabilistic load forecasting that is based on lasso (least absolute shrinkage and selection operator) estimation. The model considered can be regarded as a bivariate time-varying threshold autoregressive(AR) process for the hourly electric load and temperature. The joint modeling approach incorporates the temperature effects directly, and reflects daily, weekly, and annual seasonal patterns and public holiday effects. We provide two empirical studies, one based on the probabilistic load forecasting track of the Global Energy Forecasting Competition 2014 (GEFCom2014-L), and the other based on another recent probabilistic load forecasting competition that follows a setup similar to that of GEFCom2014-L. In both empirical case studies, the proposed methodology outperforms two multiple linear regression based benchmarks from among the top eight entries to GEFCom2014-L. "
706655884257132544,2016-03-07 01:41:06,https://t.co/37LGJ6YEYW,Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks. (arXiv:1… https://t.co/37LGJ6YEYW,2,3," Abstract: While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- Internal Covariate Shift-- the current solution has certain drawbacks. Specifically, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate for validation due to shifting parameter values (especially during initial training epochs). Also, BN cannot be used with batch-size 1 during training. We address these drawbacks by proposing a non-adaptive normalization technique for removing internal covariate shift, that we call Normalization Propagation. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers. "
706655882541735937,2016-03-07 01:41:06,https://t.co/7SVyM3BlZb,X-rank and identifiability for a polynomial decomposition model. (arXiv:1603.01566v1 [cs.IT]) https://t.co/7SVyM3BlZb,1,2," Abstract: In this paper, we study a polynomial decomposition model that arises in problems of system identification, signal processing and machine learning. We show that this decomposition is a special case of the X-rank decomposition --- a powerful novel concept in algebraic geometry that generalizes the tensor CP decomposition. We prove new results on generic/maximal rank and on identifiability of the polynomial decomposition model. In the paper, we try to make results and basic tools accessible for a general audience (assuming no knowledge of algebraic geometry or its prerequisites). "
706655881451192322,2016-03-07 01:41:05,https://t.co/Un2jEHOYlC,Integrated Sequence Tagging for Medieval Latin Using Deep Representation Learning. (arXiv:1603.01597v1 [cs.CL]) https://t.co/Un2jEHOYlC,1,3," Abstract: In this paper we consider two sequence tagging tasks for medieval Latin: part-of-speech tagging and lemmatization. These are both basic, yet foundational preprocessing steps in applications such as text re-use detection. Nevertheless, they are generally complicated by the considerable orthographic variation which is typical of medieval Latin. In Digital Classics, these tasks are traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion. For example, a lexicon is used to generate all the potential lemma-tag pairs for a token, and next, a context-aware PoS-tagger is used to select the most appropriate tag-lemma pair. Apart from the problems with out-of-lexicon items, error percolation is a major downside of such approaches. In this paper we explore the possibility to elegantly solve these tasks using a single, integrated approach. For this, we make use of a layered neural network architecture from the field of deep representation learning. "
706655879769268224,2016-03-07 01:41:05,https://t.co/j8QUdbE5dk,Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines. (arXiv:1508.00451v4 [st… https://t.co/j8QUdbE5dk,1,2," Abstract: Tackling pattern recognition problems in areas such as computer vision, bioinformatics, speech or text recognition is often done best by taking into account task-specific statistical relations between output variables. In structured prediction, this internal structure is used to predict multiple outputs simultaneously, leading to more accurate and coherent predictions. Structural support vector machines (SSVMs) are nonprobabilistic models that optimize a joint input-output function through margin-based learning. Because SSVMs generally disregard the interplay between unary and interaction factors during the training phase, final parameters are suboptimal. Moreover, its factors are often restricted to linear combinations of input features, limiting its generalization power. To improve prediction accuracy, this paper proposes: (i) Joint inference and learning by integration of back-propagation and loss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM factors to neural networks that form highly nonlinear functions of input features. Image segmentation benchmark results demonstrate improvements over conventional SSVM training methods in terms of accuracy, highlighting the feasibility of end-to-end SSVM training with neural factors. "
706655878729093120,2016-03-07 01:41:05,https://t.co/YeIKOzAKHP,Clustering by Hierarchical Nearest Neighbor Descent (H-NND). (arXiv:1509.02805v3 [stat.ML] UPDATED) https://t.co/YeIKOzAKHP,0,3," Abstract: Previously in 2014, we proposed the Nearest Descent (ND) method, capable of generating an efficient Graph, called the in-tree (IT). Due to some beautiful and effective features, this IT structure proves well suited for data clustering. Although there exist some redundant edges in IT, they usually have salient features and thus it is not hard to remove them. Subsequently, in order to prevent the seemingly redundant edges from occurring, we proposed the Nearest Neighbor Descent (NND) by adding the ""Neighborhood"" constraint on ND. Consequently, clusters automatically emerged, without the additional requirement of removing the redundant edges. However, NND proved still not perfect, since it brought in a new yet worse problem, the ""over-partitioning"" problem. Now, in this paper, we propose a method, called the Hierarchical Nearest Neighbor Descent (H-NND), which overcomes the over-partitioning problem of NND via using the hierarchical strategy. Specifically, H-NND uses ND to effectively merge the over-segmented sub-graphs or clusters that NND produces. Like ND, H-NND also generates the IT structure, in which the redundant edges once again appear. This seemingly comes back to the situation that ND faces. However, compared with ND, the redundant edges in the IT structure generated by H-NND generally become more salient, thus being much easier and more reliable to be identified even by the simplest edge-removing method which takes the edge length as the only measure. In other words, the IT structure constructed by H-NND becomes more fitted for data clustering. We prove this on several clustering datasets of varying shapes, dimensions and attributes. Besides, compared with ND, H-NND generally takes less computation time to construct the IT data structure for the input data. "
706655876313120768,2016-03-07 01:41:04,https://t.co/8UxgN3wfeO,Variational Gaussian Process. (arXiv:1511.06499v2 [stat.ML] UPDATED) https://t.co/8UxgN3wfeO,1,8," Abstract: Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We develop the variational Gaussian process (VGP), a Bayesian nonparametric variational family, which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by auto-encoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW. "
706655875197444097,2016-03-07 01:41:04,https://t.co/uFsa4flLGi],Variational Inference: A Review for Statisticians. (arXiv:1601.00670v2 [https://t.co/uFsa4flLGi] UPDATED) https://t.co/EWMJ6a17re,2,14,INDEXERROR
706655873851133956,2016-03-07 01:41:04,https://t.co/OzzdZalIpm,Statistical Foundation of Spectral Graph Theory. (arXiv:1602.03861v2 [math.ST] UPDATED) https://t.co/OzzdZalIpm,2,12," Abstract: The goal of this paper is to show that there exists a simple, yet universal statistical logic of spectral graph analysis by recasting it into a nonparametric function estimation problem. The prescribed viewpoint appears to be good enough to accommodate most of the existing spectral graph techniques as a consequence of just one single formalism and algorithm. "
706655872416686080,2016-03-07 01:41:03,https://t.co/0VPyIKdZPw,Gradient Descent Converges to Minimizers. (arXiv:1602.04915v2 [stat.ML] UPDATED) https://t.co/0VPyIKdZPw,0,6," Abstract: We show that gradient descent converges to a local minimizer, almost surely with random initialization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory. "
705567726304169984,2016-03-04 01:37:09,https://t.co/MBoOmFg2cp,A Kernel Test for Three-Variable Interactions with Random Processes. (arXiv:1603.00929v1 [stat.ML]) https://t.co/MBoOmFg2cp,1,2," Abstract: We apply a wild bootstrap method to the Lancaster three-variable interaction measure in order to detect factorisation of the joint distribution on three variables forming a stationary random process, for which the existing permutation bootstrap method fails. As in the i.i.d. case, the Lancaster test is found to outperform existing tests in cases for which two independent variables individually have a weak influence on a third, but that when considered jointly the influence is strong. The main contributions of this paper are twofold: first, we prove that the Lancaster statistic satisfies the conditions required to estimate the quantiles of the null distribution using the wild bootstrap; second, the manner in which this is proved is novel, simpler than existing methods, and can further be applied to other statistics. "
705567724710305792,2016-03-04 01:37:09,https://t.co/xoiz2t0UVx,Sparse model selection in the highly under-sampled regime. (arXiv:1603.00952v1 [stat.ML]) https://t.co/xoiz2t0UVx,0,3," Abstract: We propose a method for recovering the structure of a sparse undirected graphical model when very few samples are available. The method decides about the presence or absence of bonds between pairs of variable by considering one pair at a time and using a closed form formula, analytically derived by calculating the posterior probability for every possible model explaining a two body system using Jeffreys prior. The approach does not rely on the optimisation of any cost functions and consequently is much faster than existing algorithms. Despite this time and computational advantage, numerical results show that for several sparse topologies the algorithm is comparable to the best existing algorithms, and is more accurate in the presence of hidden variables. We apply this approach to the analysis of US stock market data and to neural data, in order to show its efficiency in recovering robust statistical dependencies in real data with non stationary correlations in time and space. "
705567723254898688,2016-03-04 01:37:08,https://t.co/id6KVR5zCz,Training Input-Output Recurrent Neural Networks through Spectral Methods. (arXiv:1603.00954v1 [cs.LG]) https://t.co/id6KVR5zCz,1,5," Abstract: We consider the problem of training input-output recurrent neural networks (RNN) for sequence labeling tasks. We propose a novel spectral approach for learning the network parameters. It is based on decomposition of the cross-moment tensor between the output and a non-linear transformation of the input, based on score functions. We guarantee consistent learning with polynomial sample and computational complexity under transparent conditions such as non-degeneracy of model parameters, polynomial activations for the neurons, and a Markovian evolution of the input sequence. We also extend our results to Bidirectional RNN which uses both previous and future information to output the label at each time point, and is employed in many NLP tasks such as POS tagging. "
705567721208094720,2016-03-04 01:37:08,https://t.co/IEdzHP2lth,Whitening-Free Least-Squares Non-Gaussian Component Analysis. (arXiv:1603.01029v1 [stat.ML]) https://t.co/IEdzHP2lth,0,4," Abstract: Non-Gaussian component analysis (NGCA) is an unsupervised linear dimension reduction method that extracts low-dimensional non-Gaussian ""signals"" from high-dimensional data contaminated with Gaussian noise. NGCA can be regarded as a generalization of projection pursuit (PP) and independent component analysis (ICA) to multi-dimensional and dependent non-Gaussian components. Indeed, seminal approaches to NGCA are based on PP and ICA. Recently, a novel NGCA approach called least-squares NGCA (LSNGCA) has been developed, which gives a solution analytically through least-squares estimation of log-density gradients and eigendecomposition. However, since pre-whitening of data is involved in LSNGCA, it performs unreliably when the data covariance matrix is ill-conditioned, which is often the case in high-dimensional data analysis. In this paper, we propose a whitening-free LSNGCA method and experimentally demonstrate its superiority. "
705567718754394112,2016-03-04 01:37:07,https://t.co/a3yihQRCZ2,Overdispersed Black-Box Variational Inference. (arXiv:1603.01140v1 [stat.ML]) https://t.co/a3yihQRCZ2,0,3," Abstract: We introduce overdispersed black-box variational inference, a method to reduce the variance of the Monte Carlo estimator of the gradient in black-box variational inference. Instead of taking samples from the variational distribution, we use importance sampling to take samples from an overdispersed distribution in the same exponential family as the variational approximation. Our approach is general since it can be readily applied to any exponential family distribution, which is the typical choice for the variational approximation. We run experiments on two non-conjugate probabilistic models to show that our method effectively reduces the variance, and the overhead introduced by the computation of the proposal parameters and the importance weights is negligible. We find that our overdispersed importance sampling scheme provides lower variance than black-box variational inference, even when the latter uses twice the number of samples. This results in faster convergence of the black-box inference procedure. "
705567716829237249,2016-03-04 01:37:07,https://t.co/Ais6K1fSzy,Sparse Precision Matrix Selection for Fitting Gaussian Random Field Models to Large Data Sets. (arXiv:1405.5576v4 … https://t.co/Ais6K1fSzy,0,3," Abstract: Iterative methods for fitting a Gaussian Random Field (GRF) model to spatial data via maximum likelihood (ML) require $\mathcal{O}(n^3)$ floating point operations per iteration, where $n$ denotes the number of data locations. For large data sets, the $\mathcal{O}(n^3)$ complexity per iteration together with the non-convexity of the ML problem render traditional ML methods inefficient for GRF fitting. The problem is even more aggravated for anisotropic GRFs where the number of covariance function parameters increases with the process domain dimension. In this paper, we propose a new two-step GRF estimation procedure when the process is second-order stationary. First, a \emph{convex} likelihood problem regularized with a weighted $\ell_1$-norm, utilizing the available distance information between observation locations, is solved to fit a sparse \emph{{precision} (inverse covariance) matrix to the observed data using the Alternating Direction Method of Multipliers. Second, the parameters of the GRF spatial covariance function are estimated by solving a least squares problem. Theoretical error bounds for the proposed estimator are provided; moreover, convergence of the estimator is shown as the number of samples per location increases. The proposed method is numerically compared with state-of-the-art methods for big $n$. Data segmentation schemes are implemented to handle large data sets. "
705567715520618497,2016-03-04 01:37:06,https://t.co/DksRPkS93t,Covariate-assisted spectral clustering. (arXiv:1411.2158v4 [stat.ML] UPDATED) https://t.co/DksRPkS93t,1,3," Abstract: Biological and social systems consist of myriad interacting units. The interactions can be represented in the form of a graph or network. Measurements of these graphs can reveal the underlying structure of these interactions, which provides insight into the systems that generated the graphs. Moreover, in applications such as connectomics, social networks, and genomics, graph data are accompanied by contextualizing measures on each node. We utilize these node covariates to help uncover latent communities in a graph, using a modification of spectral clustering. Statistical guarantees are provided under a joint mixture model that we call the node-contextualized stochastic blockmodel, including a bound on the mis-clustering rate. The bound is used to derive conditions for achieving perfect clustering. For most simulated cases, covariate-assisted spectral clustering yields results superior to regularized spectral clustering without node covariates and to an adaptation of canonical correlation analysis. We apply our clustering method to large brain graphs derived from diffusion MRI data, using the node locations or neurological region membership as covariates. In both cases, covariate-assisted spectral clustering yields clusters that are easier to interpret neurologically. "
705567713930903552,2016-03-04 01:37:06,https://t.co/uW85lfoVC0,Convex Relaxation Regression: Black-Box Optimization of Smooth Functions by Learning Their Convex Envelopes. (arXi… https://t.co/uW85lfoVC0,0,8," Abstract: Finding efficient and provable methods to solve non-convex optimization problems is an outstanding challenge in machine learning and optimization theory. A popular approach used to tackle non-convex problems is to use convex relaxation techniques to find a convex surrogate for the problem. Unfortunately, convex relaxations typically must be found on a problem-by-problem basis. Thus, providing a general-purpose strategy to estimate a convex relaxation would have a wide reaching impact. Here, we introduce Convex Relaxation Regression (CoRR), an approach for learning convex relaxations for a class of smooth functions. The main idea behind our approach is to estimate the convex envelope of a function $f$ by evaluating $f$ at a set of $T$ random points and then fitting a convex function to these function evaluations. We prove that with probability greater than $1-\delta$, the solution of our algorithm converges to the global optimizer of $f$ with error $\mathcal{O} \Big( \big(\frac{\log(1/\delta) }{T} \big)^{\alpha} \Big)$ for some $\alpha> 0$. Our approach enables the use of convex optimization tools to solve a class of non-convex optimization problems. "
705206872983343104,2016-03-03 01:43:15,https://t.co/3nKvzMW6eC,LOFS: Library of Online Streaming Feature Selection. (arXiv:1603.00531v1 [cs.LG]) https://t.co/3nKvzMW6eC,0,1," Abstract: As an emerging research direction, online streaming feature selection deals with sequentially added dimensions in a feature space while the number of data instances is fixed. Online streaming feature selection provides a new, complementary algorithmic methodology to enrich online feature selection, especially targets to high dimensionality in big data analytics. This paper introduces the first comprehensive open-source library for use in MATLAB that implements the state-of-the-art algorithms of online streaming feature selection. The library is designed to facilitate the development of new algorithms in this exciting research direction and make comparisons between the new methods and existing ones available. "
705206871779577856,2016-03-03 01:43:14,https://t.co/lwDqC8jUW4,Asymptotic behavior of $\ell_p$-based Laplacian regularization in semi-supervised learning. (arXiv:1603.00564v1 [c… https://t.co/lwDqC8jUW4,0,2," Abstract: Given a weighted graph with $N$ vertices, consider a real-valued regression problem in a semi-supervised setting, where one observes $n$ labeled vertices, and the task is to label the remaining ones. We present a theoretical study of $\ell_p$-based Laplacian regularization under a $d$-dimensional geometric random graph model. We provide a variational characterization of the performance of this regularized learner as $N$ grows to infinity while $n$ stays constant, the associated optimality conditions lead to a partial differential equation that must be satisfied by the associated function estimate $\hat{f}$. From this formulation we derive several predictions on the limiting behavior the $d$-dimensional function $\hat{f}$, including (a) a phase transition in its smoothness at the threshold $p = d + 1$, and (b) a tradeoff between smoothness and sensitivity to the underlying unlabeled data distribution $P$. Thus, over the range $p \leq d$, the function estimate $\hat{f}$ is degenerate and ""spiky,"" whereas for $p\geq d+1$, the function estimate $\hat{f}$ is smooth. We show that the effect of the underlying density vanishes monotonically with $p$, such that in the limit $p = \infty$, corresponding to the so-called Absolutely Minimal Lipschitz Extension, the estimate $\hat{f}$ is independent of the distribution $P$. Under the assumption of semi-supervised smoothness, ignoring $P$ can lead to poor statistical performance, in particular, we construct a specific example for $d=1$ to demonstrate that $p=2$ has lower risk than $p=\infty$ due to the former penalty adapting to $P$ and the latter ignoring it. We also provide simulations that verify the accuracy of our predictions for finite sample sizes. Together, these properties show that $p = d+1$ is an optimal choice, yielding a function estimate $\hat{f}$ that is both smooth and non-degenerate, while remaining maximally sensitive to $P$. "
705206870609412096,2016-03-03 01:43:14,https://t.co/OLDjGNDU2V,Without-Replacement Sampling for Stochastic Gradient Methods: Convergence Results and Application to Distributed O… https://t.co/OLDjGNDU2V,2,2," Abstract: Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled \emph{with} replacement. In practice, however, sampling \emph{without} replacement is very common, easier to implement in many cases, and often performs better. In this paper, we provide competitive convergence guarantees for without-replacement sampling, under various scenarios, for three types of algorithms: Any algorithm with online regret guarantees, stochastic gradient descent, and SVRG. A useful application of our SVRG analysis is a nearly-optimal algorithm for regularized least squares in a distributed setting, in terms of both communication complexity and runtime complexity, when the data is randomly partitioned and the condition number can be as large as the data size per machine (up to logarithmic factors). Our proof techniques combine ideas from stochastic optimization, adversarial online learning, and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems. "
705206869225316352,2016-03-03 01:43:14,https://t.co/t1KoPt6Vvh,Automatic Differentiation Variational Inference. (arXiv:1603.00788v1 [stat.ML]) https://t.co/t1KoPt6Vvh,2,8," Abstract: Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference (ADVI). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models-no conjugacy assumptions are required. We study ADVI across ten different models and apply it to a dataset with millions of observations. ADVI is integrated into Stan, a probabilistic programming system; it is available for immediate use. "
705206867702771712,2016-03-03 01:43:14,https://t.co/ouMUL9iKEO,Character-based Neural Machine Translation. (arXiv:1603.00810v1 [cs.CL]) https://t.co/ouMUL9iKEO,0,2," Abstract: Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task. "
705206866666721280,2016-03-03 01:43:13,https://t.co/Nv5b2fWSvL,Molecular Graph Convolutions: Moving Beyond Fingerprints. (arXiv:1603.00856v1 [stat.ML]) https://t.co/Nv5b2fWSvL,1,0," Abstract: Molecular ""fingerprints"" encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular ""graph convolutions"", a machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph---atoms, bonds, distances, etc.---which allows the model to take greater advantage of information in the graph structure. Although graph convolutions do not outperform all fingerprint-based methods, they (along with other graph-based methods) represent a new paradigm in ligand-based virtual screening with exciting opportunities for future improvement. "
705206865580441600,2016-03-03 01:43:13,https://t.co/AbU1sSHuNJ,Why Regularized Auto-Encoders learn Sparse Representation?. (arXiv:1505.05561v3 [stat.ML] UPDATED) https://t.co/AbU1sSHuNJ,0,3," Abstract: While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- \textit{Internal Covariate Shift}-- the current solution has certain drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (especially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size $ 1 $ during training. We address these drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call \textit{Normalization Propagation}. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers. "
705206864389271553,2016-03-03 01:43:13,https://t.co/bCFHHwVCDl,Sequential Nonparametric Testing with the Law of the Iterated Logarithm. (arXiv:1506.03486v2 [stat.ML] UPDATED) https://t.co/bCFHHwVCDl,1,4," Abstract: We propose a new algorithmic framework for sequential hypothesis testing with i.i.d. data, which includes A/B testing, nonparametric two-sample testing, and independence testing as special cases. It is novel in several ways: (a) it takes linear time and constant space to compute on the fly, (b) it has the same power guarantee as a non-sequential version of the test with the same computational constraints up to a small factor, and (c) it accesses only as many samples as are required - its stopping time adapts to the unknown difficulty of the problem. All our test statistics are constructed to be zero-mean martingales under the null hypothesis, and the rejection threshold is governed by a uniform non-asymptotic law of the iterated logarithm (LIL). For the case of nonparametric two-sample mean testing, we also provide a finite sample power analysis, and the first non-asymptotic stopping time calculations for this class of problems. We verify our predictions for type I and II errors and stopping times using simulations. "
705206863277723648,2016-03-03 01:43:12,https://t.co/dZbSa5e5mp,Bayesian representation learning with oracle constraints. (arXiv:1506.05011v4 [stat.ML] UPDATED) https://t.co/dZbSa5e5mp,0,2," Abstract: Representation learning systems typically rely on massive amounts of labeled data in order to be trained to high accuracy. Recently, high-dimensional parametric models like neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Thus, \emph{oracles} or \emph{human-in-the-loop systems}, for example crowdsourcing, are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. In this work we propose to combine \emph{generative unsupervised feature learning} with a \emph{probabilistic treatment of oracle information like triplets} in order to transfer implicit privileged oracle knowledge into explicit nonlinear Bayesian latent factor models of the observations. We use a fast variational algorithm to learn the joint model and demonstrate applicability to a well-known image dataset. We show how implicit triplet information can provide rich information to learn representations that outperform previous metric learning approaches as well as generative models without this side-information in a variety of predictive tasks. In addition, we illustrate that the proposed approach compartmentalizes the latent spaces semantically which allows interpretation of the latent variables. "
705206862162034688,2016-03-03 01:43:12,https://t.co/M35fyAMgW8,Optimal approximate matrix product in terms of stable rank. (arXiv:1507.02268v3 [cs.DS] UPDATED) https://t.co/M35fyAMgW8,0,3," Abstract: We prove, using the subspace embedding guarantee in a black box way, that one can achieve the spectral norm guarantee for approximate matrix multiplication with a dimensionality-reducing map having $m = O(\tilde{r}/\varepsilon^2)$ rows. Here $\tilde{r}$ is the maximum stable rank, i.e. squared ratio of Frobenius and operator norms, of the two matrices being multiplied. This is a quantitative improvement over previous work of [MZ11, KVZ14], and is also optimal for any oblivious dimensionality-reducing map. Furthermore, due to the black box reliance on the subspace embedding property in our proofs, our theorem can be applied to a much more general class of sketching matrices than what was known before, in addition to achieving better bounds. For example, one can apply our theorem to efficient subspace embeddings such as the Subsampled Randomized Hadamard Transform or sparse subspace embeddings, or even with subspace embedding constructions that may be developed in the future. Our main theorem, via connections with spectral error matrix multiplication shown in prior work, implies quantitative improvements for approximate least squares regression and low rank approximation. Our main result has also already been applied to improve dimensionality reduction guarantees for $k$-means clustering [CEMMP14], and implies new results for nonparametric regression [YPW15]. We also separately point out that the proof of the ""BSS"" deterministic row-sampling result of [BSS12] can be modified to show that for any matrices $A, B$ of stable rank at most $\tilde{r}$, one can achieve the spectral norm guarantee for approximate matrix multiplication of $A^T B$ by deterministically sampling $O(\tilde{r}/\varepsilon^2)$ rows that can be found in polynomial time. The original result of [BSS12] was for rank instead of stable rank. Our observation leads to a stronger version of a main theorem of [KMST10]. "
705206860987691008,2016-03-03 01:43:12,https://t.co/SXkFzHrqmM,Structured Prediction: From Gaussian Perturbations to Linear-Time Principled Algorithms. (arXiv:1508.00945v3 [stat… https://t.co/SXkFzHrqmM,1,3," Abstract: Margin-based structured prediction commonly uses a maximum loss over all possible structured outputs \cite{Altun03,Collins04b,Taskar03}. In natural language processing, recent work \cite{Zhang14,Zhang15} has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution. This method is linear-time in the number of random structured outputs and trivially parallelizable. We study this family of loss functions in the PAC-Bayes framework under Gaussian perturbations \cite{McAllester07}. Under some technical conditions and up to statistical accuracy, we show that this family of loss functions produces a tighter upper bound of the Gibbs decoder distortion than commonly used methods. Thus, using the maximum loss over random structured outputs is a principled way of learning the parameter of structured prediction models. Besides explaining the experimental success of \cite{Zhang14,Zhang15}, our theoretical results show that more general techniques are possible. "
705206859846828032,2016-03-03 01:43:12,https://t.co/lWupl7bd5d,WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet Allocation. (arXiv:1510.08628v2 [stat.ML] UPDATED) https://t.co/lWupl7bd5d,1,3," Abstract: Developing efficient and scalable algorithms for Latent Dirichlet Allocation (LDA) is of wide interest for many applications. Previous work has developed an O(1) Metropolis-Hastings sampling method for each token. However, the performance is far from being optimal due to random accesses to the parameter matrices and frequent cache misses. In this paper, we first carefully analyze the memory access efficiency of existing algorithms for LDA by the scope of random access, which is the size of the memory region in which random accesses fall, within a short period of time. We then develop WarpLDA, an LDA sampler which achieves both the best O(1) time complexity per token and the best O(K) scope of random access. Our empirical results in a wide range of testing conditions demonstrate that WarpLDA is consistently 5-15x faster than the state-of-the-art Metropolis-Hastings based LightLDA, and is comparable or faster than the sparsity aware F+LDA. With WarpLDA, users can learn up to one million topics from hundreds of millions of documents in a few hours, at an unprecedentedly throughput of 11G tokens per second. "
705206858496217088,2016-03-03 01:43:11,https://t.co/goDPDqHTiu,Streaming regularization parameter selection via stochastic gradient descent. (arXiv:1511.02187v2 [stat.ML] UPDATE… https://t.co/goDPDqHTiu,1,2," Abstract: We propose a framework to perform streaming covariance selection. Our approach employs regularization constraints where a time-varying sparsity parameter is iteratively estimated via stochastic gradient descent. This allows for the regularization parameter to be efficiently learnt in an online manner. The proposed framework is developed for linear regression models and extended to graphical models via neighbourhood selection. Under mild assumptions, we are able to obtain convergence results in a non-stochastic setting. The capabilities of such an approach are demonstrated using both synthetic data as well as neuroimaging data. "
705206857342783489,2016-03-03 01:43:11,https://t.co/fnu7I4vMQd,Model-based Dashboards for Customer Analytics. (arXiv:1511.05614v3 [stat.AP] UPDATED) https://t.co/fnu7I4vMQd,0,1," Abstract: Automating the customer analytics process is crucial for companies that manage distinct customer bases. In such data-rich and dynamic environments, visualization plays a key role in understanding events of interest. These ideas have led to the popularity of analytics dashboards, yet academic research has paid scant attention to these managerial needs. We develop a probabilistic, nonparametric framework for understanding and predicting individual-level spending using Gaussian process priors over latent functions that describe customer spending along calendar time, interpurchase time, and customer lifetime dimensions. These curves form a dashboard that provides a visual model-based representation of purchasing dynamics that is easily comprehensible. The model flexibly and automatically captures the form and duration of the impact of events that influence spend propensity, even when such events are unknown a-priori. We illustrate the use of our Gaussian Process Propensity Model (GPPM) on data from two popular mobile games. We show that the GPPM generalizes hazard and buy-till-you-die models by incorporating calendar time dynamics while simultaneously accounting for recency and lifetime effects. It therefore provides insights about spending propensity beyond those available from these models. Finally, we show that the GPPM outperforms these benchmarks both in fitting and forecasting real and simulated spend data. "
705206856159993856,2016-03-03 01:43:11,https://t.co/R8Q8VD7GVO,Metric Learning with Adaptive Density Discrimination. (arXiv:1511.05939v2 [stat.ML] UPDATED) https://t.co/R8Q8VD7GVO,1,5," Abstract: Distance metric learning (DML) approaches learn a transformation to a representation space where distance is in correspondence with a predefined notion of similarity. While such models offer a number of compelling benefits, it has been difficult for these to compete with modern classification algorithms in performance and even in feature extraction. In this work, we propose a novel approach explicitly designed to address a number of subtle yet important issues which have stymied earlier DML algorithms. It maintains an explicit model of the distributions of the different classes in representation space. It then employs this knowledge to adaptively assess similarity, and achieve local discrimination by penalizing class distribution overlap. We demonstrate the effectiveness of this idea on several tasks. Our approach achieves state-of-the-art classification results on a number of fine-grained visual recognition datasets, surpassing the standard softmax classifier and outperforming triplet loss by a relative margin of 30-40%. In terms of computational performance, it alleviates training inefficiencies in the traditional triplet loss, reaching the same error in 5-30 times fewer iterations. Beyond classification, we further validate the saliency of the learnt representations via their attribute concentration and hierarchy recovery properties, achieving 10-25% relative gains on the softmax classifier and 25-50% on triplet loss in these tasks. "
705206854956261376,2016-03-03 01:43:10,https://t.co/RFnamU17FF,Multimodal sparse representation learning and applications. (arXiv:1511.06238v3 [cs.LG] UPDATED) https://t.co/RFnamU17FF,0,5," Abstract: Unsupervised methods have proven effective for discriminative tasks in a single-modality scenario. In this paper, we present a multimodal framework for learning sparse representations that can capture semantic correlation between modalities. The framework can model relationships at a higher level by forcing the shared sparse representation. In particular, we propose the use of joint dictionary learning technique for sparse coding and formulate the joint representation for concision, cross-modal representations (in case of a missing modality), and union of the cross-modal representations. Given the accelerated growth of multimodal data posted on the Web such as YouTube, Wikipedia, and Twitter, learning good multimodal features is becoming increasingly important. We show that the shared representations enabled by our framework substantially improve the classification performance under both unimodal and multimodal settings. We further show how deep architectures built on the proposed framework are effective for the case of highly nonlinear correlations between modalities. The effectiveness of our approach is demonstrated experimentally in image denoising, multimedia event detection and retrieval on the TRECVID dataset (audio-video), category classification on the Wikipedia dataset (image-text), and sentiment classification on PhotoTweet (image-text). "
705206853630824448,2016-03-03 01:43:10,https://t.co/Nfr8IXIVQK,Regularizing RNNs by Stabilizing Activations. (arXiv:1511.08400v6 [cs.NE] UPDATED) https://t.co/Nfr8IXIVQK,0,1," Abstract: We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms. This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modeling and phoneme recognition, and outperforming weight noise and dropout. We achieve competitive performance (18.6\% PER) on the TIMIT phoneme recognition task for RNNs evaluated without beam search or an RNN transducer. With this penalty term, IRNN can achieve similar performance to LSTM on language modeling, although adding the penalty term to the LSTM results in superior performance. Our penalty term also prevents the exponential growth of IRNN's activations outside of their training horizon, allowing them to generalize to much longer sequences. "
705206850871017472,2016-03-03 01:43:10,https://t.co/pNPfiLeVzM,Parallel Bayesian Global Optimization of Expensive Functions. (arXiv:1602.05149v2 [stat.ML] UPDATED) https://t.co/pNPfiLeVzM,2,5," Abstract: We consider parallel global optimization of derivative-free expensive-to-evaluate functions, and propose an efficient method based on stochastic approximation for implementing a conceptual Bayesian optimization algorithm proposed by Ginsbourger et al. (2007). To accomplish this, we use infinitessimal perturbation analysis (IPA) to construct a stochastic gradient estimator and show that this estimator is unbiased. We also show that the stochastic gradient ascent algorithm using the constructed gradient estimator converges to a stationary point of the q-EI surface, and therefore, as the number of multiple starts of the gradient ascent algorithm and the number of steps for each start grow large, the one-step Bayes optimal set of points is recovered. We show in numerical experiments that our method for maximizing the q-EI is faster than methods based on closed-form evaluation using high-dimensional integration, when considering many parallel function evaluations, and is comparable in speed when considering few. We also show that the resulting one-step Bayes optimal algorithm for parallel global optimization finds high quality solutions with fewer evaluations that a heuristic based on approximately maximizing the q-EI. A high quality open source implementation of this algorithm is available in the open source Metrics Optimization Engine (MOE). "
704852365120282625,2016-03-02 02:14:34,https://t.co/e5uObYLUTw,Designing Domain Specific Word Embeddings: Applications to Disease Surveillance. (arXiv:1603.00106v1 [cs.LG]) https://t.co/e5uObYLUTw,0,3," Abstract: Traditional disease surveillance can be augmented with a wide variety of real-time sources such as, news and social media. However, these sources are in general unstructured and, construction of surveillance tools such as taxonomical correlations and trace mapping involves considerable human supervision. In this paper, we motivate a disease vocabulary driven word2vec model (Dis2Vec) to model diseases and constituent attributes as word embeddings from the HealthMap news corpus. We use these word embeddings to automatically create disease taxonomies and evaluate our model against corresponding human annotated taxonomies. We compare our model accuracies against several state-of-the art word2vec methods. Our results demonstrate that Dis2Vec outperforms traditional distributed vector representations in its ability to faithfully capture taxonomical attributes across different class of diseases such as endemic, emerging and rare. "
704852363971051520,2016-03-02 02:14:33,https://t.co/UWan0NRSsL,Dual Smoothing and Level Set Techniques for Variational Matrix Decomposition. (arXiv:1603.00284v1 [stat.ML]) https://t.co/UWan0NRSsL,0,1," Abstract: We focus on the robust principal component analysis (RPCA) problem, and review a range of old and new convex formulations for the problem and its variants. We then review dual smoothing and level set techniques in convex optimization, present several novel theoretical results, and apply the techniques on the RPCA problem. In the final sections, we show a range of numerical experiments for simulated and real-world problems. "
704852362628829184,2016-03-02 02:14:33,https://t.co/DdsbDFICbC,Kernel-based Tests for Joint Independence. (arXiv:1603.00285v1 [math.ST]) https://t.co/DdsbDFICbC,2,2," Abstract: We investigate the problem of testing whether $d$ random variables, which may or may not be continuous, are jointly (or mutually) independent. Our method builds on ideas of the two variable Hilbert-Schmidt independence criterion (HSIC) but allows for an arbitrary number of variables. We embed the $d$-dimensional joint distribution and the product of the marginals into a reproducing kernel Hilbert space and define the $d$-variable Hilbert-Schmidt independence criterion (dHSIC) as the squared distance between the embeddings. In the population case, the value of dHSIC is zero if and only if the $d$ variables are jointly independent, as long as the kernel is characteristic. Based on an empirical estimate of dHSIC, we define three different non-parametric hypothesis tests: a permutation test, a bootstrap test and a test based on a Gamma approximation. We prove that the permutation test achieves the significance level and that the bootstrap test achieves pointwise asymptotic significance level as well as pointwise asymptotic consistency (i.e., it is able to detect any type of fixed dependence in the large sample limit). The Gamma approximation does not come with these guarantees; however, it is computationally very fast and for small $d$, it performs well in practice. Finally, we apply the test to a problem in causal discovery. "
704852361609658368,2016-03-02 02:14:33,https://t.co/fhkQnfLa9Y,Multi-Information Source Optimization with General Model Discrepancies. (arXiv:1603.00389v1 [stat.ML]) https://t.co/fhkQnfLa9Y,0,1," Abstract: We consider Bayesian optimization of an expensive-to-evaluate black-box objective function, where we also have access to cheaper approximations of the objective. In general, such approximations arise in applications such as reinforcement learning, engineering, and the natural sciences, and are subject to an inherent, unknown bias. This model discrepancy is caused by an inadequate internal model that deviates from reality and can vary over the domain, making the utilization of these approximations a non-trivial task. We present a novel algorithm that provides a rigorous mathematical treatment of the uncertainties arising from model discrepancies and noisy observations. Its optimization decisions rely on a value of information analysis that extends the Knowledge Gradient factor to the setting of multiple information sources that vary in cost: each sampling decision maximizes the predicted benefit per unit cost. We conduct an experimental evaluation that demonstrates that the method consistently outperforms other state-of-the-art techniques: it finds designs of considerably higher objective value and additionally inflicts less cost in the exploration process. "
704852360426889216,2016-03-02 02:14:32,https://t.co/7l8iQbfadX,Noisy Activation Functions. (arXiv:1603.00391v1 [cs.LG]) https://t.co/7l8iQbfadX,0,3," Abstract: Common nonlinear activation functions used in neural networks can cause training difficulties due to the saturation behavior of the activation function, which may hide dependencies that are not visible to vanilla-SGD (using first order gradients only). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that the gradients may flow easily, even if the noiseless application of the activation function would yield zero gradient. Large noise will dominate the noise-free gradient and allow stochastic gradient descent toexplore more. By adding noise only to the problematic parts of the activation function, we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by noisy variants helps training in many contexts, yielding state-of-the-art or competitive results on different datasets and task, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results. "
704852359164338176,2016-03-02 02:14:32,https://t.co/3T5pz7NxG2,Neural Programmer: Inducing Latent Programs with Gradient Descent. (arXiv:1511.04834v2 [cs.LG] UPDATED) https://t.co/3T5pz7NxG2,0,4," Abstract: Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy. "
704852357868290048,2016-03-02 02:14:32,https://t.co/s6lE4MHj13,Reducing Overfitting in Deep Networks by Decorrelating Representations. (arXiv:1511.06068v3 [cs.LG] UPDATED) https://t.co/s6lE4MHj13,0,7," Abstract: One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization. Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout. "
704852356538744832,2016-03-02 02:14:31,https://t.co/xFHRaysT0L,Multi-task Sequence to Sequence Learning. (arXiv:1511.06114v4 [cs.LG] UPDATED) https://t.co/xFHRaysT0L,0,3," Abstract: Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought. "
704852355532120065,2016-03-02 02:14:31,https://t.co/pa7nGHQWYQ,Variational Auto-encoded Deep Gaussian Processes. (arXiv:1511.06455v2 [cs.LG] UPDATED) https://t.co/pa7nGHQWYQ,1,11, Abstract: We develop a scalable deep non-parametric generative model by augmenting deep Gaussian processes with a recognition model. Inference is performed in a novel scalable variational framework where the variational posterior distributions are reparametrized through a multilayer perceptron. The key aspect of this reformulation is that it prevents the proliferation of variational parameters which otherwise grow linearly in proportion to the sample size. We derive a new formulation of the variational lower bound that allows us to distribute most of the computation in a way that enables to handle datasets of the size of mainstream deep learning tasks. We show the efficacy of the method on a variety of challenges including deep unsupervised learning and deep Bayesian optimization. 
704852354156384256,2016-03-02 02:14:31,https://t.co/viiKh8GHCP,Cluster-Seeking James-Stein Estimators. (arXiv:1602.00542v2 [cs.IT] UPDATED) https://t.co/viiKh8GHCP,0,2," Abstract: This paper considers the problem of estimating a high-dimensional vector of parameters $\boldsymbol{\theta} \in \mathbb{R}^n$ from a noisy observation. The noise vector is i.i.d. Gaussian with known variance. For a squared-error loss function, the James-Stein (JS) estimator is known to dominate the simple maximum-likelihood (ML) estimator when the dimension $n$ exceeds two. The JS-estimator shrinks the observed vector towards the origin, and the risk reduction over the ML-estimator is greatest for $\boldsymbol{\theta}$ that lie close to the origin. JS-estimators can be generalized to shrink the data towards any target subspace. Such estimators also dominate the ML-estimator, but the risk reduction is significant only when $\boldsymbol{\theta}$ lies close to the subspace. This leads to the question: in the absence of prior information about $\boldsymbol{\theta}$, how do we design estimators that give significant risk reduction over the ML-estimator for a wide range of $\boldsymbol{\theta}$? In this paper, we propose shrinkage estimators that attempt to infer the structure of $\boldsymbol{\theta}$ from the observed data in order to construct a good attracting subspace. In particular, the components of the observed vector are separated into clusters, and the elements in each cluster shrunk towards a common attractor. The number of clusters and the attractor for each cluster are determined from the observed vector. We provide concentration results for the squared-error loss and convergence results for the risk of the proposed estimators. The results show that the estimators give significant risk reduction over the ML-estimator for a wide range of $\boldsymbol{\theta}$, particularly for large $n$. Simulation results are provided to support the theoretical claims. "
704852352583536640,2016-03-02 02:14:31,https://t.co/Q0EuiRgLzu,Contextual-MDPs for PAC-Reinforcement Learning with Rich Observations. (arXiv:1602.02722v2 [cs.LG] UPDATED) https://t.co/Q0EuiRgLzu,2,4," Abstract: We propose and study a new model for reinforcement learning with rich observations, generalizing contextual bandits to sequential decision making. These models require an agent to take actions based on observations (features) with the goal of achieving long-term performance competitive with a large set of policies. To avoid barriers to sample-efficient learning associated with large observation spaces and general POMDPs, we focus on problems that can be summarized by a small number of hidden states and have long-term rewards that are predictable by a reactive function class. In this setting, we design and analyze a new reinforcement learning algorithm, Least Squares Value Elimination by Exploration. We prove that the algorithm learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. Our result provides theoretical justification for reinforcement learning with function approximation. "
704852351530704897,2016-03-02 02:14:30,https://t.co/2hIirLg7Ts,Herding as a Learning System with Edge-of-Chaos Dynamics. (arXiv:1602.03014v2 [stat.ML] UPDATED) https://t.co/2hIirLg7Ts,0,2," Abstract: Herding defines a deterministic dynamical system at the edge of chaos. It generates a sequence of model states and parameters by alternating parameter perturbations with state maximizations, where the sequence of states can be interpreted as ""samples"" from an associated MRF model. Herding differs from maximum likelihood estimation in that the sequence of parameters does not converge to a fixed point and differs from an MCMC posterior sampling approach in that the sequence of states is generated deterministically. Herding may be interpreted as a""perturb and map"" method where the parameter perturbations are generated using a deterministic nonlinear dynamical system rather than randomly from a Gumbel distribution. This chapter studies the distinct statistical characteristics of the herding algorithm and shows that the fast convergence rate of the controlled moments may be attributed to edge of chaos dynamics. The herding algorithm can also be generalized to models with latent variables and to a discriminative learning setting. The perceptron cycling theorem ensures that the fast moment matching property is preserved in the more general framework. "
704852350180184065,2016-03-02 02:14:30,https://t.co/IZucbeUygj,Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data. (arXiv:1602.05875v2 [stat.ML] U… https://t.co/IZucbeUygj,0,8," Abstract: Traditional convolutional layers extract features from patches of data by applying a non-linearity on an affine function of the input. We propose a model that enhances this feature extraction process for the case of sequential data, by feeding patches of the data into a recurrent neural network and using the outputs or hidden states of the recurrent units to compute the extracted features. By doing so, we exploit the fact that a window containing a few frames of the sequential data is a sequence itself and this additional structure might encapsulate valuable information. In addition, we allow for more steps of computation in the feature extraction process, which is potentially beneficial as an affine function followed by a non-linearity can result in too simple features. Using our convolutional recurrent layers we obtain an improvement in performance in two audio classification tasks, compared to traditional convolutional layers. "
704485632828174336,2016-03-01 01:57:18,https://t.co/If2691atLj,On the entropy numbers of the mixed smoothness function classes. (arXiv:1602.08712v1 [math.NA]) https://t.co/If2691atLj,1,4," Abstract: Behavior of the entropy numbers of classes of multivariate functions with mixed smoothness is studied here. This problem has a long history and some fundamental problems in the area are still open. The main goal of this paper is to develop a new method of proving the upper bounds for the entropy numbers. This method is based on recent developments of nonlinear approximation, in particular, on greedy approximation. This method consists of the following two steps strategy. At the first step we obtain bounds of the best m-term approximations with respect to a dictionary. At the second step we use general inequalities relating the entropy numbers to the best m-term approximations. For the lower bounds we use the volume estimates method, which is a well known powerful method for proving the lower bounds for the entropy numbers. It was used in a number of previous papers. "
704485631620096006,2016-03-01 01:57:17,https://t.co/xKtczwTdaZ,A Structured Variational Auto-encoder for Learning Deep Hierarchies of Sparse Features. (arXiv:1602.08734v1 [stat.… https://t.co/xKtczwTdaZ,0,4," Abstract: In this note we present a generative model of natural images consisting of a deep hierarchy of layers of latent random variables, each of which follows a new type of distribution that we call rectified Gaussian. These rectified Gaussian units allow spike-and-slab type sparsity, while retaining the differentiability necessary for efficient stochastic gradient variational inference. To learn the parameters of the new model, we approximate the posterior of the latent variables with a variational auto-encoder. Rather than making the usual mean-field assumption however, the encoder parameterizes a new type of structured variational approximation that retains the prior dependencies of the generative model. Using this structured posterior approximation, we are able to perform joint training of deep models with many layers of latent random variables, without having to resort to stacking or other layerwise training procedures. "
704485630424838144,2016-03-01 01:57:17,https://t.co/uf2fhfhbU7,Stability and Structural Properties of Gene Regulation Networks with Coregulation Rules. (arXiv:1602.08753v1 [stat… https://t.co/uf2fhfhbU7,0,2," Abstract: Coregulation of the expression of groups of genes has been extensively demonstrated empirically in bacterial and eukaryotic systems. Such coregulation can arise through the use of shared regulatory motifs, which allow the coordinated expression of modules (and module groups) of functionally related genes across the genome. Coregulation can also arise through the physical association of multi-gene complexes through chromosomal looping, which are then transcribed together. We present a general formalism for modeling coregulation rules in the framework of Random Boolean Networks (RBN), and develop specific models for transcription factor networks with modular structure (including module groups, and multi-input modules (MIM) with autoregulation) and multi-gene complexes (including hierarchical differentiation between multi-gene complex members). We develop a mean-field approach to analyse the stability of large networks incorporating coregulation, and show that autoregulated MIM and hierarchical gene-complex models can achieve greater stability than networks without coregulation whose rules have matching activation frequency. We provide further analysis of the stability of small networks of both kinds through simulations. We also characterize several general properties of the transients and attractors in the hierarchical coregulation model, and show using simulations that the steady-state distribution factorizes hierarchically as a Bayesian network in a Markov Jump Process analogue of the RBN model. "
704485629619531777,2016-03-01 01:57:17,https://t.co/P4iPIKd3Kn,Structured Prediction with Test-time Budget Constraints. (arXiv:1602.08761v1 [stat.ML]) https://t.co/P4iPIKd3Kn,0,2," Abstract: We study the problem of structured prediction under test-time budget constraints. We propose a novel approach applicable to a wide range of structured prediction problems in computer vision and natural language processing. Our approach seeks to adaptively generate computationally costly features during test-time in order to reduce the computational cost of prediction while maintaining prediction performance. We show that training the adaptive feature generation system can be reduced to a series of structured learning problems, resulting in efficient training using existing structured learning algorithms. This framework provides theoretical justification for several existing heuristic approaches found in literature. We evaluate our proposed adaptive system on two structured prediction tasks, optical character recognition (OCR) and dependency parsing and show strong performance in reduction of the feature costs without degrading accuracy. "
704485628780670976,2016-03-01 01:57:17,https://t.co/IQyyo2N3TV,"Investigating practical, linear temporal difference learning. (arXiv:1602.08771v1 [cs.LG]) https://t.co/IQyyo2N3TV",0,5," Abstract: Off-policy reinforcement learning has many applications including: learning from demonstration, learning multiple goal seeking policies in parallel, and representing predictive knowledge. Recently there has been an proliferation of new policy-evaluation algorithms that fill a longstanding algorithmic void in reinforcement learning: combining robustness to off-policy sampling, function approximation, linear complexity, and temporal difference (TD) updates. This paper contains two main contributions. First, we derive two new hybrid TD policy-evaluation algorithms, which fill a gap in this collection of algorithms. Second, we perform an empirical comparison to elicit which of these new linear TD methods should be preferred in different situations, and make concrete suggestions about practical use. "
704485627836960769,2016-03-01 01:57:17,https://t.co/JkVbdAPQo2,Does quantification without adjustments work?. (arXiv:1602.08780v1 [stat.ML]) https://t.co/JkVbdAPQo2,0,2," Abstract: Classification is the task of predicting the class labels of objects based on the observation of their features. In contrast, quantification has been defined as the task of determining the prevalences of the different sorts of class labels in a target dataset. The simplest approach to quantification is Classify & Count where a classifier is optimised for classification on a training set and applied to the target dataset for the prediction of class labels. In the case of binary quantification, the number of predicted positive labels is then used as an estimate of the prevalence of the positive class in the target dataset. Since the performance of Classify & Count for quantification is known to be inferior its results typically are subject to adjustments. However, some researchers recently have suggested that Classify & Count might actually work without adjustments if it is based on a classifer that was specifically trained for quantification. We discuss the theoretical foundation for this claim and explore its potential and limitations with a numerical example based on the binormal model with equal variances. In order to identify an optimal quantifier in the binormal setting, we introduce the concept of local Bayes optimality. As a side remark, we present a complete proof of a theorem by Ye et al. (2012). "
704485626788368384,2016-03-01 01:57:16,https://t.co/baeQzSs7WD,Stochastic bandits on a social network: Collaborative learning with local information sharing. (arXiv:1602.08886v1… https://t.co/baeQzSs7WD,0,2," Abstract: We consider a collaborative online learning paradigm, wherein a group of agents connected through a social network are engaged in playing a stochastic multi-armed bandit game. Each time an agent takes an action, the corresponding reward is instantaneously observed by the agent, as well as its neighbours in the social network. We perform a regret analysis of various policies in this collaborative learning setting. A key finding of this paper is that natural extensions of widely-studied single agent learning policies to the network setting need not perform well in terms of regret. In particular, we identify a class of non-altruistic and individually consistent policies, and argue by deriving regret lower bounds that they are liable to suffer a large regret in the networked setting. We also show that the learning performance can be substantially improved if the agents exploit the structure of the network, and develop a simple learning algorithm based on dominating sets of the network. Specifically, we first consider a star network, which is a common motif in hierarchical social networks, and show analytically that the hub agent can be used as an information sink to expedite learning and improve the overall regret. We also derive networkwide regret bounds for the algorithm applied to general networks. We conduct numerical experiments on a variety of networks to corroborate our analytical results. "
704485625744007168,2016-03-01 01:57:16,https://t.co/FtvbyD5M5S,$L_2$Boosting in High-Dimensions: Rate of Convergence. (arXiv:1602.08927v1 [stat.ML]) https://t.co/FtvbyD5M5S,0,3," Abstract: Boosting is one of the most significant developments in machine learning. This paper studies the rate of convergence of $L_2$Boosting, which is tailored for regression, in a high-dimensional setting. Moreover, we introduce so-called \textquotedblleft post-Boosting\textquotedblright. This is a post-selection estimator which applies ordinary least squares to the variables selected in the first stage by $L_2$Boosting. Another variant is \textquotedblleft Orthogonal Boosting\textquotedblright\ where after each step an orthogonal projection is conducted. We show that both post-$L_2$Boosting and the orthogonal boosting achieve the same rate of convergence as LASSO in a sparse, high-dimensional setting. We show that the rate of convergence of the classical $L_2$Boosting depends on the design matrix described by a sparse eigenvalue constant. To show the latter results, we derive new approximation results for the pure greedy algorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also introduce feasible rules for early stopping, which can be easily implemented and used in applied work. Our results also allow a direct comparison between LASSO and boosting which has been missing from the literature. Finally, we present simulation studies and applications to illustrate the relevance of our theoretical results and to provide insights into the practical aspects of boosting. In these simulation studies, post-$L_2$Boosting clearly outperforms LASSO. "
704485624858976258,2016-03-01 01:57:16,https://t.co/x8mQazVgeI,Beyond CCA: Moment Matching for Multi-View Models. (arXiv:1602.09013v1 [stat.ML]) https://t.co/x8mQazVgeI,0,3," Abstract: We introduce three novel semi-parametric extensions of probabilistic canonical correlation analysis with identifiability guarantees. We consider moment matching techniques for estimation in these models. For that, by drawing explicit links between the new models and a discrete version of independent component analysis (DICA), we first extend the DICA cumulant tensors to the new discrete version of CCA. By further using a close connection with independent component analysis, we introduce generalized covariance matrices, which can replace the cumulant tensors in the moment matching framework, and, therefore, improve sample complexity and simplify derivations and algorithms significantly. As the tensor power method or orthogonal joint diagonalization are not applicable in the new setting, we use non-orthogonal joint diagonalization techniques for matching the cumulants. We demonstrate performance of the proposed models and estimation techniques on experiments with both synthetic and real datasets. "
704485624024309760,2016-03-01 01:57:16,https://t.co/oNP2Cm8UDy,Easy Monotonic Policy Iteration. (arXiv:1602.09118v1 [cs.LG]) https://t.co/oNP2Cm8UDy,0,3," Abstract: A key problem in reinforcement learning for control with general function approximators (such as deep neural networks and other nonlinear functions) is that, for many algorithms employed in practice, updates to the policy or $Q$-function may fail to improve performance---or worse, actually cause the policy performance to degrade. Prior work has addressed this for policy iteration by deriving tight policy improvement bounds; by optimizing the lower bound on policy improvement, a better policy is guaranteed. However, existing approaches suffer from bounds that are hard to optimize in practice because they include sup norm terms which cannot be efficiently estimated or differentiated. In this work, we derive a better policy improvement bound where the sup norm of the policy divergence has been replaced with an average divergence; this leads to an algorithm, Easy Monotonic Policy Iteration, that generates sequences of policies with guaranteed non-decreasing returns and is easy to implement in a sample-based framework. "
704485622979891200,2016-03-01 01:57:15,https://t.co/J7liTKgtgT,A Primal Dual Active Set Algorithm for a Class of Nonconvex Sparsity Optimization. (arXiv:1310.1147v3 [math.OC] UP… https://t.co/J7liTKgtgT,0,3," Abstract: In this paper, we consider the problem of recovering a sparse vector from noisy measurement data. Traditionally, it is formulated as a penalized least-squares problem with an $\ell^1$ penalty. Recent studies show that nonconvex penalties, e.g., $\ell^0$ and bridge, allow more effective sparse recovery. We develop an algorithm of primal dual active set type for a class of nonconvex sparsity-promoting penalties, which cover $\ell^0$, bridge, smoothly clipped absolute deviation, capped $\ell^1$ and minimax concavity penalty. First we establish the existence of a global minimizer for the class of optimization problems. Then we derive a novel necessary optimality condition for the global minimizer using the associated thresholding operator. The solutions to the optimality system are coordinate-wise minimizers, and under minor conditions, they are also local minimizers. Upon introducing the dual variable, the active set can be determined from the primal and dual variables. This relation lends itself to an iterative algorithm of active set type which at each step involves updating the primal variable only on the active set and then updating the dual variable explicitly. When combined with a continuation strategy on the regularization parameter, it has a global convergence property under the restricted isometry property. Extensive numerical experiments demonstrate its efficiency and accuracy. "
704485621964873728,2016-03-01 01:57:15,https://t.co/CsJ0SPfoO8,Distributional Smoothing with Virtual Adversarial Training. (arXiv:1507.00677v8 [stat.ML] UPDATED) https://t.co/CsJ0SPfoO8,0,2," Abstract: We propose local distributional smoothness (LDS), a new notion of smoothness for statistical model that can be used as a regularization term to promote the smoothness of the model distribution. We named the LDS based regularization as virtual adversarial training (VAT). The LDS of a model at an input datapoint is defined as the KL-divergence based robustness of the model distribution against local perturbation around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semi-supervised learning. The computational cost for VAT is relatively low. For neural network, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. When we applied our technique to supervised and semi-supervised learning for the MNIST dataset, it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We also applied our method to SVHN and NORB, and confirmed our method's superior performance over the current state of the art semi-supervised method applied to these datasets. "
704485621025382401,2016-03-01 01:57:15,https://t.co/WZWyN1VK3n,Continuous control with deep reinforcement learning. (arXiv:1509.02971v5 [cs.LG] UPDATED) https://t.co/WZWyN1VK3n,0,6," Abstract: We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs. "
704485620157194240,2016-03-01 01:57:15,https://t.co/pk7Di6DYAn,A 'Gibbs-Newton' Technique for Enhanced Inference of Multivariate Polya Parameters and Topic Models. (arXiv:1510.0… https://t.co/pk7Di6DYAn,0,2," Abstract: Hyper-parameters play a major role in the learning and inference process of latent Dirichlet allocation (LDA). In order to begin the LDA latent variables learning process, these hyper-parameters values need to be pre-determined. We propose an extension for LDA that we call 'Latent Dirichlet allocation Gibbs Newton' (LDA-GN), which places non-informative priors over these hyper-parameters and uses Gibbs sampling to learn appropriate values for them. At the heart of LDA-GN is our proposed 'Gibbs-Newton' algorithm, which is a new technique for learning the parameters of multivariate Polya distributions. We report Gibbs-Newton performance results compared with two prominent existing approaches to the latter task: Minka's fixed-point iteration method and the Moments method. We then evaluate LDA-GN in two ways: (i) by comparing it with standard LDA in terms of the ability of the resulting topic models to generalize to unseen documents; (ii) by comparing it with standard LDA in its performance on a binary classification task. "
704485619234435072,2016-03-01 01:57:14,https://t.co/idd67QLZNM,Statistical physics of inference: Thresholds and algorithms. (arXiv:1511.02476v2 [cond-mat.stat-mech] UPDATED) https://t.co/idd67QLZNM,1,7," Abstract: Many questions of fundamental interest in todays science can be formulated as inference problems: Some partial, or noisy, observations are performed over a set of variables and the goal is to recover, or infer, the values of the variables based on the indirect information contained in the measurements. For such problems, the central scientific questions are: Under what conditions is the information contained in the measurements sufficient for a satisfactory inference to be possible? What are the most efficient algorithms for this task? A growing body of work has shown that often we can understand and locate these fundamental barriers by thinking of them as phase transitions in the sense of statistical physics. Moreover, it turned out that we can use the gained physical insight to develop new promising algorithms. Connection between inference and statistical physics is currently witnessing an impressive renaissance and we review here the current state-of-the-art, with a pedagogical focus on the Ising model which formulated as an inference problem we call the planted spin glass. In terms of applications we review two classes of problems: (i) inference of clusters on graphs and networks, with community detection as a special case and (ii) estimating a signal from its noisy linear measurements, with compressed sensing as a case of sparse estimation. Our goal is to provide a pedagogical review for researchers in physics and other fields interested in this fascinating topic. "
704485618315825153,2016-03-01 01:57:14,https://t.co/4Ck8h26pLG,An Exploration of Softmax Alternatives Belonging to the Spherical Loss Family. (arXiv:1511.05042v3 [cs.NE] UPDATED) https://t.co/4Ck8h26pLG,0,3," Abstract: In a multi-class classification problem, it is standard to model the output of a neural network as a categorical distribution conditioned on the inputs. The output must therefore be positive and sum to one, which is traditionally enforced by a softmax. This probabilistic mapping allows to use the maximum likelihood principle, which leads to the well-known log-softmax loss. However the choice of the softmax function seems somehow arbitrary as there are many other possible normalizing functions. It is thus unclear why the log-softmax loss would perform better than other loss alternatives. In particular Vincent et al. (2015) recently introduced a class of loss functions, called the spherical family, for which there exists an efficient algorithm to compute the updates of the output weights irrespective of the output size. In this paper, we explore several loss functions from this family as possible alternatives to the traditional log-softmax. In particular, we focus our investigation on spherical bounds of the log-softmax loss and on two spherical log-likelihood losses, namely the log-Spherical Softmax suggested by Vincent et al. (2015) and the log-Taylor Softmax that we introduce. Although these alternatives do not yield as good results as the log-softmax loss on two language modeling tasks, they surprisingly outperform it in our experiments on MNIST and CIFAR-10, suggesting that they might be relevant in a broad range of applications. "
704485617346990080,2016-03-01 01:57:14,https://t.co/q2xP86hbrA,Deep multi-scale video prediction beyond mean square error. (arXiv:1511.05440v6 [cs.LG] UPDATED) https://t.co/q2xP86hbrA,0,1," Abstract: Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset "
704485616373915649,2016-03-01 01:57:14,https://t.co/XiAPCHNenq,SparkNet: Training Deep Networks in Spark. (arXiv:1511.06051v4 [stat.ML] UPDATED) https://t.co/XiAPCHNenq,1,6," Abstract: Training deep networks is a time-consuming process, with networks for object recognition often requiring multiple days to train. For this reason, leveraging the resources of a cluster to speed up training is an important area of work. However, widely-popular batch-processing computational frameworks like MapReduce and Spark were not designed to support the asynchronous and communication-intensive workloads of existing distributed deep learning systems. We introduce SparkNet, a framework for training deep networks in Spark. Our implementation includes a convenient interface for reading data from Spark RDDs, a Scala interface to the Caffe deep learning framework, and a lightweight multi-dimensional tensor library. Using a simple parallelization scheme for stochastic gradient descent, SparkNet scales well with the cluster size and tolerates very high-latency communication. Furthermore, it is easy to deploy and use with no parameter tuning, and it is compatible with existing Caffe models. We quantify the dependence of the speedup obtained by SparkNet on the number of machines, the communication frequency, and the cluster's communication overhead, and we benchmark our system's performance on the ImageNet dataset. "
704485615254020097,2016-03-01 01:57:14,https://t.co/hozN19rjfL,Estimating the number of unseen species: How far can one foresee?. (arXiv:1511.07428v2 [math.ST] UPDATED) https://t.co/hozN19rjfL,0,3," Abstract: Estimating the number of unseen species is an important problem in many scientific endeavors. Its most popular formulation, introduced by Fisher, uses $n$ samples to predict the number $U$ of hitherto unseen species that would be observed if $t\cdot n$ new samples were collected. Of considerable interest is the largest ratio $t$ between the number of new and existing samples for which $U$ can be accurately predicted. In seminal works, Good and Toulmin constructed an intriguing estimator that predicts $U$ for all $t\le 1$, thereby showing that the number of species can be estimated for a population twice as large as that observed. Subsequently Efron and Thisted obtained a modified estimator that empirically predicts $U$ even for some $t>1$, but without provable guarantees. We derive a class of estimators that $\textit{provably}$ predict $U$ not just for constant $t>1$, but all the way up to $t$ proportional to $\log n$. This shows that the number of species can be estimated for a population $\log n$ times larger than that observed, a factor that grows arbitrarily large as $n$ increases. We also show that this range is the best possible and that the estimators' mean-square error is optimal up to constants for any $t$. Our approach yields the first provable guarantee for the Efron-Thisted estimator and, in addition, a variant which achieves stronger theoretical and experimental performance than existing methodologies on a variety of synthetic and real datasets. The estimators we derive are simple linear estimators that are computable in time proportional to $n$. The performance guarantees hold uniformly for all distributions, and apply to all four standard sampling models commonly used across various scientific disciplines: multinomial, Poisson, hypergeometric, and Bernoulli product. "
704485614150926336,2016-03-01 01:57:13,https://t.co/o59WJAsqpi,RSG: Beating SGD without Smoothness and/or Strong Convexity. (arXiv:1512.03107v6 [math.OC] UPDATED) https://t.co/o59WJAsqpi,1,6," Abstract: In this paper, we study the efficiency of a {\bf R}estarted {\bf S}ub{\bf G}radient (RSG) method that periodically restarts the standard subgradient method (SG). We show that, when applied to a broad class of convex optimization problems, RSG method can find an $\epsilon$-optimal solution with a low complexity than SG method. In particular, we first show that RSG can reduce the dependence of SG's iteration complexity on the distance between the initial solution and the optimal set to that between the $\epsilon$-level set and the optimal set. In addition, we show the advantages of RSG over SG in solving three different families of convex optimization problems. (a) For the problems whose epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with local quadratic growth property, RSG has an $O(\frac{1}{\epsilon}\log(\frac{1}{\epsilon}))$ iteration complexity. (c) For the problems that admit a local Kurdyka-\L ojasiewicz property with a power constant of $\beta\in[0,1)$, RSG has an $O(\frac{1}{\epsilon^{2\beta}}\log(\frac{1}{\epsilon}))$ iteration complexity. On the contrary, with only the standard analysis, the iteration complexity of SG is known to be $O(\frac{1}{\epsilon^2})$ for these three classes of problems. The novelty of our analysis lies at exploiting the lower bound of the first-order optimality residual at the $\epsilon$-level set. It is this novelty that allows us to explore the local properties of functions (e.g., local quadratic growth property, local Kurdyka-\L ojasiewicz property, more generally local error bounds) to develop the improved convergence of RSG. We demonstrate the effectiveness of the proposed algorithms on several machine learning tasks including regression and classification. "
704485613228204036,2016-03-01 01:57:13,https://t.co/resgfLwl8K,Bridging the Gap between Stochastic Gradient MCMC and Stochastic Optimization. (arXiv:1512.07962v2 [stat.ML] UPDAT… https://t.co/resgfLwl8K,4,7," Abstract: Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian analogs to popular stochastic optimization methods; however, this connection is not well studied. We explore this relationship by applying simulated annealing to an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two key components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii) adaptive element-wise momentum weights. The zero-temperature limit gives a novel stochastic optimization method with adaptive element-wise momentum weights, while conventional optimization methods only have a shared, static momentum weight. Under certain assumptions, our theoretical analysis suggests the proposed simulated annealing approach converges close to the global optima. Experiments on several deep neural network models show state-of-the-art results compared to related stochastic optimization algorithms. "
704139091621842944,2016-02-29 03:00:16,https://t.co/8iIKffyZLQ,Learning to Abstain from Binary Prediction. (arXiv:1602.08151v1 [cs.LG]) https://t.co/8iIKffyZLQ,2,4," Abstract: A binary classifier capable of abstaining from making a label prediction has two goals in tension: minimizing errors, and avoiding abstaining unnecessarily often. In this work, we exactly characterize the best achievable tradeoff between these two goals in a general semi-supervised setting, given an ensemble of predictors of varying competence as well as unlabeled data on which we wish to predict or abstain. We give an algorithm for learning a classifier in this setting which trades off its errors with abstentions in a minimax optimal manner, is as efficient as linear learning and prediction, and is demonstrably practical. Our analysis extends to a large class of loss functions and other scenarios, including ensembles comprised of specialists that can themselves abstain. "
704139090602663937,2016-02-29 03:00:16,https://t.co/WQ4U2em6ue,Scalable and Sustainable Deep Learning via Randomized Hashing. (arXiv:1602.08194v1 [stat.ML]) https://t.co/WQ4U2em6ue,6,8," Abstract: Current deep learning architectures are growing larger in order to learn from enormous datasets.These architectures require giant matrix multiplication operations to train millions or billions of parameters during forward and back propagation steps. These operations are very expensive from a computational and energy standpoint. We present a novel technique to reduce the amount of computation needed to train and test deep net-works drastically. Our approach combines recent ideas from adaptive dropouts and randomized hashing for maximum inner product search to select only the nodes with the highest activation efficiently. Our new algorithm for training deep networks reduces the overall computational cost,of both feed-forward pass and backpropagation,by operating on significantly fewer nodes. As a consequence, our algorithm only requires 5% of computations (multiplications) compared to traditional algorithms, without any loss in the accuracy. Furthermore, due to very sparse gradient updates, our algorithm is ideally suited for asynchronous training leading to near linear speedup with increasing parallelism. We demonstrate the scalability and sustainability (energy efficiency) of our proposed algorithm via rigorous experimental evaluations. "
704139089398931457,2016-02-29 03:00:15,https://t.co/XVO7U0ioDR,Learning and Free Energy in Expectation Consistent Approximate Inference. (arXiv:1602.08207v1 [cs.IT]) https://t.co/XVO7U0ioDR,0,1," Abstract: Vector approximate message passing (VAMP) is a computationally simple approach to the recovery of a signal $\mathbf{x}$ from noisy linear measurements $\mathbf{y}=\mathbf{Ax}+\mathbf{w}$. Like the AMP proposed by Donoho, Maleki, and Montanari in 2009, VAMP is characterized by a rigorous state evolution (SE) that holds under certain large random matrices and that matches the replica prediction of optimality. But while AMP's SE holds only for large i.i.d. sub-Gaussian $\mathbf{A}$, VAMP's SE holds under the much larger class: right-rotationally invariant $\mathbf{A}$. To run VAMP, however, one must specify the statistical parameters of the signal and noise. This work combines VAMP with Expectation-Maximization to yield an algorithm, EM-VAMP, that can jointly recover $\mathbf{x}$ while learning those statistical parameters. The fixed points of the proposed EM-VAMP algorithm are shown to be stationary points of a certain constrained free-energy, providing a variational interpretation of the algorithm. Numerical simulations show that EM-VAMP is robust to highly ill-conditioned $\mathbf{A}$ with performance nearly matching oracle-parameter VAMP. "
704139088224452608,2016-02-29 03:00:15,https://t.co/LkGFlnsdvW,Multivariate Hawkes Processes for Large-scale Inference. (arXiv:1602.08418v1 [stat.ML]) https://t.co/LkGFlnsdvW,0,2," Abstract: In this paper, we present a framework for fitting multivariate Hawkes processes for large-scale problems both in the number of events in the observed history $n$ and the number of event types $d$ (i.e. dimensions). The proposed Low-Rank Hawkes Process (LRHP) framework introduces a low-rank approximation of the kernel matrix that allows to perform the nonparametric learning of the $d^2$ triggering kernels using at most $O(ndr^2)$ operations, where $r$ is the rank of the approximation ($r \ll d,n$). This comes as a major improvement to the existing state-of-the-art inference algorithms that are in $O(nd^2)$. Furthermore, the low-rank approximation allows LRHP to learn representative patterns of interaction between event types, which may be valuable for the analysis of such complex processes in real world datasets. The efficiency and scalability of our approach is illustrated with numerical experiments on simulated as well as real datasets. "
704139087092060161,2016-02-29 03:00:15,https://t.co/baSKj8h6Kf,Shape-aware Surface Reconstruction from Sparse Data. (arXiv:1602.08425v1 [cs.CV]) https://t.co/baSKj8h6Kf,0,2," Abstract: The reconstruction of an object's shape or surface from a set of 3D points is a common topic in materials and life sciences, computationally handled in computer graphics. Such points usually stem from optical or tactile 3D coordinate measuring equipment. Surface reconstruction also appears in medical image analysis, e.g. in anatomy reconstruction from tomographic measurements or the alignment of intra-operative navigation and preoperative planning data. In contrast to mere 3D point clouds, medical imaging yields contextual information on the 3D point data that can be used to adopt prior information on the shape that is to be reconstructed from the measurements. In this work we propose to use a statistical shape model (SSM) as a prior for surface reconstruction. The prior knowledge is represented by a point distribution model (PDM) that is associated with a surface mesh. Using the shape distribution that is modelled by the PDM, we reformulate the problem of surface reconstruction from a probabilistic perspective based on a Gaussian Mixture Model (GMM). In order to do so, the given measurements are interpreted as samples of the GMM. By using mixture components with anisotropic covariances that are oriented according to the surface normals at the PDM points, a surface-based fitting is accomplished. By estimating the parameters of the GMM in a maximum a posteriori manner, the reconstruction of the surface from the given measurements is achieved. Extensive experiments suggest that our proposed approach leads to superior surface reconstructions compared to Iterative Closest Point (ICP) methods. "
704119094438662144,2016-02-29 01:40:48,https://t.co/KrTWk74f6t,Sparse Multivariate Factor Regression. (arXiv:1502.07334v4 [stat.ML] UPDATED) https://t.co/KrTWk74f6t,0,6," Abstract: We consider the problem of multivariate regression in a setting where the relevant predictors could be shared among different responses. We propose an algorithm which decomposes the coefficient matrix into the product of a long matrix and a wide matrix, with an elastic net penalty on the former and an $\ell_1$ penalty on the latter. The first matrix linearly transforms the predictors to a set of latent factors, and the second one regresses the responses on these factors. Our algorithm simultaneously performs dimension reduction and coefficient estimation and automatically estimates the number of latent factors from the data. Our formulation results in a non-convex optimization problem, which despite its flexibility to impose effective low-dimensional structure, is difficult, or even impossible, to solve exactly in a reasonable time. We specify an optimization algorithm based on alternating minimization with three different sets of updates to solve this non-convex problem and provide theoretical results on its convergence and optimality. Finally, we demonstrate the effectiveness of our algorithm via experiments on simulated and real data. "
704119093289492488,2016-02-29 01:40:48,https://t.co/uFsa4flLGi],Layered Adaptive Importance Sampling. (arXiv:1505.04732v3 [https://t.co/uFsa4flLGi] UPDATED) https://t.co/QjWpdlCBBk,0,2,INDEXERROR
704119092282834945,2016-02-29 01:40:48,https://t.co/6xpmllxo8Q,Variance Reduced Stochastic Gradient Descent with Neighbors. (arXiv:1506.03662v4 [cs.LG] UPDATED) https://t.co/6xpmllxo8Q,1,4," Abstract: Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its slow convergence can be a computational bottleneck. Variance reduction techniques such as SAG, SVRG and SAGA have been proposed to overcome this weakness, achieving linear convergence. However, these methods are either based on computations of full gradients at pivot points, or on keeping per data point corrections in memory. Therefore speed-ups relative to SGD may need a minimal number of epochs in order to materialize. This paper investigates algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points, which offers advantages in the transient optimization phase. As a side-product we provide a unified convergence analysis for a family of variance reduction algorithms, which we call memorization algorithms. We provide experimental results supporting our theory. "
704119091309780992,2016-02-29 01:40:47,https://t.co/yrrWEIbIFv,Optimal Binary Classifier Aggregation for General Losses. (arXiv:1510.00452v4 [cs.LG] UPDATED) https://t.co/yrrWEIbIFv,1,4," Abstract: We address the problem of aggregating an ensemble of predictors with known loss bounds in a semi-supervised binary classification setting, to minimize prediction loss incurred on the unlabeled data. We find the minimax optimal predictions for a very general class of loss functions including all convex and many non-convex losses, extending a recent analysis of the problem for misclassification error. The result is a family of semi-supervised ensemble aggregation algorithms which are as efficient as linear learning by convex optimization, but are minimax optimal without any relaxations. Their decision rules take a form familiar in decision theory -- applying sigmoid functions to a notion of ensemble margin -- without the assumptions typically made in margin-based learning. "
704119089581654016,2016-02-29 01:40:47,https://t.co/Fv040TLesW,Black-box $\alpha$-divergence Minimization. (arXiv:1511.03243v2 [stat.ML] UPDATED) https://t.co/Fv040TLesW,1,4," Abstract: Black-box alpha (BB-$\alpha$) is a new approximate inference method based on the minimization of $\alpha$-divergences. BB-$\alpha$ scales to large datasets because it can be implemented using stochastic gradient descent. BB-$\alpha$ can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients. These gradients can be easily obtained using automatic differentiation. By changing the divergence parameter $\alpha$, the method is able to interpolate between variational Bayes (VB) ($\alpha \rightarrow 0$) and an algorithm similar to expectation propagation (EP) ($\alpha = 1$). Experiments on probit regression and neural network regression and classification problems show that BB-$\alpha$ with non-standard settings of $\alpha$, such as $\alpha = 0.5$, usually produces better predictions than with $\alpha \rightarrow 0$ (VB) or $\alpha = 1$ (EP). "
704119088516300800,2016-02-29 01:40:47,https://t.co/7KR5Ob2CH1,Unifying distillation and privileged information. (arXiv:1511.03643v3 [stat.ML] UPDATED) https://t.co/7KR5Ob2CH1,0,4," Abstract: Distillation (Hinton et al., 2015) and privileged information (Vapnik & Izmailov, 2015) are two techniques that enable machines to learn from other machines. This paper unifies these two techniques into generalized distillation, a framework to learn from multiple machines and data representations. We provide theoretical and causal insight about the inner workings of generalized distillation, extend it to unsupervised, semisupervised and multitask learning scenarios, and illustrate its efficacy on a variety of numerical simulations on both synthetic and real-world data. "
704119087434178564,2016-02-29 01:40:47,https://t.co/ubH8HyhdTQ,Sub-Sampled Newton Methods I: Globally Convergent Algorithms. (arXiv:1601.04737v3 [math.OC] UPDATED) https://t.co/ubH8HyhdTQ,0,6," Abstract: Large scale optimization problems are ubiquitous in machine learning and data analysis and there is a plethora of algorithms for solving such problems. Many of these algorithms employ sub-sampling, as a way to either speed up the computations and/or to implicitly implement a form of statistical regularization. In this paper, we consider second-order iterative optimization algorithms and we provide bounds on the convergence of the variants of Newton's method that incorporate uniform sub-sampling as a means to estimate the gradient and/or Hessian. Our bounds are non-asymptotic and quantitative. Our algorithms are global and are guaranteed to converge from any initial iterate. Using random matrix concentration inequalities, one can sub-sample the Hessian to preserve the curvature information. Our first algorithm incorporates Hessian sub-sampling while using the full gradient. We also give additional convergence results for when the sub-sampled Hessian is regularized by modifying its spectrum or ridge-type regularization. Next, in addition to Hessian sub-sampling, we also consider sub-sampling the gradient as a way to further reduce the computational complexity per iteration. We use approximate matrix multiplication results from randomized numerical linear algebra to obtain the proper sampling strategy. In all these algorithms, computing the update boils down to solving a large scale linear system, which can be computationally expensive. As a remedy, for all of our algorithms, we also give global convergence results for the case of inexact updates where such linear system is solved only approximately. This paper has a more advanced companion paper, [42], in which we demonstrate that, by doing a finer-grained analysis, we can get problem-independent bounds for local convergence of these algorithms and explore trade-offs to improve upon the basic results of the present paper. "
704119086524055553,2016-02-29 01:40:46,https://t.co/uilUKSYVbX,Sub-Sampled Newton Methods II: Local Convergence Rates. (arXiv:1601.04738v3 [math.OC] UPDATED) https://t.co/uilUKSYVbX,0,5," Abstract: Many data-fitting applications require the solution of an optimization problem involving a sum of large number of functions of high dimensional parameter. Here, we consider the problem of minimizing a sum of $n$ functions over a convex constraint set $\mathcal{X} \subseteq \mathbb{R}^{p}$ where both $n$ and $p$ are large. In such problems, sub-sampling as a way to reduce $n$ can offer great amount of computational efficiency. Within the context of second order methods, we first give quantitative local convergence results for variants of Newton's method where the Hessian is uniformly sub-sampled. Using random matrix concentration inequalities, one can sub-sample in a way that the curvature information is preserved. Using such sub-sampling strategy, we establish locally Q-linear and Q-superlinear convergence rates. We also give additional convergence results for when the sub-sampled Hessian is regularized by modifying its spectrum or Levenberg-type regularization. Finally, in addition to Hessian sub-sampling, we consider sub-sampling the gradient as way to further reduce the computational complexity per iteration. We use approximate matrix multiplication results from randomized numerical linear algebra (RandNLA) to obtain the proper sampling strategy and we establish locally R-linear convergence rates. In such a setting, we also show that a very aggressive sample size increase results in a R-superlinearly convergent algorithm. While the sample size depends on the condition number of the problem, our convergence rates are problem-independent, i.e., they do not depend on the quantities related to the problem. Hence, our analysis here can be used to complement the results of our basic framework from the companion paper, [38], by exploring algorithmic trade-offs that are important in practice. "
704119085282533377,2016-02-29 01:40:46,https://t.co/sdKZVyeIRg,Second Order Stochastic Optimization in Linear Time. (arXiv:1602.03943v2 [stat.ML] UPDATED) https://t.co/sdKZVyeIRg,0,6," Abstract: First-order stochastic methods are the state-of-the-art in large-scale machine learning optimization owing to efficient per-iteration complexity. Second-order methods, while able to provide faster convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop second-order stochastic methods for optimization problems in machine learning that match the per-iteration cost of gradient based methods, and in certain settings improves upon the overall running time upon the state-of-the-art. Furthermore, our algorithm has the desirable property of being implementable in time linear in the sparsity of the input data. "
703029772016865281,2016-02-26 01:32:13,https://t.co/5EE3008IWZ,Learning functions across many orders of magnitudes. (arXiv:1602.07714v1 [cs.LG]) https://t.co/5EE3008IWZ,1,2," Abstract: Most learning algorithms are not invariant to the scale of the function that is being approximated. We propose to adaptively normalize the targets used in learning. This is useful in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were all clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using the adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance. "
703029770683072512,2016-02-26 01:32:13,https://t.co/QniQZwyACG,A Compressed Sensing Based Decomposition of Electro-Dermal Activity Signals. (arXiv:1602.07754v1 [stat.ML]) https://t.co/QniQZwyACG,0,1," Abstract: The measurement and analysis of Electro-Dermal Activity (EDA) offers applications in diverse areas ranging from market research, to seizure detection, to human stress analysis. Unfortunately, the analysis of EDA signals is made difficult by the superposition of numerous components which can obscure the signal information related to a user's response to a stimulus. We show how simple pre-processing followed by a novel compressed sensing based decomposition can mitigate the effects of these noise components and help reveal the underlying physiological signal. The proposed framework allows for fast decomposition of EDA signals with provable bounds on the recovery of user responses. We test our procedure on both synthetic and real-world EDA signals from wearable sensors, and demonstrate that our approach allows for more accurate recovery of user responses as compared to the existing techniques. "
703029769663811584,2016-02-26 01:32:13,https://t.co/37pJvMSU4m,Reinforcement Learning of POMDP's using Spectral Methods. (arXiv:1602.07764v1 [cs.AI]) https://t.co/37pJvMSU4m,0,3," Abstract: We propose a new reinforcement learning algorithm for partially observable Markov decision processes (POMDP) based on spectral decomposition methods. While spectral methods have been previously employed for consistent learning of (passive) latent variable models such as hidden Markov models, POMDPs are more challenging since the learner interacts with the environment and possibly changes the future observations in the process. We devise a learning algorithm running through episodes, in each episode we employ spectral techniques to learn the POMDP parameters from a trajectory generated by a fixed policy. At the end of the episode, an optimization oracle returns the optimal memoryless planning policy which maximizes the expected reward based on the estimated POMDP model. We prove an order-optimal regret bound with respect to the optimal memoryless policy and efficient scaling with respect to the dimensionality of observation and action spaces. "
703029768690733057,2016-02-26 01:32:13,https://t.co/P5CBdzffnt,Top-N Recommendation with Novel Rank Approximation. (arXiv:1602.07783v1 [cs.IR]) https://t.co/P5CBdzffnt,0,3," Abstract: The importance of accurate recommender systems has been widely recognized by academia and industry. However, the recommendation quality is still rather low. Recently, a linear sparse and low-rank representation of the user-item matrix has been applied to produce Top-N recommendations. This approach uses the nuclear norm as a convex relaxation for the rank function and has achieved better recommendation accuracy than the state-of-the-art methods. In the past several years, solving rank minimization problems by leveraging nonconvex relaxations has received increasing attention. Some empirical results demonstrate that it can provide a better approximation to original problems than convex relaxation. In this paper, we propose a novel rank approximation to enhance the performance of Top-N recommendation systems, where the approximation error is controllable. Experimental results on real data show that the proposed rank approximation improves the Top-$N$ recommendation accuracy substantially. "
703029767591895040,2016-02-26 01:32:12,https://t.co/mohO7lUlE9,Expectation Consistent Approximate Inference: Generalizations and Convergence. (arXiv:1602.07795v1 [cs.IT]) https://t.co/mohO7lUlE9,0,1," Abstract: Approximations of loopy belief propagation, including expectation propagation and approximate message passing, have attracted considerable attention for probabilistic inference problems. This paper proposes and analyzes a generalization of Opper and Winther's expectation consistent (EC) approximate inference method. The proposed method, called Generalized Expectation Consistency (GEC), can be applied to both maximum a posteriori (MAP) and minimum mean squared error (MMSE) estimation. Here we characterize its fixed points, convergence, and performance relative to the replica prediction of optimality. "
703029766685904897,2016-02-26 01:32:12,https://t.co/MaXgVn7Yo4,Monomial Gamma Monte Carlo Sampling. (arXiv:1602.07800v1 [stat.ML]) https://t.co/MaXgVn7Yo4,0,2," Abstract: We unify slice sampling and Hamiltonian Monte Carlo (HMC) sampling, demonstrating their connection via the Hamiltonian-Jacobi equation from Hamiltonian mechanics. This insight enables extension of HMC and slice sampling to a broader family of samplers, called Monomial Gamma Samplers (MGS). We provide a theoretical analysis of the mixing performance of such samplers, proving that in the limit of a single parameter, the MGS draws decorrelated samples from the desired target distribution. We further show that as this parameter tends toward this limit, performance gains are achieved at a cost of increasing numerical difficulty and some practical convergence issues. Our theoretical results are validated with synthetic data and real-world applications. "
703029765809315840,2016-02-26 01:32:12,https://t.co/RtVvdv5jTm,Data Cleaning for XML Electronic Dictionaries via Statistical Anomaly Detection. (arXiv:1602.07807v1 [cs.DB]) https://t.co/RtVvdv5jTm,0,4," Abstract: Many important forms of data are stored digitally in XML format. Errors can occur in the textual content of the data in the fields of the XML. Fixing these errors manually is time-consuming and expensive, especially for large amounts of data. There is increasing interest in the research, development, and use of automated techniques for assisting with data cleaning. Electronic dictionaries are an important form of data frequently stored in XML format that frequently have errors introduced through a mixture of manual typographical entry errors and optical character recognition errors. In this paper we describe methods for flagging statistical anomalies as likely errors in electronic dictionaries stored in XML format. We describe six systems based on different sources of information. The systems detect errors using various signals in the data including uncommon characters, text length, character-based language models, word-based language models, tied-field length ratios, and tied-field transliteration models. Four of the systems detect errors based on expectations automatically inferred from content within elements of a single field type. We call these single-field systems. Two of the systems detect errors based on correspondence expectations automatically inferred from content within elements of multiple related field types. We call these tied-field systems. For each system, we provide an intuitive analysis of the type of error that it is successful at detecting. Finally, we describe two larger-scale evaluations using crowdsourcing with Amazon's Mechanical Turk platform and using the annotations of a domain expert. The evaluations consistently show that the systems are useful for improving the efficiency with which errors in XML electronic dictionaries can be detected. "
703029764966187008,2016-02-26 01:32:12,https://t.co/uSTAF731zU,A Bayesian baseline for belief in uncommon events. (arXiv:1602.07836v1 [stat.AP]) https://t.co/uSTAF731zU,0,2," Abstract: The plausibility of uncommon events and miracles based on testimony of such an event has been much discussed. When analyzing the probabilities involved, it has mostly been assumed that the common events can be taken as data in the calculations. However, we usually have only testimonies for the common events. While this difference does not have a significant effect on the inductive part of the inference, it has a large influence on how one should view the reliability of testimonies. In this work, a full Bayesian solution is given for the more realistic case, where one has a large number of testimonies for a common event and one testimony for an uncommon event. It is seen that, in order for there to be a large amount of testimonies for a common event, the testimonies will probably be quite reliable. For this reason, because the testimonies are quite reliable based on the testimonies for the common events, the probability for the uncommon event, given a testimony for it, is also higher. Hence, one should be more open-minded when considering the plausibility of uncommon events. "
703029763921805313,2016-02-26 01:32:12,https://t.co/hU8JReEK5L,Probably Approximately Correct Greedy Maximization. (arXiv:1602.07860v1 [cs.AI]) https://t.co/hU8JReEK5L,0,1," Abstract: Submodular function maximization finds application in a variety of real-world decision-making problems. However, most existing methods, based on greedy maximization, assume it is computationally feasible to evaluate F, the function being maximized. Unfortunately, in many realistic settings F is too expensive to evaluate exactly even once. We present probably approximately correct greedy maximization, which requires access only to cheap anytime confidence bounds on F and uses them to prune elements. We show that, with high probability, our method returns an approximately optimal set. We propose novel, cheap confidence bounds for conditional entropy, which appears in many common choices of F and for which it is difficult to find unbiased or bounded estimates. Finally, results on a real-world dataset from a multi-camera tracking system in a shopping mall demonstrate that our approach performs comparably to existing methods, but at a fraction of the computational cost. "
703029762982334464,2016-02-26 01:32:11,https://t.co/zuzo2Yl5iK,Learning Gaussian Graphical Models With Fractional Marginal Pseudo-likelihood. (arXiv:1602.07863v1 [stat.ML]) https://t.co/zuzo2Yl5iK,0,2," Abstract: We propose a Bayesian approximate inference method for learning the dependence structure of a Gaussian graphical model. Using pseudo-likelihood, we derive an analytical expression to approximate the marginal likelihood for an arbitrary graph structure without invoking any assumptions about decomposability. The majority of the existing methods for learning Gaussian graphical models are either restricted to decomposable graphs or require specification of a tuning parameter that may have a substantial impact on learned structures. By combining a simple sparsity inducing prior for the graph structures with a default reference prior for the model parameters, we obtain a fast and easily applicable scoring function that works well for even high-dimensional data. We demonstrate the favourable performance of our approach by large-scale comparisons against the leading methods for learning non-decomposable Gaussian graphical models. A theoretical justification for our method is provided by showing that it yields a consistent estimator of the graph structure. "
703029762005012480,2016-02-26 01:32:11,https://t.co/YfBdsdzluP,Projected Estimators for Robust Semi-supervised Classification. (arXiv:1602.07865v1 [stat.ML]) https://t.co/YfBdsdzluP,0,1," Abstract: For semi-supervised techniques to be applied safely in practice we at least want methods to outperform their supervised counterparts. We study this question for classification using the well-known quadratic surrogate loss function. Using a projection of the supervised estimate onto a set of constraints imposed by the unlabeled data, we find we can safely improve over the supervised solution in terms of this quadratic loss. Unlike other approaches to semi-supervised learning, the procedure does not rely on assumptions that are not intrinsic to the classifier at hand. It is theoretically demonstrated that, measured on the labeled and unlabeled training data, this semi-supervised procedure never gives a lower quadratic loss than the supervised alternative. To our knowledge this is the first approach that offers such strong, albeit conservative, guarantees for improvement over the supervised solution. The characteristics of our approach are explicated using benchmark datasets to further understand the similarities and differences between the quadratic loss criterion used in the theoretical results and the classification accuracy often considered in practice. "
703029760767737857,2016-02-26 01:32:11,https://t.co/fMvxn52c57,Measuring and Discovering Correlations in Large Data Sets. (arXiv:1602.07960v1 [stat.ME]) https://t.co/fMvxn52c57,0,2," Abstract: In this paper, a class of statistics named ART (the alternant recursive topology statistics) is proposed to measure the properties of correlation between two variables. A wide range of bi-variable correlations both linear and nonlinear can be evaluated by ART efficiently and equitably even if nothing is known about the specific types of those relationships. ART compensates the disadvantages of Reshef's model in which no polynomial time precise algorithm exists and the ""local random"" phenomenon can not be identified. As a class of nonparametric exploration statistics, ART is applied for analyzing a dataset of 10 American classical indexes, as a result, lots of bi-variable correlations are discovered. "
703029759815655424,2016-02-26 01:32:11,https://t.co/Zf4MYKTqyK,Practical Riemannian Neural Networks. (arXiv:1602.08007v1 [cs.NE]) https://t.co/Zf4MYKTqyK,0,7," Abstract: We provide the first experimental results on non-synthetic datasets for the quasi-diagonal Riemannian gradient descents for neural networks introduced in [Ollivier, 2015]. These include the MNIST, SVHN, and FACE datasets as well as a previously unpublished electroencephalogram dataset. The quasi-diagonal Riemannian algorithms consistently beat simple stochastic gradient gradient descents by a varying margin. The computational overhead with respect to simple backpropagation is around a factor $2$. Perhaps more interestingly, these methods also reach their final performance quickly, thus requiring fewer training epochs and a smaller total computation time. We also present an implementation guide to these Riemannian gradient descents for neural networks, showing how the quasi-diagonal versions can be implemented with minimal effort on top of existing routines which compute gradients. "
703029758859288576,2016-02-26 01:32:10,https://t.co/XkgHeAGVuj,Meta-learning within Projective Simulation. (arXiv:1602.08017v1 [cs.AI]) https://t.co/XkgHeAGVuj,0,1," Abstract: Learning models of artificial intelligence can nowadays perform very well on a large variety of tasks. However, in practice different task environments are best handled by different learning models, rather than a single, universal, approach. Most non-trivial models thus require the adjustment of several to many learning parameters, which is often done on a case-by-case basis by an external party. Meta-learning refers to the ability of an agent to autonomously and dynamically adjust its own learning parameters, or meta-parameters. In this work we show how projective simulation, a recently developed model of artificial intelligence, can naturally be extended to account for meta-learning in reinforcement learning settings. The projective simulation approach is based on a random walk process over a network of clips. The suggested meta-learning scheme builds upon the same design and employs clip networks to monitor the agent's performance and to adjust its meta-parameters ""on the fly"". We distinguish between ""reflexive adaptation"" and ""adaptation through learning"", and show the utility of both approaches. In addition, a trade-off between flexibility and learning-time is addressed. The extended model is examined on three different kinds of reinforcement learning tasks, in which the agent has different optimal values of the meta-parameters, and is shown to perform well, reaching near-optimal to optimal success rates in all of them, without ever needing to manually adjust any meta-parameter. "
703029757705887746,2016-02-26 01:32:10,https://t.co/6nhrSZuTmM,Kernel Mean Shrinkage Estimators. (arXiv:1405.5505v3 [stat.ML] UPDATED) https://t.co/6nhrSZuTmM,1,3," Abstract: A mean function in a reproducing kernel Hilbert space (RKHS), or a kernel mean, is central to kernel methods in that it is used by many classical algorithms such as kernel principal component analysis, and it also forms the core inference step of modern kernel methods that rely on embedding probability distributions in RKHSs. Given a finite sample, an empirical average has been used commonly as a standard estimator of the true kernel mean. Despite a widespread use of this estimator, we show that it can be improved thanks to the well-known Stein phenomenon. We propose a new family of estimators called kernel mean shrinkage estimators (KMSEs), which benefit from both theoretical justifications and good empirical performance. The results demonstrate that the proposed estimators outperform the standard one, especially in a ""large d, small n"" paradigm. "
703029756699217924,2016-02-26 01:32:10,https://t.co/90efG5PFBS,Local entropy as a measure for sampling solutions in Constraint Satisfaction Problems. (arXiv:1511.05634v2 [cond-m… https://t.co/90efG5PFBS,0,1," Abstract: We introduce a novel Entropy-driven Monte Carlo (EdMC) strategy to efficiently sample solutions of random Constraint Satisfaction Problems (CSPs). First, we extend a recent result that, using a large-deviation analysis, shows that the geometry of the space of solutions of the Binary Perceptron Learning Problem (a prototypical CSP), contains regions of very high-density of solutions. Despite being sub-dominant, these regions can be found by optimizing a local entropy measure. Building on these results, we construct a fast solver that relies exclusively on a local entropy estimate, and can be applied to general CSPs. We describe its performance not only for the Perceptron Learning Problem but also for the random $K$-Satisfiabilty Problem (another prototypical CSP with a radically different structure), and show numerically that a simple zero-temperature Metropolis search in the smooth local entropy landscape can reach sub-dominant clusters of optimal solutions in a small number of steps, while standard Simulated Annealing either requires extremely long cooling procedures or just fails. We also discuss how the EdMC can heuristically be made even more efficient for the cases we studied. "
703029755445178368,2016-02-26 01:32:10,https://t.co/KKmgEtXaBr,Fast k-means with accurate bounds. (arXiv:1602.02514v3 [stat.ML] UPDATED) https://t.co/KKmgEtXaBr,0,2," Abstract: We propose a novel accelerated exact k-means algorithm, which performs better than the current state-of-the-art low-dimensional algorithm in 18 of 22 experiments, running up to 3 times faster. We also propose a general improvement of existing state-of-the-art accelerated exact k-means algorithms through better estimates of the distance bounds used to reduce the number of distance calculations, and get a speedup in 36 of 44 experiments, up to 1.8 times faster. We have conducted experiments with our own implementations of existing methods to ensure homogeneous evaluation of performance, and we show that our implementations perform as well or better than existing available implementations. Finally, we propose simplified variants of standard approaches and show that they are faster than their fully-fledged counterparts in 59 of 62 experiments. "
702670659818807299,2016-02-25 01:45:14,https://t.co/lOIycdftKx,Parsimonious modeling with Information Filtering Networks. (arXiv:1602.07349v1 [cs.IT]) https://t.co/lOIycdftKx,0,4," Abstract: We introduce a methodology to construct parsimonious probabilistic models. This method makes use of Information Filtering Networks to produce a robust estimate of the global sparse inverse covariance from a simple sum of local inverse covariances computed on small sub-parts of the network. Being based on local and low-dimensional inversions, this method is computationally very efficient and statistically robust even for the estimation of inverse covariance of high-dimensional, noisy and short time-series. Applied to financial data our method results computationally more efficient than state-of-the-art methodologies such as Glasso producing, in a fraction of the computation time, models that can have equivalent or better performances but with a sparser inference structure. We also discuss performances with sparse factor models where we notice that relative performances decrease with the number of factors. The local nature of this approach allows us to perform computations in parallel and provides a tool for dynamical adaptation by partial updating when the properties of some variables change without the need of recomputing the whole model. This makes this approach particularly suitable to handle big datasets with large numbers of variables. Examples of practical application for forecasting, stress testing and risk allocation in financial systems are also provided. "
702670658241748992,2016-02-25 01:45:14,https://t.co/BVKPsDd3Sr,Discrete Distribution Estimation under Local Privacy. (arXiv:1602.07387v1 [stat.ML]) https://t.co/BVKPsDd3Sr,1,4," Abstract: The collection and analysis of user data drives improvements in the app and web ecosystems, but comes with risks to privacy. This paper examines discrete distribution estimation under local privacy, a setting wherein service providers can learn the distribution of a categorical statistic of interest without collecting the underlying data. We present new mechanisms, including hashed K-ary Randomized Response (KRR), that empirically meet or exceed the utility of existing mechanisms at all privacy levels. New theoretical results demonstrate the order-optimality of KRR and the existing RAPPOR mechanism at different privacy regimes. "
702670657113432064,2016-02-25 01:45:14,https://t.co/iuoN1sIrW2,Max-Margin Nonparametric Latent Feature Models for Link Prediction. (arXiv:1602.07428v1 [cs.LG]) https://t.co/iuoN1sIrW2,1,3," Abstract: Link prediction is a fundamental task in statistical network analysis. Recent advances have been made on learning flexible nonparametric Bayesian latent feature models for link prediction. In this paper, we present a max-margin learning method for such nonparametric latent feature relational models. Our approach attempts to unite the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction. It inherits the advances of nonparametric Bayesian methods to infer the unknown latent social dimension, while for discriminative link prediction, it adopts the max-margin learning principle by minimizing a hinge-loss using the linear expectation operator, without dealing with a highly nonlinear link likelihood function. For posterior inference, we develop an efficient stochastic variational inference algorithm under a truncated mean-field assumption. Our methods can scale up to large-scale real networks with millions of entities and tens of millions of positive links. We also provide a full Bayesian formulation, which can avoid tuning regularization hyper-parameters. Experimental results on a diverse range of real datasets demonstrate the benefits inherited from max-margin learning and Bayesian nonparametric inference. "
702670655968428033,2016-02-25 01:45:14,https://t.co/Het4OYAZ2X,Feature ranking for multi-label classification using Markov Networks. (arXiv:1602.07464v1 [cs.LG]) https://t.co/Het4OYAZ2X,3,3," Abstract: We propose a simple and efficient method for ranking features in multi-label classification. The method produces a ranking of features showing their relevance in predicting labels, which in turn allows to choose a final subset of features. The procedure is based on Markov Networks and allows to model the dependencies between labels and features in a direct way. In the first step we build a simple network using only labels and then we test how much adding a single feature affects the initial network. More specifically, in the first step we use the Ising model whereas the second step is based on the score statistic, which allows to test a significance of added features very quickly. The proposed approach does not require transformation of label space, gives interpretable results and allows for attractive visualization of dependency structure. We give a theoretical justification of the procedure by discussing some theoretical properties of the Ising model and the score statistic. We also discuss feature ranking procedure based on fitting Ising model using $l_1$ regularized logistic regressions. Numerical experiments show that the proposed methods outperform the conventional approaches on the considered artificial and real datasets. "
702670654718382080,2016-02-25 01:45:13,https://t.co/BAFLAMkRcG,Asymptotic consistency and order specification for logistic classifier chains in multi-label learning. (arXiv:1602… https://t.co/BAFLAMkRcG,0,1," Abstract: Classifier chains are popular and effective method to tackle a multi-label classification problem. The aim of this paper is to study the asymptotic properties of the chain model in which the conditional probabilities are of the logistic form. In particular we find conditions on the number of labels and the distribution of feature vector under which the estimated mode of the joint distribution of labels converges to the true mode. Best of our knowledge, this important issue has not yet been studied in the context of multi-label learning. We also investigate how the order of model building in a chain influences the estimation of the joint distribution of labels. We establish the link between the problem of incorrect ordering in the chain and incorrect model specification. We propose a procedure of determining the optimal ordering of labels in the chain, which is based on using measures of correct specification and allows to find the ordering such that the consecutive logistic models are best possibly specified. The other important question raised in this paper is how accurately can we estimate the joint posterior probability when the ordering of labels is wrong or the logistic models in the chain are incorrectly specified. The numerical experiments illustrate the theoretical results. "
702670653527355392,2016-02-25 01:45:13,https://t.co/x8EgL4as5T,Group Equivariant Convolutional Networks. (arXiv:1602.07576v1 [cs.LG]) https://t.co/x8EgL4as5T,5,11," Abstract: We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST. "
702670652378116097,2016-02-25 01:45:13,https://t.co/s0nhd52U8J,Online Dual Coordinate Ascent Learning. (arXiv:1602.07630v1 [math.OC]) https://t.co/s0nhd52U8J,1,4," Abstract: The stochastic dual coordinate-ascent (S-DCA) technique is a useful alternative to the traditional stochastic gradient-descent algorithm for solving large-scale optimization problems due to its scalability to large data sets and strong theoretical guarantees. However, the available S-DCA formulation is limited to finite sample sizes and relies on performing multiple passes over the same data. This formulation is not well-suited for online implementations where data keep streaming in. In this work, we develop an {\em online} dual coordinate-ascent (O-DCA) algorithm that is able to respond to streaming data and does not need to revisit the past data. This feature embeds the resulting construction with continuous adaptation, learning, and tracking abilities, which are particularly attractive for online learning scenarios. "
702670651186941952,2016-02-25 01:45:12,https://t.co/ICeGNpOtQA,Feature Selection with Annealing for Computer Vision and Big Data Learning. (arXiv:1310.2880v6 [stat.ML] UPDATED) https://t.co/ICeGNpOtQA,1,1," Abstract: Many computer vision and medical imaging problems are faced with learning from large-scale datasets, with millions of observations and features. In this paper we propose a novel efficient learning scheme that tightens a sparsity constraint by gradually removing variables based on a criterion and a schedule. The attractive fact that the problem size keeps dropping throughout the iterations makes it particularly suitable for big data learning. Our approach applies generically to the optimization of any differentiable loss function, and finds applications in regression, classification and ranking. The resultant algorithms build variable screening into estimation and are extremely simple to implement. We provide theoretical guarantees of convergence and selection consistency. In addition, one dimensional piecewise linear response functions are used to account for nonlinearity and a second order prior is imposed on these functions to avoid overfitting. Experiments on real and synthetic data show that the proposed method compares very well with other state of the art methods in regression, classification and ranking while being computationally very efficient and scalable. "
702670649114959872,2016-02-25 01:45:12,https://t.co/gu5kiNOek0,RAND-WALK: A Latent Variable Model Approach to Word Embeddings. (arXiv:1502.03520v6 [cs.LG] UPDATED) https://t.co/gu5kiNOek0,0,2," Abstract: Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of~\citet{mnih2007three}. The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by~\citet{mikolov2013efficient} and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space. "
702670648062124032,2016-02-25 01:45:12,https://t.co/i2bvZsYndx,Sparse Estimation in a Correlated Probit Model. (arXiv:1507.04777v2 [stat.ML] UPDATED) https://t.co/i2bvZsYndx,0,2," Abstract: Among the goals of statistical genetics is to find associations between genetic data and binary phenotypes, such as heritable diseases. Often, the data are obfuscated by confounders such as age, ethnicity, or population structure. Linear mixed models are linear regression models that correct for confounding by means of correlated label noise; they are widely appreciated in the field of statistical genetics. We generalize this modeling paradigm to binary classification, where we face the problem that marginalizing over the noise leads to an intractable, high-dimensional integral. We present a scalable, approximate inference algorithm that lets us fit the model to high-dimensional data sets. The algorithm selects features based on an $\ell_1$ norm regularizer which are up to 40% less confounded compared to the outcomes of uncorrected feature selection, as we show. The proposed method also outperforms Gaussian process classification and uncorrelated probit regression in terms of prediction performance. "
702670646371852289,2016-02-25 01:45:11,https://t.co/0eyrVU7gkV,adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs. (arXiv:1511.01169v5 [cs.LG] UPDATED) https://t.co/0eyrVU7gkV,0,2," Abstract: Recurrent Neural Networks (RNNs) are powerful models that achieve exceptional performance on several pattern recognition problems. However, the training of RNNs is a computationally difficult task owing to the well-known ""vanishing/exploding"" gradient problem. Algorithms proposed for training RNNs either exploit no (or limited) curvature information and have cheap per-iteration complexity, or attempt to gain significant curvature information at the cost of increased per-iteration cost. The former set includes diagonally-scaled first-order methods such as ADAGRAD and ADAM, while the latter consists of second-order algorithms like Hessian-Free Newton and K-FAC. In this paper, we present adaQN, a stochastic quasi-Newton algorithm for training RNNs. Our approach retains a low per-iteration cost while allowing for non-diagonal scaling through a stochastic L-BFGS updating scheme. The method uses a novel L-BFGS scaling initialization scheme and is judicious in storing and retaining L-BFGS curvature pairs. We present numerical experiments on two language modeling tasks and show that adaQN is competitive with popular RNN training algorithms. "
702670644954193920,2016-02-25 01:45:11,https://t.co/5wjoimeaIs,Order Matters: Sequence to sequence for sets. (arXiv:1511.06391v4 [stat.ML] UPDATED) https://t.co/5wjoimeaIs,1,3," Abstract: Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models. "
702670643297439748,2016-02-25 01:45:11,https://t.co/CqpwWFp9wl,Recurrent Gaussian Processes. (arXiv:1511.06644v6 [cs.LG] UPDATED) https://t.co/CqpwWFp9wl,1,7," Abstract: We define Recurrent Gaussian Processes (RGP) models, a general family of Bayesian nonparametric models with recurrent GP priors which are able to learn dynamical patterns from sequential data. Similar to Recurrent Neural Networks (RNNs), RGPs can have different formulations for their internal states, distinct inference methods and be extended with deep structures. In such context, we propose a novel deep RGP model whose autoregressive states are latent, thereby performing representation and dynamical learning simultaneously. To fully exploit the Bayesian nature of the RGP model we develop the Recurrent Variational Bayes (REVARB) framework, which enables efficient inference and strong regularization through coherent propagation of uncertainty across the RGP layers and states. We also introduce a RGP extension where variational parameters are greatly reduced by being reparametrized through RNN-based sequential recognition models. We apply our model to the tasks of nonlinear system identification and human motion modeling. The promising obtained results indicate that our RGP model maintains its highly flexibility while being able to avoid overfitting and being applicable even when larger datasets are not available. "
702306243772325889,2016-02-24 01:37:11,https://t.co/NsUu9xFagz,Recovering the number of clusters in data sets with noise features using feature rescaling factors. (arXiv:1602.06… https://t.co/NsUu9xFagz,0,3," Abstract: In this paper we introduce three methods for re-scaling data sets aiming at improving the likelihood of clustering validity indexes to return the true number of spherical Gaussian clusters with additional noise features. Our method obtains feature re-scaling factors taking into account the structure of a given data set and the intuitive idea that different features may have different degrees of relevance at different clusters. We experiment with the Silhouette (using squared Euclidean, Manhattan, and the p$^{th}$ power of the Minkowski distance), Dunn's, Calinski-Harabasz and Hartigan indexes on data sets with spherical Gaussian clusters with and without noise features. We conclude that our methods indeed increase the chances of estimating the true number of clusters in a data set. "
702306242870571013,2016-02-24 01:37:11,https://t.co/ogyMV6tJMr,Auditing Black-box Models by Obscuring Features. (arXiv:1602.07043v1 [stat.ML]) https://t.co/ogyMV6tJMr,1,4," Abstract: Data-trained predictive models are widely used to assist in decision making. But they are used as black boxes that output a prediction or score. It is therefore hard to acquire a deeper understanding of model behavior: and in particular how different attributes influence the model prediction. This is very important when trying to interpret the behavior of complex models, or ensure that certain problematic attributes (like race or gender) are not unduly influencing decisions. In this paper, we present a technique for auditing black-box models: we can study the extent to which existing models take advantage of particular features in the dataset without knowing how the models work. We show how a class of techniques originally developed for the detection and repair of disparate impact in classification models can be used to study the sensitivity of any model with respect to any feature subsets. Our approach does not require the black-box model to be retrained. This is important if (for example) the model is only accessible via an API, and contrasts our work with other methods that investigate feature influence like feature selection. We present experimental evidence for the effectiveness of our procedure using a variety of publicly available datasets and models. We also validate our procedure using techniques from interpretable learning and feature selection. "
702306241880657920,2016-02-24 01:37:10,https://t.co/KKFkyoM7YX,An Improved Gap-Dependency Analysis of the Noisy Power Method. (arXiv:1602.07046v1 [stat.ML]) https://t.co/KKFkyoM7YX,0,2," Abstract: We consider the noisy power method algorithm, which has wide applications in machine learning and statistics, especially those related to principal component analysis (PCA) under resource (communication, memory or privacy) constraints. Existing analysis of the noisy power method shows an unsatisfactory dependency over the ""consecutive"" spectral gap $(\sigma_k-\sigma_{k+1})$ of an input data matrix, which could be very small and hence limits the algorithm's applicability. In this paper, we present a new analysis of the noisy power method that achieves improved gap dependency for both sample complexity and noise tolerance bounds. More specifically, we improve the dependency over $(\sigma_k-\sigma_{k+1})$ to dependency over $(\sigma_k-\sigma_{q+1})$, where $q$ is an intermediate algorithm parameter and could be much larger than the target rank $k$. Our proofs are built upon a novel characterization of proximity between two subspaces that differ from canonical angle characterizations analyzed in previous works. Finally, we apply our improved bounds to distributed private PCA and memory-efficient streaming PCA and obtain bounds that are superior to existing results in the literature. "
702306240614047747,2016-02-24 01:37:10,https://t.co/zT7cz77Gzq,A Streaming Algorithm for Crowdsourced Data Classification. (arXiv:1602.07107v1 [stat.ML]) https://t.co/zT7cz77Gzq,0,4," Abstract: We propose a streaming algorithm for the binary classification of data based on crowdsourcing. The algorithm learns the competence of each labeller by comparing her labels to those of other labellers on the same tasks and uses this information to minimize the prediction error rate on each task. We provide performance guarantees of our algorithm for a fixed population of independent labellers. In particular, we show that our algorithm is optimal in the sense that the cumulative regret compared to the optimal decision with known labeller error probabilities is finite, independently of the number of tasks to label. The complexity of the algorithm is linear in the number of labellers and the number of tasks, up to some logarithmic factors. Numerical experiments illustrate the performance of our algorithm compared to existing algorithms, including simple majority voting and expectation-maximization algorithms, on both synthetic and real datasets. "
702306239603146752,2016-02-24 01:37:10,https://t.co/zKaHNJ4WKi,Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series. (arXiv:1602.07109v1 [stat.ML]) https://t.co/zKaHNJ4WKi,0,10, Abstract: Approximate variational inference has shown to be a powerful tool for modeling unknown complex probability distributions. Recent advances in the field allow us to learn probabilistic models of sequences that actively exploit spatial and temporal structure. We apply a Stochastic Recurrent Network (STORN) to learn robot time series data. Our evaluation demonstrates that we can robustly detect anomalies both off- and on-line. 
702306238688849920,2016-02-24 01:37:10,https://t.co/JXdWSJuvV6,Submodular Learning and Covering with Response-Dependent Costs. (arXiv:1602.07120v1 [cs.LG]) https://t.co/JXdWSJuvV6,0,3," Abstract: We consider interactive learning and covering problems, in a setting where actions may incur different costs, depending on the response to the action. We propose a natural greedy algorithm for response-dependent costs. We bound the approximation factor of this greedy algorithm in active learning settings as well as in the general setting. We show that a different property of the cost function controls the approximation factor in each of these scenarios. We further show that in both settings, the approximation factor of this greedy algorithm is near-optimal among all greedy algorithms. Experiments demonstrate the advantages of the proposed algorithm in the response-dependent cost setting. "
702306237808046081,2016-02-24 01:37:09,https://t.co/XqLYnyZ9IB,Lens depth function and k-relative neighborhood graph: versatile tools for ordinal data analysis. (arXiv:1602.0719… https://t.co/XqLYnyZ9IB,0,2," Abstract: In recent years it has become popular to study machine learning problems based on ordinal distance information rather than numerical distance measurements. By ordinal distance information we refer to binary answers to distance comparisons such as d(A,B) < d(C,D). For many problems in machine learning and statistics it is unclear how to solve them in such a scenario. Up to now, the main approach is to explicitly construct an ordinal embedding of the data points in the Euclidean space, an approach that has a number of drawbacks. In this paper, we propose algorithms for the problems of medoid estimation, outlier identification, classification and clustering when given only ordinal distance comparisons. They are based on estimating the lens depth function and the k-relative neighborhood graph on a data set. Our algorithms are simple, can easily be parallelized, avoid many of the drawbacks of an ordinal embedding approach and are much faster. "
702306236872724481,2016-02-24 01:37:09,https://t.co/q61pRPcsuw,Search Improves Label for Active Learning. (arXiv:1602.07265v1 [cs.LG]) https://t.co/q61pRPcsuw,0,2, Abstract: We investigate active learning with access to two distinct oracles: Label (which is standard) and Search (which is not). The Search oracle models the situation where a human searches a database to seed or counterexample an existing solution. Search is stronger than Label while being natural to implement in many situations. We show that an algorithm using both oracles can provide exponentially large problem-dependent improvements over Label alone. 
702306235983536128,2016-02-24 01:37:09,https://t.co/IHCUOXKJj0,A Simple Approach to Sparse Clustering. (arXiv:1602.07277v1 [stat.ML]) https://t.co/IHCUOXKJj0,0,5," Abstract: Consider the problem of sparse clustering, where it is assumed that only a subset of the features are useful for clustering purposes. In the framework of the COSA method of Friedman and Meulman, subsequently improved in the form of the Sparse K-means method of Witten and Tibshirani, a natural and simpler hill-climbing approach is introduced. The new method is shown to be competitive with these two methods and others. "
702306234943332353,2016-02-24 01:37:09,https://t.co/G0IL4qPWTx,The IBM 2016 Speaker Recognition System. (arXiv:1602.07291v1 [cs.SD]) https://t.co/G0IL4qPWTx,1,4," Abstract: In this paper we describe the recent advancements made in the IBM i-vector speaker recognition system for conversational speech. In particular, we identify key techniques that contribute to significant improvements in performance of our system, and quantify their contributions. The techniques include: 1) a nearest-neighbor discriminant analysis (NDA) approach that is formulated to alleviate some of the limitations associated with the conventional linear discriminant analysis (LDA) that assumes Gaussian class-conditional distributions, 2) the application of speaker- and channel-adapted features, which are derived from an automatic speech recognition (ASR) system, for speaker recognition, and 3) the use of a deep neural network (DNN) acoustic model with a large number of output units (~10k senones) to compute the frame-level soft alignments required in the i-vector estimation process. We evaluate these techniques on the NIST 2010 speaker recognition evaluation (SRE) extended core conditions involving telephone and microphone trials. Experimental results indicate that: 1) the NDA is more effective (up to 35% relative improvement in terms of EER) than the traditional parametric LDA for speaker recognition, 2) when compared to raw acoustic features (e.g., MFCCs), the ASR speaker-adapted features provide gains in speaker recognition performance, and 3) increasing the number of output units in the DNN acoustic model (i.e., increasing the senone set size from 2k to 10k) provides consistent improvements in performance (for example from 37% to 57% relative EER gains over our baseline GMM i-vector system). To our knowledge, results reported in this paper represent the best performances published to date on the NIST SRE 2010 extended core tasks. "
702306233940885504,2016-02-24 01:37:09,https://t.co/piVY3OFRmg,Maximum margin classifier working in a set of strings. (arXiv:1406.0597v3 [stat.ML] UPDATED) https://t.co/piVY3OFRmg,0,3," Abstract: Numbers and numerical vectors account for a large portion of data. However, recently the amount of string data generated has increased dramatically. Consequently, classifying string data is a common problem in many fields. The most widely used approach to this problem is to convert strings into numerical vectors using string kernels and subsequently apply a support vector machine that works in a numerical vector space. However, this non-one-to-one conversion involves a loss of information and makes it impossible to evaluate, using probability theory, the generalization error of a learning machine, considering that the given data to train and test the machine are strings generated according to probability laws. In this study, we approach this classification problem by constructing a classifier that works in a set of strings. To evaluate the generalization error of such a classifier theoretically, probability theory for strings is required. Therefore, we first extend a limit theorem on the asymptotic behavior of a consensus sequence of strings, which is the counterpart of the mean of numerical vectors, as demonstrated in the probability theory on a metric space of strings developed by one of the authors and his colleague in a previous study [18]. Using the obtained result, we then demonstrate that our learning machine classifies strings in an asymptotically optimal manner. Furthermore, we demonstrate the usefulness of our machine in practical data analysis by applying it to predicting protein--protein interactions using amino acid sequences. "
702306232963624961,2016-02-24 01:37:08,https://t.co/sZ6MU7QpLt,Permutational Rademacher Complexity: a New Complexity Measure for Transductive Learning. (arXiv:1505.02910v2 [stat… https://t.co/sZ6MU7QpLt,0,2," Abstract: Transductive learning considers situations when a learner observes $m$ labelled training points and $u$ unlabelled test points with the final goal of giving correct answers for the test points. This paper introduces a new complexity measure for transductive learning called Permutational Rademacher Complexity (PRC) and studies its properties. A novel symmetrization inequality is proved, which shows that PRC provides a tighter control over expected suprema of empirical processes compared to what happens in the standard i.i.d. setting. A number of comparison results are also provided, which show the relation between PRC and other popular complexity measures used in statistical learning theory, including Rademacher complexity and Transductive Rademacher Complexity (TRC). We argue that PRC is a more suitable complexity measure for transductive learning. Finally, these results are combined with a standard concentration argument to provide novel data-dependent risk bounds for transductive learning. "
702306231831162881,2016-02-24 01:37:08,https://t.co/V3XRvVkTMx,WordRank: Learning Word Embeddings via Robust Ranking. (arXiv:1506.02761v3 [cs.CL] UPDATED) https://t.co/V3XRvVkTMx,0,6," Abstract: Embedding words in a vector space has gained a lot of attention in recent years. While state-of-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics. Then, based on this insight, we propose a novel framework WordRank that efficiently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm is very competitive to the state-of-the- arts on large corpora, while outperforms them by a significant margin when the training set is limited (i.e., sparse and noisy). With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark. Our multi-node distributed implementation of WordRank is publicly available for general usage. "
702306230660931584,2016-02-24 01:37:08,https://t.co/bL7ovWYYD8,Unsupervised Ensemble Learning with Dependent Classifiers. (arXiv:1510.05830v2 [cs.LG] UPDATED) https://t.co/bL7ovWYYD8,0,2," Abstract: In unsupervised ensemble learning, one obtains predictions from multiple sources or classifiers, yet without knowing the reliability and expertise of each source, and with no labeled data to assess it. The task is to combine these possibly conflicting predictions into an accurate meta-learner. Most works to date assumed perfect diversity between the different sources, a property known as conditional independence. In realistic scenarios, however, this assumption is often violated, and ensemble learners based on it can be severely sub-optimal. The key challenges we address in this paper are:\ (i) how to detect, in an unsupervised manner, strong violations of conditional independence; and (ii) construct a suitable meta-learner. To this end we introduce a statistical model that allows for dependencies between classifiers. Our main contributions are the development of novel unsupervised methods to detect strongly dependent classifiers, better estimate their accuracies, and construct an improved meta-learner. Using both artificial and real datasets, we showcase the importance of taking classifier dependencies into account and the competitive performance of our approach. "
701946770092384257,2016-02-23 01:48:46,https://t.co/e7FraoEjpw,Policy Error Bounds for Model-Based Reinforcement Learning with Factored Linear Models. (arXiv:1602.06346v1 [stat.… https://t.co/e7FraoEjpw,1,4," Abstract: In this paper we study a model-based approach to calculating approximately optimal policies in Markovian Decision Processes. In particular, we derive novel bounds on the loss of using a policy derived from a factored linear model, a class of models which generalize numerous previous models out of those that come with strong computational guarantees. For the first time in the literature, we derive performance bounds for model-based techniques where the model inaccuracy is measured in weighted norms. Moreover, our bounds show a decreased sensitivity to the discount factor and, unlike similar bounds derived for other approaches, they are insensitive to measure mismatch. Similarly to previous works, our proofs are also based on contraction arguments, but with the main differences that we use carefully constructed norms building on Banach lattices, and the contraction property is only assumed for operators acting on ""compressed"" spaces, thus weakening previous assumptions, while strengthening previous results. "
701946768892755968,2016-02-23 01:48:45,https://t.co/BqcZoqEatY,"The Segmented iHMM: A Simple, Efficient Hierarchical Infinite HMM. (arXiv:1602.06349v1 [stat.ML]) https://t.co/BqcZoqEatY",0,8," Abstract: We propose the segmented iHMM (siHMM), a hierarchical infinite hidden Markov model (iHMM) that supports a simple, efficient inference scheme. The siHMM is well suited to segmentation problems, where the goal is to identify points at which a time series transitions from one relatively stable regime to a new regime. Conventional iHMMs often struggle with such problems, since they have no mechanism for distinguishing between high- and low-level dynamics. Hierarchical HMMs (HHMMs) can do better, but they require much more complex and expensive inference algorithms. The siHMM retains the simplicity and efficiency of the iHMM, but outperforms it on a variety of segmentation problems, achieving performance that matches or exceeds that of a more complicated HHMM. "
701946767907151872,2016-02-23 01:48:45,https://t.co/aVMkMuP6s1,Semidefinite Programs for Exact Recovery of a Hidden Community. (arXiv:1602.06410v1 [stat.ML]) https://t.co/aVMkMuP6s1,0,1," Abstract: We study a semidefinite programming (SDP) relaxation of the maximum likelihood estimation for exactly recovering a hidden community of cardinality $K$ from an $n \times n$ symmetric data matrix $A$, where for distinct indices $i,j$, $A_{ij} \sim P$ if $i, j$ are both in the community and $A_{ij} \sim Q$ otherwise, for two known probability distributions $P$ and $Q$. We identify a sufficient condition and a necessary condition for the success of SDP for the general model. For both the Bernoulli case ($P={{\rm Bern}}(p)$ and $Q={{\rm Bern}}(q)$ with $p>q$) and the Gaussian case ($P=\mathcal{N}(\mu,1)$ and $Q=\mathcal{N}(0,1)$ with $\mu>0$), which correspond to the problem of planted dense subgraph recovery and submatrix localization respectively, the general results lead to the following findings: (1) If $K=\omega( n /\log n)$, SDP attains the information-theoretic recovery limits with sharp constants; (2) If $K=\Theta(n/\log n)$, SDP is order-wise optimal, but strictly suboptimal by a constant factor; (3) If $K=o(n/\log n)$ and $K \to \infty$, SDP is order-wise suboptimal. The same critical scaling for $K$ is found to hold, up to constant factors, for the performance of SDP on the stochastic block model of $n$ vertices partitioned into multiple communities of equal size $K$. A key ingredient in the proof of the necessary condition is a construction of a primal feasible solution based on random perturbation of the true cluster matrix. "
701946766921433088,2016-02-23 01:48:45,https://t.co/JeoNHurBoj,Generalized Statistical Tests for mRNA and Protein Subcellular Spatial Patterning against Complete Spatial Randomn… https://t.co/JeoNHurBoj,0,1," Abstract: We derive generalized estimators for a number of spatial statistics that have been used in the analysis of spatially resolved omics data, such as Ripley's K, H and L functions, clustering index, and degree of clustering, which allow these statistics to be calculated on data modelled by arbitrary random measures (RMs). Our estimators generalize those typically used to calculate these statistics on point process data, allowing them to be calculated on RMs which assign continuous values to spatial regions, for instance to model protein intensity. The clustering index (H*) compares Ripley's H function calculated empirically to its distribution under complete spatial randomness (CSR), leading us to consider CSR null hypotheses for RMs which are not point-processes when generalizing this statistic. We thus consider restricted classes of completely random measures which can be simulated directly (Gamma processes and Marked Poisson Processes), as well as the general class of all CSR RMs, for which we derive an exact permutation-based H* estimator. We establish several properties of the estimators, including bounds on the accuracy of our general Ripley K estimator, its relationship to a previous estimator for the cross-correlation measure, and the relationship of our generalized H* estimator to previous statistics. To test the ability of our approach to identify spatial patterning, we use Fluorescent In Situ Hybridization (FISH) and Immunofluorescence (IF) data to probe for mRNA and protein subcellular localization patterns respectively in polarizing mouse fibroblasts on micropattened cells. We observe correlated patterns of clustering over time for corresponding mRNAs and proteins, suggesting a deterministic effect of mRNA localization on protein localization for several pairs tested, including one case in which spatial patterning at the mRNA level has not been previously demonstrated. "
701946765914857472,2016-02-23 01:48:45,https://t.co/ZZOyZXrF75,Burstiness Scale: a highly parsimonious model for characterizing random series of events. (arXiv:1602.06431v1 [sta… https://t.co/ZZOyZXrF75,3,5," Abstract: The problem to accurately and parsimoniously characterize random series of events (RSEs) present in the Web, such as e-mail conversations or Twitter hashtags, is not trivial. Reports found in the literature reveal two apparent conflicting visions of how RSEs should be modeled. From one side, the Poissonian processes, of which consecutive events follow each other at a relatively regular time and should not be correlated. On the other side, the self-exciting processes, which are able to generate bursts of correlated events and periods of inactivities. The existence of many and sometimes conflicting approaches to model RSEs is a consequence of the unpredictability of the aggregated dynamics of our individual and routine activities, which sometimes show simple patterns, but sometimes results in irregular rising and falling trends. In this paper we propose a highly parsimonious way to characterize general RSEs, namely the Burstiness Scale (BuSca) model. BuSca views each RSE as a mix of two independent process: a Poissonian and a self-exciting one. Here we describe a fast method to extract the two parameters of BuSca that, together, gives the burstyness scale, which represents how much of the RSE is due to bursty and viral effects. We validated our method in eight diverse and large datasets containing real random series of events seen in Twitter, Yelp, e-mail conversations, Digg, and online forums. Results showed that, even using only two parameters, BuSca is able to accurately describe RSEs seen in these diverse systems, what can leverage many applications. "
701946764564287488,2016-02-23 01:48:44,https://t.co/DhhfSN00eV,Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling Techniques. (arXiv:1602.06516v1 [cs.LG]) https://t.co/DhhfSN00eV,0,1," Abstract: Graph partitioning plays a central role in machine learning, and the development of graph partitioning algorithms is still an active area of research. The immense demand for such algorithms arises due to the abundance of applications that involve pairwise interactions or similarities among entities. Recent studies in computer vision and databases systems have emphasized on the necessity of considering multi-way interactions, and has led to the study of a more general problem in the form of hypergraph partitioning. This paper focuses on the problem of partitioning uniform hypergraphs, which arises in applications such as subspace clustering, motion segmentation etc. We show that uniform hypergraph partitioning is equivalent to a tensor trace maximization problem, and hence, a tensor based method is a natural answer to this problem. We also propose a tensor spectral method that extends the widely known spectral clustering algorithm to the case of uniform hypergraphs. While the theoretical guarantees of spectral clustering have been extensively studied, very little is known about the statistical properties of tensor based methods. To this end, we prove the consistency of the proposed algorithm under a planted partition model. The computational complexity of tensorial approaches has resulted in the use of various tensor sampling strategies. We present the first theoretical study on the effect of sampling in tensor based hypergraph partitioning. Our result justifies the empirical success of iterative sampling techniques often used in practice. We also present an iteratively sampled variant of the proposed algorithm for the purpose of subspace clustering, and demonstrate the performance of this method on a benchmark problem. "
701946763113013248,2016-02-23 01:48:44,https://t.co/1JjGPrtb3b,Active Task Selection for Multi-Task Learning. (arXiv:1602.06518v1 [stat.ML]) https://t.co/1JjGPrtb3b,0,3," Abstract: In this paper we consider the problem of multi-task learning, in which a learner is given a collection of prediction tasks that need to be solved. In contrast to previous work, we give up on the assumption that labeled training data is available for all tasks. Instead, we propose an active task selection framework, where based only on the unlabeled data, the learner can choose a, typically small, subset of tasks for which he gets some labeled examples. For the remaining tasks, which have no available annotation, solutions are found by transferring information from the selected tasks. We analyze two transfer strategies and develop generalization bounds for each of them. Based on this theoretical analysis we propose two algorithms for making the choice of labeled tasks in a principled way and show their effectiveness on synthetic and real data. "
701946762081255424,2016-02-23 01:48:44,https://t.co/adJOrhZQ6i,Multi-task and Lifelong Learning of Kernels. (arXiv:1602.06531v1 [stat.ML]) https://t.co/adJOrhZQ6i,0,2," Abstract: We consider a problem of learning kernels for use in SVM classification in the multi-task and lifelong scenarios and provide generalization bounds on the error of a large margin classifier. Our results show that, under mild conditions on the family of kernels used for learning, solving several related tasks simultaneously is beneficial over single task learning. In particular, as the number of observed tasks grows, assuming that in the considered family of kernels there exists one that yields low approximation error on all tasks, the overhead associated with learning such a kernel vanishes and the complexity converges to that of learning when this good kernel is given to the learner. "
701946760772644864,2016-02-23 01:48:43,https://t.co/U4moKcsaiR,Semi-Markov Switching Vector Autoregressive Model-based Anomaly Detection in Aviation Systems. (arXiv:1602.06550v1… https://t.co/U4moKcsaiR,0,1," Abstract: In this work we consider the problem of anomaly detection in heterogeneous, multivariate, variable-length time series datasets. Our focus is on the aviation safety domain, where data objects are flights and time series are sensor readings and pilot switches. In this context the goal is to detect anomalous flight segments, due to mechanical, environmental, or human factors in order to identifying operationally significant events and provide insights into the flight operations and highlight otherwise unavailable potential safety risks and precursors to accidents. For this purpose, we propose a framework which represents each flight using a semi-Markov switching vector autoregressive (SMS-VAR) model. Detection of anomalies is then based on measuring dissimilarities between the model's prediction and data observation. The framework is scalable, due to the inherent parallel nature of most computations, and can be used to perform online anomaly detection. Extensive experimental results on simulated and real datasets illustrate that the framework can detect various types of anomalies along with the key parameters involved. "
701946759799513089,2016-02-23 01:48:43,https://t.co/tlxhvFRUXN,Interactive Storytelling over Document Collections. (arXiv:1602.06566v1 [cs.AI]) https://t.co/tlxhvFRUXN,0,4," Abstract: Storytelling algorithms aim to 'connect the dots' between disparate documents by linking starting and ending documents through a series of intermediate documents. Existing storytelling algorithms are based on notions of coherence and connectivity, and thus the primary way by which users can steer the story construction is via design of suitable similarity functions. We present an alternative approach to storytelling wherein the user can interactively and iteratively provide 'must use' constraints to preferentially support the construction of some stories over others. The three innovations in our approach are distance measures based on (inferred) topic distributions, the use of constraints to define sets of linear inequalities over paths, and the introduction of slack and surplus variables to condition the topic distribution to preferentially emphasize desired terms over others. We describe experimental results to illustrate the effectiveness of our interactive storytelling approach over multiple text datasets. "
701946758218256384,2016-02-23 01:48:43,https://t.co/ATomEeFp1D,"2-Bit Random Projections, NonLinear Estimators, and Approximate Near Neighbor Search. (arXiv:1602.06577v1 [stat.ML… https://t.co/ATomEeFp1D",0,4," Abstract: The method of random projections has become a standard tool for machine learning, data mining, and search with massive data at Web scale. The effective use of random projections requires efficient coding schemes for quantizing (real-valued) projected data into integers. In this paper, we focus on a simple 2-bit coding scheme. In particular, we develop accurate nonlinear estimators of data similarity based on the 2-bit strategy. This work will have important practical applications. For example, in the task of near neighbor search, a crucial step (often called re-ranking) is to compute or estimate data similarities once a set of candidate data points have been identified by hash table techniques. This re-ranking step can take advantage of the proposed coding scheme and estimator. As a related task, in this paper, we also study a simple uniform quantization scheme for the purpose of building hash tables with projected data. Our analysis shows that typically only a small number of bits are needed. For example, when the target similarity level is high, 2 or 3 bits might be sufficient. When the target similarity level is not so high, it is preferable to use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a good choice for the task of sublinear time approximate near neighbor search via hash tables. Combining these results, we conclude that 2-bit random projections should be recommended for approximate near neighbor search and similarity estimation. Extensive experimental results are provided. "
701946757312290816,2016-02-23 01:48:43,https://t.co/JJn3IjiWfT,Estimating Structured Vector Autoregressive Model. (arXiv:1602.06606v1 [math.ST]) https://t.co/JJn3IjiWfT,0,2," Abstract: While considerable advances have been made in estimating high-dimensional structured models from independent data using Lasso-type models, limited progress has been made for settings when the samples are dependent. We consider estimating structured VAR (vector auto-regressive models), where the structure can be captured by any suitable norm, e.g., Lasso, group Lasso, order weighted Lasso, sparse group Lasso, etc. In VAR setting with correlated noise, although there is strong dependence over time and covariates, we establish bounds on the non-asymptotic estimation error of structured VAR parameters. Surprisingly, the estimation error is of the same order as that of the corresponding Lasso-type estimator with independent samples, and the analysis holds for any norm. Our analysis relies on results in generic chaining, sub-exponential martingales, and spectral representation of VAR models. Experimental results on synthetic data with a variety of structures as well as real aviation data are presented, validating theoretical results. "
701946756423098370,2016-02-23 01:48:42,https://t.co/zga80f1ocq,Clustering subgaussian mixtures by semidefinite programming. (arXiv:1602.06612v1 [stat.ML]) https://t.co/zga80f1ocq,0,2," Abstract: We introduce a model-free relax-and-round algorithm for k-means clustering based on a semidefinite relaxation due to Peng and Wei. The algorithm interprets the SDP output as a denoised version of the original data and then rounds this output to a hard clustering. We provide a generic method for proving performance guarantees for this algorithm, and we analyze the algorithm in the context of subgaussian mixture models. We also study the fundamental limits of estimating Gaussian centers by k-means clustering in order to compare our approximation guarantee to the theoretically optimal k-means clustering solution. "
701946755445878785,2016-02-23 01:48:42,https://t.co/sXzWcPDzaJ,Denoising and Covariance Estimation of Single Particle Cryo-EM Images. (arXiv:1602.06632v1 [cs.CV]) https://t.co/sXzWcPDzaJ,1,2," Abstract: The problem of image restoration in cryo-EM entails correcting for the effects of the Contrast Transfer Function (CTF) and noise. Popular methods for image restoration include `phase flipping', which corrects only for the Fourier phases but not amplitudes, and Wiener filtering, which requires the spectral signal to noise ratio. We propose a new image restoration method which we call `Covariance Wiener Filtering' (CWF). In CWF, the covariance matrix of the projection images is used within the classical Wiener filtering framework for solving the image restoration deconvolution problem. Our estimation procedure for the covariance matrix is new and successfully corrects for the CTF. We demonstrate the efficacy of CWF by applying it to restore both simulated and experimental cryo-EM images. Results with experimental datasets demonstrate that CWF provides a good way to evaluate the particle images and to see what the dataset contains even without 2D classification and averaging. "
701946754254643200,2016-02-23 01:48:42,https://t.co/EfjAs3PE8O,A Geometric Analysis of Phase Retrieval. (arXiv:1602.06664v1 [cs.IT]) https://t.co/EfjAs3PE8O,1,4," Abstract: Can we recover a complex signal from its Fourier magnitudes? More generally, given a set of $m$ measurements, $y_k = |\mathbf a_k^* \mathbf x|$ for $k = 1, \dots, m$, is it possible to recover $\mathbf x \in \mathbb{C}^n$ (i.e., length-$n$ complex vector)? This **generalized phase retrieval** (GPR) problem is a fundamental task in various disciplines, and has been the subject of much recent investigation. Natural nonconvex heuristics often work remarkably well for GPR in practice, but lack clear theoretical explanations. In this paper, we take a step towards bridging this gap. We prove that when the measurement vectors $\mathbf a_k$'s are generic (i.i.d. complex Gaussian) and the number of measurements is large enough ($m \ge C n \log^3 n$), with high probability, a natural least-squares formulation for GPR has the following benign geometric structure: (1) there are no spurious local minimizers, and all global minimizers are equal to the target signal $\mathbf x$, up to a global phase; and (2) the objective function has a negative curvature around each saddle point. This structure allows a number of iterative optimization methods to efficiently find a global minimizer, without special initialization. To corroborate the claim, we describe and analyze a second-order trust-region algorithm. "
701946753239687168,2016-02-23 01:48:42,https://t.co/KZzF6LD259,Preconditioning Kernel Matrices. (arXiv:1602.06693v1 [stat.ML]) https://t.co/KZzF6LD259,0,3," Abstract: The computational and storage complexity of kernel machines presents the primary barrier to their scaling to large, modern, datasets. A common way to tackle the scalability issue is to use the conjugate gradient algorithm, which relieves the constraints on both storage (the kernel matrix need not be stored) and computation (both stochastic gradients and parallelization can be used). Even so, conjugate gradient is not without its own issues: the conditioning of kernel matrices is often such that conjugate gradients will have poor convergence in practice. Preconditioning is a common approach to alleviating this issue. Here we propose preconditioned conjugate gradients for kernel machines, and develop a broad range of preconditioners particularly useful for kernel matrices. We describe a scalable approach to both solving kernel machines and learning their hyperparameters. We show this approach is exact in the limit of iterations and outperforms state-of-the-art approximations for a given computational budget. "
701946751993958401,2016-02-23 01:48:41,https://t.co/D9p9hMBGbf,Inference Networks for Sequential Monte Carlo in Graphical Models. (arXiv:1602.06701v1 [stat.ML]) https://t.co/D9p9hMBGbf,0,3," Abstract: We introduce a new approach for amortizing inference in directed graphical models by learning heuristic approximations to stochastic inverses, designed specifically for use as proposal distributions in sequential Monte Carlo methods. We describe a procedure for constructing and learning a structured neural network which represents an inverse factorization of the graphical model, resulting in a conditional density estimator that takes as input particular values of the observed random variables, and returns an approximation to the distribution of the latent variables. This recognition model can be learned offline, independent from any particular dataset, prior to performing inference. The output of these networks can be used as automatically-learned high-quality proposal distributions to accelerate sequential Monte Carlo across a diverse range of problem settings. "
701946750504984576,2016-02-23 01:48:41,https://t.co/a7gC7oB3au,Variational inference for Monte Carlo objectives. (arXiv:1602.06725v1 [cs.LG]) https://t.co/a7gC7oB3au,4,11," Abstract: Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2016) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multi-sample objectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator proposed for the single-sample variational objective, and is competitive with the currently used biased estimators. "
701946749527715841,2016-02-23 01:48:41,https://t.co/V3wntwiYaG,Convexification of Learning from Constraints. (arXiv:1602.06746v1 [cs.LG]) https://t.co/V3wntwiYaG,0,3," Abstract: Regularized empirical risk minimization with constrained labels (in contrast to fixed labels) is a remarkably general abstraction of learning. For common loss and regularization functions, this optimization problem assumes the form of a mixed integer program (MIP) whose objective function is non-convex. In this form, the problem is resistant to standard optimization techniques. We construct MIPs with the same solutions whose objective functions are convex. Specifically, we characterize the tightest convex extension of the objective function, given by the Legendre-Fenchel biconjugate. Computing values of this tightest convex extension is NP-hard. However, by applying our characterization to every function in an additive decomposition of the objective function, we obtain a class of looser convex extensions that can be computed efficiently. For some decompositions, common loss and regularization functions, we derive a closed form. "
701946748449726464,2016-02-23 01:48:41,https://t.co/9UfO080PhP,Principal Component Projection Without Principal Component Analysis. (arXiv:1602.06872v1 [cs.DS]) https://t.co/9UfO080PhP,0,7," Abstract: We show how to efficiently project a vector onto the top principal components of a matrix, without explicitly computing these components. Specifically, we introduce an iterative algorithm that provably computes the projection using few calls to any black-box routine for ridge regression. By avoiding explicit principal component analysis (PCA), our algorithm is the first with no runtime dependence on the number of top principal components. We show that it can be used to give a fast iterative method for the popular principal component regression problem, giving the first major runtime improvement over the naive method of combining PCA with regression. To achieve our results, we first observe that ridge regression can be used to obtain a ""smooth projection"" onto the top principal components. We then sharpen this approximation to true projection using a low-degree polynomial approximation to the matrix step function. Step function approximation is a topic of long-term interest in scientific computing. We extend prior theory by constructing polynomials with simple iterative structure and rigorously analyzing their behavior under limited precision. "
701946747426361344,2016-02-23 01:48:40,https://t.co/tHrNxzNSVo,Clustering with a Reject Option: Interactive Clustering as Bayesian Prior Elicitation. (arXiv:1602.06886v1 [stat.M… https://t.co/tHrNxzNSVo,0,3," Abstract: A good clustering can help a data analyst to explore and understand a data set, but what constitutes a good clustering may depend on domain-specific and application-specific criteria. These criteria can be difficult to formalize, even when it is easy for an analyst to know a good clustering when she sees one. We present a new approach to interactive clustering for data exploration, called \ciif, based on a particularly simple feedback mechanism, in which an analyst can choose to reject individual clusters and request new ones. The new clusters should be different from previously rejected clusters while still fitting the data well. We formalize this interaction in a novel Bayesian prior elicitation framework. In each iteration, the prior is adapted to account for all the previous feedback, and a new clustering is then produced from the posterior distribution. To achieve the computational efficiency necessary for an interactive setting, we propose an incremental optimization method over data minibatches using Lagrangian relaxation. Experiments demonstrate that \ciif can produce accurate and diverse clusterings. "
701946746390364160,2016-02-23 01:48:40,https://t.co/AVsZ5kTYq1,Matching Matrix Bernstein with Little Memory: Near-Optimal Finite Sample Guarantees for Oja's Algorithm. (arXiv:16… https://t.co/AVsZ5kTYq1,0,2," Abstract: This work provides improved guarantees for streaming principle component analysis (PCA). Given $A_1, \ldots, A_n\in \mathbb{R}^{d\times d}$ sampled independently from distributions satisfying $\mathbb{E}[A_i] = \Sigma$ for $\Sigma \succeq \mathbf{0}$, this work provides an $O(d)$-space linear-time single-pass streaming algorithm for estimating the top eigenvector of $\Sigma$. The algorithm nearly matches (and in certain cases improves upon) the accuracy obtained by the standard batch method that computes top eigenvector of the empirical covariance $\frac{1}{n} \sum_{i \in [n]} A_i$ as analyzed by the matrix Bernstein inequality. Moreover, to achieve constant accuracy, our algorithm improves upon the best previous known sample complexities of streaming algorithms by either a multiplicative factor of $O(d)$ or $1/\mathrm{gap}$ where $\mathrm{gap}$ is the relative distance between the top two eigenvalues of $\Sigma$. These results are achieved through a novel analysis of the classic Oja's algorithm, one of the oldest and most popular algorithms for streaming PCA. In particular, this work shows that simply picking a random initial point $w_0$ and applying the update rule $w_{i + 1} = w_i + \eta_i A_i w_i$ suffices to accurately estimate the top eigenvector, with a suitable choice of $\eta_i$. We believe our result sheds light on how to efficiently perform streaming PCA both in theory and in practice and we hope that our analysis may serve as the basis for analyzing many variants and extensions of streaming PCA. "
701946745337602050,2016-02-23 01:48:40,https://t.co/r7d5CME2tc,Learning Laplacian Matrix in Smooth Graph Signal Representations. (arXiv:1406.7842v3 [cs.LG] UPDATED) https://t.co/r7d5CME2tc,0,4," Abstract: The construction of a meaningful graph plays a crucial role in the success of many graph-based representations and algorithms for handling structured data, especially in the emerging field of graph signal processing. However, a meaningful graph is not always readily available from the data, nor easy to define depending on the application domain. In particular, it is often desirable in graph signal processing applications that a graph is chosen such that the data admit certain regularity or smoothness on the graph. In this paper, we address the problem of learning graph Laplacians, which is equivalent to learning graph topologies, such that the input data form graph signals with smooth variations on the resulting topology. To this end, we adopt a factor analysis model for the graph signals and impose a Gaussian probabilistic prior on the latent variables that control these signals. We show that the Gaussian prior leads to an efficient representation that favors the smoothness property of the graph signals. We then propose an algorithm for learning graphs that enforces such property and is based on minimizing the variations of the signals on the learned graph. Experiments on both synthetic and real world data demonstrate that the proposed graph learning framework can efficiently infer meaningful graph topologies from signal observations under the smoothness prior. "
701946744251224065,2016-02-23 01:48:40,https://t.co/gZiFydlwkV,A Spectral Algorithm for Inference in Hidden Semi-Markov Models. (arXiv:1407.3422v2 [stat.ML] UPDATED) https://t.co/gZiFydlwkV,1,6," Abstract: Hidden semi-Markov models (HSMMs) are latent variable models which allow latent state persistence and can be viewed as a generalization of the popular hidden Markov models (HMMs). In this paper, we introduce a novel spectral algorithm to perform inference in HSMMs. Unlike expectation maximization (EM), our approach correctly estimates the probability of given observation sequence based on a set of training sequences. Our approach is based on estimating moments from the sample, whose number of dimensions depends only logarithmically on the maximum length of the hidden state persistence. Moreover, the algorithm requires only a few matrix inversions and is therefore computationally efficient. Empirical evaluations on synthetic and real data demonstrate the advantage of the algorithm over EM in terms of speed and accuracy, especially for large datasets. "
701946742900654084,2016-02-23 01:48:39,https://t.co/xJs93Xs8iR,Non-linear Causal Inference using Gaussianity Measures. (arXiv:1409.4573v3 [stat.ML] UPDATED) https://t.co/xJs93Xs8iR,1,3," Abstract: We provide theoretical and empirical evidence for a type of asymmetry between causes and effects that is present when these are related via linear models contaminated with additive non-Gaussian noise. Assuming that the causes and the effects have the same distribution, we show that the distribution of the residuals of a linear fit in the anti-causal direction is closer to a Gaussian than the distribution of the residuals in the causal direction. This Gaussianization effect is characterized by reduction of the magnitude of the high-order cumulants and by an increment of the differential entropy of the residuals. The problem of non-linear causal inference is addressed by performing an embedding in an expanded feature space, in which the relation between causes and effects can be assumed to be linear. The effectiveness of a method to discriminate between causes and effects based on this type of asymmetry is illustrated in a variety of experiments using different measures of Gaussianity. The proposed method is shown to be competitive with state-of-the-art techniques for causal inference. "
701946741399166976,2016-02-23 01:48:39,https://t.co/5nnFRAFZMl,Theoretical guarantees for approximate sampling from smooth and log-concave densities. (arXiv:1412.7392v5 [sta… https://t.co/5nnFRAFZMl,1,5," Abstract: Sampling from various kinds of distributions is an issue of paramount importance in statistics since it is often the key ingredient for constructing estimators, test procedures or confidence intervals. In many situations, the exact sampling from a given distribution is impossible or computationally expensive and, therefore, one needs to resort to approximate sampling strategies. However, there is no well-developed theory providing meaningful nonasymptotic guarantees for the approximate sampling procedures, especially in the high-dimensional problems. This paper makes some progress in this direction by considering the problem of sampling from a distribution having a smooth and log-concave density defined on $\mathbb R^p$, for some integer $p>0$. We establish nonasymptotic bounds for the error of approximating the true distribution by the one obtained by the Langevin Monte Carlo method and its variants. We illustrate the effectiveness of the established guarantees with various experiments. Underlying our analysis are insights from the theory of continuous-time diffusion processes, which may be of interest beyond the framework of distributions with log-concave densities considered in the present work. "
701946739226505216,2016-02-23 01:48:38,https://t.co/jxsliUNUw8,Learning to classify with possible sensor failures. (arXiv:1507.04540v3 [cs.LG] UPDATED) https://t.co/jxsliUNUw8,0,3," Abstract: In this paper, we propose a general framework to learn a robust large-margin binary classifier when corrupt measurements, called anomalies, caused by sensor failure might be present in the training set. The goal is to minimize the generalization error of the classifier on non-corrupted measurements while controlling the false alarm rate associated with anomalous samples. By incorporating a non-parametric regularizer based on an empirical entropy estimator, we propose a Geometric-Entropy-Minimization regularized Maximum Entropy Discrimination (GEM-MED) method to learn to classify and detect anomalies in a joint manner. We demonstrate using simulated data and a real multimodal data set. Our GEM-MED method can yield improved performance over previous robust classification methods in terms of both classification accuracy and anomaly detection rate. "
701946738060480512,2016-02-23 01:48:38,https://t.co/sWOv01H8Tf,Dynamic Filtering of Time-Varying Sparse Signals via l1 Minimization. (arXiv:1507.06145v2 [math.ST] UPDATED) https://t.co/sWOv01H8Tf,0,3," Abstract: Despite the importance of sparsity signal models and the increasing prevalence of high-dimensional streaming data, there are relatively few algorithms for dynamic filtering of time-varying sparse signals. Of the existing algorithms, fewer still provide strong performance guarantees. This paper examines two algorithms for dynamic filtering of sparse signals that are based on efficient l1 optimization methods. We first present an analysis for one simple algorithm (BPDN-DF) that works well when the system dynamics are known exactly. We then introduce a novel second algorithm (RWL1-DF) that is more computationally complex than BPDN-DF but performs better in practice, especially in the case where the system dynamics model is inaccurate. Robustness to model inaccuracy is achieved by using a hierarchical probabilistic data model and propagating higher-order statistics from the previous estimate (akin to Kalman filtering) in the sparse inference process. We demonstrate the properties of these algorithms on both simulated data as well as natural video sequences. Taken together, the algorithms presented in this paper represent the first strong performance analysis of dynamic filtering algorithms for time-varying sparse signals as well as state-of-the-art performance in this emerging application. "
701946734671491072,2016-02-23 01:48:37,https://t.co/MRLiPaprZc,Document Context Language Models. (arXiv:1511.03962v4 [cs.CL] UPDATED) https://t.co/MRLiPaprZc,0,5," Abstract: Text documents are structured on multiple levels of detail: individual words are related by syntax, but larger units of text are related by discourse structure. Existing language models generally fail to account for discourse structure, but it is crucial if we are to have language models that reward coherence and generate coherent texts. We present and empirically evaluate a set of multi-level recurrent neural network language models, called Document-Context Language Models (DCLM), which incorporate contextual information both within and beyond the sentence. In comparison with word-level recurrent neural network language models, the DCLM models obtain slightly better predictive likelihoods, and considerably better assessments of document coherence. "
