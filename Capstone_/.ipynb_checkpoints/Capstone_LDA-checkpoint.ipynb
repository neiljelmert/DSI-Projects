{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ga/anaconda/envs/dsi/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"text.color\": \".1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_statml = pd.read_csv(\"StatMLPapers_tweets.csv\")\n",
    "df_statml['year'] = 2016\n",
    "df_statml['genre'] = 'ML'\n",
    "df_statml['source'] = 'Twitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_16_NIPS = pd.read_csv('/Users/ga/Desktop/Capstone/open_review_abstracts_NIPS2016.csv')\n",
    "df_16_NIPS['year'] = 2016\n",
    "df_16_NIPS['genre'] = 'ML'\n",
    "df_16_NIPS['source'] = 'NIPS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_17_ICLR = pd.read_csv('/Users/ga/Desktop/Capstone/open_review_abstracts_ICLR2017.csv')\n",
    "df_17_ICLR['year'] = 2017\n",
    "df_17_ICLR['genre'] = 'DL'\n",
    "df_17_ICLR['source'] = 'ICLR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_16_NIPS_NAMPI = pd.read_csv('/Users/ga/Desktop/Capstone/open_review_abstracts_NIPS16_NAMPI.csv')\n",
    "df_16_NIPS_NAMPI['year'] = 2016\n",
    "df_16_NIPS_NAMPI['genre'] = 'ML'\n",
    "df_16_NIPS_NAMPI['source'] = 'NIPS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_16_NIPS_MLITS = pd.read_csv('/Users/ga/Desktop/Capstone/open_review_abstracts_NIPS16_MLITS.csv')\n",
    "df_16_NIPS_MLITS['year'] = 2016\n",
    "df_16_NIPS_MLITS['genre'] = 'ML'\n",
    "df_16_NIPS_MLITS['source'] = 'NIPS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_16_ICLR = pd.read_csv('/Users/ga/Desktop/Capstone/open_review_abstracts_ICLR16.csv')\n",
    "df_16_ICLR['year'] = 2016\n",
    "df_16_ICLR['genre'] = 'DL'\n",
    "df_16_ICLR['source'] = 'ICLR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_CVPR_16 = pd.read_csv('/Users/ga/Desktop/Capstone/CVPR_2016/cvpr_2016.csv')\n",
    "df_CVPR_16['year'] = 2016\n",
    "df_CVPR_16['genre'] = 'CV'\n",
    "df_CVPR_16['source'] = 'CVPR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_CVPR_15 = pd.read_csv('/Users/ga/Desktop/Capstone/CVPR_2016/cvpr_2015.csv')\n",
    "df_CVPR_15['year'] = 2015\n",
    "df_CVPR_15['genre'] = 'CV'\n",
    "df_CVPR_15['source'] = 'CVPR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_CVPR_14 = pd.read_csv('/Users/ga/Desktop/Capstone/CVPR_2016/cvpr_2014.csv')\n",
    "df_CVPR_14['year'] = 2014\n",
    "df_CVPR_14['genre'] = 'CV'\n",
    "df_CVPR_14['source'] = 'CVPR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_CVPR_13 = pd.read_csv('/Users/ga/Desktop/Capstone/CVPR_2016/cvpr_2013.csv')\n",
    "df_CVPR_13['year'] = 2013\n",
    "df_CVPR_13['genre'] = 'CV'\n",
    "df_CVPR_13['source'] = 'CVPR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_NIPS = pd.read_csv('/Users/ga/Desktop/Capstone/NIPS/NIPS.csv')\n",
    "df_NIPS['genre'] = 'ML'\n",
    "df_NIPS['source'] = 'NIPS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_JMLR_16 = pd.read_csv('/Users/ga/Desktop/Capstone/JMLR/JMLR_16.csv')\n",
    "df_JMLR_16['genre'] = 'ML'\n",
    "df_JMLR_16['source'] = 'JMLR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_JMLR_15 = pd.read_csv('/Users/ga/Desktop/Capstone/JMLR/JMLR_15.csv')\n",
    "df_JMLR_15['genre'] = 'ML'\n",
    "df_JMLR_15['source'] = 'JMLR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_JMLR_14 = pd.read_csv('/Users/ga/Desktop/Capstone/JMLR/JMLR_14.csv')\n",
    "df_JMLR_14['genre'] = 'ML'\n",
    "df_JMLR_14['source'] = 'JMLR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_JMLR_13 = pd.read_csv('/Users/ga/Desktop/Capstone/JMLR/JMLR_13.csv')\n",
    "df_JMLR_13['genre'] = 'ML'\n",
    "df_JMLR_13['source'] = 'JMLR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_arxiv = pd.read_csv(\"/Users/ga/Desktop/Capstone/reddit/reddit_scraped_arxivs.csv\")\n",
    "df_arxiv['year'] = 2016\n",
    "df_arxiv['genre'] = 'ML'\n",
    "df_arxiv['source'] = 'arXiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_open_rev = pd.read_csv(\"/Users/ga/Desktop/Capstone/reddit/reddit_scraped_openreview.csv\")\n",
    "df_open_rev['year'] = 2016\n",
    "df_open_rev['genre'] = 'ML'\n",
    "df_open_rev['source'] = 'OpenReview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_open_rev_s = df_open_rev[['title', 'abstract', 'genre', 'source', 'year']]\n",
    "df_arxiv_s = df_arxiv[['title', 'abstract', 'genre', 'source', 'year']]\n",
    "df_statml_s = df_statml[['text', 'abstract', 'genre', 'source', 'year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "#find = re.compile(r\"^(.*?)\\..*\")\n",
    "new_text = []\n",
    "find = re.compile(r\"^([^.]*).*\")\n",
    "for l in df_statml_s['text']:\n",
    "    m = re.match(find, l)\n",
    "    new_text.append(m.group(1))\n",
    "\n",
    "df_statml_s['title'] = new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_statml_s.drop([\"text\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = [df_statml_s, df_arxiv_s, df_open_rev_s, df_JMLR_13, df_JMLR_14, df_JMLR_15,\n",
    "         df_JMLR_16, df_CVPR_13, df_CVPR_14, df_CVPR_15, df_CVPR_16, df_16_ICLR,\n",
    "         df_16_NIPS_MLITS, df_16_NIPS_NAMPI, df_17_ICLR, df_16_NIPS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7503, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6806"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result.abstract.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6759"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unik_titles = result.title.unique().tolist()\n",
    "len(result.title.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = result.drop_duplicates(\"abstract\")\n",
    "result = result.drop_duplicates(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6605, 5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6605"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result.title.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result['abstract'] = result['abstract'].str.replace('Abstract:?' , '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result['title'] = result['title'].str.replace('\\[R\\]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3722, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result.year == 2016].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(867, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result.year == 2015].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(849, 5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result.year == 2014].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(753, 5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result.year == 2013].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 5)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result.year == 2017].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genre</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CV</th>\n",
       "      <td>2247</td>\n",
       "      <td>2247</td>\n",
       "      <td>2247</td>\n",
       "      <td>2247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DL</th>\n",
       "      <td>518</td>\n",
       "      <td>518</td>\n",
       "      <td>518</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ML</th>\n",
       "      <td>3839</td>\n",
       "      <td>3840</td>\n",
       "      <td>3840</td>\n",
       "      <td>3840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       abstract  source  title  year\n",
       "genre                               \n",
       "CV         2247    2247   2247  2247\n",
       "DL          518     518    518   518\n",
       "ML         3839    3840   3840  3840"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.groupby(['genre']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2013    0.114005\n",
       "2014    0.128539\n",
       "2015    0.131264\n",
       "2016    0.563512\n",
       "2017    0.062680\n",
       "Name: title, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.groupby(['year'])['title'].count()/result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152,\n",
       "            6153, 6154, 6569, 6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577,\n",
       "            6578, 6579, 6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588,\n",
       "            6589, 6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599,\n",
       "            6600, 6601, 6602, 6603, 6604],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['source'] == 'NIPS'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6605, 5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = pd.DataFrame(result.groupby(['year', 'genre'])[\"title\"].count()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11a0bc410>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAE4CAYAAABVMDj3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119eb1910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result.groupby(['year', 'genre']).count()[\"title\"].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_abs = result['abstract'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant \n",
    "import inflect\n",
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'analysi'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.singular_noun(\"analysis\", count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_words = ['tasks', 'methods', 'using', 'propose', 'based', 'problem',\n",
    "            'results', 'method', 'different', 'demonstrate', 'paper',\n",
    "            'approach', 'task', 'used', 'proposed', 'provide', 'study'\n",
    "            'use', 'problems', 'work', 'present', \"es\", \"al\", \"et\", \"em\", \"ed\", \"ca\", \n",
    "            \"ial\", \"rl\", \"i\", \"ii\", \"iii\", \"res\", \"ing\", \"ts\", 'consider', \n",
    "            'include', 'issue', 'follow', 'remain', 'reation', \n",
    "            'achiev', 'techniqu', 'mage', 'cifar', 'learnng', 'ful', \n",
    "            'gorithms', 'ths', 'gorithm', 'featur', 'ful', 'subs', 'rs', 'ell']\n",
    "\n",
    "bad_symb = ['\\\\',',','^',';',':','/','`','*','_','`','~','%','&','|','@',\n",
    "               '{','}','[',']','(',')','>','<','\"','#','+','.','!','=','?','$','\\'']\n",
    "\n",
    "def abstract_cleaner(abstract):\n",
    "    is_english = enchant.Dict(\"en_US\")\n",
    "    abstract = \"\".join([i for i in abstract if not i.isdigit()])\n",
    "    abstract = abstract.replace(\"\\n\", \" \")\n",
    "    abstract = abstract.replace(\"-\", \"\")\n",
    "    for char in bad_symb:\n",
    "        if char in abstract:\n",
    "            abstract = abstract.replace(char, \"\")\n",
    "    for char in bad_words:\n",
    "        if char.strip() in abstract.split(\" \"):\n",
    "            abstract = abstract.replace(char, \"\")\n",
    "    for word in abstract:\n",
    "        if not is_english.check(word):\n",
    "            abstract = abstract.replace(word, \"\")\n",
    "#     for word in abstract.split(\" \"):\n",
    "#         if p.singular_noun(word, count=None) is not False:\n",
    "#             abstract = abstract.replace(word, p.singular_noun(word, count=None))\n",
    "    abstract = filter(None, abstract)\n",
    "    return abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_abs = map(str, result_abs)\n",
    "result_abs = map(abstract_cleaner, result_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dl_abs = result[result.genre=='DL']['abstract'].tolist()\n",
    "dl_abs_join = \" \".join(str(v) for v in dl_abs)\n",
    "dl_abs_join = \"\".join([i for i in dl_abs_join if not i.isdigit()])\n",
    "dl_abs_join = dl_abs_join.replace(\"\\n\", \" \")\n",
    "dl_abs_join = dl_abs_join.replace(\".\", \"\")\n",
    "dl_abs_split = dl_abs_join.split(\" \")\n",
    "dl_abs_split = filter(None, dl_abs_split) # fastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_abs = result[result.genre=='ML']['abstract'].tolist()\n",
    "ml_abs_join = \" \".join(str(v) for v in ml_abs)\n",
    "ml_abs_join = \"\".join([i for i in ml_abs_join if not i.isdigit()])\n",
    "ml_abs_join = ml_abs_join.replace(\"\\n\", \" \")\n",
    "ml_abs_join = ml_abs_join.replace(\".\", \"\")\n",
    "ml_abs_split = ml_abs_join.split(\" \")\n",
    "ml_abs_split = filter(None, ml_abs_split) # fastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_abs = result[result.genre=='CV']['abstract'].tolist()\n",
    "cv_abs_join = \" \".join(str(v) for v in cv_abs)\n",
    "cv_abs_join = \"\".join([i for i in cv_abs_join if not i.isdigit()])\n",
    "cv_abs_join = cv_abs_join.replace(\"\\n\", \" \")\n",
    "cv_abs_join = cv_abs_join.replace(\".\", \"\")\n",
    "cv_abs_split = cv_abs_join.split(\" \")\n",
    "cv_abs_split = filter(None, cv_abs_split) # fastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_abs_join = ml_abs_join.replace('infinitedimensional', 'infinite dimensional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "s=set(stopwords.words('english'))\n",
    "\n",
    "new_s = s\n",
    "for sw in list(s):\n",
    "    new_s.add(sw.title())\n",
    "    \n",
    "nonstop_dl = filter(lambda w: not w in new_s, dl_abs_split)\n",
    "nonstop_ml = filter(lambda w: not w in new_s, ml_abs_split)\n",
    "nonstop_cv = filter(lambda w: not w in new_s, cv_abs_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# counts = Counter(nonstop_dl)\n",
    "# print counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from itertools import dropwhile\n",
    "\n",
    "# for key, count in dropwhile(lambda key_count: key_count[1] >= 2, counts.most_common()):\n",
    "#     del counts[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# labels, values = zip(*counts.items())\n",
    "# values = sorted(values)\n",
    "\n",
    "# indexes = np.arange(len(labels))\n",
    "# width = 1\n",
    "\n",
    "# plt.bar(indexes, values, width)\n",
    "# #plt.xticks(indexes + width * 0.1, labels)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dl = ' '.join(nonstop_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml = ' '.join(nonstop_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = ' '.join(nonstop_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.97, max_features=None, min_df=0.005,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvec = CountVectorizer(stop_words='english', min_df=0.005, max_df=0.97, ngram_range=(1,3))\n",
    "cvec.fit(result_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_countvect  = pd.DataFrame(cvec.transform(result_abs).todense(),\n",
    "             columns=cvec.get_feature_names())\n",
    "\n",
    "#df = df.transpose().sort_values(0, ascending=False).transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Twitter', 'arXiv', 'OpenReview', 'JMLR', 'CVPR', 'ICLR', 'NIPS'], dtype=object)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.source.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_count_NIPS = df_countvect.iloc[result[result.source=='NIPS'].index,:]\n",
    "df_count_Twitter = df_countvect.iloc[result[result.source=='Twitter'].index,:]\n",
    "df_count_arXiv = df_countvect.iloc[result[result.source=='arXiv'].index,:]\n",
    "df_count_OpenReview = df_countvect.iloc[result[result.source=='OpenReview'].index,:]\n",
    "df_count_CVPR = df_countvect.iloc[result[result.source=='CVPR'].index,:]\n",
    "df_count_ICLR = df_countvect.iloc[result[result.source=='ICLR'].index,:]\n",
    "df_count_JMLR = df_countvect.iloc[result[result.source=='JMLR'].index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plurals = ['algorithms', 'models', 'features', 'datasets', 'errors', 'functions',\n",
    "          'variables', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data                     2810\n",
       "learning                 1966\n",
       "model                    1828\n",
       "algorithm                1429\n",
       "models                   1396\n",
       "new                       898\n",
       "algorithms                892\n",
       "performance               842\n",
       "analysis                  737\n",
       "number                    721\n",
       "optimization              676\n",
       "function                  651\n",
       "linear                    625\n",
       "inference                 623\n",
       "time                      619\n",
       "matrix                    611\n",
       "information               609\n",
       "neural                    607\n",
       "set                       580\n",
       "use                       575\n",
       "networks                  575\n",
       "large                     559\n",
       "classification            552\n",
       "framework                 552\n",
       "training                  552\n",
       "network                   535\n",
       "stochastic                531\n",
       "deep                      519\n",
       "datasets                  516\n",
       "machine                   513\n",
       "                         ... \n",
       "bounding boxes              1\n",
       "patch                       1\n",
       "conditional random          1\n",
       "face recognition            1\n",
       "pose estimation             1\n",
       "blur                        1\n",
       "input image                 1\n",
       "illumination                1\n",
       "challenging datasets        1\n",
       "rgb                         1\n",
       "lighting                    1\n",
       "correspondences             1\n",
       "pascal voc                  0\n",
       "midlevel                    0\n",
       "pascal                      0\n",
       "human pose                  0\n",
       "rgbd                        0\n",
       "motions                     0\n",
       "bounding box                0\n",
       "stereo                      0\n",
       "action recognition          0\n",
       "nonrigid                    0\n",
       "new dataset                 0\n",
       "deformable                  0\n",
       "semantic segmentation       0\n",
       "voc                         0\n",
       "object segmentation         0\n",
       "superpixels                 0\n",
       "detections                  0\n",
       "pedestrian                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count_Twitter.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Twit_vocab = df_count_Twitter.sum(axis=0).sort_values(ascending=False).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {}\n",
    "for word1 in Twit_vocab:\n",
    "    for word2 in Twit_vocab:\n",
    "        if (word1 in word2 and len(word2.split(\" \")) == 1 and word1 != word2 and\n",
    "           word1 not in [\"es\", \"al\", \"et\", \"em\", \"ed\", \"ca\", \n",
    "                         \"ial\", \"rl\", \"i\", \"ii\", \"iii\",\n",
    "                         \"res\", \"ing\", \"lie\", \"ts\"]):\n",
    "            d.setdefault(word1, []).append(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "represent represents\n",
      "concept concepts\n",
      "gp gpu\n",
      "dynamic dynamics\n",
      "code codes\n",
      "generalize generalized\n",
      "generalize generalizes\n",
      "computation computations\n",
      "semantic semantics\n",
      "depend depends\n",
      "graph graphs\n",
      "technique techniques\n",
      "program programs\n",
      "advantage advantages\n",
      "activation activations\n",
      "rise arise\n",
      "choice choices\n",
      "difference differences\n",
      "condition conditions\n",
      "level levels\n",
      "sensor sensors\n",
      "solution solutions\n",
      "large larger\n",
      "vector vectors\n",
      "enhance enhanced\n",
      "equation equations\n",
      "prediction predictions\n",
      "approximation approximations\n",
      "rate rates\n",
      "cost costs\n",
      "aim aims\n",
      "reconstruction reconstructions\n",
      "net nets\n",
      "coefficient coefficients\n",
      "segmentation segmentations\n",
      "component components\n",
      "explore explored\n",
      "explore explores\n",
      "change changes\n",
      "prior priors\n",
      "prior priori\n",
      "action actions\n",
      "projection projections\n",
      "select selects\n",
      "environment environments\n",
      "use uses\n",
      "use user\n",
      "prove proved\n",
      "prove proven\n",
      "dnn dnns\n",
      "camera cameras\n",
      "encode encoder\n",
      "encode encoded\n",
      "encode encodes\n",
      "type types\n",
      "benchmark benchmarks\n",
      "limitation limitations\n",
      "hold holds\n",
      "effort efforts\n",
      "sample samples\n",
      "sample sampled\n",
      "sample sampler\n",
      "word words\n",
      "cluster clusters\n",
      "challenge challenges\n",
      "descriptor descriptors\n",
      "learn learns\n",
      "learn learnt\n",
      "example examples\n",
      "control controls\n",
      "compare compared\n",
      "compare compares\n",
      "stream streams\n",
      "predict predicts\n",
      "attribute attributes\n",
      "share shared\n",
      "agent agents\n",
      "topic topics\n",
      "refine refined\n",
      "arise arises\n",
      "occur occurs\n",
      "guarantee guarantees\n",
      "guarantee guaranteed\n",
      "motion motions\n",
      "end tend\n",
      "means kmeans\n",
      "feature features\n",
      "machine machines\n",
      "forest forests\n",
      "answer answers\n",
      "parameter parameters\n",
      "ensemble ensembles\n",
      "map maps\n",
      "description descriptions\n",
      "outlier outliers\n",
      "variant variants\n",
      "collection collections\n",
      "produce produces\n",
      "produce produced\n",
      "response responses\n",
      "correspond corresponds\n",
      "element elements\n",
      "allow allows\n",
      "correlation correlations\n",
      "representation representations\n",
      "discover discovery\n",
      "order orders\n",
      "help helps\n",
      "insight insights\n",
      "utilize utilizes\n",
      "utilize utilized\n",
      "suffer suffers\n",
      "derive derived\n",
      "group groups\n",
      "pixel pixels\n",
      "correspondence correspondences\n",
      "combination combinations\n",
      "introduce introduced\n",
      "introduce introduces\n",
      "scene scenes\n",
      "framework frameworks\n",
      "term terms\n",
      "document documents\n",
      "difficult difficulty\n",
      "classifier classifiers\n",
      "mean means\n",
      "subset subsets\n",
      "domain domains\n",
      "square squares\n",
      "square squared\n",
      "weight weights\n",
      "experiment experiments\n",
      "reduce reduced\n",
      "reduce reduces\n",
      "idea ideas\n",
      "idea ideal\n",
      "involve involves\n",
      "involve involved\n",
      "measure measures\n",
      "measure measured\n",
      "operation operations\n",
      "event events\n",
      "network networks\n",
      "space spaces\n",
      "gradient gradients\n",
      "increase increases\n",
      "increase increased\n",
      "size sizes\n",
      "evaluation evaluations\n",
      "architecture architectures\n",
      "million millions\n",
      "reason reasons\n",
      "base based\n",
      "base bases\n",
      "estimate estimates\n",
      "estimate estimated\n",
      "generate generated\n",
      "generate generates\n",
      "advance advances\n",
      "advance advanced\n",
      "interaction interactions\n",
      "filter filters\n",
      "turn turns\n",
      "number numbers\n",
      "transform transforms\n",
      "illustrate illustrated\n",
      "leverage leverages\n",
      "annotation annotations\n",
      "assumption assumptions\n",
      "scheme schemes\n",
      "relationship relationships\n",
      "formulate formulated\n",
      "attempt attempts\n",
      "distance distances\n",
      "kind kinds\n",
      "target targets\n",
      "require requires\n",
      "require required\n",
      "iteration iterations\n",
      "video videos\n",
      "outcome outcomes\n",
      "exhibit exhibits\n",
      "raw draw\n",
      "need needs\n",
      "seek seeks\n",
      "strength strengths\n",
      "hyperparameter hyperparameters\n",
      "mechanism mechanisms\n",
      "instance instances\n",
      "potential potentials\n",
      "build builds\n",
      "objective objectives\n",
      "performance performances\n",
      "detector detectors\n",
      "tool tools\n",
      "channel channels\n",
      "play plays\n",
      "normal normals\n",
      "object objects\n",
      "position positions\n",
      "pair pairs\n",
      "segment segments\n",
      "cause caused\n",
      "observation observations\n",
      "statistic statistics\n",
      "face faces\n",
      "principle principled\n",
      "principle principles\n",
      "incorporate incorporates\n",
      "incorporate incorporated\n",
      "speedup speedups\n",
      "converge converges\n",
      "relation relations\n",
      "tend tends\n",
      "implementation implementations\n",
      "version versions\n",
      "factor factors\n",
      "integrate integrated\n",
      "integrate integrates\n",
      "achieve achieves\n",
      "achieve achieved\n",
      "mixture mixtures\n",
      "handle handles\n",
      "viewpoint viewpoints\n",
      "reveal reveals\n",
      "outperform outperforms\n",
      "yield yields\n",
      "investigate investigated\n",
      "contain contains\n",
      "view views\n",
      "kernel kernels\n",
      "requirement requirements\n",
      "set sets\n",
      "tree trees\n",
      "frame frames\n",
      "detection detections\n",
      "individual individuals\n",
      "result results\n",
      "fail fails\n",
      "close closed\n",
      "subject subjects\n",
      "expert experts\n",
      "pattern patterns\n",
      "label labels\n",
      "state states\n",
      "score scores\n",
      "preserve preserves\n",
      "extend extends\n",
      "article particle\n",
      "configuration configurations\n",
      "come comes\n",
      "improve improved\n",
      "improve improves\n",
      "region regions\n",
      "bandit bandits\n",
      "connection connections\n",
      "stateoftheart stateofthearts\n",
      "context contexts\n",
      "contribution contributions\n",
      "expression expressions\n",
      "minima minimax\n",
      "minima minimal\n",
      "autoencoder autoencoders\n",
      "point points\n",
      "simple simpler\n",
      "aspect aspects\n",
      "exploit exploits\n",
      "maximize maximizes\n",
      "path paths\n",
      "create created\n",
      "proposal proposals\n",
      "define defined\n",
      "direction directions\n",
      "enable enables\n",
      "subspace subspaces\n",
      "observe observed\n",
      "case cases\n",
      "value values\n",
      "deformation deformations\n",
      "employ employs\n",
      "behavior behaviors\n",
      "error errors\n",
      "procedure procedures\n",
      "operate operates\n",
      "layer layers\n",
      "metric metrics\n",
      "surface surfaces\n",
      "characterize characterized\n",
      "capture captures\n",
      "capture captured\n",
      "recover recovery\n",
      "recover recovers\n",
      "baseline baselines\n",
      "perform performs\n",
      "suggest suggests\n",
      "make makes\n",
      "embedding embeddings\n",
      "occlusion occlusions\n",
      "effect effects\n",
      "moment moments\n",
      "user users\n",
      "validate validated\n",
      "scenario scenarios\n",
      "database databases\n",
      "minimize minimizes\n",
      "edge edges\n",
      "solve solved\n",
      "solve solves\n",
      "solve solver\n",
      "model models\n",
      "reward rewards\n",
      "dimension dimensions\n",
      "manifold manifolds\n",
      "sentence sentences\n",
      "obtain obtains\n",
      "shape shapes\n",
      "human humans\n",
      "alternative alternatives\n",
      "heuristic heuristics\n",
      "candidate candidates\n",
      "application applications\n",
      "improvement improvements\n",
      "source sources\n",
      "rgb rgbd\n",
      "combine combined\n",
      "combine combines\n",
      "location locations\n",
      "input inputs\n",
      "transformation transformations\n",
      "rnn rnns\n",
      "evaluate evaluated\n",
      "early nearly\n",
      "finding findings\n",
      "game games\n",
      "signal signals\n",
      "benefit benefits\n",
      "output outputs\n",
      "deal ideal\n",
      "sequence sequences\n",
      "accelerate accelerated\n",
      "scale scales\n",
      "lead leads\n",
      "decision decisions\n",
      "unit units\n",
      "cnn cnns\n",
      "core score\n",
      "run runs\n",
      "pose poses\n",
      "step steps\n",
      "optimize optimized\n",
      "optimize optimizes\n",
      "stage stages\n",
      "comparison comparisons\n",
      "algorthm algorthms\n",
      "extension extensions\n",
      "constraint constraints\n",
      "simulation simulations\n",
      "estimator estimators\n",
      "neighbor neighbors\n",
      "predictor predictors\n",
      "block blocks\n",
      "image images\n",
      "bound bounds\n",
      "dataset datasets\n",
      "determine determined\n",
      "operator operators\n",
      "ridge bridge\n",
      "area areas\n",
      "support supports\n",
      "question questions\n",
      "approximate approximated\n",
      "approximate approximates\n",
      "low flow\n",
      "low slow\n",
      "way ways\n",
      "analyze analyzed\n",
      "function functions\n",
      "form forms\n",
      "offer offers\n",
      "link links\n",
      "gain gains\n",
      "compute computed\n",
      "compute computer\n",
      "compute computes\n",
      "consist consists\n",
      "characteristic characteristics\n",
      "limit limits\n",
      "distribution distributions\n",
      "measurement measurements\n",
      "exist exists\n",
      "formulation formulations\n",
      "field fields\n",
      "setting settings\n",
      "test tests\n",
      "imag image\n",
      "node nodes\n",
      "draw drawn\n",
      "update updates\n",
      "update updated\n",
      "variation variations\n",
      "variable variables\n",
      "structure structured\n",
      "structure structures\n",
      "algorithm algorithms\n",
      "assume assumed\n",
      "assume assumes\n",
      "solver solvers\n",
      "curve curves\n",
      "rule rules\n",
      "tradeoff tradeoffs\n",
      "time times\n",
      "avoid avoids\n"
     ]
    }
   ],
   "source": [
    "for word in d.keys():\n",
    "    for candidate in d[word]:\n",
    "        if levenshteinDistance(word, candidate) == 1:\n",
    "            print word, candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def levenshteinDistance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshteinDistance(\"algorithm\", \"algorithmic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=0.005,\n",
       "        ngram_range=(1, 3), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer(stop_words='english', min_df=0.005, max_df=0.95, ngram_range=(1, 3))\n",
    "tvec.fit(result_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df  = pd.DataFrame(tvec.transform(result_abs).todense(),\n",
    "                   columns=tvec.get_feature_names(),\n",
    "                   index=result_abs)\n",
    "\n",
    "#df.transpose().sort_values('dl', ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'new method': 1738,\n",
       " u'fit': 1040,\n",
       " u'combinatorial': 402,\n",
       " u'dnn': 773,\n",
       " u'optimization framework': 1829,\n",
       " u'chain': 328,\n",
       " u'proposed algorithms': 2106,\n",
       " u'exact': 907,\n",
       " u'norm': 1755,\n",
       " u'hash': 1174,\n",
       " u'learn': 1438,\n",
       " u'influence': 1307,\n",
       " u'forest': 1057,\n",
       " u'coherence': 390,\n",
       " u'illustrate': 1237,\n",
       " u'runs': 2347,\n",
       " u'baselines': 227,\n",
       " u'descriptors': 689,\n",
       " u'dependencies': 672,\n",
       " u'advantages': 60,\n",
       " u'transition': 2750,\n",
       " u'using standard': 2814,\n",
       " u'metric': 1618,\n",
       " u'relevant': 2261,\n",
       " u'features extracted': 1015,\n",
       " u'visual recognition': 2871,\n",
       " u'new model': 1739,\n",
       " u'physics': 1948,\n",
       " u'similar': 2462,\n",
       " u'correlated': 558,\n",
       " u'camera': 295,\n",
       " u'decision making': 626,\n",
       " u'finite sample': 1037,\n",
       " u'vision tasks': 2868,\n",
       " u'root': 2341,\n",
       " u'unsupervised learning': 2795,\n",
       " u'assigned': 173,\n",
       " u'automatically': 200,\n",
       " u'ensembles': 861,\n",
       " u'new framework': 1737,\n",
       " u'considering': 498,\n",
       " u'risk minimization': 2333,\n",
       " u'linear': 1484,\n",
       " u'deep neural': 638,\n",
       " u'simulation results': 2475,\n",
       " u'convolutional': 543,\n",
       " u'subsequently': 2606,\n",
       " u'like': 1474,\n",
       " u'direct': 740,\n",
       " u'parallel': 1894,\n",
       " u'variational': 2841,\n",
       " u'surfaces': 2644,\n",
       " u'direction': 742,\n",
       " u'employing': 840,\n",
       " u'spatio temporal': 2522,\n",
       " u'attention': 188,\n",
       " u'questions': 2149,\n",
       " u'suggests': 2623,\n",
       " u'heavily': 1179,\n",
       " u'collections': 397,\n",
       " u'parameter': 1895,\n",
       " u'order magnitude': 1840,\n",
       " u'task learning': 2671,\n",
       " u'ad': 34,\n",
       " u'al': 72,\n",
       " u'relationship': 2255,\n",
       " u'focusing': 1049,\n",
       " u'video': 2857,\n",
       " u'stochastic gradient': 2572,\n",
       " u'inferred': 1304,\n",
       " u'index': 1291,\n",
       " u'frequency': 1074,\n",
       " u'interactions': 1337,\n",
       " u'documents': 776,\n",
       " u'trajectory': 2742,\n",
       " u'achieve state': 14,\n",
       " u'reproducing': 2285,\n",
       " u'variety': 2844,\n",
       " u'latent': 1425,\n",
       " u'window': 2895,\n",
       " u'storage': 2575,\n",
       " u'trees': 2756,\n",
       " u'aims': 71,\n",
       " u'interestingly': 1341,\n",
       " u'required': 2289,\n",
       " u'requires': 2292,\n",
       " u'spatiotemporal': 2523,\n",
       " u'existing methods': 920,\n",
       " u'addition': 43,\n",
       " u'latent variable models': 1429,\n",
       " u'expansion': 924,\n",
       " u'lstm': 1530,\n",
       " u'identify': 1228,\n",
       " u'important role': 1260,\n",
       " u'analyze': 103,\n",
       " u'propose new': 2099,\n",
       " u'starting': 2549,\n",
       " u'achieving': 20,\n",
       " u'individual': 1293,\n",
       " u'alignment': 83,\n",
       " u'forward': 1066,\n",
       " u'optimal solution': 1822,\n",
       " u'drawn': 786,\n",
       " u'passing': 1912,\n",
       " u'center': 325,\n",
       " u'tests': 2691,\n",
       " u'limitation': 1478,\n",
       " u'neighboring': 1712,\n",
       " u'groups': 1158,\n",
       " u'connectivity': 488,\n",
       " u'rate': 2168,\n",
       " u'arise': 151,\n",
       " u'demonstrate performance': 661,\n",
       " u'weights': 2888,\n",
       " u'photometric': 1946,\n",
       " u'respect': 2301,\n",
       " u'new approach': 1734,\n",
       " u'neural networks': 1728,\n",
       " u'methodology': 1612,\n",
       " u'areas': 149,\n",
       " u'accuracy': 10,\n",
       " u'correlations': 561,\n",
       " u'model trained': 1654,\n",
       " u'training set': 2739,\n",
       " u'assess': 171,\n",
       " u'desirable': 693,\n",
       " u'schemes': 2372,\n",
       " u'true': 2758,\n",
       " u'poisson': 1962,\n",
       " u'decompose': 630,\n",
       " u'heavy': 1180,\n",
       " u'reliability': 2262,\n",
       " u'art techniques': 164,\n",
       " u'phenomenon': 1944,\n",
       " u'explored': 957,\n",
       " u'precisely': 2000,\n",
       " u'cases': 316,\n",
       " u'based': 218,\n",
       " u'bases': 228,\n",
       " u'enables': 843,\n",
       " u'manifolds': 1549,\n",
       " u'mixtures': 1642,\n",
       " u'bounds': 279,\n",
       " u'presents novel': 2023,\n",
       " u'availability': 203,\n",
       " u'locally': 1501,\n",
       " u'iid': 1233,\n",
       " u'iii': 1234,\n",
       " u'poorly': 1970,\n",
       " u'represented': 2282,\n",
       " u'weak': 2880,\n",
       " u'procedure': 2065,\n",
       " u'environment': 867,\n",
       " u'discovering': 753,\n",
       " u'generative': 1112,\n",
       " u'cope': 551,\n",
       " u'outcome': 1847,\n",
       " u'irrelevant': 1374,\n",
       " u'coupled': 574,\n",
       " u'style': 2599,\n",
       " u'feed': 1016,\n",
       " u'large scale problems': 1420,\n",
       " u'inference algorithm': 1302,\n",
       " u'characteristic': 343,\n",
       " u'problems': 2064,\n",
       " u'work present': 2901,\n",
       " u'histogram': 1206,\n",
       " u'acquisition': 22,\n",
       " u'object': 1781,\n",
       " u'slightly': 2486,\n",
       " u'validity': 2827,\n",
       " u'note': 1761,\n",
       " u'scientific': 2374,\n",
       " u'events': 904,\n",
       " u'input': 1316,\n",
       " u'oracle': 1838,\n",
       " u'partial': 1901,\n",
       " u'singular': 2481,\n",
       " u'propose novel approach': 2101,\n",
       " u'factor': 991,\n",
       " u'rule': 2343,\n",
       " u'proper': 2086,\n",
       " u'bounding boxes': 278,\n",
       " u'kernel methods': 1391,\n",
       " u'analytical': 101,\n",
       " u'refine': 2230,\n",
       " u'benefit': 246,\n",
       " u'energy minimization': 854,\n",
       " u'actually': 33,\n",
       " u'targets': 2669,\n",
       " u'alpha': 90,\n",
       " u'emerging': 830,\n",
       " u'log likelihood': 1505,\n",
       " u'unseen': 2792,\n",
       " u'presents': 2022,\n",
       " u'overhead': 1866,\n",
       " u'balance': 214,\n",
       " u'super': 2628,\n",
       " u'understood': 2776,\n",
       " u'technique': 2675,\n",
       " u'finally': 1028,\n",
       " u'additive': 46,\n",
       " u'theoretical guarantees': 2700,\n",
       " u'views': 2864,\n",
       " u'recurrent neural networks': 2220,\n",
       " u'ranking': 2164,\n",
       " u'yield': 2912,\n",
       " u'classification performance': 362,\n",
       " u'geometric': 1120,\n",
       " u'different': 725,\n",
       " u'measuring': 1592,\n",
       " u'practical': 1994,\n",
       " u'models': 1660,\n",
       " u'probability distributions': 2057,\n",
       " u'desired': 694,\n",
       " u'phase': 1943,\n",
       " u'preliminary': 2014,\n",
       " u'et al': 894,\n",
       " u'data demonstrate': 606,\n",
       " u'trajectories': 2741,\n",
       " u'cnn': 382,\n",
       " u'efforts': 820,\n",
       " u'suboptimal': 2604,\n",
       " u'vector machine': 2849,\n",
       " u'expectation': 925,\n",
       " u'propose simple': 2102,\n",
       " u'recursive': 2221,\n",
       " u'data point': 610,\n",
       " u'linear regression': 1488,\n",
       " u'theoretical results': 2702,\n",
       " u'capabilities': 301,\n",
       " u'scales': 2365,\n",
       " u'sensitivity': 2406,\n",
       " u'times': 2715,\n",
       " u'internet': 1344,\n",
       " u'dimension': 733,\n",
       " u'clusters': 381,\n",
       " u'models using': 1661,\n",
       " u'exponential': 960,\n",
       " u'empirical': 831,\n",
       " u'considerably': 495,\n",
       " u'considerable': 494,\n",
       " u'process': 2067,\n",
       " u'message passing': 1599,\n",
       " u'web': 2884,\n",
       " u'high dimensional data': 1195,\n",
       " u'detecting': 700,\n",
       " u'counterparts': 573,\n",
       " u'divergence': 770,\n",
       " u'different types': 726,\n",
       " u'monte': 1668,\n",
       " u'marginal': 1557,\n",
       " u'large class': 1413,\n",
       " u'null': 1772,\n",
       " u'maximum posteriori': 1581,\n",
       " u'maximizes': 1577,\n",
       " u'powerful': 1993,\n",
       " u'achieve': 13,\n",
       " u'broad': 285,\n",
       " u'regression problem': 2241,\n",
       " u'incorporating': 1281,\n",
       " u'model based': 1650,\n",
       " u'occlusion': 1799,\n",
       " u'networks cnns': 1722,\n",
       " u'inequalities': 1298,\n",
       " u'autoencoder': 196,\n",
       " u'enforcing': 855,\n",
       " u'shape': 2430,\n",
       " u'cut': 602,\n",
       " u'access': 6,\n",
       " u'composed': 440,\n",
       " u'paper introduces': 1879,\n",
       " u'boundary': 274,\n",
       " u'paper focus': 1877,\n",
       " u'trained': 2732,\n",
       " u'discovery': 754,\n",
       " u'numerical': 1778,\n",
       " u'indoor': 1295,\n",
       " u'gaussian process': 1095,\n",
       " u'short term': 2438,\n",
       " u'ubiquitous': 2769,\n",
       " u'lighting': 1473,\n",
       " u'bandit': 215,\n",
       " u'systematically': 2659,\n",
       " u'experiments benchmark': 937,\n",
       " u'language modeling': 1409,\n",
       " u'general': 1098,\n",
       " u'real data': 2176,\n",
       " u'main contribution': 1539,\n",
       " u'cumulative': 594,\n",
       " u'steps': 2569,\n",
       " u'visual': 2869,\n",
       " u'grouping': 1157,\n",
       " u'case': 315,\n",
       " u'contrast': 525,\n",
       " u'information': 1308,\n",
       " u'corresponds': 566,\n",
       " u'specifically': 2529,\n",
       " u'sufficiently': 2620,\n",
       " u'studying': 2598,\n",
       " u'data driven': 608,\n",
       " u'kernel hilbert': 1389,\n",
       " u'article': 165,\n",
       " u'recovering': 2213,\n",
       " u'random fields': 2154,\n",
       " u'heuristics': 1186,\n",
       " u'depends': 676,\n",
       " u'gaussian noise': 1094,\n",
       " u'interested': 1339,\n",
       " u'contrast existing': 526,\n",
       " u'speed': 2536,\n",
       " u'coverage': 579,\n",
       " u'extends': 969,\n",
       " u'fields': 1023,\n",
       " u'mechanisms': 1594,\n",
       " u'continuous': 523,\n",
       " u'factors': 993,\n",
       " u'paths': 1917,\n",
       " u'private': 2050,\n",
       " u'learning algorithm': 1442,\n",
       " u'multi label classification': 1679,\n",
       " u'underlying': 2773,\n",
       " u'straightforward': 2576,\n",
       " u'specific': 2528,\n",
       " u'core': 552,\n",
       " u'regarding': 2233,\n",
       " u'reveals': 2319,\n",
       " u'changing': 339,\n",
       " u'heuristic': 1185,\n",
       " u'provided': 2123,\n",
       " u'provides': 2124,\n",
       " u'difficulty': 731,\n",
       " u'smoothness': 2494,\n",
       " u'inherent': 1311,\n",
       " u'added': 41,\n",
       " u'sequential': 2418,\n",
       " u'batch': 231,\n",
       " u'model selection': 1653,\n",
       " u'demonstrate effectiveness approach': 657,\n",
       " u'patients': 1918,\n",
       " u'neural network cnn': 1727,\n",
       " u'rank matrix': 2163,\n",
       " u'rectified': 2216,\n",
       " u'optimal': 1821,\n",
       " u'square': 2539,\n",
       " u'matrix factorization': 1573,\n",
       " u'refer': 2227,\n",
       " u'sensors': 2408,\n",
       " u'fundamental': 1081,\n",
       " u'subject': 2601,\n",
       " u'probability distribution': 2056,\n",
       " u'extreme': 982,\n",
       " u'differentiable': 727,\n",
       " u'log': 1504,\n",
       " u'segment': 2386,\n",
       " u'calculated': 291,\n",
       " u'implementation': 1252,\n",
       " u'publicly': 2129,\n",
       " u'perform': 1928,\n",
       " u'attribute': 191,\n",
       " u'high computational': 1193,\n",
       " u'remaining': 2269,\n",
       " u'active learning': 29,\n",
       " u'analyzed': 104,\n",
       " u'seen': 2385,\n",
       " u'seek': 2383,\n",
       " u'transforms': 2749,\n",
       " u'emerged': 829,\n",
       " u'allocation': 85,\n",
       " u'objects': 1788,\n",
       " u'implicit': 1255,\n",
       " u'greater': 1150,\n",
       " u'controls': 531,\n",
       " u'constructing': 512,\n",
       " u'shot learning': 2441,\n",
       " u'crafted': 581,\n",
       " u'functional': 1079,\n",
       " u'learning model': 1451,\n",
       " u'propose efficient': 2096,\n",
       " u'generalization': 1101,\n",
       " u'image data': 1242,\n",
       " u'best knowledge': 249,\n",
       " u'developed': 713,\n",
       " u'general framework': 1099,\n",
       " u'instead': 1326,\n",
       " u'class': 355,\n",
       " u'novel approach': 1765,\n",
       " u'reinforcement': 2250,\n",
       " u'similarly': 2465,\n",
       " u'compared state': 420,\n",
       " u'derive': 678,\n",
       " u'manifold': 1548,\n",
       " u'select': 2390,\n",
       " u'data points': 611,\n",
       " u'experiments synthetic': 942,\n",
       " u'imposed': 1262,\n",
       " u'tools': 2717,\n",
       " u'efficiently': 818,\n",
       " u'deep reinforcement learning': 642,\n",
       " u'mean field': 1584,\n",
       " u'forests': 1058,\n",
       " u'dimensional space': 736,\n",
       " u'extending': 968,\n",
       " u'clinical': 372,\n",
       " u'optimisation': 1825,\n",
       " u'variables': 2836,\n",
       " u'learning': 1441,\n",
       " u'comparison': 425,\n",
       " u'analyzing': 105,\n",
       " u'convex relaxation': 540,\n",
       " u'event': 903,\n",
       " u'solvers': 2505,\n",
       " u'plausible': 1957,\n",
       " u'structure': 2589,\n",
       " u'connected': 485,\n",
       " u'computed': 459,\n",
       " u'computes': 464,\n",
       " u'arbitrarily': 144,\n",
       " u'non trivial': 1751,\n",
       " u'overcome': 1864,\n",
       " u'upper bound': 2801,\n",
       " u'background': 211,\n",
       " u'statistic': 2563,\n",
       " u'stacked': 2544,\n",
       " u'running': 2346,\n",
       " u'neighborhood': 1711,\n",
       " u'share': 2432,\n",
       " u'big': 255,\n",
       " u'recognition': 2197,\n",
       " u'computations': 457,\n",
       " u'hypotheses': 1221,\n",
       " u'adapting': 38,\n",
       " u'alternatives': 95,\n",
       " u'modified': 1665,\n",
       " u'classification accuracy': 361,\n",
       " u'minimize': 1630,\n",
       " u'relaxation': 2259,\n",
       " u'framework': 1071,\n",
       " u'generation': 1111,\n",
       " u'finally demonstrate': 1029,\n",
       " u'fine tuning': 1035,\n",
       " u'change': 337,\n",
       " u'modal': 1645,\n",
       " u'leveraging': 1468,\n",
       " u'sequence': 2416,\n",
       " u'dependence': 671,\n",
       " u'dependency': 673,\n",
       " u'projection': 2080,\n",
       " u'effects': 811,\n",
       " u'learning techniques': 1458,\n",
       " u'adapt': 35,\n",
       " u'rewards': 2323,\n",
       " u'deep learning': 635,\n",
       " u'lasso': 1424,\n",
       " u'convergence rate': 535,\n",
       " u'visually': 2873,\n",
       " u'formulation': 1064,\n",
       " u'method based': 1603,\n",
       " u'columns': 399,\n",
       " u'confidence': 479,\n",
       " u'integrating': 1331,\n",
       " u'interaction': 1336,\n",
       " u'similarities': 2463,\n",
       " u'embedded': 826,\n",
       " u'calibrated': 292,\n",
       " u'showing': 2443,\n",
       " u'propose algorithm': 2095,\n",
       " u'learning framework': 1448,\n",
       " u'demonstrates': 664,\n",
       " u'nuclear norm': 1771,\n",
       " u'domain adaptation': 781,\n",
       " u'language processing': 1410,\n",
       " u'symmetry': 2650,\n",
       " u'combination': 400,\n",
       " u'programs': 2077,\n",
       " u'refined': 2231,\n",
       " u'method significantly': 1608,\n",
       " u'label': 1400,\n",
       " u'identifying': 1229,\n",
       " u'depending': 675,\n",
       " u'constant': 506,\n",
       " u'present approach': 2017,\n",
       " u'knowledge': 1398,\n",
       " u'excellent': 912,\n",
       " u'description': 686,\n",
       " u'low dimensional': 1523,\n",
       " u'support vector': 2638,\n",
       " u'fine grained': 1034,\n",
       " u'probabilistic': 2051,\n",
       " u'convex optimization': 539,\n",
       " u'include': 1274,\n",
       " u'convolutional neural network': 547,\n",
       " u'superior': 2630,\n",
       " u'matrix completion': 1572,\n",
       " u'auxiliary': 202,\n",
       " u'measurements': 1590,\n",
       " u'predictions': 2007,\n",
       " u'combined': 404,\n",
       " u'results real': 2312,\n",
       " u'spatial': 2518,\n",
       " u'bayesian optimization': 236,\n",
       " u'fewer': 1020,\n",
       " u'gained': 1086,\n",
       " u'near': 1699,\n",
       " u'encodes': 848,\n",
       " u'encoder': 847,\n",
       " u'encoded': 846,\n",
       " u'orthogonal': 1846,\n",
       " u'space': 2512,\n",
       " u'affine': 64,\n",
       " u'referred': 2229,\n",
       " u'finding': 1030,\n",
       " u'state art performance': 2555,\n",
       " u'cloud': 377,\n",
       " u'auto': 195,\n",
       " u'exhibit': 913,\n",
       " u'modeling': 1658,\n",
       " u'potentials': 1991,\n",
       " u'assume': 177,\n",
       " u'encode': 845,\n",
       " u'random forest': 2155,\n",
       " u'personalized': 1939,\n",
       " u'computational complexity': 451,\n",
       " u'online learning': 1809,\n",
       " u'learning method': 1449,\n",
       " u'fisher': 1039,\n",
       " u'capacity': 304,\n",
       " u'criteria': 585,\n",
       " u'corrupted': 567,\n",
       " u'millions': 1625,\n",
       " u'test time': 2688,\n",
       " u'color': 398,\n",
       " u'design': 690,\n",
       " u'competitive': 428,\n",
       " u'multipliers': 1687,\n",
       " u'occur': 1801,\n",
       " u'proposing': 2115,\n",
       " u'shallow': 2429,\n",
       " u'planning': 1956,\n",
       " u'optimally': 1824,\n",
       " u'types': 2766,\n",
       " u'easier': 798,\n",
       " u'verify': 2853,\n",
       " u'demonstrate approach': 655,\n",
       " u'revisit': 2321,\n",
       " u'average': 205,\n",
       " u'inter': 1335,\n",
       " u'recent years': 2193,\n",
       " u'belief': 240,\n",
       " u'pedestrian': 1923,\n",
       " u'normals': 1759,\n",
       " u'technical': 2674,\n",
       " u'resulting': 2308,\n",
       " u'effectively': 806,\n",
       " u'principal': 2038,\n",
       " u'object detection': 1782,\n",
       " u'learned': 1439,\n",
       " u'terms': 2686,\n",
       " u'matching': 1567,\n",
       " u'valuable': 2828,\n",
       " u'remains': 2270,\n",
       " u'experimental': 931,\n",
       " u'shot': 2440,\n",
       " u'source code': 2510,\n",
       " u'ensemble': 860,\n",
       " u'foreground': 1056,\n",
       " u'adaptively': 40,\n",
       " u'gene': 1097,\n",
       " u'normalized': 1758,\n",
       " u'posteriori': 1988,\n",
       " u'pattern': 1919,\n",
       " u'ridge': 2326,\n",
       " u'fourier': 1067,\n",
       " u'rank': 2162,\n",
       " u'epsilon': 869,\n",
       " u'quasi': 2144,\n",
       " u'behaviour': 239,\n",
       " u'goal': 1131,\n",
       " u'gamma': 1090,\n",
       " u'model used': 1655,\n",
       " u'incomplete': 1277,\n",
       " u'characterize': 346,\n",
       " u'graphical models': 1147,\n",
       " u'complete': 431,\n",
       " u'social': 2495,\n",
       " u'carefully': 310,\n",
       " u'chosen': 353,\n",
       " u'regularizer': 2248,\n",
       " u'does': 777,\n",
       " u'word': 2897,\n",
       " u'work': 2899,\n",
       " u'art algorithms': 159,\n",
       " u'hessian': 1183,\n",
       " u'compression': 447,\n",
       " u'annotation': 109,\n",
       " u'semantics': 2398,\n",
       " u'outperforming': 1855,\n",
       " u'geometry': 1121,\n",
       " u'adopted': 55,\n",
       " u'secondly': 2382,\n",
       " u'candidate': 297,\n",
       " u'dense': 667,\n",
       " u'benefits': 247,\n",
       " u'fine': 1033,\n",
       " u'feature representation': 1010,\n",
       " u'lines': 1491,\n",
       " u'progress': 2078,\n",
       " u'finite': 1036,\n",
       " u'expressive': 965,\n",
       " u'simulated real': 2473,\n",
       " u'constructed': 511,\n",
       " u'neighbors': 1713,\n",
       " u'insights': 1322,\n",
       " u'art performance': 162,\n",
       " u'components': 439,\n",
       " u'far': 999,\n",
       " u'challenging': 333,\n",
       " u'converge': 533,\n",
       " u'feasible': 1006,\n",
       " u'wild': 2894,\n",
       " u'sensing': 2404,\n",
       " u'deep network': 636,\n",
       " u'locations': 1503,\n",
       " u'commonly used': 411,\n",
       " u'carlo': 311,\n",
       " u'unified framework': 2780,\n",
       " u'data real': 612,\n",
       " u'entropy': 866,\n",
       " u'learner': 1440,\n",
       " u'introduce new': 1356,\n",
       " u'losses': 1520,\n",
       " u'close': 373,\n",
       " u'sentences': 2410,\n",
       " u'hidden markov': 1188,\n",
       " u'positions': 1979,\n",
       " u'allowed': 87,\n",
       " u'low rank': 1525,\n",
       " u'operates': 1812,\n",
       " u'data distribution': 607,\n",
       " u'making': 1547,\n",
       " u'inferring': 1305,\n",
       " u'tuning': 2762,\n",
       " u'proposed method': 2109,\n",
       " u'separable': 2412,\n",
       " u'expert': 944,\n",
       " u'validation': 2826,\n",
       " u'representative': 2281,\n",
       " u'preserving': 2026,\n",
       " u'computationally expensive': 456,\n",
       " u'parameterized': 1897,\n",
       " u'relies': 2265,\n",
       " u'previous methods': 2029,\n",
       " u'family': 998,\n",
       " u'difficult': 729,\n",
       " u'unique': 2783,\n",
       " u'introducing': 1360,\n",
       " u'applications': 121,\n",
       " u'bayesian model': 235,\n",
       " u'image': 1239,\n",
       " u'science': 2373,\n",
       " u'evaluations': 902,\n",
       " u'structure motion': 2590,\n",
       " u'localization': 1499,\n",
       " u'defined': 644,\n",
       " u'naturally': 1697,\n",
       " u'dirichlet process': 750,\n",
       " u'classifying': 369,\n",
       " u'popular': 1971,\n",
       " u'criterion': 586,\n",
       " u'supervision': 2636,\n",
       " u'comes': 408,\n",
       " u'media': 1595,\n",
       " u'central': 326,\n",
       " u'stochastic gradient descent': 2573,\n",
       " u'evaluate': 896,\n",
       " u'signal': 2447,\n",
       " u'vector machines': 2850,\n",
       " u'pixel': 1951,\n",
       " u'modeled': 1657,\n",
       " u'hierarchical': 1189,\n",
       " u'affect': 63,\n",
       " u'gradient descent': 1138,\n",
       " u'mid level': 1622,\n",
       " u'tracker': 2724,\n",
       " u'multi label': 1678,\n",
       " u'grained': 1142,\n",
       " u'black': 261,\n",
       " u'feature selection': 1012,\n",
       " u'significant': 2451,\n",
       " u'consequently': 491,\n",
       " u'computer vision applications': 462,\n",
       " u'named': 1692,\n",
       " u'attributes': 192,\n",
       " u'recently proposed': 2196,\n",
       " u'approximate inference': 137,\n",
       " u'promising results': 2083,\n",
       " u'flow': 1045,\n",
       " u'categorization': 320,\n",
       " u'np': 1768,\n",
       " u'modes': 1663,\n",
       " u'model': 1647,\n",
       " u'strategy': 2578,\n",
       " u'facial': 988,\n",
       " u'tends': 2681,\n",
       " u'paper presents': 1883,\n",
       " u'armed': 155,\n",
       " u'target domain': 2668,\n",
       " u'families': 997,\n",
       " u'argue': 150,\n",
       " u'maximizing': 1578,\n",
       " u'previous state art': 2031,\n",
       " u'tasks': 2672,\n",
       " u'nonparametric': 1754,\n",
       " u'establishing': 884,\n",
       " u'policies': 1963,\n",
       " u'aspects': 170,\n",
       " u'comprehensive': 444,\n",
       " u'surrogate': 2646,\n",
       " u'asymptotic': 183,\n",
       " u'challenge': 331,\n",
       " u'crucial': 592,\n",
       " u'architectures': 147,\n",
       " u'infinite': 1306,\n",
       " u'theoretical analysis': 2699,\n",
       " u'regularizing': 2249,\n",
       " u'highly': 1203,\n",
       " u'total': 2722,\n",
       " u'uniform': 2781,\n",
       " u'directly': 747,\n",
       " u'orientation': 1844,\n",
       " u'deformation': 649,\n",
       " u'conditions': 476,\n",
       " u'collected': 395,\n",
       " u'introduce novel': 1357,\n",
       " u'processes': 2068,\n",
       " u'humans': 1215,\n",
       " u'corresponding': 565,\n",
       " u'optimization problems': 1832,\n",
       " u'paper investigate': 1880,\n",
       " u'wide range': 2890,\n",
       " u'probabilistic model': 2052,\n",
       " u'linear time': 1489,\n",
       " u'using synthetic': 2815,\n",
       " u'difference': 723,\n",
       " u'gradient descent sgd': 1139,\n",
       " u'simple': 2466,\n",
       " u'consuming': 514,\n",
       " u'taking': 2666,\n",
       " u'completely': 432,\n",
       " u'condition': 471,\n",
       " u'transform': 2745,\n",
       " u'significantly improve': 2457,\n",
       " u'component analysis': 438,\n",
       " u'posterior distribution': 1986,\n",
       " u'known': 1399,\n",
       " u'investigate': 1367,\n",
       " u'quantization': 2143,\n",
       " u'treated': 2753,\n",
       " u'hypothesis': 1222,\n",
       " u'limiting': 1481,\n",
       " u'motions': 1671,\n",
       " u'estimators': 892,\n",
       " u'brings': 284,\n",
       " u'feedback': 1018,\n",
       " u'wide': 2889,\n",
       " u'publicly available': 2130,\n",
       " u'crowdsourcing': 591,\n",
       " u'viewpoint': 2862,\n",
       " u'quite': 2151,\n",
       " u'stationary': 2562,\n",
       " u'mean': 1583,\n",
       " u'viewing': 2861,\n",
       " u'numerical experiments': 1779,\n",
       " u'probabilistic models': 2053,\n",
       " u'generative model': 1115,\n",
       " u'combine': 403,\n",
       " u'consisting': 504,\n",
       " u'distributions': 769,\n",
       " u'involve': 1370,\n",
       " u'world applications': 2906,\n",
       " u'develop new': 711,\n",
       " u'privacy': 2049,\n",
       " u'mixture models': 1641,\n",
       " u'similarity': 2464,\n",
       " u'active': 28,\n",
       " u'arms': 157,\n",
       " u'scoring': 2377,\n",
       " u'utilize': 2819,\n",
       " u'affinity': 65,\n",
       " u'neural networks rnns': 1730,\n",
       " u'precise': 1999,\n",
       " u'linear models': 1487,\n",
       " u'transformations': 2747,\n",
       " u'error bounds': 876,\n",
       " u'outlier': 1850,\n",
       " u'approximating': 141,\n",
       " u'adversarial': 61,\n",
       " u'random forests': 2156,\n",
       " u'naive': 1691,\n",
       " u'andor': 106,\n",
       " u'captures': 307,\n",
       " u'captured': 306,\n",
       " u'challenging datasets': 334,\n",
       " u'art': 158,\n",
       " u'arm': 154,\n",
       " u'called': 294,\n",
       " u'purpose': 2133,\n",
       " u'crowd': 590,\n",
       " u'research': 2294,\n",
       " u'clustering': 379,\n",
       " u'dynamics': 795,\n",
       " u'makes': 1546,\n",
       " u'concept': 468,\n",
       " u'logistic regression': 1508,\n",
       " u'real world': 2179,\n",
       " u'fully': 1076,\n",
       " u'problem using': 2063,\n",
       " u'address': 47,\n",
       " u'including': 1276,\n",
       " u'special': 2524,\n",
       " u'prediction accuracy': 2006,\n",
       " u'weakly': 2882,\n",
       " u'predicting': 2004,\n",
       " u'pseudo': 2127,\n",
       " u'box': 280,\n",
       " u'extended': 967,\n",
       " u'novel framework': 1766,\n",
       " u'minimum': 1633,\n",
       " u'visual features': 2870,\n",
       " u'entire': 863,\n",
       " u'automatic': 199,\n",
       " u'invariant': 1365,\n",
       " u'natural language': 1695,\n",
       " u'selected': 2391,\n",
       " u'networks rnns': 1723,\n",
       " u'encouraging': 850,\n",
       " u'error rate': 877,\n",
       " u'generalize': 1103,\n",
       " u'gpu': 1135,\n",
       " u'gps': 1134,\n",
       " u'factorization': 992,\n",
       " u'separately': 2414,\n",
       " u'long': 1509,\n",
       " u'approach allows': 129,\n",
       " u'carried': 312,\n",
       " u'vector': 2848,\n",
       " u'score': 2375,\n",
       " u'people': 1926,\n",
       " u'exist': 915,\n",
       " u'studies': 2595,\n",
       " u'matches': 1566,\n",
       " u'matched': 1565,\n",
       " u'objective function': 1786,\n",
       " u'vocabulary': 2875,\n",
       " u'conditional random': 473,\n",
       " u'insight': 1321,\n",
       " u'distance metric': 762,\n",
       " u'reconstruction': 2208,\n",
       " u'scenario': 2367,\n",
       " u'adding': 42,\n",
       " u'shown': 2444,\n",
       " u'large datasets': 1415,\n",
       " u'shows': 2445,\n",
       " u'impossible': 1264,\n",
       " u'real': 2175,\n",
       " u'efficient algorithm': 815,\n",
       " u'step size': 2568,\n",
       " u'object recognition': 1783,\n",
       " u'probability': 2055,\n",
       " u'method outperforms state': 1607,\n",
       " u'usefulness': 2807,\n",
       " u'inner': 1315,\n",
       " u'method called': 1604,\n",
       " u'outperforms state art': 1860,\n",
       " u'designed': 691,\n",
       " u'hyperparameter': 1219,\n",
       " u'arts': 168,\n",
       " u'solve problem': 2502,\n",
       " u'examples': 911,\n",
       " u'dirichlet': 748,\n",
       " u'exploited': 952,\n",
       " u'signal processing': 2448,\n",
       " u'defining': 645,\n",
       " u'approach problem': 133,\n",
       " u'align': 81,\n",
       " u'calibration': 293,\n",
       " u'empirical results': 834,\n",
       " u'channels': 341,\n",
       " u'approximations': 143,\n",
       " u'art results': 163,\n",
       " u'quantitative': 2139,\n",
       " u'unit': 2784,\n",
       " u'explore': 956,\n",
       " u'binary': 257,\n",
       " u'leverage': 1466,\n",
       " u'having': 1176,\n",
       " u'software': 2498,\n",
       " u'likelihood': 1475,\n",
       " u'inspired': 1323,\n",
       " u'visualization': 2872,\n",
       " u'paper propose novel': 1887,\n",
       " u'semi supervised learning': 2401,\n",
       " u'comparable': 416,\n",
       " u'quantify': 2138,\n",
       " u'distance': 761,\n",
       " u'extracting': 980,\n",
       " u'recognize': 2200,\n",
       " u'map': 1553,\n",
       " u'entries': 865,\n",
       " u'extend': 966,\n",
       " u'number samples': 1776,\n",
       " u'extremely': 983,\n",
       " u'optimize': 1833,\n",
       " u'network cnn': 1719,\n",
       " u'report': 2275,\n",
       " u'model parameters': 1652,\n",
       " u'focus': 1046,\n",
       " u'generative adversarial': 1113,\n",
       " u'induced': 1296,\n",
       " u'increasingly': 1286,\n",
       " u'deep neural network': 639,\n",
       " u'learning approaches': 1446,\n",
       " u'real time': 2178,\n",
       " u'attractive': 190,\n",
       " u'consists': 505,\n",
       " u'classify': 368,\n",
       " u'pre': 1997,\n",
       " u'posed': 1976,\n",
       " u'poses': 1977,\n",
       " u'offer': 1804,\n",
       " u'consensus': 489,\n",
       " u'respectively': 2302,\n",
       " u'great': 1149,\n",
       " u'covariance matrix': 576,\n",
       " u'bounded': 275,\n",
       " u'overlapping': 1867,\n",
       " u'curve': 600,\n",
       " u'cameras': 296,\n",
       " u'building': 288,\n",
       " u'recurrent neural network': 2219,\n",
       " u'youtube': 2915,\n",
       " u'coordinate descent': 550,\n",
       " u'ground truth': 1155,\n",
       " u'policy': 1964,\n",
       " u'reasons': 2187,\n",
       " u'qualitatively': 2136,\n",
       " u'bag': 213,\n",
       " u'learning based': 1447,\n",
       " u'manual': 1551,\n",
       " u'maximization': 1575,\n",
       " u'inputs': 1320,\n",
       " u'tractable': 2726,\n",
       " u'lower bounds': 1529,\n",
       " u'instances': 1325,\n",
       " u'built': 290,\n",
       " u'build': 287,\n",
       " u'principal component': 2039,\n",
       " u'methods use': 1616,\n",
       " u'patches': 1915,\n",
       " u'risk': 2332,\n",
       " u'rise': 2331,\n",
       " u'observed data': 1793,\n",
       " u'single image': 2480,\n",
       " u'computation': 448,\n",
       " u'denoising': 666,\n",
       " u'investigated': 1368,\n",
       " u'intra': 1352,\n",
       " u'regret': 2242,\n",
       " u'action recognition': 24,\n",
       " u'choose': 351,\n",
       " u'typical': 2767,\n",
       " u'absolute': 2,\n",
       " u'online': 1808,\n",
       " u'past': 1913,\n",
       " u'pass': 1911,\n",
       " u'experience': 929,\n",
       " u'tested': 2689,\n",
       " u'existing state art': 922,\n",
       " u'definition': 647,\n",
       " u'pairs': 1869,\n",
       " u'matrix': 1571,\n",
       " u'length': 1462,\n",
       " u'obtained': 1796,\n",
       " u'recovery': 2215,\n",
       " u'recovers': 2214,\n",
       " u'riemannian': 2327,\n",
       " u'high quality': 1198,\n",
       " u'applicability': 118,\n",
       " u'algorithms proposed': 80,\n",
       " u'dynamically': 794,\n",
       " u'discovered': 752,\n",
       " u'alleviate': 84,\n",
       " u'using': 2811,\n",
       " u'experiments real': 941,\n",
       " u'systems': 2660,\n",
       " u'existing state': 921,\n",
       " u'success': 2613,\n",
       " u'cost function': 569,\n",
       " u'svm': 2648,\n",
       " u'dealing': 624,\n",
       " ...}"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6605, 2917)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>structural</th>\n",
       "      <th>form</th>\n",
       "      <th>forms</th>\n",
       "      <th>intuitive</th>\n",
       "      <th>discovery</th>\n",
       "      <th>bias</th>\n",
       "      <th>initial</th>\n",
       "      <th>tree</th>\n",
       "      <th>knowledge</th>\n",
       "      <th>human</th>\n",
       "      <th>computational</th>\n",
       "      <th>learn</th>\n",
       "      <th>structure</th>\n",
       "      <th>supports</th>\n",
       "      <th>assumes</th>\n",
       "      <th>discovered</th>\n",
       "      <th>right</th>\n",
       "      <th>understood</th>\n",
       "      <th>consequence</th>\n",
       "      <th>connectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Both scientists and children make important structural discoveries yet their computational underpinnings are not well understood Structure discovery has previously been formalized as probabilistic inference about the right structural form     where form could be a tree ring chain grid etc Kemp  Tenenbaum  The discovery of structural form PNAS    While this approach can learn intuitive organizations including a tree for animals and a ring for the color circle it assumes a strong inductive bias that considers only these particular forms and each form is explicitly provided as initial knowledge Here we introduce a new computational model of how organizing structure can be discovered utilizing a broad hypothesis space with a preference for sparse connectivity Given that the inductive bias is more general the models initial knowledge shows little qualitative resemblance to some of the discoveries it supports As a consequence the model can also learn complex structures for domains that lack intuitive description as well as predict human property induction judgments without explicit structural forms By allowing form to emerge from sparsity our approach clarifies how both the richness and flexibility of human conceptual organization can coexist</th>\n",
       "      <td>0.372479</td>\n",
       "      <td>0.358872</td>\n",
       "      <td>0.221204</td>\n",
       "      <td>0.217130</td>\n",
       "      <td>0.204885</td>\n",
       "      <td>0.195566</td>\n",
       "      <td>0.193394</td>\n",
       "      <td>0.187002</td>\n",
       "      <td>0.144667</td>\n",
       "      <td>0.141165</td>\n",
       "      <td>0.131838</td>\n",
       "      <td>0.124883</td>\n",
       "      <td>0.119940</td>\n",
       "      <td>0.118867</td>\n",
       "      <td>0.117513</td>\n",
       "      <td>0.116246</td>\n",
       "      <td>0.115057</td>\n",
       "      <td>0.115057</td>\n",
       "      <td>0.114302</td>\n",
       "      <td>0.111868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We consider joint estimation of multiple graphical models arising from heterogeneous and high dimensional observations Unlike most previous approaches which assume that the cluster structure is given in advance an appealing feature of our method is to learn cluster structure while estimating heterogeneous graphical models This is achieved via a high dimensional version of Expectation Conditional Maximization ECM algorithm Meng and Rubin  A joint graphical lasso penalty is imposed in the conditional maximization step to extract both homogeneity and heterogeneity components across all clusters Our algorithm is computationally efficient due to fast sparse learning routines and can be implemented without unsupervised learning knowledge The superior performance of our method is demonstrated by extensive experiments and its application to a Glioblastoma cancer dataset reveals some new insights in understanding the Glioblastoma cancer In theory a non asymptotic error bound is established for the output directly from our high dimensional ECM algorithm and it consists of two quantities statistical error statistical accuracy and optimization error computational complexity Such a result gives a theoretical guideline in terminating our ECM iterations</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065716</td>\n",
       "      <td>0.062250</td>\n",
       "      <td>0.119572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU neural networks define piecewise linear functions of their inputs However initializing and training a neural network is very different from fitting a linear spline In this paper we expand empirically upon previous theoretical work to demonstrate features of trained neural networks Standard network initialization and training produce networks vastly simpler than a naive parameter count would suggest and can impart odd features to the trained network However we also show the forced simplicity is beneficial and indeed critical for the wide success of these networks</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural networks with rectified linear unit activations are essentially multivariate linear splines As such one of many ways to measure the complexity or expressivity of a neural network is to count the number of knots in the spline model We study the number of knots in fully connected feedforward neural networks with rectified linear unit activation functions We intentionally keep the neural networks very simple so as to make theoretical analyses more approachable An induction on the number of layers l reveals a tight upper bound on the number of knots in mathbbR to mathbbRp deep neural networks With ni gg  neurons in layer i   dots l the upper bound is approximately n dots nl We then show that the exact upper bound is tight and we demonstrate the upper bound with an example The purpose of these analyses is to pave a path for understanding the behavior of general mathbbRq to mathbbRp neural networks</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We propose a new framework for manifold denoising based on processing in the graph Fourier frequency domain derived from the spectral decomposition of the discrete graph Laplacian Our approach uses the Spectral Graph Wavelet transform in order to per  form non iterative denoising directly in the graph frequency domain an approach inspired by conventional wavelet based signal denoising methods We theoretically justify our approach based on the fact that for smooth manifolds the coordinate information energy is localized in the low spectral graph wavelet sub bands while the noise affects all frequency bands in a similar way Experimental results show that our proposed manifold frequency denoising MFD approach significantly outperforms the state of the art denoising meth  ods and is robust to a wide range of parameter selections eg the choice of k nearest neighbor connectivity of the graph</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>An associative memory is a framework of content addressable memory that stores a collection of message vectors or a dataset over a neural network while enabling a neurally feasible mechanism to recover any message in the dataset from its noisy version Designing an associative memory requires addressing two main tasks  learning phase given a dataset learn a concise representation of the dataset in the form of a graphical model or a neural network  recall phase given a noisy version of a message vector from the dataset output the correct message vector via a neurally feasible algorithm over the network learnt during the learning phase This paper studies the problem of designing a class of neural associative memories which learns a network representation for a large dataset that ensures correction against a large number of adversarial errors during the recall phase Specifically the associative memories designed in this paper can store dataset containing expn n length message vectors over a network with On nodes and can tolerate Omegafracnrm polylog n adversarial errors This paper carries out this memory design by mapping the learning phase and recall phase to the tasks of dictionary learning with a square dictionary and iterative error correction in an expander code respectively</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variational auto encoders VAE are scalable and powerful generative models However the choice of the variational posterior determines tractability and flexibility of the VAE Commonly latent variables are modeled using the normal distribution with a diagonal covariance matrix This results in computational efficiency but typically it is not flexible enough to match the true posterior distribution One fashion of enriching the variational posterior distribution is application of normalizing flows ie a series of invertible transformations to latent variables with a simple posterior In this paper we follow this line of thinking and propose a volume preserving flow that uses a series of Householder transformations We show empirically on MNIST dataset and histopathology data that the proposed flow allows to obtain more flexible variational posterior and highly competitive results comparing to other normalizing flows</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Increasing availability of vehicle GPS data has created potentially transformative opportunities for traffic management route planning and other location based services Critical to the utility of the data is their accuracy Map matching is the process of improving the accuracy by aligning GPS data with the road network In this paper we propose a purely probabilistic approach to map matching based on a sequential Monte Carlo algorithm known as particle filters The approach performs map matching by producing a range of candidate solutions each with an associated probability score We outline implementation details and thoroughly validate the technique on GPS data of varied quality</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We address the issue of speeding up the training of convolutional networks Here we study a distributed method adapted to stochastic gradient descent SGD The parallel optimization setup uses several threads each applying individual gradient descents on a local variable We propose a new way to share information between different threads inspired by gossip algorithms and showing good consensus convergence properties Our method called GoSGD has the advantage to be fully asynchronous and decentralized We compared our method to the recent EASGD in citeelastic on CIFAR  show encouraging results</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phase synchronisation in multichannel EEG is known as the manifestation of functional brain connectivity Traditional phase synchronisation studies are mostly based on time average synchrony measures hence do not preserve the temporal evolution of the phase difference Here we propose a new method to show the existence of a small set of unique phase synchronised patterns or states in multi channel EEG recordings each state being stable of the order of ms from typical and pathological subjects during face perception tasks The proposed methodology bridges the concepts of EEG microstates and phase synchronisation in time and frequency domain respectively The analysis is reported for four groups of children including typical Autism Spectrum Disorder ASD low and high anxiety subjects   a total of  subjects In all cases we observe consistent existence of these states   termed as synchrostates   within specific cognition related frequency bands beta and gamma bands though the topographies of these synchrostates differ for different subject groups with different pathological conditions The inter synchrostate switching follows a well defined sequence capturing the underlying inter electrode phase relation dynamics in stimulus  and person centric manner Our study is motivated from the well known EEG microstate exhibiting stable potential maps over the scalp However here we report a similar observation of quasi stable phase synchronised states in multichannel EEG The existence of the synchrostates coupled with their unique switching sequence characteristics could be considered as a potentially new field over contemporary EEG phase synchronisation studies</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In this paper we propose a new primal dual algorithm for minimizing fxgxhAx where f g and h are convex functions f is differentiable with a Lipschitz continuous gradient and A is a bounded linear operator It has some famous primal dual algorithms for minimizing the sum of two functions as special cases For example it reduces to the Chambolle Pock algorithm when f and a primal dual fixed point algorithm in P Chen J Huang and X Zhang A primal dual fixed point algorithm for convex separable minimization with applications to image restoration Inverse Problems   p when g In addition it recovers the three operator splitting scheme in D Davis and W Yin A three operator splitting scheme and its optimization applications arXiv  when A is the identity operator We prove the convergence of this new algorithm for the general case by showing that the iteration is a nonexpansive operator and derive the linear convergence rate with additional assumptions Comparing to other primal dual algorithms for solving the same problem this algorithm extends the range of acceptable parameters to ensure the convergence and has a smaller per iteration cost The numerical experiments show the efficiency of this new algorithm by comparing to other primal dual algorithms</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Co adaptation is a special form of on line learning where an algorithm mathcalA must assist an unknown algorithm mathcalB to perform some task This is a general framework and has applications in recommendation systems search education and much more Today the most common use of co adaptive algorithms is in brain computer interfacing BCI where algorithms help patients gain and maintain control over prosthetic devices While previous studies have shown strong empirical results Kowalski et al  Orsborn et al  or have been studied in specific examples Merel et al   there is no general analysis of the co adaptive learning problem Here we will study the co adaptive learning problem in the online closed loop setting We will prove that with high probability co adaptive learning is guaranteed to outperform learning with a fixed decoder as long as a particular condition is met</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humans are remarkably adept at interpreting the gaze direction of other individuals in their surroundings This skill is at the core of the ability to engage in joint visual attention which is essential for establishing social interactions How accurate are humans in determining the gaze direction of others in lifelike scenes when they can move their heads and eyes freely and what are the sources of information for the underlying perceptual processes These questions pose a challenge from both empirical and computational perspectives due to the complexity of the visual input in real life situations Here we measure empirically human accuracy in perceiving the gaze direction of others in lifelike scenes and study computationally the sources of information and representations underlying this cognitive capacity We show that humans perform better in face to face conditions compared with recorded conditions and that this advantage is not due to the availability of input dynamics We further show that humans are still performing well when only the eyes region is visible rather than the whole face We develop a computational model which replicates the pattern of human performance including the finding that the eyes region contains on its own the required information for estimating both head orientation and direction of gaze Consistent with neurophysiological findings on task specific face regions in the brain the learned computational representations reproduce perceptual effects such as the Wollaston illusion when trained to estimate direction of gaze but not when trained to recognize objects or faces</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124163</td>\n",
       "      <td>0.173940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plants sense their environment by producing electrical signals which in essence represent changes in underlying physiological processes These electrical signals when monitored show both stochastic and deterministic dynamics In this paper we compute  statistical features from the raw non stationary plant electrical signal time series to classify the stimulus applied causing the electrical signal By using different discriminant analysis based classification techniques we successfully establish that there is enough information in the raw electrical signal to classify the stimuli In the process we also propose two standard features which consistently give good classification results for three types of stimuli   Sodium Chloride NaCl Sulphuric Acid HSO and Ozone O This may facilitate reduction in the complexity involved in computing all the features for online classification of similar external stimuli in future</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We introduce a new large scale music dataset MusicNet to serve as a source of supervision and evaluation of machine learning methods for music research MusicNet consists of hundreds of freely licensed classical music recordings by  composers written for  instruments together with instrumentnote annotations resulting in over  million temporal labels on  hours of chamber music performances under various studio and microphone conditions We define a multi label classification task to predict notes in musical recordings along with an evaluation protocol We benchmark several machine learning architectures for this task i learning from hand crafted spectrogram features ii end to end learning with a neural net iii end to end learning with a convolutional neural net We show that several end to end learning proposals outperform approaches based on learning from hand crafted audio features</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demanding sparsity in estimated models has become a routine practice in statistics In many situations we wish to require that the sparsity patterns attained honor certain problem specific constraints Hierarchical sparse modeling HSM refers to situations in which these constraints specify that one set of parameters be set to zero whenever another is set to zero In recent years numerous papers have developed convex regularizers for this form of sparsity structure which arises in many areas of statistics including interaction modeling time series analysis and covariance estimation In this paper we observe that these methods fall into two frameworks the group lasso GL and latent overlapping group lasso LOG which have not been systematically compared in the context of HSM The purpose of this paper is to provide a side by side comparison of these two frameworks for HSM in terms of their statistical properties and computational efficiency We call special attention to GLs more aggressive shrinkage of parameters deep in the hierarchy a property not shared by LOG In terms of computation we introduce a finite step algorithm that exactly solves the proximal operator of LOG for a certain simple HSM structure we later exploit this to develop a novel path based BCD scheme for general HSM structures Both algorithms greatly improve the computational performance of LOG Finally we compare the two methods in the context of covariance estimation where we introduce a new sparsely banded estimator using LOG which we show achieves the statistical advantages of an existing GL based method but is simpler to express and more efficient to compute</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In this paper we study the efficiency of a bf Restarted bf Subbf Gradient RSG method that periodically restarts the standard subgradient method SG We show that when applied to a broad class of convex optimization problems RSG method can find an epsilon optimal solution with a low complexity than SG method In particular we first show that RSG can reduce the dependence of SGs iteration complexity on the distance between the initial solution and the optimal set to that between the epsilon level set and the optimal set In addition we show the advantages of RSG over SG in solving three different families of convex optimization problems a For the problems whose epigraph is a polyhedron RSG is shown to converge linearly b For the problems with local quadratic growth property RSG has an Ofracepsilonlogfracepsilon iteration complexity c For the problems that admit a local Kurdyka L ojasiewicz property with a power constant of betain RSG has an Ofracepsilonbetalogfracepsilon iteration complexity On the contrary with only the standard analysis the iteration complexity of SG is known to be Ofracepsilon for these three classes of problems The novelty of our analysis lies at exploiting the lower bound of the first order optimality residual at the epsilon level set It is this novelty that allows us to explore the local properties of functions eg local quadratic growth property local Kurdyka L ojasiewicz property more generally local error bounds to develop the improved convergence of RSG We demonstrate the effectiveness of the proposed algorithms on several machine learning tasks including regression and classification</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adaptive schemes where tasks are assigned based on the data collected thus far are widely used in practical crowdsourcing systems to efficiently allocate the budget However existing theoretical analyses of crowdsourcing systems suggest that the gain of adaptive task assignments is minimal To bridge this gap we investigate this question under a strictly more general probabilistic model which has been recently introduced to model practical crowdsourcing datasets Under this generalized Dawid Skene model we characterize the fundamental trade off between budget and accuracy We introduce a novel adaptive scheme that matches this fundamental limit A given budget is allocated over multiple rounds In each round a subset of tasks with high enough confidence are classified and increasing budget is allocated on remaining ones that are potentially more difficult On each round decisions are made based on the leading eigenvector of weighted non backtracking operator corresponding to the bipartite assignment graph We further quantify the gain of adaptivity by comparing the tradeoff with the one for non adaptive schemes and confirm that the gain is significant and can be made arbitrarily large depending on the distribution of the difficulty level of the tasks at hand</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A binary classifier capable of abstaining from making a label prediction has two goals in tension minimizing errors and avoiding abstaining unnecessarily often In this work we exactly characterize the best achievable tradeoff between these two goals in a general semi supervised setting given an ensemble of predictors of varying competence as well as unlabeled data on which we wish to predict or abstain We give an algorithm for learning a classifier in this setting which trades off its errors with abstentions in a minimax optimal manner is as efficient as linear learning and prediction and is demonstrably practical Our analysis extends to a large class of loss functions and other scenarios including ensembles comprised of specialists that can themselves abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In this paper we aim at recovering an undirected weighted graph of N vertices from the knowledge of a perturbed version of the eigenspaces of its adjacency matrix W Our approach is based on minimizing a cost function given by the Frobenius norm of the commutator AB BA between symmetric matrices A and B In the ErdHos Renyi model with no self loops we show that identifiability ie the ability to reconstruct W from the knowledge of its eigenspaces follows a sharp phase transition on the expected number of edges with threshold function Nlog N Given an estimation of the eigenspaces based on a n sample we provide support selection procedures from theoretical and practical point of views In particular when deleting an edge from the active support our study unveils that our test statistic is the order of On when we overestimate the true support and lower bounded by a positive constant when the estimated support is smaller than the true support This feature leads to a powerful practical support estimation procedure when properly thresholding Simulated and real life numerical experiments assert our new methodology</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A typical problem in causal modeling is the instability of model structure learning ie small changes in finite data can result in completely different optimal models The present work introduces a novel causal modeling algorithm for longitudinal data that is robust for finite samples based on recent advances in stability selection using subsampling and selection algorithms Our approach uses exploratory search but allows incorporation of prior knowledge eg that causal relationships do not go back in time We represent causal relationships using structural equation models Models are scored along two objectives the model fit and the model complexity Since both objectives are often conflicting we apply a multi objective evolutionary algorithm to search for Pareto optimal models To handle the instability of small finite data samples we repeatedly subsample the data and select those substructures from the optimal models that are both stable and parsimonious These substructures can be visualized through a causal graph Our more exploratory approach outperforms state of the art alternative approaches on a simulated data set with a known ground truth We also present the results of our method on three real world longitudinal data sets on chronic fatigue syndrome Alzheimer disease and chronic kidney disease The findings obtained with our approach are generally in line with results from more hypothesis driven analyses in earlier studies and do suggest some novel relationships that deserve further research</th>\n",
       "      <td>0.080591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian processes GPs are flexible distributions over functions that enable high level assumptions about unknown functions to be encoded in a parsimonious flexible and general way Although elegant the application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non Gaussian models Consequently a wealth of GP approximation schemes have been developed over the last  years to address these key limitations Many of these schemes employ a small set of pseudo data points to summarise the actual data In this paper we develop a new pseudo point approximation framework using Power Expectation Propagation Power EP that unifies a large number of these pseudo point approximations Unlike much of the previous venerable work in this area the new framework is built on standard methods for approximate inference variational free energy EP and power EP methods rather than employing approximations to the probabilistic generative model itself In this way all of approximation is performed at inference time rather than at modelling time resolving awkward philosophical and empirical questions that trouble previous approaches Crucially we demonstrate that the new framework includes new pseudo point approximation methods that outperform current approaches on regression classification and state space modelling tasks</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modern discriminative predictors have been shown to match natural intelligences in specific perceptual tasks in image classification object and part detection boundary extraction etc However a major advantage that natural intelligences still have is that they work well for all perceptual problems together solving them efficiently and coherently in an integrated manner In order to capture some of these advantages in machine perception we ask two questions whether deep neural networks can learn universal image representations useful not only for a single task but for all of them and how the solutions to the different tasks can be integrated in this framework We answer by proposing a new architecture which we call MultiNet in which not only deep image features are shared between tasks but where tasks can interact in a recurrent manner by encoding the results of their analysis in a common shared representation of the data In this manner we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then more significantly by integrating their solutions in the common representation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relational learning deals with data that are characterized by relational structures An important task is collective classification which is to jointly classify networked objects While it holds a great promise to produce a better accuracy than non collective classifiers collective classification is computational challenging and has not leveraged on the recent breakthroughs of deep learning We present Column Network CLN a novel deep learning model for collective classification in multi relational domains CLN has many desirable theoretical properties i it encodes multi relations between any two instances ii it is deep and compact allowing complex functions to be approximated at the network level with a small set of free parameters iii local and relational features are learned simultaneously iv long range higher order dependencies between instances are supported naturally and v crucially learning and inference are efficient linear in the size of the network and the number of relations We evaluate CLN on multiple real world applications a delay prediction in software projects b PubMed Diabetes publication classification and c film genre classification In all applications CLN demonstrates a higher accuracy than state of the art rivals</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sources of variability in experimentally derived data include measurement error in addition to the physical phenomena of interest This measurement error is a combination of systematic components originating from the measuring instrument and random measurement errors Several novel biological technologies such as mass cytometry and single cell RNA seq are plagued with systematic errors that may severely affect statistical analysis if the data is not properly calibrated Here we propose a novel deep learning approach for removing systematic batch effects Our method is based on a residual network trained to minimize the Maximum Mean Discrepancy MMD between the multivariate distributions of two replicates measured in different batches We apply our method to mass cytometry and single cell RNA seq datasets and demonstrate that it effectively attenuates batch effects and outperforms several popular methods</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We provide a general framework for privacy preserving variational Bayes VB for a large class of probabilistic models called the conjugate exponential CE family Our primary observation is that when models are in the CE family we can privatise the variational posterior distributions simply by perturbing the expected sufficient statistics of the complete data likelihood For widely used non CE models with binomial likelihoods eg logistic regression we exploit the Polya Gamma data augmentation scheme to bring such models into the CE family such that inferences in the modified model resemble the original private variational Bayes algorithm as closely as possible The iterative nature of variational Bayes presents a further challenge for privacy preservation as each iteration increases the amount of noise needed We overcome this challenge by combining  a relaxed notion of differential privacy called itconcentrated differential privacy which provides a tight bound on the privacy cost of multiple VB iterations and thus significantly decreases the amount of additive noise and  the privacy amplification effect resulting from subsampling mini batches from large scale data in stochastic learning We empirically demonstrate the effectiveness of our method in CE and non CE models including latent Dirichlet allocation LDA and Bayesian logistic regression evaluated on real world datasets</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Various active learning methods based on logistic regression have been proposed In this paper we investigate seven state of the art strategies present an extensive benchmark and provide a better understanding of their underlying characteristics Experiments are carried out both on  synthetic datasets and  real world datasets providing insights into the behaviour of these active learning methods with respect to classification accuracy and their computational cost</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The introduction of data analytics into medicine has changed the nature of treatment In this patients are asked to disclose personal information such as genetic markers lifestyle habits and clinical history This data is then used by statistical models to predict personalized treatments However due to privacy concerns patients often desire to withhold sensitive information This self censorship can impede proper diagnosis and treatment which may lead to serious health complications and even death In this paper we present privacy distillation a mechanism which allows patients to control the type and amount of information they wish to disclose to the healthcare providers for use in statistical models Meanwhile it retains the accuracy of models that have access to all patient data under a sufficient but not full set of privacy relevant information We validate privacy distillation using a corpus of patients prescribed to warfarin for a personalized dosage We use a deep neural network to implement privacy distillation for training and making dose predictions We find that privacy distillation with sufficient privacy relevant information i retains accuracy almost as good as having all patient data only  worse and ii is effective at preventing errors that introduce health related risks yielding on average  of under  or over prescriptions</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This work investigates the case of a network of agents that attempt to learn some unknown state of the world amongst the finitely many possibilities At each time step agents all receive random independently distributed private signals whose distributions are dependent on the unknown state of the world However it may be the case that some or any of the agents cannot distinguish between two or more of the possible states based only on their private observations as when several states result in the same distribution of the private signals In our model the agents form some initial belief probability distribution about the unknown state and then refine their beliefs in accordance with their private observations as well as the beliefs of their neighbors An agent learns the unknown state when her belief converges to a point mass that is concentrated at the true state A rational agent would use the Bayes rule to incorporate her neighbors beliefs and own private signals over time While such repeated applications of the Bayes rule in networks can become computationally intractable in this paper we show that in the canonical cases of directed star circle or path networks and their combinations one can derive a class of memoryless update rules that replicate that of a single Bayesian agent but replace the self beliefs with the beliefs of the neighbors This way one can realize an exponentially fast rate of learning similar to the case of Bayesian fully rational agents The proposed rules are a special case of the Learning without Recall</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi step reasoning The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database To our knowledge this paper presents the first weakly supervised end to end neural network model to induce such programs on a real world dataset We enhance the objective function of Neural Programmer a neural network with built in discrete operations and apply it on WikiTableQuestions a natural language question answering dataset The model is trained end to end with weak supervision of question answer pairs and does not require domain specific grammars rules or annotations that are key elements in previous approaches to program induction The main experimental result in this paper is that a single Neural Programmer model achieves  accuracy using only  examples with weak supervision An ensemble of  models with a trivial combination technique achieves  accuracy which is competitive to the current state of the art accuracy of  obtained by a traditional natural language semantic parser</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning to solve complex sequences of tasks  while both leveraging transfer and avoiding catastrophic forgetting  remains a key obstacle to achieving human level intelligence The progressive networks approach represents a step forward in this direction they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features We evaluate this architecture extensively on a wide variety of reinforcement learning tasks Atari and D maze games and show that it outperforms common baselines based on pretraining and finetuning Using a novel sensitivity measure we demonstrate that transfer occurs at both low level sensory and high level control layers of the learned policy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102578</td>\n",
       "      <td>0.100095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We introduce a simple recurrent variational autoencoder architecture that significantly improves image modeling The system represents the stateof the art in latent variable models for both the ImageNet and Omniglot datasets We show that it naturally separates global conceptual information from lower level details thus addressing one of the fundamentally desired properties of unsupervised learning Furthermore the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality conceptual compression</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We discuss relations between Residual Networks ResNet Recurrent Neural Networks RNNs and the primate visual cortex We begin with the observation that a shallow RNN is exactly equivalent to a very deep ResNet with weight sharing among the layers A direct implementation of such a RNN although having orders of magnitude fewer parameters leads to a performance similar to the corresponding ResNet We propose  a generalization of both RNN and ResNet architectures and  the conjecture that a class of moderately deep RNNs is a biologically plausible model of the ventral stream in visual cortex We demonstrate the effectiveness of the architectures by testing them on the CIFAR  dataset</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inspired by the recent success of methods that employ shape priors to achieve robust D reconstructions we propose a novel recurrent neural network architecture that we call the D Recurrent Reconstruction Neural Network D RN The network learns a mapping from images of objects to their underlying D shapes from a large collection of synthetic data  Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a D occupancy grid Unlike most of the previous works our network does not require any image annotations or object class labels for training or testing Our extensive experimental analysis shows that our reconstruction framework i outperforms the state of theart methods for single view reconstruction and ii enables the D reconstruction of objects in situations when traditional SFMSLAM methods fail because of lack of texture andor wide baseline</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual networks family with hundreds or even thousands of layers dominate major image recognition tasks but building a network by simply stacking residual blocks inevitably limits its optimization ability This paper proposes a novel residual network architecture Residual networks of Residual networks RoR to dig the optimization ability of residual networks RoR substitutes optimizing residual mapping of residual mapping for optimizing original residual mapping in particular adding level wise shortcut connections upon original residual networks to promote the learning capability of residual networks More importantly RoR can be applied to various kinds of residual networks Pre ResNets and WRN and significantly boost their performance Our experiments demonstrate the effectiveness and versatility of RoR where it achieves the best performance in all residual network like structures Our RoR  WRN  models achieve new state of the art results on CIFAR  CIFAR  and SVHN with test errors   and  respectively These results outperform  layer Pre ResNets by  on CIFAR  and  on CIFAR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We propose local binary convolution LBC an efficient alternative to convolutional layers in standard convolutional neural networks CNN The design principles of LBC are motivated by local binary patterns LBP The LBC layer comprises of a set of fixed sparse pre defined binary convolutional filters that are not updated during the training process a non linear activation function and a set of learnable linear weights The linear weights combine the activated filter responses to approximate the corresponding activated filter responses of a standard convolutional layer The LBC layer affords significant parameter savings  to  in the number of learnable parameters compared to a standard convolutional layer Furthermore due to lower model complexity and sparse and binary nature of the weights also results in up to  to  savings in model size compared to a standard convolutional layer We demonstrate both theoretically and experimentally that our local binary convolution layer is a good approximation of a standard convolutional layer Empirically CNNs with LBC layers called local binary convolutional neural networks LBCNN reach state of the art performance on a range of visual datasets MNIST SVHN CIFAR  and a subset of ImageNet while enjoying significant computational savings</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We present a unified framework for learning continuous control policies using backpropagation It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise The product is a spectrum of general policy gradient algorithms that range from model free methods with value functions to model based methods without value functions We use learned models but only require observations from the environment instead of observations from model predicted trajectories minimizing the impact of compounded model errors We apply these algorithms first to a toy stochastic control problem and then to several physics based control problems in simulation One of these variants SVG shows the effectiveness of learning models value functions and policies simultaneously in continuous domains</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recent advances in machine learning have dramatically improved the accuracy of object recognition in static images However humans and animals are rarely confronted with images that are completely static and typically perform a variety of interventions to aid in object recognition from changes in viewpoint to active manipulation such as turning an object around to see it from all sides or removing occluders Such active perception is particularly important in robotics where correct object classification is often crucial and active interventions are available to aid in recognition In this paper we present a method that can learn such active interventions and evaluate our method on a simulated environment that we call Occluded MNIST which requires the agent to push distractor objects out of the way to perform successful recognition We evaluate a variety of solutions based on reinforcement learning and demonstrate that active intervention substantially improves recognition accuracy over a passive baseline</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kalman Filters are one of the most influential models of time varying phenomena They admit an intuitive probabilistic interpretation have a simple functional form and enjoy widespread adoption in a variety of disciplines Motivated by recent variational methods for learning deep generative models we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters Of particular interest is the use of temporal generative models for counterfactual inference We investigate the efficacy of such models for counterfactual inference and to that end we introduce the Healing MNIST dataset where long term structure noise and actions are applied to sequences of digits We show the efficacy of our method for modeling this dataset We further show how our model can be used for counterfactual inference for patients based on electronic health record data of  patients over  years</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.077689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Many tasks in AI require the collaboration of multiple agents Typically the communication protocol between agents is manually specified and not altered during training In this paper we explore a simple neural model called CommNN that uses continuous communication for fully cooperative tasks The model consists of multiple agents and the communication between them is learned alongside their policy We apply this model to a diverse set of tasks demonstrating the ability of the agents to learn to communicate amongst themselves yielding improved performance over non communicative agents and baselines In some cases it is possible to interpret the language devised by the agents revealing simple but effective strategies for solving the task at hand</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recurrent neural networks such as the GRU and LSTM found wide adoption in natural language processing and achieve state of the art results for many tasks These models are characterized by a memory state that can be written to and read from by applying gated composition operations to the current input and the previous state However they only cover a small subset of potentially useful compositions We propose Multi Function Recurrent Units MuFuRUs that allow for arbitrary differentiable functions as composition operations Furthermore MuFuRUs allow for an input  and state dependent choice of these composition operations that is learned Our experiments demonstrate that the additional functionality helps in different sequence modeling tasks including the evaluation of propositional logic formulae language modeling and sentiment analysis</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep neural networks DNNs have demonstrated state of the art results on many pattern recognition tasks especially vision classification problems Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own rightsimilar to why we study the human brainand will enable researchers to further improve DNNs One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect One such method is called activation maximization AM which synthesizes an input eg an image that highly activates a neuron Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful learned prior a deep generator network DGN The algorithm  generates qualitatively state of the art synthetic images that look almost real  reveals the features learned by each neuron in an interpretable way  generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned and  can be considered as a high quality generative method in this case by generating novel creative interesting recognizable images</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073230</td>\n",
       "      <td>0.068392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We introduce SE Nets which are deep networks designed to model rigid body motion from raw point cloud data Based only on pairs of depth images along with an action vector and point wise data associations SE Nets learn to segment effected object parts and predict their motion resulting from the applied force Rather than learning point wise flow vectors SE Nets predict SE transformations for different parts of the scene Using simulated depth data of a table top scene and a robot manipulator we show that the structure underlying SE  Nets enables them to generate a far more consistent prediction of object motion than traditional flow based networks</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071145</td>\n",
       "      <td>0.068329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We trained a convolutional neural network CNN to map raw pixels from a single front facing camera directly to steering commands This end to end approach proved surprisingly powerful With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal We never explicitly trained it to detect for example the outline of roads Compared to explicit decomposition of the problem such as lane marking detection path planning and control our end to end system optimizes all processing steps simultaneously We argue that this will eventually lead to better performance and smaller systems Better performance will result because the internal components self optimize to maximize overall system performance instead of optimizing human selected intermediate criteria e g lane detection Such criteria understandably are selected for ease of human interpretation which doesnt automatically guarantee maximum system performance Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps We used an NVIDIA DevBox and Torch  for training and an NVIDIA DRIVETM PX self driving car computer also running Torch  for determining where to drive The system operates at  frames per second FPS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Despite recent breakthroughs in the applications of deep neural networks one setting that presents a persistent challenge is that of one shot learning Traditional gradient based networks require a lot of data to learn often through extensive iterative training When new data is encountered the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference Architectures with augmented memory capacities such as Neural Turing Machines NTMs offer the ability to quickly encode and retrieve new information and hence can potentially obviate the downsides of conventional models Here we demonstrate the ability of a memory augmented neural network to rapidly assimilate new data and leverage this data to make accurate predictions after only a few samples We also introduce a new method for accessing an external memory that focuses on memory content unlike previous methods that additionally use memory location based focusing mechanisms</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Video games are a compelling source of annotated data as they can readily provide fine grained groundtruth for diverse tasks However it is not clear whether the synthetically generated data has enough resemblance to the real world images to improve the performance of computer vision models in practice We present experiments assessing the effectiveness on real world data of systems trained on synthetic RGB images that are extracted from a video game We collected over  synthetic samples from a modern video game with similar conditions to the real world CamVid and Cityscapes datasets We provide several experiments to demonstrate that the synthetically generated RGB images can be used to improve the performance of deep neural networks on both image segmentation and depth estimation These results show that a convolutional network trained on synthetic data achieves a similar test error to a network that is trained on real world data for dense image classification Furthermore the synthetically generated RGB images can provide similar or better results compared to the real world datasets if a simple domain adaptation technique is applied Our results suggest that collaboration with game developers for an accessible interface to gather data is potentially a fruitful direction for future work in computer vision</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Physical viability in particular energy efficiency is a key challenge in realizing the true potential of Deep Neural Networks DNNs In this paper we aim to incorporate the energy dimension as a design parameter in the higher level hierarchy of DNN training and execution to optimize for the energy resources and constraints We use energy characterization to bound the network size in accordance to the pertinent physical resources An automated customization methodology is proposed to adaptively conform the DNN configurations to the underlying hardware characteristics while minimally affecting the inference accuracy The key to our approach is a new context and resource aware projection of data to a lower dimensional embedding by which learning the correlation between data samples requires significantly smaller number of neurons We leverage the performance gain achieved as a result of the data projection to enable the training of different DNN architectures which can be aggregated together to further boost the inference accuracy Accompanying APIs are provided to facilitate rapid prototyping of an arbitrary DNN application customized to the underlying platform Proof of concept evaluations for deployment of different visual audio and smart sensing benchmarks demonstrate up to  fold energy improvement compared to the prior art DL solutions</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training directed neural networks typically requires forward propagating data through a computation graph followed by backpropagating error signal to produce weight updates All layers or more generally modules of the network are therefore locked in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph These models predict what the result of the modelled subgraph will produce using only local information In particular we focus on modelling error gradients by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs and can update them independently and asynchronously ie we realise decoupled neural interfaces We show results for feed forward models where every layer is trained asynchronously recurrent neural networks RNNs where predicting ones future gradient extends the time over which the RNN can effectively model and also a hierarchical RNN system with ticking at different timescales Finally we demonstrate that in addition to predicting gradients the same framework can be used to predict inputs resulting in models which are decoupled in both the forward and backwards pass    amounting to independent networks which co learn such that they can be composed into a single functioning corporation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This work explores conditional image generation with a new image density model based on the PixelCNN architecture The model can be conditioned on any vector including descriptive labels or tags or latent embeddings created by other networks When conditioned on class labels from the ImageNet database the model is able to generate diverse realistic scenes representing distinct animals objects landscapes and structures When conditioned on an embedding produced by a convolutional network given a single image of an unseen face it generates a variety of new portraits of the same person with different facial expressions poses and lighting conditions We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder Additionally the gated convolutional layers in the proposed model improve the log likelihood of PixelCNN to match the state of the art performance of PixelRNN on ImageNet with greatly reduced computational cost</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The move from hand designed features to learned features in machine learning has been wildly successful In spite of this optimization algorithms are still designed by hand In this paper we show how the design of an optimization algorithm can be cast as a learning problem allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way Our learned algorithms implemented by LSTMs outperform generic hand designed competitors on the tasks for which they are trained and also generalize well to new tasks with similar structure We demonstrate this on a number of tasks including simple convex problems training neural networks and styling images with neural art</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090435</td>\n",
       "      <td>0.173711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scalable and effective exploration remains a key challenge in reinforcement learning RL While there are methods with optimality guarantees in the setting of discrete state and action spaces these methods cannot be applied in high dimensional deep RL scenarios As such most contemporary RL relies on simple heuristics such as epsilon greedy exploration or adding Gaussian noise to the controls This paper introduces Variational Information Maximizing Exploration VIME an exploration strategy based on maximization of information gain about the agents belief of environment dynamics We propose a practical implementation using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces VIME modifies the MDP reward function and can be applied with several different underlying RL algorithms We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms including tasks with very sparse rewards</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In this work we introduce a novel interpretation of residual networks showing they are exponential ensembles This observation is supported by a large scale lesion study that demonstrates they behave just like ensembles at test time Subsequently we perform an analysis showing these ensembles mostly consist of networks that are each relatively shallow For example contrary to our expectations most of the gradient in a residual network with  layers comes from an ensemble of very short networks ie only   layers deep This suggests that in addition to describing neural networks in terms of width and depth there is a third dimension multiplicity the size of the implicit ensemble Ultimately residual networks do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network   rather they avoid the problem simply by ensembling many short networks together This insight reveals that depth is still an open research question and invites the exploration of the related notion of multiplicity</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We present an approach to training neural networks to generate sequences using actor critic methods from reinforcement learning RL Current log likelihood training methods are limited by the discrepancy between their training and testing modes as models must generate tokens conditioned on their previous guesses rather than the ground truth tokens We address this problem by introducing a critic network that is trained to predict the value of an output token given the policy of an actor network This results in a training procedure that is much closer to the test phase and allows us to directly optimize for a task specific score such as BLEU Crucially since we leverage these techniques in the supervised learning setting rather than the traditional RL setting we condition the critic network on the ground truth output We show that our method leads to improved performance on both a synthetic task and for German English machine translation Our analysis paves the way for such methods to be applied in natural language generation tasks such as machine translation caption generation and dialogue modelling</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks GANs framework We focus on two applications of GANs semi supervised learning and the generation of images that humans find visually realistic Unlike most work on generative models our primary goal is not to train a model that assigns high likelihood to test data nor do we require the model to be able to learn well without using any labels Using our new techniques we achieve state of the art results in semi supervised classification on MNIST CIFAR  and SVHN The generated images are of high quality as confirmed by a visual Turing test our model generates MNIST samples that humans cannot distinguish from real data and CIFAR  samples that yield a human error rate of  We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Directly reading documents and being able to answer questions from them is a key problem To avoid its inherent difficulty question answering QA has been directed towards using Knowledge Bases KBs instead which has proven effective Unfortunately KBs suffer from often being too restrictive as the schema cannot support certain types of answers and too sparse eg Wikipedia contains much more information than Freebase In this work we introduce a new method Key Value Memory Networks that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation To compare using KBs information extraction or Wikipedia documents directly in a single framework we construct an analysis tool MovieQA a QA dataset in the domain of movies Our method closes the gap between all three settings It also achieves state of the art results on the existing WikiQA benchmark</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This paper introduces Adaptive Computation Time ACT an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output ACT requires minimal changes to the network architecture is deterministic and differentiable and does not add any noise to the parameter gradients Experimental results are provided for four synthetic problems determining the parity of binary vectors applying binary logic operations adding integers and sorting real numbers Overall performance is dramatically improved by the use of ACT which successfully adapts the number of computational steps to the requirements of the problem We also present character level language modelling results on the Hutter prize Wikipedia dataset In this case ACT does not yield large gains in performance however it does provide intriguing insight into the structure of the data with more computation allocated to harder to predict transitions such as spaces between words and ends of sentences This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150243</td>\n",
       "      <td>0.071158</td>\n",
       "      <td>0.068342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reinforcement learning offers a promising methodology for developing skills for simulated characters but typically requires working with sparse hand crafted features Building on recent progress in deep reinforcement learning DeepRL we introduce a mixture of actor critic experts MACE approach that learns terrain adaptive dynamic locomotion skills using high dimensional state and terrain descriptions as input and parameterized leaps or steps as output actions MACE learns more quickly than a single actor critic approach and results in actor critic experts that exhibit specialization Additional elements of our solution that contribute towards efficient learning include Boltzmann exploration and the use of initial actor biases to encourage specialization Results are demonstrated for multiple planar characters and terrain classes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The ability to transfer knowledge gained in previous tasks into new contexts is one of the most important mechanisms of human learning Despite this adapting autonomous behavior to be reused in partially similar settings is still an open problem in current robotics research In this paper we take a small step in this direction and propose a generic framework for learning transferable motion policies Our goal is to solve a learning problem in a target domain by utilizing the training data in a different but related source domain We present this in the context of an autonomous MAV flight using monocular reactive control and demonstrate the efficacy of our proposed approach through extensive real world flight experiments in outdoor cluttered environments</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109579</td>\n",
       "      <td>0.106927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In a given scene humans can often easily predict a set of immediate future events that might happen However generalized pixel level anticipation in computer vision systems is difficult because machine learning struggles with the ambiguity inherent in predicting the future In this paper we focus on predicting the dense trajectory of pixels in a scene specifically what will move in the scene where it will travel and how it will deform over the course of one second We propose a conditional variational autoencoder as a solution to this problem In this framework direct inference from the image shapes the distribution of possible trajectories while latent variables encode any necessary information that is not available in the image We show that our method is able to successfully predict events in a wide variety of scenes and can produce multiple different predictions when the future is ambiguous Our algorithm is trained on thousands of diverse realistic videos and requires absolutely no human labeling In addition to non semantic action prediction we find that our method learns a representation that is applicable to semantic vision tasks</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks Although the unmatched expressiveness of the many layers can be highly desirable at test time training very deep networks comes with its own set of challenges The gradients can vanish the forward flow often diminishes and the training time can be painfully slow To address these problems we propose stochastic depth a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time We start with very deep networks but during training for each mini batch randomly drop a subset of layers and bypass them with the identity function This simple approach complements the recent success of residual networks It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation With stochastic depth we can increase the depth of residual networks even beyond  layers and still yield meaningful improvements in test error  on CIFAR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6605 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    structural      form  \\\n",
       "  Both scientists and children make important s...    0.372479  0.358872   \n",
       "  We consider joint estimation of multiple grap...    0.000000  0.000000   \n",
       "  ReLU neural networks define piecewise linear ...    0.000000  0.000000   \n",
       "  Neural networks with rectified linear unit ac...    0.000000  0.000000   \n",
       "  We propose a new framework for manifold denoi...    0.000000  0.064165   \n",
       "  An associative memory is a framework of conte...    0.000000  0.051021   \n",
       "  Variational auto encoders VAE are scalable an...    0.000000  0.000000   \n",
       "  Increasing availability of vehicle GPS data h...    0.000000  0.000000   \n",
       "  We address the issue of speeding up the train...    0.000000  0.000000   \n",
       "  Phase synchronisation in multichannel EEG is ...    0.000000  0.000000   \n",
       "  In this paper we propose a new primal dual al...    0.000000  0.000000   \n",
       "  Co adaptation is a special form of on line le...    0.000000  0.078993   \n",
       "  Humans are remarkably adept at interpreting t...    0.000000  0.000000   \n",
       "  Plants sense their environment by producing e...    0.000000  0.000000   \n",
       "  We introduce a new large scale music dataset ...    0.000000  0.000000   \n",
       "  Demanding sparsity in estimated models has be...    0.000000  0.061749   \n",
       "  In this paper we study the efficiency of a bf...    0.000000  0.000000   \n",
       "  Adaptive schemes where tasks are assigned bas...    0.000000  0.000000   \n",
       "  A binary classifier capable of abstaining fro...    0.000000  0.000000   \n",
       "  In this paper we aim at recovering an undirec...    0.000000  0.000000   \n",
       "  A typical problem in causal modeling is the i...    0.080591  0.000000   \n",
       "  Gaussian processes GPs are flexible distribut...    0.000000  0.000000   \n",
       "  Modern discriminative predictors have been sh...    0.000000  0.000000   \n",
       "  Relational learning deals with data that are ...    0.000000  0.000000   \n",
       "  Sources of variability in experimentally deri...    0.000000  0.000000   \n",
       "  We provide a general framework for privacy pr...    0.000000  0.000000   \n",
       "  Various active learning methods based on logi...    0.000000  0.000000   \n",
       "  The introduction of data analytics into medic...    0.000000  0.000000   \n",
       "  This work investigates the case of a network ...    0.000000  0.052215   \n",
       "  Learning a natural language interface for dat...    0.000000  0.000000   \n",
       "...                                                        ...       ...   \n",
       "Learning to solve complex sequences of tasks  w...    0.000000  0.000000   \n",
       "We introduce a simple recurrent variational aut...    0.000000  0.000000   \n",
       "We discuss relations between Residual Networks ...    0.000000  0.000000   \n",
       "Inspired by the recent success of methods that ...    0.000000  0.074885   \n",
       "Residual networks family with hundreds or even ...    0.000000  0.000000   \n",
       "We propose local binary convolution LBC an effi...    0.000000  0.000000   \n",
       "We present a unified framework for learning con...    0.000000  0.000000   \n",
       "Recent advances in machine learning have dramat...    0.000000  0.000000   \n",
       "Kalman Filters are one of the most influential ...    0.000000  0.092981   \n",
       "Many tasks in AI require the collaboration of m...    0.000000  0.000000   \n",
       "Recurrent neural networks such as the GRU and L...    0.000000  0.000000   \n",
       "Deep neural networks DNNs have demonstrated sta...    0.000000  0.000000   \n",
       "We introduce SE Nets which are deep networks de...    0.000000  0.000000   \n",
       "We trained a convolutional neural network CNN t...    0.000000  0.000000   \n",
       "Despite recent breakthroughs in the application...    0.000000  0.000000   \n",
       "Video games are a compelling source of annotate...    0.000000  0.000000   \n",
       "Physical viability in particular energy efficie...    0.000000  0.000000   \n",
       "Training directed neural networks typically req...    0.000000  0.000000   \n",
       "This work explores conditional image generation...    0.000000  0.000000   \n",
       "The move from hand designed features to learned...    0.000000  0.000000   \n",
       "Scalable and effective exploration remains a ke...    0.000000  0.000000   \n",
       "In this work we introduce a novel interpretatio...    0.000000  0.000000   \n",
       "We present an approach to training neural netwo...    0.000000  0.000000   \n",
       "We present a variety of new architectural featu...    0.000000  0.000000   \n",
       "Directly reading documents and being able to an...    0.000000  0.000000   \n",
       "This paper introduces Adaptive Computation Time...    0.000000  0.000000   \n",
       "Reinforcement learning offers a promising metho...    0.000000  0.000000   \n",
       "The ability to transfer knowledge gained in pre...    0.000000  0.000000   \n",
       "In a given scene humans can often easily predic...    0.000000  0.000000   \n",
       "Very deep convolutional networks with hundreds ...    0.000000  0.000000   \n",
       "\n",
       "                                                       forms  intuitive  \\\n",
       "  Both scientists and children make important s...  0.221204   0.217130   \n",
       "  We consider joint estimation of multiple grap...  0.000000   0.000000   \n",
       "  ReLU neural networks define piecewise linear ...  0.000000   0.000000   \n",
       "  Neural networks with rectified linear unit ac...  0.000000   0.000000   \n",
       "  We propose a new framework for manifold denoi...  0.000000   0.000000   \n",
       "  An associative memory is a framework of conte...  0.000000   0.000000   \n",
       "  Variational auto encoders VAE are scalable an...  0.000000   0.000000   \n",
       "  Increasing availability of vehicle GPS data h...  0.000000   0.000000   \n",
       "  We address the issue of speeding up the train...  0.000000   0.000000   \n",
       "  Phase synchronisation in multichannel EEG is ...  0.000000   0.000000   \n",
       "  In this paper we propose a new primal dual al...  0.000000   0.000000   \n",
       "  Co adaptation is a special form of on line le...  0.000000   0.000000   \n",
       "  Humans are remarkably adept at interpreting t...  0.000000   0.000000   \n",
       "  Plants sense their environment by producing e...  0.000000   0.000000   \n",
       "  We introduce a new large scale music dataset ...  0.000000   0.000000   \n",
       "  Demanding sparsity in estimated models has be...  0.000000   0.000000   \n",
       "  In this paper we study the efficiency of a bf...  0.000000   0.000000   \n",
       "  Adaptive schemes where tasks are assigned bas...  0.000000   0.000000   \n",
       "  A binary classifier capable of abstaining fro...  0.000000   0.000000   \n",
       "  In this paper we aim at recovering an undirec...  0.000000   0.000000   \n",
       "  A typical problem in causal modeling is the i...  0.000000   0.000000   \n",
       "  Gaussian processes GPs are flexible distribut...  0.000000   0.000000   \n",
       "  Modern discriminative predictors have been sh...  0.000000   0.000000   \n",
       "  Relational learning deals with data that are ...  0.000000   0.000000   \n",
       "  Sources of variability in experimentally deri...  0.000000   0.000000   \n",
       "  We provide a general framework for privacy pr...  0.000000   0.000000   \n",
       "  Various active learning methods based on logi...  0.000000   0.000000   \n",
       "  The introduction of data analytics into medic...  0.000000   0.000000   \n",
       "  This work investigates the case of a network ...  0.000000   0.000000   \n",
       "  Learning a natural language interface for dat...  0.089898   0.000000   \n",
       "...                                                      ...        ...   \n",
       "Learning to solve complex sequences of tasks  w...  0.000000   0.000000   \n",
       "We introduce a simple recurrent variational aut...  0.000000   0.000000   \n",
       "We discuss relations between Residual Networks ...  0.000000   0.000000   \n",
       "Inspired by the recent success of methods that ...  0.000000   0.000000   \n",
       "Residual networks family with hundreds or even ...  0.000000   0.000000   \n",
       "We propose local binary convolution LBC an effi...  0.000000   0.000000   \n",
       "We present a unified framework for learning con...  0.000000   0.000000   \n",
       "Recent advances in machine learning have dramat...  0.000000   0.000000   \n",
       "Kalman Filters are one of the most influential ...  0.000000   0.140641   \n",
       "Many tasks in AI require the collaboration of m...  0.000000   0.000000   \n",
       "Recurrent neural networks such as the GRU and L...  0.000000   0.000000   \n",
       "Deep neural networks DNNs have demonstrated sta...  0.000000   0.000000   \n",
       "We introduce SE Nets which are deep networks de...  0.000000   0.000000   \n",
       "We trained a convolutional neural network CNN t...  0.000000   0.000000   \n",
       "Despite recent breakthroughs in the application...  0.000000   0.000000   \n",
       "Video games are a compelling source of annotate...  0.000000   0.000000   \n",
       "Physical viability in particular energy efficie...  0.000000   0.000000   \n",
       "Training directed neural networks typically req...  0.000000   0.000000   \n",
       "This work explores conditional image generation...  0.000000   0.000000   \n",
       "The move from hand designed features to learned...  0.000000   0.000000   \n",
       "Scalable and effective exploration remains a ke...  0.000000   0.000000   \n",
       "In this work we introduce a novel interpretatio...  0.000000   0.000000   \n",
       "We present an approach to training neural netwo...  0.000000   0.000000   \n",
       "We present a variety of new architectural featu...  0.000000   0.000000   \n",
       "Directly reading documents and being able to an...  0.000000   0.000000   \n",
       "This paper introduces Adaptive Computation Time...  0.000000   0.000000   \n",
       "Reinforcement learning offers a promising metho...  0.000000   0.000000   \n",
       "The ability to transfer knowledge gained in pre...  0.000000   0.000000   \n",
       "In a given scene humans can often easily predic...  0.000000   0.000000   \n",
       "Very deep convolutional networks with hundreds ...  0.000000   0.000000   \n",
       "\n",
       "                                                    discovery      bias  \\\n",
       "  Both scientists and children make important s...   0.204885  0.195566   \n",
       "  We consider joint estimation of multiple grap...   0.000000  0.000000   \n",
       "  ReLU neural networks define piecewise linear ...   0.000000  0.000000   \n",
       "  Neural networks with rectified linear unit ac...   0.000000  0.000000   \n",
       "  We propose a new framework for manifold denoi...   0.000000  0.000000   \n",
       "  An associative memory is a framework of conte...   0.000000  0.000000   \n",
       "  Variational auto encoders VAE are scalable an...   0.000000  0.000000   \n",
       "  Increasing availability of vehicle GPS data h...   0.000000  0.000000   \n",
       "  We address the issue of speeding up the train...   0.000000  0.000000   \n",
       "  Phase synchronisation in multichannel EEG is ...   0.000000  0.000000   \n",
       "  In this paper we propose a new primal dual al...   0.000000  0.000000   \n",
       "  Co adaptation is a special form of on line le...   0.000000  0.000000   \n",
       "  Humans are remarkably adept at interpreting t...   0.000000  0.000000   \n",
       "  Plants sense their environment by producing e...   0.000000  0.000000   \n",
       "  We introduce a new large scale music dataset ...   0.000000  0.000000   \n",
       "  Demanding sparsity in estimated models has be...   0.000000  0.000000   \n",
       "  In this paper we study the efficiency of a bf...   0.000000  0.000000   \n",
       "  Adaptive schemes where tasks are assigned bas...   0.000000  0.000000   \n",
       "  A binary classifier capable of abstaining fro...   0.000000  0.000000   \n",
       "  In this paper we aim at recovering an undirec...   0.000000  0.000000   \n",
       "  A typical problem in causal modeling is the i...   0.000000  0.000000   \n",
       "  Gaussian processes GPs are flexible distribut...   0.000000  0.000000   \n",
       "  Modern discriminative predictors have been sh...   0.000000  0.000000   \n",
       "  Relational learning deals with data that are ...   0.000000  0.000000   \n",
       "  Sources of variability in experimentally deri...   0.000000  0.000000   \n",
       "  We provide a general framework for privacy pr...   0.000000  0.000000   \n",
       "  Various active learning methods based on logi...   0.000000  0.000000   \n",
       "  The introduction of data analytics into medic...   0.000000  0.000000   \n",
       "  This work investigates the case of a network ...   0.000000  0.000000   \n",
       "  Learning a natural language interface for dat...   0.000000  0.000000   \n",
       "...                                                       ...       ...   \n",
       "Learning to solve complex sequences of tasks  w...   0.000000  0.000000   \n",
       "We introduce a simple recurrent variational aut...   0.000000  0.000000   \n",
       "We discuss relations between Residual Networks ...   0.000000  0.000000   \n",
       "Inspired by the recent success of methods that ...   0.000000  0.000000   \n",
       "Residual networks family with hundreds or even ...   0.000000  0.000000   \n",
       "We propose local binary convolution LBC an effi...   0.000000  0.000000   \n",
       "We present a unified framework for learning con...   0.000000  0.000000   \n",
       "Recent advances in machine learning have dramat...   0.000000  0.000000   \n",
       "Kalman Filters are one of the most influential ...   0.000000  0.000000   \n",
       "Many tasks in AI require the collaboration of m...   0.000000  0.000000   \n",
       "Recurrent neural networks such as the GRU and L...   0.000000  0.000000   \n",
       "Deep neural networks DNNs have demonstrated sta...   0.000000  0.000000   \n",
       "We introduce SE Nets which are deep networks de...   0.000000  0.000000   \n",
       "We trained a convolutional neural network CNN t...   0.000000  0.000000   \n",
       "Despite recent breakthroughs in the application...   0.000000  0.000000   \n",
       "Video games are a compelling source of annotate...   0.000000  0.000000   \n",
       "Physical viability in particular energy efficie...   0.000000  0.000000   \n",
       "Training directed neural networks typically req...   0.000000  0.000000   \n",
       "This work explores conditional image generation...   0.000000  0.000000   \n",
       "The move from hand designed features to learned...   0.000000  0.000000   \n",
       "Scalable and effective exploration remains a ke...   0.000000  0.000000   \n",
       "In this work we introduce a novel interpretatio...   0.000000  0.000000   \n",
       "We present an approach to training neural netwo...   0.000000  0.000000   \n",
       "We present a variety of new architectural featu...   0.000000  0.000000   \n",
       "Directly reading documents and being able to an...   0.000000  0.000000   \n",
       "This paper introduces Adaptive Computation Time...   0.000000  0.000000   \n",
       "Reinforcement learning offers a promising metho...   0.000000  0.000000   \n",
       "The ability to transfer knowledge gained in pre...   0.000000  0.000000   \n",
       "In a given scene humans can often easily predic...   0.000000  0.000000   \n",
       "Very deep convolutional networks with hundreds ...   0.000000  0.000000   \n",
       "\n",
       "                                                     initial      tree  \\\n",
       "  Both scientists and children make important s...  0.193394  0.187002   \n",
       "  We consider joint estimation of multiple grap...  0.000000  0.000000   \n",
       "  ReLU neural networks define piecewise linear ...  0.000000  0.000000   \n",
       "  Neural networks with rectified linear unit ac...  0.000000  0.000000   \n",
       "  We propose a new framework for manifold denoi...  0.000000  0.000000   \n",
       "  An associative memory is a framework of conte...  0.000000  0.000000   \n",
       "  Variational auto encoders VAE are scalable an...  0.000000  0.000000   \n",
       "  Increasing availability of vehicle GPS data h...  0.000000  0.000000   \n",
       "  We address the issue of speeding up the train...  0.000000  0.000000   \n",
       "  Phase synchronisation in multichannel EEG is ...  0.000000  0.000000   \n",
       "  In this paper we propose a new primal dual al...  0.000000  0.000000   \n",
       "  Co adaptation is a special form of on line le...  0.000000  0.000000   \n",
       "  Humans are remarkably adept at interpreting t...  0.000000  0.000000   \n",
       "  Plants sense their environment by producing e...  0.000000  0.000000   \n",
       "  We introduce a new large scale music dataset ...  0.000000  0.000000   \n",
       "  Demanding sparsity in estimated models has be...  0.000000  0.000000   \n",
       "  In this paper we study the efficiency of a bf...  0.077867  0.000000   \n",
       "  Adaptive schemes where tasks are assigned bas...  0.000000  0.000000   \n",
       "  A binary classifier capable of abstaining fro...  0.000000  0.000000   \n",
       "  In this paper we aim at recovering an undirec...  0.000000  0.000000   \n",
       "  A typical problem in causal modeling is the i...  0.000000  0.000000   \n",
       "  Gaussian processes GPs are flexible distribut...  0.000000  0.000000   \n",
       "  Modern discriminative predictors have been sh...  0.000000  0.000000   \n",
       "  Relational learning deals with data that are ...  0.000000  0.000000   \n",
       "  Sources of variability in experimentally deri...  0.000000  0.000000   \n",
       "  We provide a general framework for privacy pr...  0.000000  0.000000   \n",
       "  Various active learning methods based on logi...  0.000000  0.000000   \n",
       "  The introduction of data analytics into medic...  0.000000  0.000000   \n",
       "  This work investigates the case of a network ...  0.070346  0.000000   \n",
       "  Learning a natural language interface for dat...  0.000000  0.000000   \n",
       "...                                                      ...       ...   \n",
       "Learning to solve complex sequences of tasks  w...  0.000000  0.000000   \n",
       "We introduce a simple recurrent variational aut...  0.000000  0.000000   \n",
       "We discuss relations between Residual Networks ...  0.000000  0.000000   \n",
       "Inspired by the recent success of methods that ...  0.000000  0.000000   \n",
       "Residual networks family with hundreds or even ...  0.000000  0.000000   \n",
       "We propose local binary convolution LBC an effi...  0.000000  0.000000   \n",
       "We present a unified framework for learning con...  0.000000  0.000000   \n",
       "Recent advances in machine learning have dramat...  0.000000  0.000000   \n",
       "Kalman Filters are one of the most influential ...  0.000000  0.000000   \n",
       "Many tasks in AI require the collaboration of m...  0.000000  0.000000   \n",
       "Recurrent neural networks such as the GRU and L...  0.000000  0.000000   \n",
       "Deep neural networks DNNs have demonstrated sta...  0.000000  0.000000   \n",
       "We introduce SE Nets which are deep networks de...  0.000000  0.000000   \n",
       "We trained a convolutional neural network CNN t...  0.000000  0.000000   \n",
       "Despite recent breakthroughs in the application...  0.000000  0.000000   \n",
       "Video games are a compelling source of annotate...  0.000000  0.000000   \n",
       "Physical viability in particular energy efficie...  0.000000  0.000000   \n",
       "Training directed neural networks typically req...  0.000000  0.000000   \n",
       "This work explores conditional image generation...  0.000000  0.000000   \n",
       "The move from hand designed features to learned...  0.000000  0.000000   \n",
       "Scalable and effective exploration remains a ke...  0.000000  0.000000   \n",
       "In this work we introduce a novel interpretatio...  0.000000  0.000000   \n",
       "We present an approach to training neural netwo...  0.000000  0.000000   \n",
       "We present a variety of new architectural featu...  0.000000  0.000000   \n",
       "Directly reading documents and being able to an...  0.000000  0.000000   \n",
       "This paper introduces Adaptive Computation Time...  0.000000  0.000000   \n",
       "Reinforcement learning offers a promising metho...  0.127878  0.000000   \n",
       "The ability to transfer knowledge gained in pre...  0.000000  0.000000   \n",
       "In a given scene humans can often easily predic...  0.000000  0.000000   \n",
       "Very deep convolutional networks with hundreds ...  0.000000  0.000000   \n",
       "\n",
       "                                                    knowledge     human  \\\n",
       "  Both scientists and children make important s...   0.144667  0.141165   \n",
       "  We consider joint estimation of multiple grap...   0.072111  0.000000   \n",
       "  ReLU neural networks define piecewise linear ...   0.000000  0.000000   \n",
       "  Neural networks with rectified linear unit ac...   0.000000  0.000000   \n",
       "  We propose a new framework for manifold denoi...   0.000000  0.000000   \n",
       "  An associative memory is a framework of conte...   0.000000  0.000000   \n",
       "  Variational auto encoders VAE are scalable an...   0.000000  0.000000   \n",
       "  Increasing availability of vehicle GPS data h...   0.000000  0.000000   \n",
       "  We address the issue of speeding up the train...   0.000000  0.000000   \n",
       "  Phase synchronisation in multichannel EEG is ...   0.000000  0.000000   \n",
       "  In this paper we propose a new primal dual al...   0.000000  0.000000   \n",
       "  Co adaptation is a special form of on line le...   0.000000  0.000000   \n",
       "  Humans are remarkably adept at interpreting t...   0.000000  0.124163   \n",
       "  Plants sense their environment by producing e...   0.000000  0.000000   \n",
       "  We introduce a new large scale music dataset ...   0.000000  0.000000   \n",
       "  Demanding sparsity in estimated models has be...   0.000000  0.000000   \n",
       "  In this paper we study the efficiency of a bf...   0.000000  0.000000   \n",
       "  Adaptive schemes where tasks are assigned bas...   0.000000  0.000000   \n",
       "  A binary classifier capable of abstaining fro...   0.000000  0.000000   \n",
       "  In this paper we aim at recovering an undirec...   0.153037  0.000000   \n",
       "  A typical problem in causal modeling is the i...   0.062601  0.000000   \n",
       "  Gaussian processes GPs are flexible distribut...   0.000000  0.000000   \n",
       "  Modern discriminative predictors have been sh...   0.000000  0.000000   \n",
       "  Relational learning deals with data that are ...   0.000000  0.000000   \n",
       "  Sources of variability in experimentally deri...   0.000000  0.000000   \n",
       "  We provide a general framework for privacy pr...   0.000000  0.000000   \n",
       "  Various active learning methods based on logi...   0.000000  0.000000   \n",
       "  The introduction of data analytics into medic...   0.000000  0.000000   \n",
       "  This work investigates the case of a network ...   0.000000  0.000000   \n",
       "  Learning a natural language interface for dat...   0.058793  0.000000   \n",
       "...                                                       ...       ...   \n",
       "Learning to solve complex sequences of tasks  w...   0.102578  0.100095   \n",
       "We introduce a simple recurrent variational aut...   0.000000  0.000000   \n",
       "We discuss relations between Residual Networks ...   0.000000  0.000000   \n",
       "Inspired by the recent success of methods that ...   0.000000  0.000000   \n",
       "Residual networks family with hundreds or even ...   0.000000  0.000000   \n",
       "We propose local binary convolution LBC an effi...   0.000000  0.000000   \n",
       "We present a unified framework for learning con...   0.000000  0.000000   \n",
       "Recent advances in machine learning have dramat...   0.000000  0.000000   \n",
       "Kalman Filters are one of the most influential ...   0.000000  0.000000   \n",
       "Many tasks in AI require the collaboration of m...   0.000000  0.000000   \n",
       "Recurrent neural networks such as the GRU and L...   0.000000  0.000000   \n",
       "Deep neural networks DNNs have demonstrated sta...   0.000000  0.073230   \n",
       "We introduce SE Nets which are deep networks de...   0.000000  0.000000   \n",
       "We trained a convolutional neural network CNN t...   0.000000  0.177893   \n",
       "Despite recent breakthroughs in the application...   0.000000  0.000000   \n",
       "Video games are a compelling source of annotate...   0.000000  0.000000   \n",
       "Physical viability in particular energy efficie...   0.000000  0.000000   \n",
       "Training directed neural networks typically req...   0.000000  0.000000   \n",
       "This work explores conditional image generation...   0.000000  0.000000   \n",
       "The move from hand designed features to learned...   0.000000  0.000000   \n",
       "Scalable and effective exploration remains a ke...   0.000000  0.000000   \n",
       "In this work we introduce a novel interpretatio...   0.000000  0.000000   \n",
       "We present an approach to training neural netwo...   0.000000  0.000000   \n",
       "We present a variety of new architectural featu...   0.000000  0.074444   \n",
       "Directly reading documents and being able to an...   0.087868  0.000000   \n",
       "This paper introduces Adaptive Computation Time...   0.000000  0.000000   \n",
       "Reinforcement learning offers a promising metho...   0.000000  0.000000   \n",
       "The ability to transfer knowledge gained in pre...   0.109579  0.106927   \n",
       "In a given scene humans can often easily predic...   0.000000  0.078752   \n",
       "Very deep convolutional networks with hundreds ...   0.000000  0.000000   \n",
       "\n",
       "                                                    computational     learn  \\\n",
       "  Both scientists and children make important s...       0.131838  0.124883   \n",
       "  We consider joint estimation of multiple grap...       0.065716  0.062250   \n",
       "  ReLU neural networks define piecewise linear ...       0.000000  0.000000   \n",
       "  Neural networks with rectified linear unit ac...       0.000000  0.000000   \n",
       "  We propose a new framework for manifold denoi...       0.000000  0.000000   \n",
       "  An associative memory is a framework of conte...       0.000000  0.044387   \n",
       "  Variational auto encoders VAE are scalable an...       0.068724  0.000000   \n",
       "  Increasing availability of vehicle GPS data h...       0.000000  0.000000   \n",
       "  We address the issue of speeding up the train...       0.000000  0.000000   \n",
       "  Phase synchronisation in multichannel EEG is ...       0.000000  0.000000   \n",
       "  In this paper we propose a new primal dual al...       0.000000  0.000000   \n",
       "  Co adaptation is a special form of on line le...       0.000000  0.000000   \n",
       "  Humans are remarkably adept at interpreting t...       0.173940  0.000000   \n",
       "  Plants sense their environment by producing e...       0.000000  0.000000   \n",
       "  We introduce a new large scale music dataset ...       0.000000  0.000000   \n",
       "  Demanding sparsity in estimated models has be...       0.113423  0.000000   \n",
       "  In this paper we study the efficiency of a bf...       0.000000  0.000000   \n",
       "  Adaptive schemes where tasks are assigned bas...       0.000000  0.000000   \n",
       "  A binary classifier capable of abstaining fro...       0.000000  0.000000   \n",
       "  In this paper we aim at recovering an undirec...       0.000000  0.000000   \n",
       "  A typical problem in causal modeling is the i...       0.000000  0.000000   \n",
       "  Gaussian processes GPs are flexible distribut...       0.053501  0.000000   \n",
       "  Modern discriminative predictors have been sh...       0.000000  0.071786   \n",
       "  Relational learning deals with data that are ...       0.073266  0.000000   \n",
       "  Sources of variability in experimentally deri...       0.000000  0.000000   \n",
       "  We provide a general framework for privacy pr...       0.000000  0.000000   \n",
       "  Various active learning methods based on logi...       0.103156  0.000000   \n",
       "  The introduction of data analytics into medic...       0.000000  0.000000   \n",
       "  This work investigates the case of a network ...       0.000000  0.045426   \n",
       "  Learning a natural language interface for dat...       0.000000  0.000000   \n",
       "...                                                           ...       ...   \n",
       "Learning to solve complex sequences of tasks  w...       0.000000  0.000000   \n",
       "We introduce a simple recurrent variational aut...       0.000000  0.000000   \n",
       "We discuss relations between Residual Networks ...       0.000000  0.000000   \n",
       "Inspired by the recent success of methods that ...       0.000000  0.000000   \n",
       "Residual networks family with hundreds or even ...       0.000000  0.000000   \n",
       "We propose local binary convolution LBC an effi...       0.048662  0.000000   \n",
       "We present a unified framework for learning con...       0.000000  0.000000   \n",
       "Recent advances in machine learning have dramat...       0.000000  0.064615   \n",
       "Kalman Filters are one of the most influential ...       0.000000  0.080890   \n",
       "Many tasks in AI require the collaboration of m...       0.000000  0.067338   \n",
       "Recurrent neural networks such as the GRU and L...       0.000000  0.000000   \n",
       "Deep neural networks DNNs have demonstrated sta...       0.068392  0.000000   \n",
       "We introduce SE Nets which are deep networks de...       0.000000  0.071145   \n",
       "We trained a convolutional neural network CNN t...       0.000000  0.000000   \n",
       "Despite recent breakthroughs in the application...       0.000000  0.068771   \n",
       "Video games are a compelling source of annotate...       0.000000  0.000000   \n",
       "Physical viability in particular energy efficie...       0.000000  0.000000   \n",
       "Training directed neural networks typically req...       0.000000  0.060177   \n",
       "This work explores conditional image generation...       0.070770  0.000000   \n",
       "The move from hand designed features to learned...       0.000000  0.090435   \n",
       "Scalable and effective exploration remains a ke...       0.000000  0.000000   \n",
       "In this work we introduce a novel interpretatio...       0.000000  0.000000   \n",
       "We present an approach to training neural netwo...       0.000000  0.000000   \n",
       "We present a variety of new architectural featu...       0.000000  0.131715   \n",
       "Directly reading documents and being able to an...       0.000000  0.000000   \n",
       "This paper introduces Adaptive Computation Time...       0.150243  0.071158   \n",
       "Reinforcement learning offers a promising metho...       0.000000  0.000000   \n",
       "The ability to transfer knowledge gained in pre...       0.000000  0.000000   \n",
       "In a given scene humans can often easily predic...       0.000000  0.000000   \n",
       "Very deep convolutional networks with hundreds ...       0.000000  0.000000   \n",
       "\n",
       "                                                    structure  supports  \\\n",
       "  Both scientists and children make important s...   0.119940  0.118867   \n",
       "  We consider joint estimation of multiple grap...   0.119572  0.000000   \n",
       "  ReLU neural networks define piecewise linear ...   0.000000  0.000000   \n",
       "  Neural networks with rectified linear unit ac...   0.000000  0.000000   \n",
       "  We propose a new framework for manifold denoi...   0.000000  0.000000   \n",
       "  An associative memory is a framework of conte...   0.000000  0.000000   \n",
       "  Variational auto encoders VAE are scalable an...   0.000000  0.000000   \n",
       "  Increasing availability of vehicle GPS data h...   0.000000  0.000000   \n",
       "  We address the issue of speeding up the train...   0.000000  0.000000   \n",
       "  Phase synchronisation in multichannel EEG is ...   0.000000  0.000000   \n",
       "  In this paper we propose a new primal dual al...   0.000000  0.000000   \n",
       "  Co adaptation is a special form of on line le...   0.000000  0.000000   \n",
       "  Humans are remarkably adept at interpreting t...   0.000000  0.000000   \n",
       "  Plants sense their environment by producing e...   0.000000  0.000000   \n",
       "  We introduce a new large scale music dataset ...   0.000000  0.000000   \n",
       "  Demanding sparsity in estimated models has be...   0.103187  0.000000   \n",
       "  In this paper we study the efficiency of a bf...   0.000000  0.000000   \n",
       "  Adaptive schemes where tasks are assigned bas...   0.000000  0.000000   \n",
       "  A binary classifier capable of abstaining fro...   0.000000  0.000000   \n",
       "  In this paper we aim at recovering an undirec...   0.000000  0.000000   \n",
       "  A typical problem in causal modeling is the i...   0.051902  0.000000   \n",
       "  Gaussian processes GPs are flexible distribut...   0.000000  0.000000   \n",
       "  Modern discriminative predictors have been sh...   0.000000  0.000000   \n",
       "  Relational learning deals with data that are ...   0.000000  0.000000   \n",
       "  Sources of variability in experimentally deri...   0.000000  0.000000   \n",
       "  We provide a general framework for privacy pr...   0.000000  0.000000   \n",
       "  Various active learning methods based on logi...   0.000000  0.000000   \n",
       "  The introduction of data analytics into medic...   0.000000  0.000000   \n",
       "  This work investigates the case of a network ...   0.000000  0.000000   \n",
       "  Learning a natural language interface for dat...   0.000000  0.000000   \n",
       "...                                                       ...       ...   \n",
       "Learning to solve complex sequences of tasks  w...   0.000000  0.000000   \n",
       "We introduce a simple recurrent variational aut...   0.000000  0.000000   \n",
       "We discuss relations between Residual Networks ...   0.000000  0.000000   \n",
       "Inspired by the recent success of methods that ...   0.000000  0.000000   \n",
       "Residual networks family with hundreds or even ...   0.000000  0.000000   \n",
       "We propose local binary convolution LBC an effi...   0.000000  0.000000   \n",
       "We present a unified framework for learning con...   0.000000  0.138014   \n",
       "Recent advances in machine learning have dramat...   0.000000  0.000000   \n",
       "Kalman Filters are one of the most influential ...   0.077689  0.000000   \n",
       "Many tasks in AI require the collaboration of m...   0.000000  0.000000   \n",
       "Recurrent neural networks such as the GRU and L...   0.000000  0.000000   \n",
       "Deep neural networks DNNs have demonstrated sta...   0.000000  0.000000   \n",
       "We introduce SE Nets which are deep networks de...   0.068329  0.000000   \n",
       "We trained a convolutional neural network CNN t...   0.000000  0.000000   \n",
       "Despite recent breakthroughs in the application...   0.000000  0.000000   \n",
       "Video games are a compelling source of annotate...   0.000000  0.000000   \n",
       "Physical viability in particular energy efficie...   0.000000  0.000000   \n",
       "Training directed neural networks typically req...   0.000000  0.000000   \n",
       "This work explores conditional image generation...   0.000000  0.000000   \n",
       "The move from hand designed features to learned...   0.173711  0.000000   \n",
       "Scalable and effective exploration remains a ke...   0.000000  0.000000   \n",
       "In this work we introduce a novel interpretatio...   0.000000  0.000000   \n",
       "We present an approach to training neural netwo...   0.000000  0.000000   \n",
       "We present a variety of new architectural featu...   0.000000  0.000000   \n",
       "Directly reading documents and being able to an...   0.000000  0.000000   \n",
       "This paper introduces Adaptive Computation Time...   0.068342  0.000000   \n",
       "Reinforcement learning offers a promising metho...   0.000000  0.000000   \n",
       "The ability to transfer knowledge gained in pre...   0.000000  0.000000   \n",
       "In a given scene humans can often easily predic...   0.000000  0.000000   \n",
       "Very deep convolutional networks with hundreds ...   0.000000  0.000000   \n",
       "\n",
       "                                                     assumes  discovered  \\\n",
       "  Both scientists and children make important s...  0.117513    0.116246   \n",
       "  We consider joint estimation of multiple grap...  0.000000    0.000000   \n",
       "  ReLU neural networks define piecewise linear ...  0.000000    0.000000   \n",
       "  Neural networks with rectified linear unit ac...  0.000000    0.000000   \n",
       "  We propose a new framework for manifold denoi...  0.000000    0.000000   \n",
       "  An associative memory is a framework of conte...  0.000000    0.000000   \n",
       "  Variational auto encoders VAE are scalable an...  0.000000    0.000000   \n",
       "  Increasing availability of vehicle GPS data h...  0.000000    0.000000   \n",
       "  We address the issue of speeding up the train...  0.000000    0.000000   \n",
       "  Phase synchronisation in multichannel EEG is ...  0.000000    0.000000   \n",
       "  In this paper we propose a new primal dual al...  0.000000    0.000000   \n",
       "  Co adaptation is a special form of on line le...  0.000000    0.000000   \n",
       "  Humans are remarkably adept at interpreting t...  0.000000    0.000000   \n",
       "  Plants sense their environment by producing e...  0.000000    0.000000   \n",
       "  We introduce a new large scale music dataset ...  0.000000    0.000000   \n",
       "  Demanding sparsity in estimated models has be...  0.000000    0.000000   \n",
       "  In this paper we study the efficiency of a bf...  0.000000    0.000000   \n",
       "  Adaptive schemes where tasks are assigned bas...  0.000000    0.000000   \n",
       "  A binary classifier capable of abstaining fro...  0.000000    0.000000   \n",
       "  In this paper we aim at recovering an undirec...  0.000000    0.000000   \n",
       "  A typical problem in causal modeling is the i...  0.000000    0.000000   \n",
       "  Gaussian processes GPs are flexible distribut...  0.000000    0.000000   \n",
       "  Modern discriminative predictors have been sh...  0.000000    0.000000   \n",
       "  Relational learning deals with data that are ...  0.000000    0.000000   \n",
       "  Sources of variability in experimentally deri...  0.000000    0.000000   \n",
       "  We provide a general framework for privacy pr...  0.000000    0.000000   \n",
       "  Various active learning methods based on logi...  0.000000    0.000000   \n",
       "  The introduction of data analytics into medic...  0.000000    0.000000   \n",
       "  This work investigates the case of a network ...  0.000000    0.000000   \n",
       "  Learning a natural language interface for dat...  0.000000    0.000000   \n",
       "...                                                      ...         ...   \n",
       "Learning to solve complex sequences of tasks  w...  0.000000    0.000000   \n",
       "We introduce a simple recurrent variational aut...  0.000000    0.000000   \n",
       "We discuss relations between Residual Networks ...  0.000000    0.000000   \n",
       "Inspired by the recent success of methods that ...  0.000000    0.000000   \n",
       "Residual networks family with hundreds or even ...  0.000000    0.000000   \n",
       "We propose local binary convolution LBC an effi...  0.000000    0.000000   \n",
       "We present a unified framework for learning con...  0.000000    0.000000   \n",
       "Recent advances in machine learning have dramat...  0.000000    0.000000   \n",
       "Kalman Filters are one of the most influential ...  0.000000    0.000000   \n",
       "Many tasks in AI require the collaboration of m...  0.000000    0.000000   \n",
       "Recurrent neural networks such as the GRU and L...  0.000000    0.000000   \n",
       "Deep neural networks DNNs have demonstrated sta...  0.000000    0.000000   \n",
       "We introduce SE Nets which are deep networks de...  0.000000    0.000000   \n",
       "We trained a convolutional neural network CNN t...  0.000000    0.000000   \n",
       "Despite recent breakthroughs in the application...  0.000000    0.000000   \n",
       "Video games are a compelling source of annotate...  0.000000    0.000000   \n",
       "Physical viability in particular energy efficie...  0.000000    0.000000   \n",
       "Training directed neural networks typically req...  0.000000    0.000000   \n",
       "This work explores conditional image generation...  0.000000    0.000000   \n",
       "The move from hand designed features to learned...  0.000000    0.000000   \n",
       "Scalable and effective exploration remains a ke...  0.000000    0.000000   \n",
       "In this work we introduce a novel interpretatio...  0.000000    0.000000   \n",
       "We present an approach to training neural netwo...  0.000000    0.000000   \n",
       "We present a variety of new architectural featu...  0.000000    0.000000   \n",
       "Directly reading documents and being able to an...  0.000000    0.000000   \n",
       "This paper introduces Adaptive Computation Time...  0.000000    0.000000   \n",
       "Reinforcement learning offers a promising metho...  0.000000    0.000000   \n",
       "The ability to transfer knowledge gained in pre...  0.000000    0.000000   \n",
       "In a given scene humans can often easily predic...  0.000000    0.000000   \n",
       "Very deep convolutional networks with hundreds ...  0.000000    0.000000   \n",
       "\n",
       "                                                       right  understood  \\\n",
       "  Both scientists and children make important s...  0.115057    0.115057   \n",
       "  We consider joint estimation of multiple grap...  0.000000    0.000000   \n",
       "  ReLU neural networks define piecewise linear ...  0.000000    0.000000   \n",
       "  Neural networks with rectified linear unit ac...  0.000000    0.000000   \n",
       "  We propose a new framework for manifold denoi...  0.000000    0.000000   \n",
       "  An associative memory is a framework of conte...  0.000000    0.000000   \n",
       "  Variational auto encoders VAE are scalable an...  0.000000    0.000000   \n",
       "  Increasing availability of vehicle GPS data h...  0.000000    0.000000   \n",
       "  We address the issue of speeding up the train...  0.000000    0.000000   \n",
       "  Phase synchronisation in multichannel EEG is ...  0.000000    0.000000   \n",
       "  In this paper we propose a new primal dual al...  0.000000    0.000000   \n",
       "  Co adaptation is a special form of on line le...  0.000000    0.000000   \n",
       "  Humans are remarkably adept at interpreting t...  0.000000    0.000000   \n",
       "  Plants sense their environment by producing e...  0.000000    0.000000   \n",
       "  We introduce a new large scale music dataset ...  0.000000    0.000000   \n",
       "  Demanding sparsity in estimated models has be...  0.000000    0.000000   \n",
       "  In this paper we study the efficiency of a bf...  0.000000    0.000000   \n",
       "  Adaptive schemes where tasks are assigned bas...  0.000000    0.000000   \n",
       "  A binary classifier capable of abstaining fro...  0.000000    0.000000   \n",
       "  In this paper we aim at recovering an undirec...  0.000000    0.000000   \n",
       "  A typical problem in causal modeling is the i...  0.000000    0.000000   \n",
       "  Gaussian processes GPs are flexible distribut...  0.000000    0.000000   \n",
       "  Modern discriminative predictors have been sh...  0.000000    0.000000   \n",
       "  Relational learning deals with data that are ...  0.000000    0.000000   \n",
       "  Sources of variability in experimentally deri...  0.000000    0.000000   \n",
       "  We provide a general framework for privacy pr...  0.000000    0.000000   \n",
       "  Various active learning methods based on logi...  0.000000    0.000000   \n",
       "  The introduction of data analytics into medic...  0.000000    0.000000   \n",
       "  This work investigates the case of a network ...  0.000000    0.000000   \n",
       "  Learning a natural language interface for dat...  0.000000    0.000000   \n",
       "...                                                      ...         ...   \n",
       "Learning to solve complex sequences of tasks  w...  0.000000    0.000000   \n",
       "We introduce a simple recurrent variational aut...  0.000000    0.000000   \n",
       "We discuss relations between Residual Networks ...  0.000000    0.000000   \n",
       "Inspired by the recent success of methods that ...  0.000000    0.000000   \n",
       "Residual networks family with hundreds or even ...  0.000000    0.000000   \n",
       "We propose local binary convolution LBC an effi...  0.000000    0.000000   \n",
       "We present a unified framework for learning con...  0.000000    0.000000   \n",
       "Recent advances in machine learning have dramat...  0.000000    0.000000   \n",
       "Kalman Filters are one of the most influential ...  0.000000    0.000000   \n",
       "Many tasks in AI require the collaboration of m...  0.000000    0.000000   \n",
       "Recurrent neural networks such as the GRU and L...  0.000000    0.000000   \n",
       "Deep neural networks DNNs have demonstrated sta...  0.000000    0.000000   \n",
       "We introduce SE Nets which are deep networks de...  0.000000    0.000000   \n",
       "We trained a convolutional neural network CNN t...  0.000000    0.000000   \n",
       "Despite recent breakthroughs in the application...  0.000000    0.000000   \n",
       "Video games are a compelling source of annotate...  0.000000    0.000000   \n",
       "Physical viability in particular energy efficie...  0.000000    0.000000   \n",
       "Training directed neural networks typically req...  0.000000    0.000000   \n",
       "This work explores conditional image generation...  0.000000    0.000000   \n",
       "The move from hand designed features to learned...  0.000000    0.000000   \n",
       "Scalable and effective exploration remains a ke...  0.000000    0.000000   \n",
       "In this work we introduce a novel interpretatio...  0.000000    0.000000   \n",
       "We present an approach to training neural netwo...  0.000000    0.000000   \n",
       "We present a variety of new architectural featu...  0.000000    0.000000   \n",
       "Directly reading documents and being able to an...  0.000000    0.000000   \n",
       "This paper introduces Adaptive Computation Time...  0.000000    0.000000   \n",
       "Reinforcement learning offers a promising metho...  0.000000    0.000000   \n",
       "The ability to transfer knowledge gained in pre...  0.000000    0.000000   \n",
       "In a given scene humans can often easily predic...  0.000000    0.000000   \n",
       "Very deep convolutional networks with hundreds ...  0.000000    0.000000   \n",
       "\n",
       "                                                    consequence  connectivity  \n",
       "  Both scientists and children make important s...     0.114302      0.111868  \n",
       "  We consider joint estimation of multiple grap...     0.000000      0.000000  \n",
       "  ReLU neural networks define piecewise linear ...     0.000000      0.000000  \n",
       "  Neural networks with rectified linear unit ac...     0.000000      0.000000  \n",
       "  We propose a new framework for manifold denoi...     0.000000      0.100008  \n",
       "  An associative memory is a framework of conte...     0.000000      0.000000  \n",
       "  Variational auto encoders VAE are scalable an...     0.000000      0.000000  \n",
       "  Increasing availability of vehicle GPS data h...     0.000000      0.000000  \n",
       "  We address the issue of speeding up the train...     0.000000      0.000000  \n",
       "  Phase synchronisation in multichannel EEG is ...     0.000000      0.080201  \n",
       "  In this paper we propose a new primal dual al...     0.000000      0.000000  \n",
       "  Co adaptation is a special form of on line le...     0.000000      0.000000  \n",
       "  Humans are remarkably adept at interpreting t...     0.000000      0.000000  \n",
       "  Plants sense their environment by producing e...     0.000000      0.000000  \n",
       "  We introduce a new large scale music dataset ...     0.000000      0.000000  \n",
       "  Demanding sparsity in estimated models has be...     0.000000      0.000000  \n",
       "  In this paper we study the efficiency of a bf...     0.000000      0.000000  \n",
       "  Adaptive schemes where tasks are assigned bas...     0.000000      0.000000  \n",
       "  A binary classifier capable of abstaining fro...     0.000000      0.000000  \n",
       "  In this paper we aim at recovering an undirec...     0.000000      0.000000  \n",
       "  A typical problem in causal modeling is the i...     0.000000      0.000000  \n",
       "  Gaussian processes GPs are flexible distribut...     0.000000      0.000000  \n",
       "  Modern discriminative predictors have been sh...     0.000000      0.000000  \n",
       "  Relational learning deals with data that are ...     0.000000      0.000000  \n",
       "  Sources of variability in experimentally deri...     0.000000      0.000000  \n",
       "  We provide a general framework for privacy pr...     0.000000      0.000000  \n",
       "  Various active learning methods based on logi...     0.000000      0.000000  \n",
       "  The introduction of data analytics into medic...     0.000000      0.000000  \n",
       "  This work investigates the case of a network ...     0.000000      0.000000  \n",
       "  Learning a natural language interface for dat...     0.000000      0.000000  \n",
       "...                                                         ...           ...  \n",
       "Learning to solve complex sequences of tasks  w...     0.000000      0.000000  \n",
       "We introduce a simple recurrent variational aut...     0.000000      0.000000  \n",
       "We discuss relations between Residual Networks ...     0.000000      0.000000  \n",
       "Inspired by the recent success of methods that ...     0.000000      0.000000  \n",
       "Residual networks family with hundreds or even ...     0.000000      0.000000  \n",
       "We propose local binary convolution LBC an effi...     0.000000      0.000000  \n",
       "We present a unified framework for learning con...     0.000000      0.000000  \n",
       "Recent advances in machine learning have dramat...     0.000000      0.000000  \n",
       "Kalman Filters are one of the most influential ...     0.000000      0.000000  \n",
       "Many tasks in AI require the collaboration of m...     0.000000      0.000000  \n",
       "Recurrent neural networks such as the GRU and L...     0.000000      0.000000  \n",
       "Deep neural networks DNNs have demonstrated sta...     0.000000      0.000000  \n",
       "We introduce SE Nets which are deep networks de...     0.000000      0.000000  \n",
       "We trained a convolutional neural network CNN t...     0.000000      0.000000  \n",
       "Despite recent breakthroughs in the application...     0.000000      0.000000  \n",
       "Video games are a compelling source of annotate...     0.000000      0.000000  \n",
       "Physical viability in particular energy efficie...     0.000000      0.000000  \n",
       "Training directed neural networks typically req...     0.000000      0.000000  \n",
       "This work explores conditional image generation...     0.000000      0.000000  \n",
       "The move from hand designed features to learned...     0.000000      0.000000  \n",
       "Scalable and effective exploration remains a ke...     0.000000      0.000000  \n",
       "In this work we introduce a novel interpretatio...     0.000000      0.000000  \n",
       "We present an approach to training neural netwo...     0.000000      0.000000  \n",
       "We present a variety of new architectural featu...     0.000000      0.000000  \n",
       "Directly reading documents and being able to an...     0.000000      0.000000  \n",
       "This paper introduces Adaptive Computation Time...     0.000000      0.000000  \n",
       "Reinforcement learning offers a promising metho...     0.000000      0.000000  \n",
       "The ability to transfer knowledge gained in pre...     0.000000      0.000000  \n",
       "In a given scene humans can often easily predic...     0.000000      0.000000  \n",
       "Very deep convolutional networks with hundreds ...     0.000000      0.000000  \n",
       "\n",
       "[6605 rows x 20 columns]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.transpose().sort_values(result_abs[0], ascending=False).head(20).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
