votes,title,url,abstract,submitted,link,rank,top_comment
19,[R][1611.01491] Understanding Deep Neural Networks with Rectified Linear Units,https://www.reddit.com/r/MachineLearning/comments/5bmpkw/r161101491_understanding_deep_neural_networks/," Abstract: In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give the first-ever polynomial time (in the size of data) algorithm to train a ReLU DNN with one hidden layer to global optimality. This follows from our complete characterization of the ReLU DNN function class whereby we show that a $\mathbb{R}^n \to \mathbb{R}$ function is representable by a ReLU DNN if and only if it is a continuous piecewise linear function. The main tool used to prove this characterization is an elegant result from tropical geometry. Further, for the $n=1$ case, we show that a single hidden layer suffices to express all piecewise linear functions, and we give tight bounds for the size of such a ReLU DNN.We follow up with gap results showing that there is a smoothly parameterized family of $\mathbb{R}\to \mathbb{R}$ ""hard"" functions that lead to an exponential blow-up in size, if the number of layers is decreased by a small amount. An example consequence of our gap theorem is that for every natural number $N$, there exists a function representable by a ReLU DNN with depth $N^2+1$ and total size $N^3$, such that any ReLU DNN with depth at most $N + 1$ will require at least $\frac12N^{N+1}-1$ total nodes. Finally, we construct a family of $\mathbb{R}^n\to \mathbb{R}$ functions for $n\geq 2$ (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension $n$ at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on $n$ has not been achieved by previous families of ""hard"" functions in the neural nets literature. ",Mon Nov 7 15:23:23 2016 UTC,https://arxiv.org/abs/1611.01491,13,This is a really nice theoretical paper. The two main contributions are :  The equivalence between functions learned by multilayers ReLU networks and piecewise linear functions A new algorithm to train 2 layers relu nets based on this observation  Also it's the first paper I ever saw in DL that uses tropical geometry 
15,[Research] Improving Stochastic Gradient Descent with Feedback,https://www.reddit.com/r/MachineLearning/comments/5bsmvn/research_improving_stochastic_gradient_descent/," Abstract: In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process. ",Tue Nov 8 11:14:12 2016 UTC,http://arxiv.org/abs/1611.01505,3,Somone already implements .https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/Eve 
21,[R] Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences,https://www.reddit.com/r/MachineLearning/comments/5bmfw5/r_phased_lstm_accelerating_recurrent_network/," Abstract: Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime. ",Mon Nov 7 14:33:08 2016 UTC,http://arxiv.org/abs/1610.09513,15,"Looks compelling but why wouldn't they also include a penn tree bank or Hutter data set sequence prediction benchmark? Seems like most of the other ""LSTM improvement"" papers do, and would be nice to see a head-to-head comparison with the ByteNet decoder and the new LSTM substitute block from the Neural Architecture Search with Reinforcement Learning paper. "
22,[R]Entropy-SGD: Biasing Gradient Descent Into Wide Valleys,https://www.reddit.com/r/MachineLearning/comments/5bqzu2/rentropysgd_biasing_gradient_descent_into_wide/," Abstract: This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape at solutions found by gradient descent. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local entropy based objective that favors well-generalizable solutions lying in the flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Our algorithm resembles two nested loops of SGD, where we use Langevin dynamics to compute the gradient of local entropy at each update of the weights. We prove that incorporating local entropy into the objective function results in a smoother energy landscape and use uniform stability to show improved generalization bounds over SGD. Our experiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training. ",Tue Nov 8 03:40:52 2016 UTC,http://arxiv.org/abs/1611.01838,6,Uhh.... sounds like they re-invented Flat Minimum Search.  
47,[R][1611.01146] Finding Local Minima for Nonconvex Optimization in Linear Time,https://www.reddit.com/r/MachineLearning/comments/5b0rl3/r161101146_finding_local_minima_for_nonconvex/, Abstract: We design a non-convex second-order optimization algorithm that is guaranteed to return an approximate local minimum in time which is linear in the input representation. The time complexity of our algorithm to find an approximate local minimum is even faster than that of gradient descent to find a critical point. Our algorithm applies to a general class of optimization problems including training a neural network and other non-convex objectives arising in machine learning. ,Fri Nov 4 01:12:07 2016 UTC,https://arxiv.org/abs/1611.01146,53,
34,[R] [1610.09615] Compressed Learning: A Deep Neural Network Approach,https://www.reddit.com/r/MachineLearning/comments/5ar80g/r_161009615_compressed_learning_a_deep_neural/," Abstract: Compressed Learning (CL) is a joint signal processing and machine learning framework for inference from a signal, using a small number of measurements obtained by linear projections of the signal. In this paper we present an end-to-end deep learning approach for CL, in which a network composed of fully-connected layers followed by convolutional layers perform the linear sensing and non-linear inference stages. During the training phase, the sensing matrix and the non-linear inference operator are jointly optimized, and the proposed approach outperforms state-of-the-art for the task of image classification. For example, at a sensing rate of 1% (only 8 measurements of 28 X 28 pixels images), the classification error for the MNIST handwritten digits dataset is 6.46% compared to 41.06% with state-of-the-art. ",Wed Nov 2 17:23:21 2016 UTC,https://arxiv.org/abs/1610.09615,74,"I can't really tell if the paper is badly written or not. It is definitely short and I even thought to the point. The really lost me at the selection off he architecture for the net. I mean the first layer was supposed to convert the measurements into the sparse realm... And after that there are about 12 layers behind with almost no explanation of why. Idk if they are standard ""lenets"" but if they are not the authors should have spent a couple of paragraphs explaining the architecture (IMO) "
16,[R] [1610.02995] Extrapolation and learning equations,https://www.reddit.com/r/MachineLearning/comments/5av8hh/r_161002995_extrapolation_and_learning_equations/," Abstract: In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified. ",Thu Nov 3 06:19:17 2016 UTC,https://arxiv.org/abs/1610.02995,72,"Is there a way to regress a sine wave of a particular frequency latent in the data? Can this do that? For example: the data is sin(3x) + noise, how would I regress this data with some parametric differentiable function? "
40,[R][1611.00712] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables,https://www.reddit.com/r/MachineLearning/comments/5aubhn/r161100712_the_concrete_distribution_a_continuous/," Abstract: The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack continuous reparameterizations due to the discontinuous nature of discrete states. In this work we introduce concrete random variables -- continuous relaxations of discrete random variables. The concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate effectiveness of concrete relaxations on density estimation and structured prediction tasks using neural networks. ",Thu Nov 3 02:37:02 2016 UTC,https://arxiv.org/abs/1611.00712,70,"This is a beautiful result. Here’s the crux: Sampling a concrete random variable is as easy as taking the softmax of the noisy logits (log α_k +G_k)/λ. Here, the latent (learned) variable is α_k, k is the dimensionality, λ is an empirically-chosen hyperparameter that varies from continous to discrete, and G_k is a random number drawn from the Gumbel distribution (which is also easy to make: log(-log(Uniform[0, 1]))).  Great stuff! Allows for backprop through (kind of) discrete variables. "
18,[R] [1606.02492v3] Convolutional Neural Fabrics,https://www.reddit.com/r/MachineLearning/comments/5avy16/r_160602492v3_convolutional_neural_fabrics/," Abstract: Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a ""fabric"" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset. ",Thu Nov 3 10:22:25 2016 UTC,https://arxiv.org/abs/1606.02492v3,67,the idea is interesting but the experiment results seem rather underwhelming 
4,[R][1611.01142] Using a Deep Reinforcement Learning Agent for Traffic Signal Control,https://www.reddit.com/r/MachineLearning/comments/5b31z9/r161101142_using_a_deep_reinforcement_learning/," Abstract: Ensuring transportation systems are efficient is a priority for modern society. Technological advances have made it possible for transportation systems to collect large volumes of varied data on an unprecedented scale. We propose a traffic signal control system which takes advantage of this new, high quality data, with minimal abstraction compared to other proposed systems. We apply modern deep reinforcement learning methods to build a truly adaptive traffic signal control agent in the traffic microsimulator SUMO. We propose a new state space, the discrete traffic state encoding, which is information dense. The discrete traffic state encoding is used as input to a deep convolutional neural network, trained using Q-learning with experience replay. Our agent was compared against a one hidden layer neural network traffic signal control agent and reduces average cumulative delay by 82%, average queue length by 66% and average travel time by 20%. ",Fri Nov 4 11:40:20 2016 UTC,https://arxiv.org/abs/1611.01142,56,Video 
23,[R] [1611.01144] Categorical Reparameterization with Gumbel-Softmax,https://www.reddit.com/r/MachineLearning/comments/5b1cwf/r_161101144_categorical_reparameterization_with/," Abstract: Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification. ",Fri Nov 4 03:15:56 2016 UTC,https://arxiv.org/abs/1611.01144,54,"I'm one of the authors on this paper. I'd be happy to answer any questions! Coincidentally, Deepmind just published a paper on pretty much the same idea. https://arxiv.org/abs/1611.00712 Congratulations to Chris Maddison and co. as well. "
55,[R][1610.09027] Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes [DeepMind],https://www.reddit.com/r/MachineLearning/comments/5abcd4/r161009027_scaling_memoryaugmented_neural/," Abstract: Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows --- limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs $1,\!000\times$ faster and with $3,\!000\times$ less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring $100,\!000$s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer. ",Mon Oct 31 09:32:45 2016 UTC,https://arxiv.org/abs/1610.09027,97,
19,[Research] [1610.09585] Conditional Image Synthesis With Auxiliary Classifier GANs,https://www.reddit.com/r/MachineLearning/comments/5ahi35/research_161009585_conditional_image_synthesis/," Abstract: Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data. ",Tue Nov 1 05:36:30 2016 UTC,https://arxiv.org/abs/1610.09585,91,"From the team that brought you the Deconv Checkerboard Artifacts article.  This is an idea which I (and I wouldn't be surprised if many others) have thought of before, but never thought it would actually improve results--I'm glad that these guys pursued it and used it to good effect. Ideas are cheap! I'm impressed, but not sure that I'm entirely sold: browsing through the full set of samples, it seems that it doesn't work any better than any other Imagenet GAN for a pretty large majority of classes. This looks more representative of most of the classes, but the fact that they're getting global coherence on some of the classes (it seems to like hotdogs/vegetables and flowers) suggests that this is a worthwhile track to pursue. I'll be throwing a celebA experiment in the blender after the ICLR deadline, keen to see how it turns out with facial attribute labels. "
72,[Research] [1610.10099] Neural Machine Translation in Linear Time,https://www.reddit.com/r/MachineLearning/comments/5agopr/research_161010099_neural_machine_translation_in/," Abstract: We present a neural architecture for sequence processing. The ByteNet is a stack of two dilated convolutional neural networks, one to encode the source sequence and one to decode the target sequence, where the target network unfolds dynamically to generate variable length outputs. The ByteNet has two core properties: it runs in time that is linear in the length of the sequences and it preserves the sequences' temporal resolution. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent neural networks. The ByteNet also achieves a performance on raw character-level machine translation that approaches that of the best neural translation models that run in quadratic time. The implicit structure learnt by the ByteNet mirrors the expected alignments between the sequences. ",Tue Nov 1 02:20:59 2016 UTC,https://arxiv.org/abs/1610.10099,89,Is this a fair characterization?  PixelRNN: dilated convolutions applied to sequential prediction of 2-dimensional data WaveNet: dilated convolutions applied to sequential prediction of 1-dimensional data ByteNet: dilated convolutions applied to seq2seq predictions of 1-dimensional data  Pretty amazing set of results from a pretty robust core insight...! What's next? Video frame prediction as dilated convolutions on 3-dimensional data? (they did that too!) 
9,[R][1611.00328] The Chi-Divergence for Approximate Inference,https://www.reddit.com/r/MachineLearning/comments/5an91t/r161100328_the_chidivergence_for_approximate/," Abstract: Variational inference enables Bayesian analysis for complex probabilistic models with massive data sets. It works by positing a family of distributions and finding the member in the family that is closest to the posterior. While successful, variational methods can run into pathologies; for example, they typically underestimate posterior uncertainty. We propose CHI-VI, a complementary algorithm to traditional variational inference with KL($q$ || $p$) and an alternative algorithm to EP. CHI-VI is a black box algorithm that minimizes the $\chi$-divergence from the posterior to the family of approximating distributions. In EP, only local minimization of the KL($p$ || $q$) objective is possible. In contrast, CHI-VI optimizes a well-defined global objective. It directly minimizes an upper bound to the model evidence that equivalently minimizes the $\chi$-divergence. In experiments, we illustrate the utility of the upper bound for sandwich estimating the model evidence. We also compare several probabilistic models and a Cox process for basketball data. We find CHI-VI often yields better classification error rates and better posterior uncertainty. ",Wed Nov 2 01:44:01 2016 UTC,https://arxiv.org/abs/1611.00328,86,"This paper has interesting exposition on the Chi Divergence, but ultimately isn't it doing the same thing as the recent Renyi Divergence paper?  Just with the Renyi alpha parameter set to -1? "
18,[R] [1611.00020] Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision,https://www.reddit.com/r/MachineLearning/comments/5aoscn/r_161100020_neural_symbolic_machines_learning/," Abstract: Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural ""programmer"", and a non-differentiable ""computer"" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge. ",Wed Nov 2 08:28:42 2016 UTC,https://arxiv.org/abs/1611.00020,80,Might want to fix some of those references 
23,[Research] [1610.09033] Operator Variational Inference,https://www.reddit.com/r/MachineLearning/comments/5ab2zg/research_161009033_operator_variational_inference/," Abstract: Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images. ",Mon Oct 31 07:52:46 2016 UTC,https://arxiv.org/abs/1610.09033,101,
3,[R] [1610.08613] Can Active Memory Replace Attention?,https://www.reddit.com/r/MachineLearning/comments/59stao/r_161008613_can_active_memory_replace_attention/," Abstract: Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice. ",Fri Oct 28 05:16:08 2016 UTC,https://arxiv.org/abs/1610.08613,125,"While I like the idea in this paper, I don't really get the naming scheme the authors use. Neural GPUs? Active Memory?  Just call it a convolutional memory. "
9,[Research] Learning to Reason With Adaptive Computation,https://www.reddit.com/r/MachineLearning/comments/59sfz8/research_learning_to_reason_with_adaptive/," Abstract: Multi-hop inference is necessary for machine learning systems to successfully solve tasks such as Recognising Textual Entailment and Machine Reading. In this work, we demonstrate the effectiveness of adaptive computation for learning the number of inference steps required for examples of different complexity and that learning the correct number of inference steps is difficult. We introduce the first model involving Adaptive Computation Time which provides a small performance benefit on top of a similar model without an adaptive component as well as enabling considerable insight into the reasoning process of the model. ",Fri Oct 28 03:40:22 2016 UTC,https://arxiv.org/abs/1610.07647,124,"/u/MarkusDeNeutoy (and anyone else who wants to chime in), how does your adaptive attention based on ACT (that can learn to halt) compare with the REINFORCE based adaptive attention used in Reasonet [https://arxiv.org/pdf/1609.05284.pdf], the adaptive attention used in Neural Semantic Encoders w/ adaptive computation [https://arxiv.org/pdf/1610.06454.pdf] cc:/u/tsendsuren, and the stop action used in WebNav [https://arxiv.org/pdf/1602.02261.pdf] I've been trying to figure out which adaptive attention mechanisms (that can learn to stop, forget, etc.) (such as ACT, REINFORCE-trained stopping gates, and simple stop action vectors from WebNav that can be selected instead of a next memory to attend to) work best for different use cases. "
32,[R] [1610.06918] Learning to Protect Communications with Adversarial Neural Cryptography,https://www.reddit.com/r/MachineLearning/comments/59v9ua/r_161006918_learning_to_protect_communications/," Abstract: We ask whether neural networks can learn to use secret keys to protect information from other neural networks. Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary. Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks; instead, we train end-to-end, adversarially. We demonstrate that the neural networks can learn how to perform forms of encryption and decryption, and also how to apply these operations selectively in order to meet confidentiality goals. ",Fri Oct 28 16:12:57 2016 UTC,https://arxiv.org/abs/1610.06918,116,"I had a go at implementing the model in this paper. I only read the comments in this thread after I had done so. In general I agree that it probably isn't that useful an experiment in real world terms, but still it was a good learning experience for me which I gained a lot from. "
5,[R] [NIPS:ArXiv:1608.04042] Can Peripheral Representations Improve Clutter Metrics on Complex Scenes?,https://www.reddit.com/r/MachineLearning/comments/5a3a5o/r_nipsarxiv160804042_can_peripheral/," Abstract: Previous studies have proposed image-based clutter measures that correlate with human search times and/or eye movements. However, most models do not take into account the fact that the effects of clutter interact with the foveated nature of the human visual system: visual clutter further from the fovea has an increasing detrimental influence on perception. Here, we introduce a new foveated clutter model to predict the detrimental effects in target search utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz et al.) as our non foveated clutter model, and we stack a peripheral architecture on top of Feature Congestion for our foveated model. We introduce the Peripheral Integration Feature Congestion (PIFC) coefficient, as a fundamental ingredient of our model that modulates clutter as a non-linear gain contingent on eccentricity. We finally show that Foveated Feature Congestion (FFC) clutter scores r(44) = -0.82 correlate better with target detection (hit rate) than regular Feature Congestion r(44) = -0.19 in forced fixation search. Thus, our model allows us to enrich clutter perception research by computing fixation specific clutter maps. A toolbox for creating peripheral architectures: Piranhas: Peripheral Architectures for Natural, Hybrid and Artificial Systems will be made available. ",Sat Oct 29 23:25:32 2016 UTC,https://arxiv.org/pdf/1608.04042v1.pdf,111,0
4,[R][arXiv:1610.06998] Ranking of classification algorithms in terms of mean-standard deviation using A-TOPSIS,https://www.reddit.com/r/MachineLearning/comments/5a6xv7/rarxiv161006998_ranking_of_classification/," Abstract: In classification problems when multiples algorithms are applied to different benchmarks a difficult issue arises, i.e., how can we rank the algorithms? In machine learning it is common run the algorithms several times and then a statistic is calculated in terms of means and standard deviations. In order to compare the performance of the algorithms, it is very common to employ statistical tests. However, these tests may also present limitations, since they consider only the means and not the standard deviations of the obtained results. In this paper, we present the so called A-TOPSIS, based on TOPSIS (Technique for Order Preference by Similarity to Ideal Solution), to solve the problem of ranking and comparing classification algorithms in terms of means and standard deviations. We use two case studies to illustrate the A-TOPSIS for ranking classification algorithms and the results show the suitability of A-TOPSIS to rank the algorithms. The presented approach is general and can be applied to compare the performance of stochastic algorithms in machine learning. Finally, to encourage researchers to use the A-TOPSIS for ranking algorithms we also presented in this work an easy-to-use A-TOPSIS web framework. ",Sun Oct 30 16:54:50 2016 UTC,https://arxiv.org/ftp/arxiv/papers/1610/1610.06998.pdf,108,"Hello guys, I and my advisor developed this framework for ranking classification algorithms. You can try it on www.inf.ufes.br/~agcpacheco/alg-ranking If you have some question about something feel free for do it. Thanks "
15,[R] [1610.09296] Improving Sampling from Generative Autoencoders with Markov Chains,https://www.reddit.com/r/MachineLearning/comments/5a9dmy/r_161009296_improving_sampling_from_generative/," Abstract: We focus on generative autoencoders, such as variational or adversarial autoencoders, which jointly learn a generative model alongside an inference model. Generative autoencoders are those which are trained to softly enforce a prior on the latent distribution learned by the inference model. However, the inference model may not always map inputs to latent samples that are consistent with the prior. We formulate a Markov chain Monte Carlo (MCMC) sampling process, equivalent to iteratively encoding and decoding, which allows us to sample from the learned latent distribution. Using this, we can improve the quality of samples drawn from the model, especially when the learned distribution is far from the prior. Using MCMC sampling, we are able to reveal previously unseen differences between generative autoencoders trained either with or without a denoising criterion. ",Mon Oct 31 00:34:23 2016 UTC,https://arxiv.org/abs/1610.09296,105,0
30,[R] [1609.04309] Efficient softmax approximation for GPUs (Facebook AI Research),https://www.reddit.com/r/MachineLearning/comments/59dme6/r_160904309_efficient_softmax_approximation_for/," Abstract: We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. ",Tue Oct 25 21:16:00 2016 UTC,https://arxiv.org/abs/1609.04309,137,"Abs:  We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. Torch code: https://github.com/facebookresearch/adaptive-softmax (I'm not affiliated) "
6,[R] [1610.07675] Surprisal-Driven Zoneout,https://www.reddit.com/r/MachineLearning/comments/59j7t4/r_161007675_surprisaldriven_zoneout/," Abstract: We propose a novel method of regularization for recurrent neural networks called suprisal-driven zoneout. In this method, states zoneout (maintain their previous value rather than updating), when the suprisal (discrepancy between the last state's prediction and target) is small. Thus regularization is adaptive and input-driven on a per-neuron basis. We demonstrate the effectiveness of this idea by achieving state-of-the-art bits per character of 1.31 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to the best known highly-engineered compression methods. ",Wed Oct 26 18:33:04 2016 UTC,https://arxiv.org/abs/1610.07675,135,"Hi, nice paper, I also like the effort you into producing strong baseline results for LSTM and other methods. Regarding the surprisal methodology, my understanding is that during test evaluation, to calculate the bpc on the test set, your model uses information about the prediction errors for each prediction on the test set, and then incorporate this information back into your model in order to make more accurate future predictions. Could we not also take a standard lstm, and also incorporate this prediction error to fine tune the model's weights during test evaluation?  This sort of online training is called Dynamic Evaluation and I believe recently obtained close to ~1.2bpc on this dataset already, using a multiplicative rnn model.  Shouldn't you compare your model to the dynamic evaluation framework instead, since it is also a method to take the test prediction error information (assuming we have that) to improve the model's prediction accuracy during inference? One of the concerns I have, after reading recent papers (hierarcal memory rnn), is that this method may not be directly comparable with other RNN baselines that do not have access to this error information from the test set that your model has access to.  I think the concerns are valid, as in many practical applications, we indeed do not have access to a test set to give us the error for each prediction character during inference. For instance, I can take an LSTM, train it on classical music midi files, and create a music generator with it, and then use it to generate a five minute long midi file from whatever statistical information it learns from the training set (without ever looking at a test set).  It seems for your approach, in order to generate samples of the quality in your results (1.32bpc?), you would require a teacher to tell your algorithm how accurate each note is immediately after it gets generated, so that is why I think it may not be comparable directly with methods that do not require such a real-time teacher. In most applications we may not even be able to get the error for each character generated. For the set of applications where we do have access to this real time prediction error (say, in financial markets prediction, or predicting user's clicks), I believe one can also apply the dynamic evaluation technique earlier and get much better results as well. However I do appreciate that it does something different than modifying weights, sgd style, from a thought level.  Is there any way to modify the approach so that it doesn't require the test error signal?  Perhaps there can be some opportunity there.  For instance, Ba and Hinton's recent fast weights paper attempt to indirectly modify weights using self-attention techniques rather than errors from the test set during inference. Just my two cents. "
37,[1610.07629] A Learned Representation For Artistic Style,https://www.reddit.com/r/MachineLearning/comments/59iiz9/161007629_a_learned_representation_for_artistic/," Abstract: The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style. ",Wed Oct 26 16:35:44 2016 UTC,https://arxiv.org/abs/1610.07629,133,"https://research.googleblog.com/2016/10/supercharging-style-transfer.html additional examples and a video are given in the blog post, looks like they will also release the TensorFlow source code, should be fun to play with, I hope we see this on mobile devices soon.  "
62,[Research] The small universal perturbation perturbation causes the image to be misclassified,https://www.reddit.com/r/MachineLearning/comments/59yug9/research_the_small_universal_perturbation/," Abstract: Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images. ",Sat Oct 29 04:12:19 2016 UTC,http://arxiv.org/abs/1610.08401,113,"What if these these ""universal perturbations"" are used in data augmentation while training the classifier? Wouldn't it make the classifier resistant against/invariant under such perturbations? "
2,[arXiv:1610.07448] A Framework for Parallel and Distributed Training of Neural Networks,https://www.reddit.com/r/MachineLearning/comments/59nkaa/arxiv161007448_a_framework_for_parallel_and/," Abstract: The aim of this paper is to develop a general framework for training neural networks (NNs) in a distributed environment, where training data is partitioned over a set of agents that communicate with each other through a sparse, possibly time-varying, connectivity pattern. In such distributed scenario, the training problem can be formulated as the (regularized) optimization of a non-convex social cost function, given by the sum of local (non-convex) costs, where each agent contributes with a single error term defined with respect to its local dataset. To devise a flexible and efficient solution, we customize a recently proposed framework for non-convex optimization over networks, which hinges on a (primal) convexification-decomposition technique to handle non-convexity, and a dynamic consensus procedure to diffuse information among the agents. Several typical choices for the training criterion (e.g., squared loss, cross entropy, etc.) and regularization (e.g., $\ell_2$ norm, sparsity inducing penalties, etc.) are included in the framework and explored along the paper. Convergence to a stationary solution of the social non-convex problem is guaranteed under mild assumptions. Additionally, we show a principled way allowing each agent to exploit a multi-core architecture (e.g., a local cloud) in order to parallelize its local optimization step, resulting in strategies that are both distributed (across the agents) and parallel (inside each agent) in nature. A comprehensive set of experimental results validate the proposed approach. ",Thu Oct 27 11:57:36 2016 UTC,https://arxiv.org/abs/1610.07448,131,0
19,[R] [1610.06402] A Growing Long-term Episodic & Semantic Memory,https://www.reddit.com/r/MachineLearning/comments/59obye/r_161006402_a_growing_longterm_episodic_semantic/," Abstract: The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this bounds the total amount of knowledge that can be learned and stored. Though this is not normally a problem for a neural network designed for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. To address this, we describe a lifelong learning system that leverages a fast, though non-differentiable, content-addressable memory which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge over many episodes for an unbounded number of domains. This opens the door for investigation into transfer learning, and leveraging prior knowledge that has been learned over a lifetime of experiences to new domains. ",Thu Oct 27 14:42:56 2016 UTC,https://arxiv.org/abs/1610.06402,127,"I'm having troubles figuring out what they did. Section 3 is supposed to explain their method, but it is a single page without any equation or diagram, and I find the prose unclear. Can somebody please explain it to me like I'm five? "
15,[Research] [1610.08123] Socratic Learning,https://www.reddit.com/r/MachineLearning/comments/59oxwn/research_161008123_socratic_learning/," Abstract: Modern machine learning techniques, such as deep learning, often use discriminative models that require large amounts of labeled data. An alternative approach is to use a generative model, which leverages heuristics from domain experts to train on unlabeled data. Domain experts often prefer to use generative models because they ""tell a story"" about their data. Unfortunately, generative models are typically less accurate than discriminative models. Several recent approaches combine both types of model to exploit their strengths. In this setting, a misspecified generative model can hurt the performance of subsequent discriminative training. To address this issue, we propose a framework called Socratic learning that automatically uses information from the discriminative model to correct generative model misspecification. Furthermore, this process provides users with interpretable feedback about how to improve their generative model. We evaluate Socratic learning on real-world relation extraction tasks and observe an immediate improvement in classification accuracy that could otherwise require several weeks of effort by domain experts. ",Thu Oct 27 16:31:15 2016 UTC,https://arxiv.org/abs/1610.08123,126,(Posted partly for the fancy name) 
42,[1610.09038] Professor Forcing: A New Algorithm for Training Recurrent Networks,https://www.reddit.com/r/MachineLearning/comments/5a9zle/161009038_professor_forcing_a_new_algorithm_for/," Abstract: The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar. ",Mon Oct 31 02:43:28 2016 UTC,https://arxiv.org/abs/1610.09038,102,"Normally I wouldn't, but it's u/alexmlamb so I feel obliged: Did your professor force you into submitting your NIPS? It's okay, you can tell us. They can be real handsy if you don't meet in public places. (Good work, I dig applying GAN stuff to sequence generation) "
1,Stochastic Gradient MCMC with Stale Gradients,https://www.reddit.com/r/MachineLearning/comments/59f9w3/stochastic_gradient_mcmc_with_stale_gradients/," Abstract: Stochastic gradient MCMC (SG-MCMC) has played an important role in large-scale Bayesian learning, with well-developed theoretical convergence properties. In such applications of SG-MCMC, it is becoming increasingly popular to employ distributed systems, where stochastic gradients are computed based on some outdated parameters, yielding what are termed stale gradients. While stale gradients could be directly used in SG-MCMC, their impact on convergence properties has not been well studied. In this paper we develop theory to show that while the bias and MSE of an SG-MCMC algorithm depend on the staleness of stochastic gradients, its estimation variance (relative to the expected estimate, based on a prescribed number of samples) is independent of it. In a simple Bayesian distributed system with SG-MCMC, where stale gradients are computed asynchronously by a set of workers, our theory indicates a linear speedup on the decrease of estimation variance w.r.t. the number of workers. Experiments on synthetic data and deep neural networks validate our theory, demonstrating the effectiveness and scalability of SG-MCMC with stale gradients. ",Wed Oct 26 02:50:34 2016 UTC,http://arxiv.org/abs/1610.06664,145,0
16,[R] Visual Explanations from Deep Networks via Gradient-based Localization,https://www.reddit.com/r/MachineLearning/comments/58xmnt/r_visual_explanations_from_deep_networks_via/," Abstract: We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing the regions of input that are ""important"" for predictions from these models - or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. Grad-CAM is a strict generalization of the Class Activation Mapping. Unlike CAM, Grad-CAM requires no re-training and is broadly applicable to any CNN-based architectures. We also show how Grad-CAM may be combined with existing pixel-space visualizations to create a high-resolution class-discriminative visualization (Guided Grad-CAM). We generate Grad-CAM and Guided Grad-CAM visual explanations to better understand image classification, image captioning, and visual question answering (VQA) models. In the context of image classification models, our visualizations (a) lend insight into their failure modes showing that seemingly unreasonable predictions have reasonable explanations, and (b) outperform pixel-space gradient visualizations (Guided Backpropagation and Deconvolution) on the ILSVRC-15 weakly supervised localization task. For image captioning and VQA, our visualizations expose the somewhat surprising insight that common CNN + LSTM models can often be good at localizing discriminative input image regions despite not being trained on grounded image-text pairs. Finally, we design and conduct human studies to measure if Guided Grad-CAM explanations help users establish trust in the predictions made by deep networks. Interestingly, we show that Guided Grad-CAM helps untrained users successfully discern a ""stronger"" deep network from a ""weaker"" one even when both networks make identical predictions. ",Sun Oct 23 07:44:00 2016 UTC,https://arxiv.org/abs/1610.02391,168,"This is very nice. Class activation maps can give great visualizations, but restrict you to certain network architectures (the output from the conv layer should go to global average pooling, and there can't be fully connected layers). In this paper they generalize CAMs and show a way to get nice visualizations of which image parts triggered a certain network output, for any network type. I made a repository with simple Keras code to try this out, if anyone is interested: http://github.com/jacobgil/keras-grad-cam "
43,[Research][1610.06258] Using Fast Weights to Attend to the Recent Past,https://www.reddit.com/r/MachineLearning/comments/58qjiw/research161006258_using_fast_weights_to_attend_to/," Abstract: Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These ""fast weights"" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns. ",Sat Oct 22 00:11:32 2016 UTC,https://arxiv.org/abs/1610.06258,177,
7,[R] Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data,https://www.reddit.com/r/MachineLearning/comments/58e2o2/r_semisupervised_knowledge_transfer_for_deep/," Abstract: Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data. The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as ""teachers"" for a ""student"" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning. ",Thu Oct 20 01:14:25 2016 UTC,https://arxiv.org/abs/1610.05755,194,Isn't this more or less the same as using drop-out? 
14,[Research] [1610.05683] Rejection Sampling Variational Inference,https://www.reddit.com/r/MachineLearning/comments/58fm3j/research_161005683_rejection_sampling_variational/," Abstract: Variational inference using the reparameterization trick has enabled large-scale approximate Bayesian inference in complex probabilistic models, leveraging stochastic optimization to sidestep intractable expectations. The reparameterization trick is applicable when we can simulate a random variable by applying a (differentiable) deterministic function on an auxiliary random variable whose distribution is fixed. For many distributions of interest (such as the gamma or Dirichlet), simulation of random variables relies on rejection sampling. The discontinuity introduced by the accept--reject step means that standard reparameterization tricks are not applicable. We propose a new method that lets us leverage reparameterization gradients even when variables are outputs of a rejection sampling algorithm. Our approach enables reparameterization on a larger class of variational distributions. In several studies of real and synthetic data, we show that the variance of the estimator of the gradient is significantly lower than other state-of-the-art methods. This leads to faster convergence of stochastic optimization variational inference. ",Thu Oct 20 07:22:35 2016 UTC,https://arxiv.org/abs/1610.05683,189,"Exciting VI research! Question from the village stats idiot: can I implement this for the latent layer of a VAE, and if so, would I want to? (Particularly if I'm, say, generating images?)  I sort of get the feeling that the idea is that this would work better if one believes that the underlying process that describes the latents is not isotropic gaussian but is instead from a distribution that we'd need rejection sampling for. Is that right, and if so, what intuition would lead you towards selecting one distribution over another? "
31,[R] Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation,https://www.reddit.com/r/MachineLearning/comments/58kdpo/r_equilibrium_propagation_bridging_the_gap/," Abstract: We introduce Equilibrium Propagation (e-prop), a learning algorithm for energy-based models. This algorithm involves only one kind of neural computation both for the first phase (when the prediction is made) and the second phase (after the target is revealed) of training. Contrary to backpropagation in feedforward networks, there is no need for special computation in the second phase of our learning algorithm. Equilibrium Propagation combines features of Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: the algorithm computes the exact gradient of a well defined objective function. Because the objective function is defined in terms of local perturbations, the second phase of e-prop corresponds to only nudging the first-phase fixed point towards a configuration that has lower cost value. In the case of a multi-layer supervised neural network, the output units are slightly nudged towards their target, and the perturbation introduced at the output layer propagates backward in the network. The theory developed in this paper shows that the signal 'back-propagated' during this second phase actually contains information about the error derivatives, which we use to implement a learning rule proved to perform gradient descent with respect to the objective function. Thus, this work makes it more plausible that a mechanism similar to backpropagation could be implemented by brains. ",Fri Oct 21 00:23:10 2016 UTC,https://arxiv.org/abs/1602.05179,185,Here is a talk Bengio gives which covers this paper 
17,[1610.06454v1] Reasoning with Memory Augmented Neural Networks for Language Comprehension,https://www.reddit.com/r/MachineLearning/comments/58n9hl/161006454v1_reasoning_with_memory_augmented/," Abstract: Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets. ",Fri Oct 21 13:41:34 2016 UTC,https://arxiv.org/abs/1610.06454v1,182,Memory-augmented nets seem to be the flavor of the year. 
44,[R] Achieving Human Parity in Conversational Speech Recognition,https://www.reddit.com/r/MachineLearning/comments/58414p/r_achieving_human_parity_in_conversational_speech/," Abstract: Conversational speech recognition has served as a flagship speech recognition task since the release of the DARPA Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find that our latest automated system has reached human parity. The error rate of professional transcriptionists is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion where friends and family members have open-ended conversations. In both cases, our automated system establishes a new state-of-the-art, and edges past the human benchmark. This marks the first time that human parity has been reported for conversational speech. The key to our system's performance is the systematic use of convolutional and LSTM neural networks, combined with a novel spatial smoothing method and lattice-free MMI acoustic training. ",Tue Oct 18 14:21:11 2016 UTC,https://arxiv.org/abs/1610.05256,202,
17,[R] Temporal Ensembling for Semi-Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/57rq9f/r_temporal_ensembling_for_semisupervised_learning/," Abstract: In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally demonstrate good tolerance to incorrect labels. ",Sun Oct 16 14:50:33 2016 UTC,https://arxiv.org/abs/1610.02242,224,Interesting paper. The implementation is very simple. 
24,[R] Sim-to-Real Robot Learning from Pixels with Progressive Nets (DeepMind),https://www.reddit.com/r/MachineLearning/comments/57w3fp/r_simtoreal_robot_learning_from_pixels_with/," Abstract: Applying end-to-end learning to solve complex, interactive, pixel-driven control tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms are too slow to achieve performance on a real robot, but their potential has been demonstrated in simulated environments. We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. The progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks, enabling a compositional, yet simple, approach to building complex skills. We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap. Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator. Moreover, rather than relying on model-based trajectory optimisation, the task learning is accomplished using only deep reinforcement learning and sparse rewards. ",Mon Oct 17 07:08:57 2016 UTC,https://arxiv.org/abs/1610.04286,215,"If someone is interested in Progressive Nets, there is a very nice presentation that Raia Hadsell presented on ICML this year. "
11,[R]Big Batch SGD: Automated Inference using Adaptive Batch Sizes,https://www.reddit.com/r/MachineLearning/comments/58fbwj/rbig_batch_sgd_automated_inference_using_adaptive/," Abstract: Classical stochastic gradient methods for optimization rely on noisy gradient approximations that become progressively less accurate as iterates approach a solution. The large noise and small signal in the resulting gradients makes it difficult to use them for adaptive stepsize selection and automatic stopping. We propose alternative ""big batch"" SGD schemes that adaptively grow the batch size over time to maintain a nearly constant signal-to-noise ratio in the gradient approximation. The resulting methods have similar convergence rates to classical SGD methods without requiring convexity of the objective function. The high fidelity gradients enable automated learning rate selection and do not require stepsize decay. For this reason, big batch methods are easily automated and can run with little or no user oversight. ",Thu Oct 20 05:54:57 2016 UTC,http://arxiv.org/abs/1610.05792,191,Probably a good way for scaling up gradient based machine learning problems without performance drop.  
48,[R] Why Deep Neural Networks? [arXiv:1610.04161],https://www.reddit.com/r/MachineLearning/comments/57vzsc/r_why_deep_neural_networks_arxiv161004161/," Abstract: Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. In this paper, we show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\varepsilon$) require $\Omega(\text{poly}(1/\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\varepsilon$) require $\mathcal{O}(\text{polylog}(1/\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on this simple observation that the binary approximation of a real number in the interval $[0,1]$ can be represented by a deep neural network which uses a ""small"" number of neurons. ",Mon Oct 17 06:34:44 2016 UTC,https://arxiv.org/abs/1610.04161,214,"An interesting result, particularly since in its simplest form is very easy to state: fixed depth neural networks require exponentially more neurons to approximate the function f(x)=x2 on the interval [0,1] to within epsilon accuracy compared to arbitrary depth networks.  Even more importantly they show it for a very large and important class. I'm not an expert so I'd be interested to hear what the experts have to say about this? "
26,[R] [1610.04490] Amortised MAP Inference for Image Super-Resolution: Connects GANs to MAP and Variational inference,https://www.reddit.com/r/MachineLearning/comments/57xaem/r_161004490_amortised_map_inference_for_image/," Abstract: Image Super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data, achieving particularly good results in photo-realistic texture SR. ",Mon Oct 17 13:23:44 2016 UTC,https://arxiv.org/abs/1610.04490,213,"The trick to limit the range of the resulting function to functions which actually are valid super-resolution functions, was really cool I thought. It's one of those things you wonder why no one did before. I would think that when the range is all possible functions from n by n images to 4n by 4n, it's got to be a lot harder. "
15,[R] Learning in Implicit Generative Models,https://www.reddit.com/r/MachineLearning/comments/585wnd/r_learning_in_implicit_generative_models/," Abstract: Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination. ",Tue Oct 18 19:47:24 2016 UTC,https://arxiv.org/abs/1610.03483,204,"What does {G(z) ≤ x} mean? The set where the generator output G(z) is less than data x? These are high dimensional vectors, so what does ≤ mean in this case? "
3,[Research] Generative Adversarial Nets from a Density Ratio Estimation Perspective,https://www.reddit.com/r/MachineLearning/comments/57na6g/research_generative_adversarial_nets_from_a/," Abstract: Generative adversarial networks (GANs) are successful deep generative models. GANs are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful. ",Sat Oct 15 18:09:14 2016 UTC,https://arxiv.org/abs/1610.02920,238,Quick notes on their method (please correct me if I'm wrong):  Density ratio estimation refers to estimating a ratio between probability densities without knowing the densities themselves (i.e. estimating p(x)/p(q) without knowing p(x) or p(q)) Using density ratio estimation methods is inspired by the fact that the optimal discriminator is p(x)/(p(x) + q(x)) so training the discriminator is actually a density ratio estimation. They view the GAN as follows:   the generator minimizes the f-divergence between p(x) and q(x). This builds on the ideas from the f-GAN paper (very short notes on that) the discriminator estimates the density-ratio between p(x) and q(x).  In practice the density ratio that they estimate is actually p/(αp + (1-α)q) (small α) to address some of the training instabilities.  
8,Hadamard Product for Low-rank Bilinear Pooling,https://www.reddit.com/r/MachineLearning/comments/57uph7/hadamard_product_for_lowrank_bilinear_pooling/," Abstract: Bilinear models provide rich representations compared with linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks with the state-of-the-art results on the VQA dataset, having a better parsimonious property. ",Mon Oct 17 01:01:43 2016 UTC,http://arxiv.org/abs/1610.04325,220,Welcome any feedbacks and questions! 
33,[Research] Driving in the Matrix: Can Virtual Worlds Replace Human-Generated Annotations for Real World Tasks?,https://www.reddit.com/r/MachineLearning/comments/573a8e/research_driving_in_the_matrix_can_virtual_worlds/," Abstract: Deep learning has rapidly transformed the state of the art algorithms used to address a variety of problems in computer vision and robotics. These breakthroughs have however relied upon massive amounts of human annotated training data. This time-consuming process has begun impeding the progress of these deep learning efforts. This paper describes a method to incorporate photo-realistic computer images from a simulation engine to rapidly generate annotated data that can be used for training of machine learning algorithms. We demonstrate that a state of the art architecture, which is trained only using these synthetic annotations, performs better than the identical architecture trained on human annotated real-world data, when tested on the KITTI data set for vehicle detection. By training machine learning algorithms on a rich virtual world, this paper illustrates that real objects in real scenes can be learned and classified using synthetic data. This approach offers the possibility of accelerating deep learning's application to sensor based classification problems like those that appear in self-driving cars. ",Wed Oct 12 09:44:17 2016 UTC,https://arxiv.org/abs/1610.01983,263,
48,[Research] Fully Character-Level Neural Machine Translation without Explicit Segmentation,https://www.reddit.com/r/MachineLearning/comments/5725kd/research_fully_characterlevel_neural_machine/," Abstract: Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment. ",Wed Oct 12 03:27:26 2016 UTC,https://arxiv.org/abs/1610.03017,268,This looks like a breakthrough. Any other recent breakthroughs in character-level models that I am missing? Good summary here: https://www.commonlounge.com/discussion/c077af2ac89349fbb7517daa9329d831/main 
20,[Research] cleverhans v0.1: an adversarial machine learning library,https://www.reddit.com/r/MachineLearning/comments/56yaku/research_cleverhans_v01_an_adversarial_machine/," Abstract: cleverhans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the cleverhans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system. ",Tue Oct 11 14:37:27 2016 UTC,https://arxiv.org/abs/1610.00768,279,
24,[1610.02357] Deep Learning with Separable Convolutions,https://www.reddit.com/r/MachineLearning/comments/56plkb/161002357_deep_learning_with_separable/," Abstract: We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the \textit{depthwise separable convolution} operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameter as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. ",Mon Oct 10 01:57:21 2016 UTC,https://arxiv.org/abs/1610.02357,301,
20,[1610.01644] Understanding intermediate layers using linear classifier probes,https://www.reddit.com/r/MachineLearning/comments/56el4x/161001644_understanding_intermediate_layers_using/," Abstract: Neural network models have a reputation for being black boxes. We propose a new method to understand better the roles and dynamics of the intermediate layers. This has direct consequences on the design of such models and it enables the expert to be able to justify certain heuristics (such as the auxiliary heads in the Inception model). Our method uses linear classifiers, referred to as ""probes"", where a probe can only use the hidden units of a given intermediate layer as discriminating features. Moreover, these probes cannot affect the training phase of a model, and they are generally added after training. They allow the user to visualize the state of the model at multiple steps of training. We demonstrate how this can be used to develop a better intuition about a known model and to diagnose potential problems. ",Sat Oct 8 00:33:52 2016 UTC,https://arxiv.org/abs/1610.01644,325,
10,[Research] Supervision via Competition: Robot Adversaries for Learning Tasks,https://www.reddit.com/r/MachineLearning/comments/56nq5x/research_supervision_via_competition_robot/," Abstract: There has been a recent paradigm shift in robotics to data-driven learning for planning and control. Due to large number of experiences required for training, most of these approaches use a self-supervised paradigm: using sensors to measure success/failure. However, in most cases, these sensors provide weak supervision at best. In this work, we propose an adversarial learning framework that pits an adversary against the robot learning the task. In an effort to defeat the adversary, the original robot learns to perform the task with more robustness leading to overall improved performance. We show that this adversarial framework forces the the robot to learn a better grasping model in order to overcome the adversary. By grasping 82% of presented novel objects compared to 68% without an adversary, we demonstrate the utility of creating adversaries. We also demonstrate via experiments that having robots in adversarial setting might be a better learning strategy as compared to having collaborative multiple robots. ",Sun Oct 9 19:11:53 2016 UTC,https://arxiv.org/abs/1610.01685,309,
0,[Research][1610.02306] Optimization of Convolutional Neural Network using Microcanonical Annealing Algorithm,https://www.reddit.com/r/MachineLearning/comments/56tkpd/research161002306_optimization_of_convolutional/," Abstract: Convolutional neural network (CNN) is one of the most prominent architectures and algorithm in Deep Learning. It shows a remarkable improvement in the recognition and classification of objects. This method has also been proven to be very effective in a variety of computer vision and machine learning problems. As in other deep learning, however, training the CNN is interesting yet challenging. Recently, some metaheuristic algorithms have been used to optimize CNN using Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing and Harmony Search. In this paper, another type of metaheuristic algorithms with different strategy has been proposed, i.e. Microcanonical Annealing to optimize Convolutional Neural Network. The performance of the proposed method is tested using the MNIST and CIFAR-10 datasets. Although experiment results of MNIST dataset indicate the increase in computation time (1.02x - 1.38x), nevertheless this proposed method can considerably enhance the performance of the original CNN (up to 4.60\%). On the CIFAR10 dataset, currently, state of the art is 96.53\% using fractional pooling, while this proposed method achieves 99.14\%. ",Mon Oct 10 18:40:33 2016 UTC,https://arxiv.org/abs/1610.02306,302,
39,[1610.01945] Connecting Generative Adversarial Networks and Actor-Critic Methods,https://www.reddit.com/r/MachineLearning/comments/569h88/161001945_connecting_generative_adversarial/," Abstract: Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities. ",Fri Oct 7 03:37:28 2016 UTC,https://arxiv.org/abs/1610.01945,330,
14,Exploration Potential,https://www.reddit.com/r/MachineLearning/comments/561en3/exploration_potential/," Abstract: We introduce exploration potential, a quantity for that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem's reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation. ",Wed Oct 5 19:49:08 2016 UTC,https://arxiv.org/pdf/1609.04994.pdf,350,
38,"[1609.08913] ""we must either continue to develop new learning methods year after year or move towards highly parameterized models that are both flexible and sensitive to their hyperparameters""",https://www.reddit.com/r/MachineLearning/comments/566eea/160908913_we_must_either_continue_to_develop_new/," Abstract: No Free Lunch theorems show that the average performance across any closed-under-permutation set of problems is fixed for all algorithms, under appropriate conditions. Extending these results, we demonstrate that the proportion of favorable problems is itself strictly bounded, such that no single algorithm can perform well over a large fraction of possible problems. Our results explain why we must either continue to develop new learning methods year after year or move towards highly parameterized models that are both flexible and sensitive to their hyperparameters. ",Thu Oct 6 16:58:02 2016 UTC,https://arxiv.org/abs/1609.08913,337,
86,"Accelerating Deep Convolutional Networks using low-precision and sparsity - Intel paper, 2-bit ResNets",https://www.reddit.com/r/MachineLearning/comments/55sqv6/accelerating_deep_convolutional_networks_using/," Abstract: We explore techniques to significantly improve the compute efficiency and performance of Deep Convolution Networks without impacting their accuracy. To improve the compute efficiency, we focus on achieving high accuracy with extremely low-precision (2-bit) weight networks, and to accelerate the execution time, we aggressively skip operations on zero-values. We achieve the highest reported accuracy of 76.6% Top-1/93% Top-5 on the Imagenet object classification challenge with low-precision network\footnote{github release of the source code coming soon} while reducing the compute requirement by ~3x compared to a full-precision network that achieves similar accuracy. Furthermore, to fully exploit the benefits of our low-precision networks, we build a deep learning accelerator core, dLAC, that can achieve up to 1 TFLOP/mm^2 equivalent for single-precision floating-point operations (~2 TFLOP/mm^2 for half-precision). ",Tue Oct 4 09:50:42 2016 UTC,https://arxiv.org/abs/1610.00324,363,
3,YouTube 8M: Large-Scale Video Classification from Google Research,https://www.reddit.com/r/MachineLearning/comments/55tu9i/youtube_8m_largescale_video_classification_from/," Abstract: Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets. In this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of ~8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. To get the videos and their labels, we used a YouTube video annotation system, which labels videos with their main topics. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. We trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using TensorFlow. We plan to release code for training a TensorFlow model and for computing metrics. ",Tue Oct 4 14:50:13 2016 UTC,https://arxiv.org/pdf/1609.08675.pdf,375,
67,DeepMind: Video Pixel Networks,https://www.reddit.com/r/MachineLearning/comments/55r81w/deepmind_video_pixel_networks/," Abstract: We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects. ",Tue Oct 4 01:49:17 2016 UTC,https://arxiv.org/abs/1610.00527,372,
8,X-CNN: Cross-modal Convolutional Neural Networks for Sparse Datasets,https://www.reddit.com/r/MachineLearning/comments/55turk/xcnn_crossmodal_convolutional_neural_networks_for/," Abstract: In this paper we propose cross-modal convolutional neural networks (X-CNNs), a novel biologically inspired type of CNN architectures, treating gradient descent-specialised CNNs as individual units of processing in a larger-scale network topology, while allowing for unconstrained information flow and/or weight sharing between analogous hidden layers of the network---thus generalising the already well-established concept of neural network ensembles (where information typically may flow only between the output layers of the individual networks). The constituent networks are individually designed to learn the output function on their own subset of the input data, after which cross-connections between them are introduced after each pooling operation to periodically allow for information exchange between them. This injection of knowledge into a model (by prior partition of the input data through domain knowledge or unsupervised methods) is expected to yield greatest returns in sparse data environments, which are typically less suitable for training CNNs. For evaluation purposes, we have compared a standard four-layer CNN as well as a sophisticated FitNet4 architecture against their cross-modal variants on the CIFAR-10 and CIFAR-100 datasets with differing percentages of the training data being removed, and find that at lower levels of data availability, the X-CNNs significantly outperform their baselines (typically providing a 2--6% benefit, depending on the dataset size and whether data augmentation is used), while still maintaining an edge on all of the full dataset tests. ",Tue Oct 4 14:53:09 2016 UTC,https://arxiv.org/abs/1610.00163,369,
16,[paper] Deep Reinforcement Learning for Robotic Manipulation,https://www.reddit.com/r/MachineLearning/comments/55r9s2/paper_deep_reinforcement_learning_for_robotic/," Abstract: Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations. ",Tue Oct 4 02:01:10 2016 UTC,https://arxiv.org/abs/1610.00633,377,
2,"Universal classifier for Binary, Multi-class and Multi-label Classification",https://www.reddit.com/r/MachineLearning/comments/55mti5/universal_classifier_for_binary_multiclass_and/," Abstract: Classification involves the learning of the mapping function that associates input samples to corresponding target label. There are two major categories of classification problems: Single-label classification and Multi-label classification. Traditional binary and multi-class classifications are sub-categories of single-label classification. Several classifiers are developed for binary, multi-class and multi-label classification problems, but there are no classifiers available in the literature capable of performing all three types of classification. In this paper, a novel online universal classifier capable of performing all the three types of classification is proposed. Being a high speed online classifier, the proposed technique can be applied to streaming data applications. The performance of the developed classifier is evaluated using datasets from binary, multi-class and multi-label problems. The results obtained are compared with state-of-the-art techniques from each of the classification types. ",Mon Oct 3 08:59:53 2016 UTC,https://arxiv.org/abs/1609.00843,397,
28,Quantum-Chemical Insights from Deep Tensor Neural Networks,https://www.reddit.com/r/MachineLearning/comments/55k7m8/quantumchemical_insights_from_deep_tensor_neural/," Abstract: Learning from data has led to paradigm shifts in a multitude of disciplines, including web, text, and image search, speech recognition, as well as bioinformatics. Can machine learning enable similar breakthroughs in understanding quantum many-body systems? Here we develop an efficient deep learning approach that enables spatially and chemically resolved insights into quantum-mechanical observables of molecular systems. We unify concepts from many-body Hamiltonians with purpose-designed deep tensor neural networks (DTNN), which leads to size-extensive and uniformly accurate (1 kcal/mol) predictions in compositional and configurational chemical space for molecules of intermediate size. As an example of chemical relevance, the DTNN model reveals a classification of aromatic rings with respect to their stability -- a useful property that is not contained as such in the training dataset. Further applications of DTNN for predicting atomic energies and local chemical potentials in molecules, reliable isomer energies, and molecules with peculiar electronic structure demonstrate the high potential of machine learning for revealing novel insights into complex quantum-chemical systems. ",Sun Oct 2 20:55:22 2016 UTC,https://arxiv.org/pdf/1609.08259.pdf,393,
18,DecomposeMe: Simplifying ConvNets for End-to-End Learning,https://www.reddit.com/r/MachineLearning/comments/55nfqa/decomposeme_simplifying_convnets_for_endtoend/," Abstract: Deep learning and convolutional neural networks (ConvNets) have been successfully applied to most relevant tasks in the computer vision community. However, these networks are computationally demanding and not suitable for embedded devices where memory and time consumption are relevant. In this paper, we propose DecomposeMe, a simple but effective technique to learn features using 1D convolutions. The proposed architecture enables both simplicity and filter sharing leading to increased learning capacity. A comprehensive set of large-scale experiments on ImageNet and Places2 demonstrates the ability of our method to improve performance while significantly reducing the number of parameters required. Notably, on Places2, we obtain an improvement in relative top-1 classification accuracy of 7.7\% with an architecture that requires 92% fewer parameters compared to VGG-B. The proposed network is also demonstrated to generalize to other tasks by converting existing networks. ",Mon Oct 3 12:36:01 2016 UTC,https://arxiv.org/abs/1606.05426,388,
42,HyperNetworks,https://www.reddit.com/r/MachineLearning/comments/5566fo/hypernetworks/," Abstract: This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of language modeling tasks with Character-Level Penn Treebank and Hutter Prize Wikipedia datasets, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters. ",Fri Sep 30 03:54:56 2016 UTC,https://arxiv.org/abs/1609.09106,420,
49,[1609.09475v1] Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge,https://www.reddit.com/r/MachineLearning/comments/55e7kk/160909475v1_multiview_selfsupervised_deep/," Abstract: Robot warehouse automation has attracted significant interest in recent years, perhaps most visibly in the Amazon Picking Challenge (APC). A fully autonomous warehouse pick-and-place system requires robust vision that reliably recognizes and locates objects amid cluttered environments, self-occlusions, sensor noise, and a large variety of objects. In this paper we present an approach that leverages multi-view RGB-D data and self-supervised, data-driven learning to overcome those difficulties. The approach was part of the MIT-Princeton Team system that took 3rd- and 4th- place in the stowing and picking tasks, respectively at APC 2016. In the proposed approach, we segment and label multiple views of a scene with a fully convolutional neural network, and then fit pre-scanned 3D object models to the resulting segmentation to get the 6D object pose. Training a deep neural network for segmentation typically requires a large amount of training data. We propose a self-supervised method to generate a large labeled dataset without tedious manual segmentation. We demonstrate that our system can reliably estimate the 6D pose of objects under a variety of scenarios. All code, data, and benchmarks are available at this http URL ",Sat Oct 1 17:31:08 2016 UTC,http://arxiv.org/abs/1609.09475v1,402,
17,[1609.09049v1] Deep Reinforcement Learning for Tensegrity Robot Locomotion,https://www.reddit.com/r/MachineLearning/comments/55e7yc/160909049v1_deep_reinforcement_learning_for/," Abstract: Tensegrity robots, composed of rigid rods connected by elastic cables, have a number of unique properties that make them appealing for use as planetary exploration rovers. However, control of tensegrity robots remains a difficult problem due to their unusual structures and complex dynamics. In this work, we show how locomotion gaits can be learned automatically using a novel extension of mirror descent guided policy search (MDGPS) applied to periodic locomotion movements, and we demonstrate the effectiveness of our approach on tensegrity robot locomotion. We evaluate our method with real-world and simulated experiments on the SUPERball tensegrity robot, showing that the learned policies generalize to changes in system parameters, unreliable sensor measurements, and variation in environmental conditions, including varied terrains and a range of different gravities. Our experiments demonstrate that our method not only learns fast, power-efficient feedback policies for rolling gaits, but that these policies can succeed with only the limited onboard sensing provided by SUPERball's accelerometers. We compare the learned feedback policies to learned open-loop policies and hand-engineered controllers, and demonstrate that the learned policy enables the first continuous, reliable locomotion gait for the real SUPERball robot. ",Sat Oct 1 17:33:30 2016 UTC,http://arxiv.org/abs/1609.09049v1,405,
4,Fathom: Reference Workloads for Modern Deep Learning Methods,https://www.reddit.com/r/MachineLearning/comments/54toih/fathom_reference_workloads_for_modern_deep/," Abstract: Deep learning has been popularized by its recent successes on challenging artificial intelligence problems. One of the reasons for its dominance is also an ongoing challenge: the need for immense amounts of computational power. Hardware architects have responded by proposing a wide array of promising ideas, but to date, the majority of the work has focused on specific algorithms in somewhat narrow application domains. While their specificity does not diminish these approaches, there is a clear need for more flexible solutions. We believe the first step is to examine the characteristics of cutting edge models from across the deep learning community. Consequently, we have assembled Fathom: a collection of eight archetypal deep learning workloads for study. Each of these models comes from a seminal work in the deep learning community, ranging from the familiar deep convolutional neural network of Krizhevsky et al., to the more exotic memory networks from Facebook's AI research group. Fathom has been released online, and this paper focuses on understanding the fundamental performance characteristics of each model. We use a set of application-level modeling tools built around the TensorFlow deep learning framework in order to analyze the behavior of the Fathom workloads. We present a breakdown of where time is spent, the similarities between the performance profiles of our models, an analysis of behavior in inference and training, and the effects of parallelism on scaling. ",Wed Sep 28 00:12:27 2016 UTC,https://arxiv.org/abs/1608.06581,451,
7,[1609.07236] On the (im)possibility of fairness,https://www.reddit.com/r/MachineLearning/comments/54o2l6/160907236_on_the_impossibility_of_fairness/," Abstract: What does it mean for an algorithm to be fair? Different papers use different notions of algorithmic fairness, and although these appear internally consistent, they also seem mutually incompatible. We present a mathematical setting in which the distinctions in previous papers can be made formal. In addition to characterizing the spaces of inputs (the ""observed"" space) and outputs (the ""decision"" space), we introduce the notion of a construct space: a space that captures unobservable, but meaningful variables for the prediction. We show that in order to prove desirable properties of the entire decision-making process, different mechanisms for fairness require different assumptions about the nature of the mapping from construct space to decision space. The results in this paper imply that future treatments of algorithmic fairness should more explicitly state assumptions about the relationship between constructs and observations. ",Tue Sep 27 01:08:41 2016 UTC,https://arxiv.org/abs/1609.07236,475,
10,[1609.07843] Pointer Sentinel Mixture Models; sota for language modeling while using less parameters than lstms,https://www.reddit.com/r/MachineLearning/comments/54o9p1/160907843_pointer_sentinel_mixture_models_sota/, Abstract: Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus. ,Tue Sep 27 01:53:02 2016 UTC,http://arxiv.org/abs/1609.07843,472,"Hey /r/MachineLearning. I'm the first author on this paper and happy to answer questions that people may have! ^_^ The pointer sentinel mixture model allows you to use a pointer network but to back off to a softmax vocabulary if the pointer network is not confident. The pointer network enables longer term dependencies and also improves the handling of rare words. Backing off to the softmax vocabulary allows the pointer sentinel mixture model to generate words that may not be in the pointer window. The paper also introduces the WikiText-2 and WikiText-103 datasets. We hope these will replace the Mikolov processed Penn Treebank dataset as a standard benchmark for long term dependency language modeling as it has many advantages (numbers instead of N, true casing, punctuation, ...) and no usage restrictions. The WikiText datasets were created such that they're essentially drop-in replacements to Mikolov PTB. The WikiText dataset page is live, containing examples with download links for WikiText-2 and WikiText-103. "
13,[1609.07061] Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations,https://www.reddit.com/r/MachineLearning/comments/54p6zq/160907061_quantized_neural_networks_training/," Abstract: We introduce a method to train Quantized Neural Networks (QNNs) --- neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At train-time the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves $51\%$ top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online. ",Tue Sep 27 05:44:59 2016 UTC,http://arxiv.org/abs/1609.07061,465,"What does the <<>> operator do? The paper says it does ""both left and right binary shift"". What does this mean? "
103,[1609.08144] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,https://www.reddit.com/r/MachineLearning/comments/54oju9/160908144_googles_neural_machine_translation/," Abstract: Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system. ",Tue Sep 27 02:54:44 2016 UTC,http://arxiv.org/abs/1609.08144,455,"The power of data, infrastructure and engineering techniques! e.g. ""Given the larger volume of training data available in the Google corpora, dropout is also not needed in these experiments."" "
13,[1609.07072] The Many-Body Expansion Combined with Neural Networks,https://www.reddit.com/r/MachineLearning/comments/54rt4l/160907072_the_manybody_expansion_combined_with/," Abstract: Fragmentation methods such as the many-body expansion (MBE) are a common strategy to model large systems by partitioning energies into a hierarchy of decreasingly significant contributions. The number of fragments required for chemical accuracy is still prohibitively expensive for ab-initio MBE to compete with force field approximations for applications beyond single-point energies. Alongside the MBE, empirical models of ab-initio potential energy surfaces have improved, especially non-linear models based on neural networks (NN) which can reproduce ab-initio potential energy surfaces rapidly and accurately. Although they are fast, NNs suffer from their own curse of dimensionality; they must be trained on a representative sample of chemical space. In this paper we examine the synergy of the MBE and NN's, and explore their complementarity. The MBE offers a systematic way to treat systems of arbitrary size and intelligently sample chemical space. NN's reduce, by a factor in excess of $10^6$ the computational overhead of the MBE and reproduce the accuracy of ab-initio calculations without specialized force fields. We show they are remarkably general, providing comparable accuracy with drastically different chemical embeddings. To assess this we test a new chemical embedding which can be inverted to predict molecules with desired properties. ",Tue Sep 27 17:47:07 2016 UTC,http://arxiv.org/abs/1609.07072,452,"My first paper using machine learning. I know it's more application of neural nets than development, but what do you guys think? "
10,[1609.07152] Input Convex Neural Networks,https://www.reddit.com/r/MachineLearning/comments/54ksk8/160907152_input_convex_neural_networks/," Abstract: This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with only minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases. ",Mon Sep 26 14:19:16 2016 UTC,http://arxiv.org/abs/1609.07152,482,"No usual benchmarks like MNIST, CIFAR-10 and ATARI? "
5,[1609.06616] Gov2Vec: Learning Distributed Representations of Institutions and Their Legal Text,https://www.reddit.com/r/MachineLearning/comments/540u1t/160906616_gov2vec_learning_distributed/," Abstract: We compare policy differences across institutions by embedding representations of the entire legal corpus of each institution and the vocabulary shared across all corpora into a continuous vector space. We apply our method, Gov2Vec, to Supreme Court opinions, Presidential actions, and official summaries of Congressional bills. The model discerns meaningful differences between government branches. We also learn representations for more fine-grained word sources: individual Presidents and (2-year) Congresses. The similarities between learned representations of Congresses over time and sitting Presidents are negatively correlated with the bill veto rate, and the temporal ordering of Presidents and Congresses was implicitly learned from only text. With the resulting vectors we answer questions such as: how does Obama and the 113th House differ in addressing climate change and how does this vary from environmental or economic perspectives? Our work illustrates vector-arithmetic-based investigations of complex relationships between word sources based on their texts. We are extending this to create a more comprehensive legal semantic map. ",Thu Sep 22 19:48:32 2016 UTC,https://arxiv.org/abs/1609.06616,524,"Apply standard technique to slightly new dataset and call it an innovation, Gov2Vec! Oh yeah don't forget to throw in the word ""distributed"" for good measure "
15,[1609.07088v1] Learning Modular Neural Network Policies for Multi-Task and Multi-Robot Transfer,https://www.reddit.com/r/MachineLearning/comments/549fw3/160907088v1_learning_modular_neural_network/," Abstract: Reinforcement learning (RL) can automate a wide variety of robotic skills, but learning each new skill requires considerable real-world data collection and manual representation engineering to design policy classes or features. Using deep reinforcement learning to train general purpose neural network policies alleviates some of the burden of manual representation engineering by using expressive policy classes, but exacerbates the challenge of data collection, since such methods tend to be less efficient than RL with low-dimensional, hand-designed representations. Transfer learning can mitigate this problem by enabling us to transfer information from one skill to another and even from one robot to another. We show that neural network policies can be decomposed into ""task-specific"" and ""robot-specific"" modules, where the task-specific modules are shared across robots, and the robot-specific modules are shared across all tasks on that robot. This allows for sharing task information, such as perception, between robots and sharing robot information, such as dynamics and kinematics, between tasks. We exploit this decomposition to train mix-and-match modules that can solve new robot-task combinations that were not seen during training. Using a novel neural network architecture, we demonstrate the effectiveness of our transfer method for enabling zero-shot generalization with a variety of robots and tasks in simulation for both visual and non-visual tasks. ",Sat Sep 24 09:16:36 2016 UTC,http://arxiv.org/abs/1609.07088v1,501,Here is the video. https://sites.google.com/site/modularpolicynetworks/ 
20,Is the deconvolution layer the same as a convolutional layer?,https://www.reddit.com/r/MachineLearning/comments/545d9c/is_the_deconvolution_layer_the_same_as_a/," Abstract: In this note, we want to focus on aspects related to two questions most people asked us at CVPR about the network we presented. Firstly, What is the relationship between our proposed layer and the deconvolution layer? And secondly, why are convolutions in low-resolution (LR) space a better choice? These are key questions we tried to answer in the paper, but we were not able to go into as much depth and clarity as we would have liked in the space allowance. To better answer these questions in this note, we first discuss the relationships between the deconvolution layer in the forms of the transposed convolution layer, the sub-pixel convolutional layer and our efficient sub-pixel convolutional layer. We will refer to our efficient sub-pixel convolutional layer as a convolutional layer in LR space to distinguish it from the common sub-pixel convolutional layer. We will then show that for a fixed computational budget and complexity, a network with convolutions exclusively in LR space has more representation power at the same speed than a network that first upsamples the input in high resolution space. ",Fri Sep 23 15:50:32 2016 UTC,http://arxiv.org/abs/1609.07009,509,This is some additional insights we have about the network architecture we proposed in the following paper https://arxiv.org/abs/1609.05158  
14,SoftTarget Regularization,https://www.reddit.com/r/MachineLearning/comments/53wd2f/softtarget_regularization/," Abstract: Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures. ",Thu Sep 22 00:29:43 2016 UTC,http://arxiv.org/abs/1609.06693,530,"This seems like trying to increase the entropy of the predictions (make sure the predictions don't get too ""spikey"", separating various types of dog breeds from each other very sharply). Minimum entropy semisupervised learning is similar to transductive SVM -- trying to increase the margin on unlabeled data and making the model more confident of its predictions. Am I misunderstanding things or is this sort of the opposite of a minimum entropy criterion? "
29,[1609.05473] SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,https://www.reddit.com/r/MachineLearning/comments/53og64/160905473_seqgan_sequence_generative_adversarial/," Abstract: As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines. ",Tue Sep 20 17:15:22 2016 UTC,http://arxiv.org/abs/1609.05473,548,Any samples of text? 
7,Neural Coarse-Graining: Extracting slowly-varying latent degrees of freedom with neural networks,https://www.reddit.com/r/MachineLearning/comments/53rdyz/neural_coarsegraining_extracting_slowlyvarying/," Abstract: We present a loss function for neural networks that encompasses an idea of trivial versus non-trivial predictions, such that the network jointly determines its own prediction goals and learns to satisfy them. This permits the network to choose sub-sets of a problem which are most amenable to its abilities to focus on solving, while discarding 'distracting' elements that interfere with its learning. To do this, the network first transforms the raw data into a higher-level categorical representation, and then trains a predictor from that new time series to its future. To prevent a trivial solution of mapping the signal to zero, we introduce a measure of non-triviality via a contrast between the prediction error of the learned model with a naive model of the overall signal statistics. The transform can learn to discard uninformative and unpredictable components of the signal in favor of the features which are both highly predictive and highly predictable. This creates a coarse-grained model of the time-series dynamics, focusing on predicting the slowly varying latent parameters which control the statistics of the time-series, rather than predicting the fast details directly. The result is a semi-supervised algorithm which is capable of extracting latent parameters, segmenting sections of time-series with differing statistics, and building a higher-level representation of the underlying dynamics from unlabeled data. ",Wed Sep 21 03:54:22 2016 UTC,http://arxiv.org/abs/1609.00116,546,0
41,[1609.04836v1] On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,https://www.reddit.com/r/MachineLearning/comments/53lexr/160904836v1_on_largebatch_training_for_deep/," Abstract: The stochastic gradient descent method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, usually $32$--$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize. There have been some attempts to investigate the cause for this generalization drop in the large-batch regime, however the precise answer for this phenomenon is, hitherto unknown. In this paper, we present ample numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions -- and that sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We also discuss several empirical strategies that help large-batch methods eliminate the generalization gap and conclude with a set of future research ideas and open questions. ",Tue Sep 20 03:04:21 2016 UTC,https://arxiv.org/abs/1609.04836v1,566,code to reproduce Figures 1 and 2 on github https://github.com/keskarnitish/large-batch-training 
12,A Cheap Linear Attention Mechanism with Fast Lookups and Fixed-Size Representations (from MILA),https://www.reddit.com/r/MachineLearning/comments/53s5iq/a_cheap_linear_attention_mechanism_with_fast/," Abstract: The softmax content-based attention mechanism has proven to be very beneficial in many applications of recurrent neural networks. Nevertheless it suffers from two major computational limitations. First, its computations for an attention lookup scale linearly in the size of the attended sequence. Second, it does not encode the sequence into a fixed-size representation but instead requires to memorize all the hidden states. These two limitations restrict the use of the softmax attention mechanism to relatively small-scale applications with short sequences and few lookups per sequence. In this work we introduce a family of linear attention mechanisms designed to overcome the two limitations listed above. We show that removing the softmax non-linearity from the traditional attention formulation yields constant-time attention lookups and fixed-size representations of the attended sequences. These properties make these linear attention mechanisms particularly suitable for large-scale applications with extreme query loads, real-time requirements and memory constraints. Early experiments on a question answering task show that these linear mechanisms yield significantly better accuracy results than no attention, but obviously worse than their softmax alternative. ",Wed Sep 21 08:11:47 2016 UTC,http://arxiv.org/abs/1609.05866,542,0
23,[1609.05518] Towards Deep Symbolic Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/53m92m/160905518_towards_deep_symbolic_reinforcement/," Abstract: Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game. ",Tue Sep 20 07:21:20 2016 UTC,https://arxiv.org/abs/1609.05518,563,0
123,[1609.02943] Stealing Machine Learning Models via Prediction APIs,https://www.reddit.com/r/MachineLearning/comments/53s2f3/160902943_stealing_machine_learning_models_via/," Abstract: Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (""predictive analytics"") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis. The tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., ""steal"") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures. ",Wed Sep 21 07:36:03 2016 UTC,http://arxiv.org/abs/1609.02943,534,"This is an issue with any API that depends on constant data. Usually APIs have a limit to how many requests you can send per day or charge by the request. This is, in practice, no different  than someone offering data as a service and you copying the data. It's not a ML problem. "
16,[Google DeepMind] Safe and Efficient Off-Policy Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/53mrei/google_deepmind_safe_and_efficient_offpolicy/," Abstract: In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace($\lambda$), with three desired properties: (1) low variance; (2) safety, as it safely uses samples collected from any behaviour policy, whatever its degree of ""off-policyness""; and (3) efficiency, as it makes the best use of samples collected from near on-policy behaviour policies. We analyse the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. To our knowledge, this is the first return-based off-policy control algorithm converging a.s. to $Q^*$ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q($\lambda$), which was still an open problem. We illustrate the benefits of Retrace($\lambda$) on a standard suite of Atari 2600 games. ",Tue Sep 20 10:40:56 2016 UTC,https://arxiv.org/abs/1606.02647,560,0
27,[1609.05566] Label-Free Supervision of Neural Networks with Physics and Domain Knowledge,https://www.reddit.com/r/MachineLearning/comments/53ulrt/160905566_labelfree_supervision_of_neural/," Abstract: In many machine learning applications, labeled data is scarce and obtaining more labels is expensive. We introduce a new approach to supervising neural networks by specifying constraints that should hold over the output space, rather than direct examples of input-output pairs. These constraints are derived from prior domain knowledge, e.g., from known laws of physics. We demonstrate the effectiveness of this approach on real world and simulated computer vision tasks. We are able to train a convolutional neural network to detect and track objects without any labeled examples. Our approach can significantly reduce the need for labeled training data, but introduces new challenges for encoding prior knowledge into appropriate loss functions. ",Wed Sep 21 18:22:30 2016 UTC,http://arxiv.org/abs/1609.05566,532,How different is this from invariance theory (which I also know little about)? I believe poggio or one of his students first proposed it and it helps explain why translation invariance of a CNN is useful.  
7,[1609.05672] Multi-Residual Networks,https://www.reddit.com/r/MachineLearning/comments/53m9is/160905672_multiresidual_networks/," Abstract: In this article, we take one step toward understanding the learning behavior of deep residual networks, and supporting the hypothesis that deep residual networks are exponential ensembles by construction. We examine the effective range of ensembles by introducing multi-residual networks that significantly improve classification accuracy of residual networks. The multi-residual networks increase the number of residual functions in the residual blocks. This is shown to improve the accuracy of the residual network when the network is deeper than a threshold. Based on a series of empirical studies on CIFAR-10 and CIFAR-100 datasets, the proposed multi-residual network yield $6\%$ and $10\%$ improvement with respect to the residual networks with identity mappings. Comparing with other state-of-the-art models, the proposed multi-residual network obtains a test error rate of $3.92\%$ on CIFAR-10. ",Tue Sep 20 07:26:16 2016 UTC,http://arxiv.org/abs/1609.05672,572,"I think the DenseNet paper reported better accuracy, no? "
55,[1609.01596] Direct Feedback Alignment Provides Learning in Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/52v1vx/160901596_direct_feedback_alignment_provides/," Abstract: Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task. ",Thu Sep 15 07:15:29 2016 UTC,https://arxiv.org/abs/1609.01596,624,Torch code to reproduce the results in the paper is now available here: https://github.com/anokland/dfa-torch 
10,[1609.00222] Ternary Neural Networks for Resource-Efficient AI Applications,https://www.reddit.com/r/MachineLearning/comments/52w3bj/160900222_ternary_neural_networks_for/," Abstract: The computation and storage requirements for Deep Neural Networks (DNNs) are usually high. This issue limit their deployability on ubiquitous computing devices such as smart phones or wearables. In this paper, we propose ternary neural networks (TNNs) in order to make deep learning more resource-efficient. We train these TNNs using a teacher-student approach. Using only ternary weights and ternary neurons, with a step activation function of two-thresholds, the student ternary network learns to mimic the behaviour of its teacher network. We propose a novel, layer-wise greedy methodology for training TNNs. During training, a ternary neural network inherently prunes the smaller weights by setting them to zero. This makes them even more compact thus more resource-friendly. We devise a purpose-built hardware design for TNNs and implement it on FPGA. The benchmark results with our purpose-built hardware running TNNs reveal that, with only 1.24 microjoules per image, we can achieve 97.76% accuracy with 5.37 microsecond latency and with a rate of 255K images per second on MNIST. ",Thu Sep 15 13:18:56 2016 UTC,https://arxiv.org/abs/1609.00222,629," 255K images per second on MNIST  Resource efficient MNIST? Cmon now, not testing on something that is resource intensive (like ImageNet) makes me think this doesn't actually generalize well at all... "
46,[1609.03193] Wav2Letter: an End-to-End ConvNet-based Speech Recognition System,https://www.reddit.com/r/MachineLearning/comments/536mg3/160903193_wav2letter_an_endtoend_convnetbased/," Abstract: This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform. ",Sat Sep 17 09:47:41 2016 UTC,http://arxiv.org/abs/1609.03193,609,"I would love if someone created a state of the art, free as in freedom OCR based on modern tech. Right now it's basically Tesseract or Cuneiform from 15 years ago, both of which suck. "
8,Convexified Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/52jztj/convexified_convolutional_neural_networks/," Abstract: We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve performance competitive with CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods. ",Tue Sep 13 11:39:19 2016 UTC,https://arxiv.org/pdf/1609.01000v1.pdf,667,arXiv landing page: https://arxiv.org/abs/1609.01000v1 
29,[1609.01326] UnrealCV: Connecting Computer Vision to Unreal Engine,https://www.reddit.com/r/MachineLearning/comments/52xoad/160901326_unrealcv_connecting_computer_vision_to/," Abstract: Computer graphics can not only generate synthetic images and ground truth but it also offers the possibility of constructing virtual worlds in which: (i) an agent can perceive, navigate, and take actions guided by AI algorithms, (ii) properties of the worlds can be modified (e.g., material and reflectance), (iii) physical simulations can be performed, and (iv) algorithms can be learnt and evaluated. But creating realistic virtual worlds is not easy. The game industry, however, has spent a lot of effort creating 3D worlds, which a player can interact with. So researchers can build on these resources to create virtual worlds, provided we can access and modify the internal data structures of the games. To enable this we created an open-source plugin UnrealCV (this http URL) for a popular game engine Unreal Engine 4 (UE4). We show two applications: (i) a proof of concept image dataset, and (ii) linking Caffe with the virtual world to test deep network algorithms. ",Thu Sep 15 18:43:39 2016 UTC,http://arxiv.org/abs/1609.01326,621,I suddenly need to play with this.  
27,[1609.04382v1] Warped Convolutions: Efficient Invariance to Spatial Transformations,https://www.reddit.com/r/MachineLearning/comments/52zb4f/160904382v1_warped_convolutions_efficient/," Abstract: Convolutional Neural Networks (CNNs) are extremely efficient, since they exploit the inherent translation-invariance of natural images. However, translation is just one of a myriad of useful spatial transformations. Can the same efficiency be attained when considering other spatial invariances? Such generalized convolutions have been considered in the past, but at a high computational cost. We present a construction that is simple and exact, yet has the same computational complexity that standard convolutions enjoy. It consists of a constant image warp followed by a simple convolution, which are standard blocks in deep learning toolboxes. With a carefully crafted warp, the resulting architecture can be made invariant to one of a wide range of spatial transformations. We show encouraging results in realistic scenarios, including the estimation of vehicle poses in the Google Earth dataset (rotation and scale), and face poses in Annotated Facial Landmarks in the Wild (3D rotations under perspective). ",Fri Sep 16 00:21:18 2016 UTC,http://arxiv.org/abs/1609.04382v1,619,How do these compare to spatial transformer networks? 
12,""" Energy-based Generative Adversarial Network"" (stabler GAN training)",https://www.reddit.com/r/MachineLearning/comments/52idhf/energybased_generative_adversarial_network/," Abstract: We introduce the ""Energy-based Generative Adversarial Network"" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images. ",Tue Sep 13 03:04:15 2016 UTC,https://arxiv.org/abs/1609.03126,681,"I like the concept--extra stability is good, and I'll need to dig deeper into this paper later today. One nitpick though--at a quick glance their celebA comparisons look unfair. DCGAN produces way better results than the ones compared against. It sort of makes me feel like they picked an architecture which works well for their framework and doesn't work so well with DCGAN... Here's an example of the original DCGAN after 12 epochs on 64x64 celebA and here's an example after 24 epochs. Not sure why this one adds a washed-out contrast (happens after around 13 epochs every time I used Radford's source code) but I feel like even these samples are better than the ones the paper compares its samples against. "
22,[1609.03971] Feynman Machine: The Universal Dynamical Systems Computer,https://www.reddit.com/r/MachineLearning/comments/52pajt/160903971_feynman_machine_the_universal_dynamical/," Abstract: Efforts at understanding the computational processes in the brain have met with limited success, despite their importance and potential uses in building intelligent machines. We propose a simple new model which draws on recent findings in Neuroscience and the Applied Mathematics of interacting Dynamical Systems. The Feynman Machine is a Universal Computer for Dynamical Systems, analogous to the Turing Machine for symbolic computing, but with several important differences. We demonstrate that networks and hierarchies of simple interacting Dynamical Systems, each adaptively learning to forecast its evolution, are capable of automatically building sensorimotor models of the external and internal world. We identify such networks in mammalian neocortex, and show how existing theories of cortical computation combine with our model to explain the power and flexibility of mammalian intelligence. These findings lead directly to new architectures for machine intelligence. A suite of software implementations has been built based on these principles, and applied to a number of spatiotemporal learning tasks. ",Wed Sep 14 06:56:06 2016 UTC,http://arxiv.org/abs/1609.03971,646,Quite bold to name something like that with so little empirical evidence of its use. 
24,[1609.03528] The Microsoft 2016 Conversational Speech Recognition System (word error rate of 6.9% on the NIST 2000),https://www.reddit.com/r/MachineLearning/comments/52tk31/160903528_the_microsoft_2016_conversational/," Abstract: We describe Microsoft's conversational speech recognition system, in which we combine recent developments in neural-network-based acoustic and language modeling to advance the state of the art on the Switchboard recognition task. Inspired by machine learning ensemble techniques, the system uses a range of convolutional and recurrent neural networks. I-vector modeling and lattice-free MMI training provide significant gains for all acoustic model architectures. Language model rescoring with multiple forward and backward running RNNLMs, and word posterior-based system combination provide a 20% boost. The best single system uses a ResNet architecture acoustic model with RNNLM rescoring, and achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The combined system has an error rate of 6.3%, representing an improvement over previously reported results on this benchmark task. ",Thu Sep 15 00:08:39 2016 UTC,http://arxiv.org/abs/1609.03528,638," a bunch of AM models combined together they advertise CNTK 1-bit SGD is something that is maybe their own personal achievemnt RNNLM resocoring of n-best (a bit lazy IMO) - seems like they don't have their own implementation of that that and most other things are available in other toolkits (like Kaldi)  I'm not terribly impressed (most of these things exists in other forms, like Kaldi, but unlike CNTK, they actually have fully working setups). Seems the paper is there more to show they can also achieve state-of-the-art, rather than begin something new to the table. EDIT: they say they use a 30k vocabulary - that's nothing. Maybe people should stop using over a decade old benchmarks and demonstrate something that actually has some real-life applications? "
16,[1606.08571] Learning Generative ConvNet with Continuous Latent Factors by Alternating Back-Propagation,https://www.reddit.com/r/MachineLearning/comments/52w55c/160608571_learning_generative_convnet_with/," Abstract: This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a non-linear generalization of factor analysis. In this model, the mapping from the latent factors to the observed vector is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates between the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dynamics or gradient descent. (2) Learning back-propagation, which updates the parameters given the inferred latent factors by gradient descent. The gradient computations in both steps are powered by back-propagation, and they share most of their code in common. We show that the alternating back-propagation algorithm can learn realistic generator models of natural images, video sequences, and sounds. Moreover, it can also be used to learn from incomplete or indirect training data. ",Thu Sep 15 13:30:50 2016 UTC,http://arxiv.org/abs/1606.08571,628,"Interesting paper! But I have a question: learning both parameters of the neural network and latent vectors at the same time is problematic as the neural network, at the beginning, is going to be trained to reduce the error on the basis of randomly initialized vectors. Likewise, the latent vectors are going to be optimized on the basis of a randomly-initialized neural network. Is there anything in literature that poses and discusses this problem? "
6,[1609.03947v1] Associating Grasping with Convolutional Neural Network Features,https://www.reddit.com/r/MachineLearning/comments/52xn66/160903947v1_associating_grasping_with/," Abstract: In this work, we provide a solution for pre-shaping a human-like robot hand for grasping based on visual information. Our approach uses convolutional neural networks (CNNs) to define a mapping between images and grasps. Applying CNNs to robotics applications is non-trivial for two reasons. First, collecting enough robot data to train a CNN at the same scale as the models trained in the vision community is extremely difficult. In this work, we demonstrate that by using a pre-trained CNN, a small set of grasping examples is sufficient for generalizing across different objects of similar shapes. Second, the final output of a CNN contains little location information of the observed object, which is essential for the robot to manipulate the object. We take advantage of the hierarchical nature of CNN features and identify the 3D position of a mid-level feature using an approach we call targeted back propagation. Targeted back propagation traces the activations of higher level features in a CNN backwards through the network to discover the locations in the observation that were responsible for making them fire, thus localizing important manipulatives in the environment. We showed that this approach outperforms approaches without targeted backpropagation in a cluttered scene. We further implemented a hierarchical controller that controls fingers and palms based on features located in different CNN layers for pre-shaping the robot hand and demonstrated that this approach outperforms a pointcloud based approach on a grasping task on Robonaut 2. ",Thu Sep 15 18:37:13 2016 UTC,http://arxiv.org/abs/1609.03947v1,626,0
106,[1609.04802] Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (Twitter Cortex / Magic Pony),https://www.reddit.com/r/MachineLearning/comments/530hjn/160904802_photorealistic_single_image/," Abstract: Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? During image downsampling information is lost, making super-resolution a highly ill-posed inverse problem with a large set of possible solutions. The behavior of optimization-based super-resolution methods is therefore principally driven by the choice of objective function. Recent work has largely focussed on minimizing the mean squared reconstruction error (MSE). The resulting estimates have high peak signal-to-noise-ratio (PSNR), but they are often overly smoothed, lack high-frequency detail, making them perceptually unsatisfying. In this paper, we present super-resolution generative adversarial network (SRGAN). To our knowledge, it is the first framework capable of recovering photo-realistic natural images from 4 times downsampling. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss function motivated by perceptual similarity instead of similarity in pixel space. Trained on 350K images using the perceptual loss function, our deep residual network was able to recover photo-realistic textures from heavily downsampled images on public benchmarks. ",Fri Sep 16 05:34:34 2016 UTC,http://arxiv.org/abs/1609.04802,616,What is this sorcery 
17,[1609.02036] Deep Markov Random Field for Image Modeling,https://www.reddit.com/r/MachineLearning/comments/52lba2/160902036_deep_markov_random_field_for_image/," Abstract: Markov Random Fields (MRFs), a formulation widely used in generative image modeling, have long been plagued by the lack of expressive power. This issue is primarily due to the fact that conventional MRFs formulations tend to use simplistic factors to capture local patterns. In this paper, we move beyond such limitations, and propose a novel MRF model that uses fully-connected neurons to express the complex interactions among pixels. Through theoretical analysis, we reveal an inherent connection between this model and recurrent neural networks, and thereon derive an approximated feed-forward network that couples multiple RNNs along opposite directions. This formulation combines the expressive power of deep neural networks and the cyclic dependency structure of MRF in a unified model, bringing the modeling capability to a new level. The feed-forward approximation also allows it to be efficiently learned from data. Experimental results on a variety of low-level vision tasks show notable improvement over state-of-the-arts. ",Tue Sep 13 16:15:35 2016 UTC,http://arxiv.org/abs/1609.02036,657,0
2,[1609.03068] Multiplex visibility graphs to investigate recurrent neural networks dynamics,https://www.reddit.com/r/MachineLearning/comments/52laih/160903068_multiplex_visibility_graphs_to/," Abstract: A recurrent neural network (RNN) is a universal approximator of dynamical systems, whose performance often depends on sensitive hyperparameters. Tuning of such hyperparameters may be difficult and, typically, based on a trial-and-error approach. In this work, we adopt a graph-based framework to interpret and characterize the internal RNN dynamics. Through this insight, we are able to design a principled unsupervised method to derive configurations with maximized performances, in terms of prediction error and memory capacity. In particular, we propose to model time series of neurons activations with the recently introduced horizontal visibility graphs, whose topological properties reflect important dynamical features of the underlying dynamic system. Successively, each graph becomes a layer of a larger structure, called multiplex. We show that topological properties of such a multiplex reflect important features of RNN dynamics and are used to guide the tuning procedure. To validate the proposed method, we consider a class of RNNs called echo state networks. We perform experiments and discuss results on several benchmarks and real-world dataset of call data records. ",Tue Sep 13 16:11:39 2016 UTC,http://arxiv.org/abs/1609.03068,673,0
96,SARM (Stacked Approximated Regression Machine) withdrawn,https://www.reddit.com/r/MachineLearning/comments/51ut79/sarm_stacked_approximated_regression_machine/," Abstract: With the agreement of my coauthors, I Zhangyang Wang would like to withdraw the manuscript ""Stacked Approximated Regression Machine: A Simple Deep Learning Approach"". Some experimental procedures were not included in the manuscript, which makes a part of important claims not meaningful. In the relevant research, I was solely responsible for carrying out the experiments; the other coauthors joined in the discussions leading to the main algorithm. Please see the updated text for more details. ",Fri Sep 9 01:17:22 2016 UTC,https://arxiv.org/abs/1608.04062,755,"I agree with /u/fchollet on this:   That's the part that saddens me the most about this paper: even after reading it multiple times and discussing it with several researchers who have also read it multiple times, it seems impossible to tell with certainty what the algo they are testing really does. That is no way to write a research paper. Yet, somehow it got into NIPS?  This paper was very difficult to parse, don't understand how the reviewers pushed this through. "
17,[1609.01596v1] Direct Feedback Alignment Provides Learning in Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/51tw0u/160901596v1_direct_feedback_alignment_provides/," Abstract: Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task. ",Thu Sep 8 21:58:53 2016 UTC,https://arxiv.org/abs/1609.01596v1,770,"Personally, I think this paper is fantastic.  "
21,[1609.02993] Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks,https://www.reddit.com/r/MachineLearning/comments/52i1dp/160902993_episodic_exploration_for_deep/," Abstract: We consider scenarios from the real-time strategy game StarCraft as new benchmarks for reinforcement learning algorithms. We propose micromanagement tasks, which present the problem of the short-term, low-level control of army members during a battle. From a reinforcement learning point of view, these scenarios are challenging because the state-action space is very large, and because there is no obvious feature representation for the state-action evaluation function. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. In addition, we present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm allows for the collection of traces for learning using deterministic policies, which appears much more efficient than, for example, {\epsilon}-greedy exploration. Experiments show that with this algorithm, we successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle. ",Tue Sep 13 01:46:55 2016 UTC,http://arxiv.org/abs/1609.02993,677,"Not related to the paper but, I remember Google Deepmind a few months ago saying they were working on a policy agent for Starcraft, did they ever happen to release any results(blog post / research paper)? "
39,[1609.02200] Discrete Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/52cq74/160902200_discrete_variational_autoencoders/," Abstract: Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We introduce a novel class of probabilistic models, comprising an undirected discrete component and a directed hierarchical continuous component, that can be trained efficiently using the variational autoencoder framework. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, OMNIGLOT, and Caltech-101 Silhouettes datasets. ",Mon Sep 12 04:57:31 2016 UTC,http://arxiv.org/abs/1609.02200,707,Has anyone tried using one of the binary neuron gradient estimators (like straight through) with a VAE? 
54,[1609.02907] Semi-Supervised Classification with Graph Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/52d8ms/160902907_semisupervised_classification_with/, Abstract: We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin. ,Mon Sep 12 07:50:44 2016 UTC,http://arxiv.org/abs/1609.02907,701,"Thanks for posting - I had not heard of graph-learning techniques with neural networks before, and this was an interesting read.  In case anyone is interested, I calculated the correlations between properties of the datasets and classification accuracy. The number of nodes per class and label rate are most highly correlated: (http://imgur.com/a/5GiFc) "
37,[1609.01704] Hierarchical Multiscale Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/51k6e7/160901704_hierarchical_multiscale_recurrent/," Abstract: Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling. ",Wed Sep 7 07:58:56 2016 UTC,http://arxiv.org/abs/1609.01704,811,"Fantastic! I've been working on a similar concept for a while, but haven't had success in actual implementation - as an amateur, it's extremely heartening to see that I was actually in the vicinity of a viable concept, and very educational to see how actual researchers were able to solve the problems I wasn't sure how to approach. "
votes,title,url,abstract,submitted,link,rank,top_comment
2,[R] [1610.02365] Deep Learning with Coherent Nanophotonic Circuits,https://www.reddit.com/r/MachineLearning/comments/5fb4zh/r_161002365_deep_learning_with_coherent/," Abstract: Artificial Neural Networks are computational network models inspired by signal processing in the brain. These models have dramatically improved the performance of many learning tasks, including speech and object recognition. However, today's computing hardware is inefficient at implementing neural networks, in large part because much of it was designed for von Neumann computing schemes. Significant effort has been made to develop electronic architectures tuned to implement artificial neural networks that improve upon both computational speed and energy efficiency. Here, we propose a new architecture for a fully-optical neural network that, using unique advantages of optics, promises a computational speed enhancement of at least two orders of magnitude over the state-of-the-art and three orders of magnitude in power efficiency for conventional learning tasks. We experimentally demonstrate essential parts of our architecture using a programmable nanophotonic processor. ",Mon Nov 28 11:12:58 2016 UTC,https://arxiv.org/abs/1610.02365,8,
23,[R][1611.07709] Fully Convolutional Instance-aware Semantic Segmentation,https://www.reddit.com/r/MachineLearning/comments/5erwq1/r161107709_fully_convolutional_instanceaware/," Abstract: We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation and instance mask proposal. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The proposed network is highly integrated and achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. The code would be released at \url{this https URL}. ",Fri Nov 25 05:11:58 2016 UTC,https://arxiv.org/abs/1611.07709,31,
30,[R] STDP-based spiking deep neural networks for object recognition,https://www.reddit.com/r/MachineLearning/comments/5ei18v/r_stdpbased_spiking_deep_neural_networks_for/," Abstract: Previous studies have shown that spike-timing-dependent plasticity (STDP) can be used in spiking neural networks (SNN) to extract visual features of low or intermediate complexity in an unsupervised manner. These studies, however, used relatively shallow architectures, and only one layer was trainable. Another line of research has demonstrated - using rate-based neural networks trained with back-propagation - that having many layers increases the recognition robustness, an approach known as deep learning. We thus designed a deep SNN, comprising several convolutional (trainable with STDP) and pooling layers. We used a temporal coding scheme where the most strongly activated neurons fire first, and less activated neurons fire later or not at all. The network was exposed to natural images. Thanks to STDP, neurons progressively learned features corresponding to prototypical patterns that were both salient and frequent. Only a few tens of examples per category were required and no label was needed. After learning, the complexity of the extracted features increased along the hierarchy, from edge detectors in the first layer to object prototypes in the last layer. Coding was very sparse, with only a few thousands spikes per image, and in some cases the object category could be reasonably well inferred from the activity of a single higher-order neuron. More generally, the activity of a few hundreds of such neurons contained robust category information, as demonstrated using a classifier on Caltech 101, ETH-80, and MNIST databases. We think that the combination of STDP with latency coding is key to understanding the way that the primate visual system learns, its remarkable processing speed and its low energy consumption. These mechanisms are also interesting for artificial vision systems, particularly for hardware solutions. ",Wed Nov 23 16:04:54 2016 UTC,https://arxiv.org/pdf/1611.01421.pdf,44,
12,[R] [1611.05118] The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives,https://www.reddit.com/r/MachineLearning/comments/5ekc5x/r_161105118_the_amazing_mysteries_of_the_gutter/," Abstract: Visual narrative is often a combination of explicit information and judicious omissions, relying on the viewer to supply missing details. In comics, most movements in time and space are hidden in the ""gutters"" between panels. To follow the story, readers logically connect panels together by inferring unseen actions through a process called ""closure"". While computers can now describe the content of natural images, in this paper we examine whether they can understand the closure-driven narratives conveyed by stylized artwork and dialogue in comic book panels. We collect a dataset, COMICS, that consists of over 1.2 million panels (120 GB) paired with automatic textbox transcriptions. An in-depth analysis of COMICS demonstrates that neither text nor image alone can tell a comic book story, so a computer must understand both modalities to keep up with the plot. We introduce three cloze-style tasks that ask models to predict narrative and character-centric aspects of a panel given n preceding panels as context. Various deep neural architectures underperform human baselines on these tasks, suggesting that COMICS contains fundamental challenges for both vision and language. ",Wed Nov 23 22:52:12 2016 UTC,https://arxiv.org/abs/1611.05118,42,
20,[R][1611.07715] Deep Feature Flow for Video Recognition,https://www.reddit.com/r/MachineLearning/comments/5eryf7/r161107715_deep_feature_flow_for_video_recognition/," Abstract: Deep convolutional neutral networks have achieved great success on image recognition tasks. Yet, it is non-trivial to transfer the state-of-the-art image recognition networks to videos as per-frame evaluation is too slow and unaffordable. We present deep feature flow, a fast and accurate framework for video recognition. It runs the expensive convolutional sub-network only on sparse key frames and propagates their deep feature maps to other frames via a flow field. It achieves significant speedup as flow computation is relatively fast. The end-to-end training of the whole architecture significantly boosts the recognition accuracy. Deep feature flow is flexible and general. It is validated on two recent large scale video datasets. It makes a large step towards practical video recognition. ",Fri Nov 25 05:23:39 2016 UTC,https://arxiv.org/abs/1611.07715,32,
24,[R] [1611.06355] Invertible Conditional GANs for image editing (NIPS 2016 Workshop),https://www.reddit.com/r/MachineLearning/comments/5eacwb/r_161106355_invertible_conditional_gans_for_image/," Abstract: Generative Adversarial Networks (GANs) have recently demonstrated to successfully approximate complex data distributions. A relevant extension of this model is conditional GANs (cGANs), where the introduction of external information allows to determine specific representations of the generated images. In this work, we evaluate encoders to inverse the mapping of a cGAN, i.e., mapping a real image into a latent space and a conditional representation. This allows, for example, to reconstruct and modify real images of faces conditioning on arbitrary attributes. Additionally, we evaluate the design of cGANs. The combination of an encoder with a cGAN, which we call Invertible cGAN (IcGAN), enables to re-generate real images with deterministic complex modifications. ",Tue Nov 22 12:04:23 2016 UTC,https://arxiv.org/abs/1611.06355,56,
28,[R] PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection (NIPS 2016 Workshop),https://www.reddit.com/r/MachineLearning/comments/5dwlk8/r_pvanet_deep_but_lightweight_neural_networks_for/," Abstract: This paper presents how we can achieve the state-of-the-art accuracy in multi-category object detection task while minimizing the computational cost by adapting and combining recent technical innovations. Following the common pipeline of ""CNN feature extraction + region proposal + RoI classification"", we mainly redesign the feature extraction part, since region proposal part is not computationally expensive and classification part can be efficiently compressed with common techniques like truncated SVD. Our design principle is ""less channels with more layers"" and adoption of some building blocks including concatenated ReLU, Inception, and HyperNet. The designed network is deep and thin and trained with the help of batch normalization, residual connections, and learning rate scheduling based on plateau detection. We obtained solid results on well-known object detection benchmarks: 83.8% mAP (mean average precision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only 750ms/image on Intel i7-6700K CPU with a single core and 46ms/image on NVIDIA Titan X GPU. Theoretically, our network requires only 12.3% of the computational cost compared to ResNet-101, the winner on VOC2012. ",Sun Nov 20 05:07:29 2016 UTC,https://arxiv.org/abs/1608.08021,68,
12,[R] [1611.06080] A Generalized Stochastic Variational Bayesian Hyperparameter Learning Framework for Sparse Spectrum Gaussian Process Regression,https://www.reddit.com/r/MachineLearning/comments/5e4ahu/r_161106080_a_generalized_stochastic_variational/," Abstract: While much research effort has been dedicated to scaling up sparse Gaussian process (GP) models based on inducing variables for big data, little attention is afforded to the other less explored class of low-rank GP approximations that exploit the sparse spectral representation of a GP kernel. This paper presents such an effort to advance the state of the art of sparse spectrum GP models to achieve competitive predictive performance for massive datasets. Our generalized framework of stochastic variational Bayesian sparse spectrum GP (sVBSSGP) models addresses their shortcomings by adopting a Bayesian treatment of the spectral frequencies to avoid overfitting, modeling these frequencies jointly in its variational distribution to enable their interaction a posteriori, and exploiting local data for boosting the predictive performance. However, such structural improvements result in a variational lower bound that is intractable to be optimized. To resolve this, we exploit a variational parameterization trick to make it amenable to stochastic optimization. Interestingly, the resulting stochastic gradient has a linearly decomposable structure that can be exploited to refine our stochastic optimization method to incur constant time per iteration while preserving its property of being an unbiased estimator of the exact gradient of the variational lower bound. Empirical evaluation on real-world datasets shows that sVBSSGP outperforms state-of-the-art stochastic implementations of sparse GP models. ",Mon Nov 21 14:31:58 2016 UTC,https://arxiv.org/abs/1611.06080,64,
7,[Research][Ethics] Automated Inference on Criminality using Face Images,https://www.reddit.com/r/MachineLearning/comments/5eciuj/researchethics_automated_inference_on_criminality/," Abstract: We study, for the first time, automated inference on criminality based solely on still face images. Via supervised machine learning, we build four classifiers (logistic regression, KNN, SVM, CNN) using facial images of 1856 real persons controlled for race, gender, age and facial expressions, nearly half of whom were convicted criminals, for discriminating between criminals and non-criminals. All four classifiers perform consistently well and produce evidence for the validity of automated face-induced inference on criminality, despite the historical controversy surrounding the topic. Also, we find some discriminating structural features for predicting criminality, such as lip curvature, eye inner corner distance, and the so-called nose-mouth angle. Above all, the most important discovery of this research is that criminal and non-criminal face images populate two quite distinctive manifolds. The variation among criminal faces is significantly greater than that of the non-criminal faces. The two manifolds consisting of criminal and non-criminal faces appear to be concentric, with the non-criminal manifold lying in the kernel with a smaller span, exhibiting a law of normality for faces of non-criminals. In other words, the faces of general law-biding public have a greater degree of resemblance compared with the faces of criminals, or criminals have a higher degree of dissimilarity in facial appearance than normal people. ",Tue Nov 22 19:10:03 2016 UTC,https://arxiv.org/abs/1611.04135,58,
63,[R] Learning to reinforcement learn,https://www.reddit.com/r/MachineLearning/comments/5dm7yu/r_learning_to_reinforcement_learn/," Abstract: In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience. ",Fri Nov 18 12:51:08 2016 UTC,https://arxiv.org/abs/1611.05763,79,
6,[R] How to scale distributed deep learning?,https://www.reddit.com/r/MachineLearning/comments/5dcrrh/r_how_to_scale_distributed_deep_learning/," Abstract: Training time on large datasets for deep neural networks is the principal workflow bottleneck in a number of important applications of deep learning, such as object classification and detection in automatic driver assistance systems (ADAS). To minimize training time, the training of a deep neural network must be scaled beyond a single machine to as many machines as possible by distributing the optimization method used for training. While a number of approaches have been proposed for distributed stochastic gradient descent (SGD), at the current time synchronous approaches to distributed SGD appear to be showing the greatest performance at large scale. Synchronous scaling of SGD suffers from the need to synchronize all processors on each gradient step and is not resilient in the face of failing or lagging processors. In asynchronous approaches using parameter servers, training is slowed by contention to the parameter server. In this paper we compare the convergence of synchronous and asynchronous SGD for training a modern ResNet network architecture on the ImageNet classification problem. We also propose an asynchronous method, gossiping SGD, that aims to retain the positive features of both systems by replacing the all-reduce collective operation of synchronous training with a gossip aggregation algorithm. We find, perhaps counterintuitively, that asynchronous SGD, including both elastic averaging and gossiping, converges faster at fewer nodes (up to about 32 nodes), whereas synchronous SGD scales better to more nodes (up to about 100 nodes). ",Thu Nov 17 00:18:19 2016 UTC,https://arxiv.org/abs/1611.04581,100,
10,[R] Aggregated Residual Transformations for Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5ddb4g/r_aggregated_residual_transformations_for_deep/," Abstract: We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call ""cardinality"" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, codenamed ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. ",Thu Nov 17 02:09:50 2016 UTC,https://arxiv.org/abs/1611.05431,99,
29,[R] Reinforcement Learning with Unsupervised Auxiliary Tasks,https://www.reddit.com/r/MachineLearning/comments/5dds2l/r_reinforcement_learning_with_unsupervised/," Abstract: Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\% expert human performance, and a challenging suite of first-person, three-dimensional \emph{Labyrinth} tasks leading to a mean speedup in learning of 10$\times$ and averaging 87\% expert human performance on Labyrinth. ",Thu Nov 17 03:54:17 2016 UTC,https://arxiv.org/abs/1611.05397,97,
41,16x Faster RNN: QUASI-RECURRENT NEURAL NETWORKS,https://www.reddit.com/r/MachineLearning/comments/5dep6x/16x_faster_rnn_quasirecurrent_neural_networks/," Abstract: Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks. ",Thu Nov 17 08:05:00 2016 UTC,https://arxiv.org/pdf/1611.01576v1.pdf,93,
13,[Research][1611.05209] Deep Variational Inference Without Pixel-Wise Reconstruction,https://www.reddit.com/r/MachineLearning/comments/5dfzue/research161105209_deep_variational_inference/," Abstract: Variational autoencoders (VAEs), that are built upon deep neural networks have emerged as popular generative models in computer vision. Most of the work towards improving variational autoencoders has focused mainly on making the approximations to the posterior flexible and accurate, leading to tremendous progress. However, there have been limited efforts to replace pixel-wise reconstruction, which have known shortcomings. In this work, we use real-valued non-volume preserving transformations (real NVP) to exactly compute the conditional likelihood of the data given the latent distribution. We show that a simple VAE with this form of reconstruction is competitive with complicated VAE structures, on image modeling tasks. As part of our model, we develop powerful conditional coupling layers that enable real NVP to learn with fewer intermediate layers. ",Thu Nov 17 14:14:19 2016 UTC,https://arxiv.org/abs/1611.05209,92,
13,[R] Generative Image Modeling using Style and Structure Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/5dki5m/r_generative_image_modeling_using_style_and/," Abstract: Current generative frameworks use end-to-end learning and generate images by sampling from uniform noise distribution. However, these approaches ignore the most basic principle of image formation: images are product of: (a) Structure: the underlying 3D model; (b) Style: the texture mapped onto structure. In this paper, we factorize the image generation process and propose Style and Structure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has two components: the Structure-GAN generates a surface normal map; the Style-GAN takes the surface normal map as input and generates the 2D image. Apart from a real vs. generated loss function, we use an additional loss with computed surface normals from generated images. The two GANs are first trained independently, and then merged together via joint learning. We show our S^2-GAN model is interpretable, generates more realistic images and can be used to learn unsupervised RGBD representations. ",Fri Nov 18 04:17:25 2016 UTC,https://arxiv.org/abs/1603.05631,86,
6,Rapid and Accurate Image Super Resolution via ML,https://www.reddit.com/r/MachineLearning/comments/5d951g/rapid_and_accurate_image_super_resolution_via_ml/," Abstract: Given an image, we wish to produce an image of larger size with significantly more pixels and higher image quality. This is generally known as the Single Image Super-Resolution (SISR) problem. The idea is that with sufficient training data (corresponding pairs of low and high resolution images) we can learn set of filters (i.e. a mapping) that when applied to given image that is not in the training set, will produce a higher resolution version of it, where the learning is preferably low complexity. In our proposed approach, the run-time is more than one to two orders of magnitude faster than the best competing methods currently available, while producing results comparable or better than state-of-the-art. A closely related topic is image sharpening and contrast enhancement, i.e., improving the visual quality of a blurry image by amplifying the underlying details (a wide range of frequencies). Our approach additionally includes an extremely efficient way to produce an image that is significantly sharper than the input blurry one, without introducing artifacts such as halos and noise amplification. We illustrate how this effective sharpening algorithm, in addition to being of independent interest, can be used as a pre-processing step to induce the learning of more effective upscaling filters with built-in sharpening and contrast enhancement effect. ",Wed Nov 16 13:21:37 2016 UTC,https://arxiv.org/abs/1606.01299,108,
5,[Research] Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,https://www.reddit.com/r/MachineLearning/comments/5d5e4q/research_tying_word_vectors_and_word_classifiers/," Abstract: Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our LSTM model lowers the state of the art word-level perplexity on the Penn Treebank to 68.5. ",Tue Nov 15 22:03:02 2016 UTC,https://arxiv.org/abs/1611.01462,118,
6,[R] [1611.02344] A Convolutional Encoder Model for Neural Machine Translation,https://www.reddit.com/r/MachineLearning/comments/5d72in/r_161102344_a_convolutional_encoder_model_for/, Abstract: The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. In this paper we present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation. Our convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ,Wed Nov 16 03:32:53 2016 UTC,https://arxiv.org/abs/1611.02344,114,
5,[R] Finding a global minimizer of an unknown (black-box) loss function,https://www.reddit.com/r/MachineLearning/comments/5dldao/r_finding_a_global_minimizer_of_an_unknown/," Abstract: We present a learning to learn approach for training recurrent neural networks to perform black-box global optimization. In the meta-learning phase we use a large set of smooth target functions to learn a recurrent neural network (RNN) optimizer, which is either a long-short term memory network or a differentiable neural computer. After learning, the RNN can be applied to learn policies in reinforcement learning, as well as other black-box learning tasks, including continuous correlated bandits and experimental design. We compare this approach to Bayesian optimization, with emphasis on the issues of computation speed, horizon length, and exploration-exploitation trade-offs. ",Fri Nov 18 08:19:23 2016 UTC,http://arxiv.org/abs/1611.03824,88,0
57,[R] Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation,https://www.reddit.com/r/MachineLearning/comments/5d05ay/r_googles_multilingual_neural_machine_translation/," Abstract: We propose a simple, elegant solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English$\rightarrow$French and surpasses state-of-the-art results for English$\rightarrow$German. Similarly, a single multilingual model surpasses state-of-the-art results for French$\rightarrow$English and German$\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages. ",Tue Nov 15 02:39:21 2016 UTC,https://arxiv.org/abs/1611.04558,129,
7,[R][1611.01260] Learning Identity Mappings with Residual Gates,https://www.reddit.com/r/MachineLearning/comments/5cxukf/r161101260_learning_identity_mappings_with/," Abstract: We propose a technique to augment network layers by adding a linear gating mechanism, which provides a way to learn identity mappings by optimizing only one parameter. We also introduce a new metric which served as basis for the technique. It captures the difficulty involved in learning identity mappings for different types of network models, and provides a new theoretical intuition for the increased depths of models such as Highway and Residual Networks. We propose a new model, the Gated Residual Network, which is the result when augmenting Residual Networks. Experimental results show that augmenting layers grants increased performance, less issues with depth, and more layer independence -- fully removing them does not cripple the model. We evaluate our method on MNIST using fully-connected networks and on CIFAR-10 using Wide ResNets, achieving a relative error reduction of more than 8% in the latter when compared to the original model. ",Mon Nov 14 19:31:58 2016 UTC,https://arxiv.org/abs/1611.01260,141,
23,[R] [1611.03530] Understanding deep learning requires rethinking generalization,https://www.reddit.com/r/MachineLearning/comments/5cw3lr/r_161103530_understanding_deep_learning_requires/," Abstract: Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models. ",Mon Nov 14 14:07:56 2016 UTC,https://arxiv.org/abs/1611.03530,140,
33,[R] DeepCoder: Learning to Write Programs,https://www.reddit.com/r/MachineLearning/comments/5cvwfh/r_deepcoder_learning_to_write_programs/," Abstract: We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites. ",Mon Nov 14 13:20:56 2016 UTC,https://arxiv.org/abs/1611.01989,136,
9,[R] Learning to Perform Physics Experiments via Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/5d1gm6/r_learning_to_perform_physics_experiments_via/," Abstract: When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations. ",Tue Nov 15 08:14:53 2016 UTC,https://arxiv.org/abs/1611.01843,131,
1,Safety Verification of Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5csffq/safety_verification_of_deep_neural_networks/," Abstract: Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it. With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety. We develop the first SMT-based automated verification framework for feed-forward multi-layer neural networks that works directly with the code of the network, exploring it layer by layer. We define safety for a region around a data point in a given layer by requiring that all points in the region are assigned the same class label. Working with a notion of a manipulation, a mapping between points that intuitively corresponds to a modification of an image, we employ discretisation to enable exhaustive search of the region. Our method can guarantee that adversarial examples are found for the given region and set of manipulations. If found, adversarial examples can be shown to human testers and/or used to fine-tune the network, and otherwise the network is declared safe for the given parameters. We implement the techniques using Z3 and evaluate them on state-of-the-art networks, including regularised and deep learning networks. ",Sun Nov 13 22:13:55 2016 UTC,https://arxiv.org/abs/1610.06940,151,
6,[R] Discovering Blind Spots of Predictive Models: Representations and Policies for Guided Exploration,https://www.reddit.com/r/MachineLearning/comments/5cetkk/r_discovering_blind_spots_of_predictive_models/," Abstract: Predictive models deployed in the world may assign incorrect labels to instances with high confidence. Such errors or unknown unknowns are rooted in model incompleteness, and typically arise because of the mismatch between training data and the cases seen in the open world. As the models are blind to such errors, input from an oracle is needed to identify these failures. In this paper, we formulate and address the problem of optimizing the discovery of unknown unknowns of any predictive model under a fixed budget, which limits the number of times an oracle can be queried for true labels. We propose a model-agnostic methodology which uses feedback from an oracle to both identify unknown unknowns and to intelligently guide the discovery. We employ a two-phase approach which first organizes the data into multiple partitions based on instance similarity, and then utilizes an explore-exploit strategy for discovering unknown unknowns across these partitions. We demonstrate the efficacy of our framework by varying the underlying causes of unknown unknowns across various applications. To the best of our knowledge, this paper presents the first algorithmic approach to the problem of discovering unknown unknowns of predictive models. ",Fri Nov 11 14:47:48 2016 UTC,https://arxiv.org/abs/1610.09064,164,
39,[R] [1611.03214] Ultimate tensorization: compressing convolutional and FC layers alike,https://www.reddit.com/r/MachineLearning/comments/5cdt6o/r_161103214_ultimate_tensorization_compressing/," Abstract: Convolutional neural networks excel in image recognition tasks, but this comes at the cost of high computational and memory complexity. To tackle this problem, [1] developed a tensor factorization framework to compress fully-connected layers. In this paper, we focus on compressing convolutional layers. We show that while the direct application of the tensor framework [1] to the 4-dimensional kernel of convolution does compress the layer, we can do better. We reshape the convolutional kernel into a tensor of higher order and factorize it. We combine the proposed approach with the previous work to compress both convolutional and fully-connected layers of a network and achieve 80x network compression rate with 1.1% accuracy drop on the CIFAR-10 dataset. ",Fri Nov 11 10:18:49 2016 UTC,https://arxiv.org/abs/1611.03214,160,
11,[R] Low-effort place recognition with WiFi fingerprints using deep learning,https://www.reddit.com/r/MachineLearning/comments/5c7c9i/r_loweffort_place_recognition_with_wifi/," Abstract: Using WiFi signals for indoor localization is the main localization modality of the existing personal indoor localization systems operating on mobile devices. WiFi fingerprinting is also used for mobile robots, as WiFi signals are usually available indoors and can provide rough initial position estimate or can be used together with other positioning systems. Currently, the best solutions rely on filtering, manual data analysis, and time-consuming parameter tuning to achieve reliable and accurate localization. In this work, we propose to use deep neural networks to significantly lower the work-force burden of the localization system design, while still achieving satisfactory results. Assuming the state-of-the-art hierarchical approach, we employ the DNN system for building/floor classification. We show that stacked autoencoders allow to efficiently reduce the feature space in order to achieve robust and precise classification. The proposed architecture is verified on the publicly available UJIIndoorLoc dataset and the results are compared with other solutions. ",Thu Nov 10 11:20:34 2016 UTC,https://arxiv.org/abs/1611.02049,171,
12,[R][1611.02554] The Neural Noisy Channel,https://www.reddit.com/r/MachineLearning/comments/5c77r4/r161102554_the_neural_noisy_channel/," Abstract: We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use. ",Thu Nov 10 10:39:51 2016 UTC,https://arxiv.org/abs/1611.02554,170,
3,[R] Unsupervised Cross-Domain Image Generation,https://www.reddit.com/r/MachineLearning/comments/5bwf02/r_unsupervised_crossdomain_image_generation/," Abstract: We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity. ",Tue Nov 8 22:23:51 2016 UTC,https://arxiv.org/abs/1611.02200v1,193,
23,[R]Entropy-SGD: Biasing Gradient Descent Into Wide Valleys,https://www.reddit.com/r/MachineLearning/comments/5bqzu2/rentropysgd_biasing_gradient_descent_into_wide/," Abstract: This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape at solutions found by gradient descent. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local entropy based objective that favors well-generalizable solutions lying in the flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Our algorithm resembles two nested loops of SGD, where we use Langevin dynamics to compute the gradient of local entropy at each update of the weights. We prove that incorporating local entropy into the objective function results in a smoother energy landscape and use uniform stability to show improved generalization bounds over SGD. Our experiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training. ",Tue Nov 8 03:40:52 2016 UTC,http://arxiv.org/abs/1611.01838,197,
29,[Research] Improving Stochastic Gradient Descent with Feedback,https://www.reddit.com/r/MachineLearning/comments/5bsmvn/research_improving_stochastic_gradient_descent/," Abstract: In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process. ",Tue Nov 8 11:14:12 2016 UTC,http://arxiv.org/abs/1611.01505,194,
22,[R][1611.01491] Understanding Deep Neural Networks with Rectified Linear Units,https://www.reddit.com/r/MachineLearning/comments/5bmpkw/r161101491_understanding_deep_neural_networks/," Abstract: In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give the first-ever polynomial (in data size and circuit size) time algorithm to train a ReLU DNN with one hidden layer and a single input to global optimality. This follows from our complete characterization of the ReLU DNN function class whereby we show that a $\mathbb{R}^n \to \mathbb{R}$ function is representable by a ReLU DNN if and only if it is a continuous piecewise linear function. The main tool used to prove this characterization is an elegant result from tropical geometry. Further, for the $n=1$ case, we show that a single hidden layer suffices to express all piecewise linear functions, and we give tight bounds for the size of such a ReLU DNN.We follow up with gap results showing that there is a smoothly parameterized family of $\mathbb{R}\to \mathbb{R}$ ""hard"" functions that lead to an exponential blow-up in size, if the number of layers is decreased by a small amount. An example consequence of our gap theorem is that for every natural number $N$, there exists a function representable by a ReLU DNN with depth $N^2+1$ and total size $N^3$, such that any ReLU DNN with depth at most $N + 1$ will require at least $\frac12N^{N+1}-1$ total nodes. Finally, we construct a family of $\mathbb{R}^n\to \mathbb{R}$ functions for $n\geq 2$ (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension $n$ at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on $n$ has not been achieved by previous families of ""hard"" functions in the neural nets literature. ",Mon Nov 7 15:23:23 2016 UTC,https://arxiv.org/abs/1611.01491,204,
27,[R] Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences,https://www.reddit.com/r/MachineLearning/comments/5bmfw5/r_phased_lstm_accelerating_recurrent_network/," Abstract: Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime. ",Mon Nov 7 14:33:08 2016 UTC,http://arxiv.org/abs/1610.09513,203,
48,[R][1611.01146] Finding Local Minima for Nonconvex Optimization in Linear Time,https://www.reddit.com/r/MachineLearning/comments/5b0rl3/r161101146_finding_local_minima_for_nonconvex/, Abstract: We design a non-convex second-order optimization algorithm that is guaranteed to return an approximate local minimum in time which is linear in the input representation. The time complexity of our algorithm to find an approximate local minimum is even faster than that of gradient descent to find a critical point. Our algorithm applies to a general class of optimization problems including training a neural network and other non-convex objectives arising in machine learning. ,Fri Nov 4 01:12:07 2016 UTC,https://arxiv.org/abs/1611.01146,242,
3,[R][1611.01142] Using a Deep Reinforcement Learning Agent for Traffic Signal Control,https://www.reddit.com/r/MachineLearning/comments/5b31z9/r161101142_using_a_deep_reinforcement_learning/," Abstract: Ensuring transportation systems are efficient is a priority for modern society. Technological advances have made it possible for transportation systems to collect large volumes of varied data on an unprecedented scale. We propose a traffic signal control system which takes advantage of this new, high quality data, with minimal abstraction compared to other proposed systems. We apply modern deep reinforcement learning methods to build a truly adaptive traffic signal control agent in the traffic microsimulator SUMO. We propose a new state space, the discrete traffic state encoding, which is information dense. The discrete traffic state encoding is used as input to a deep convolutional neural network, trained using Q-learning with experience replay. Our agent was compared against a one hidden layer neural network traffic signal control agent and reduces average cumulative delay by 82%, average queue length by 66% and average travel time by 20%. ",Fri Nov 4 11:40:20 2016 UTC,https://arxiv.org/abs/1611.01142,245,
25,[R] [1611.01144] Categorical Reparameterization with Gumbel-Softmax,https://www.reddit.com/r/MachineLearning/comments/5b1cwf/r_161101144_categorical_reparameterization_with/," Abstract: Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification. ",Fri Nov 4 03:15:56 2016 UTC,https://arxiv.org/abs/1611.01144,243,
18,[R] [1606.02492v3] Convolutional Neural Fabrics,https://www.reddit.com/r/MachineLearning/comments/5avy16/r_160602492v3_convolutional_neural_fabrics/," Abstract: Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a ""fabric"" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset. ",Thu Nov 3 10:22:25 2016 UTC,https://arxiv.org/abs/1606.02492v3,256,
10,[R][1611.00328] The Chi-Divergence for Approximate Inference,https://www.reddit.com/r/MachineLearning/comments/5an91t/r161100328_the_chidivergence_for_approximate/," Abstract: Variational inference enables Bayesian analysis for complex probabilistic models with massive data sets. It works by positing a family of distributions and finding the member in the family that is closest to the posterior. While successful, variational methods can run into pathologies; for example, they typically underestimate posterior uncertainty. We propose CHI-VI, a complementary algorithm to traditional variational inference with KL($q$ || $p$) and an alternative algorithm to EP. CHI-VI is a black box algorithm that minimizes the $\chi$-divergence from the posterior to the family of approximating distributions. In EP, only local minimization of the KL($p$ || $q$) objective is possible. In contrast, CHI-VI optimizes a well-defined global objective. It directly minimizes an upper bound to the model evidence that equivalently minimizes the $\chi$-divergence. In experiments, we illustrate the utility of the upper bound for sandwich estimating the model evidence. We also compare several probabilistic models and a Cox process for basketball data. We find CHI-VI often yields better classification error rates and better posterior uncertainty. ",Wed Nov 2 01:44:01 2016 UTC,https://arxiv.org/abs/1611.00328,275,
16,[R] [1611.00020] Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision,https://www.reddit.com/r/MachineLearning/comments/5aoscn/r_161100020_neural_symbolic_machines_learning/," Abstract: Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural ""programmer"", and a non-differentiable ""computer"" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge. ",Wed Nov 2 08:28:42 2016 UTC,https://arxiv.org/abs/1611.00020,269,
29,[R] [1610.09615] Compressed Learning: A Deep Neural Network Approach,https://www.reddit.com/r/MachineLearning/comments/5ar80g/r_161009615_compressed_learning_a_deep_neural/," Abstract: Compressed Learning (CL) is a joint signal processing and machine learning framework for inference from a signal, using a small number of measurements obtained by linear projections of the signal. In this paper we present an end-to-end deep learning approach for CL, in which a network composed of fully-connected layers followed by convolutional layers perform the linear sensing and non-linear inference stages. During the training phase, the sensing matrix and the non-linear inference operator are jointly optimized, and the proposed approach outperforms state-of-the-art for the task of image classification. For example, at a sensing rate of 1% (only 8 measurements of 28 X 28 pixels images), the classification error for the MNIST handwritten digits dataset is 6.46% compared to 41.06% with state-of-the-art. ",Wed Nov 2 17:23:21 2016 UTC,https://arxiv.org/abs/1610.09615,263,
16,[R] [1610.02995] Extrapolation and learning equations,https://www.reddit.com/r/MachineLearning/comments/5av8hh/r_161002995_extrapolation_and_learning_equations/," Abstract: In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified. ",Thu Nov 3 06:19:17 2016 UTC,https://arxiv.org/abs/1610.02995,261,
44,[R][1611.00712] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables,https://www.reddit.com/r/MachineLearning/comments/5aubhn/r161100712_the_concrete_distribution_a_continuous/," Abstract: The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack continuous reparameterizations due to the discontinuous nature of discrete states. In this work we introduce concrete random variables -- continuous relaxations of discrete random variables. The concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate effectiveness of concrete relaxations on density estimation and structured prediction tasks using neural networks. ",Thu Nov 3 02:37:02 2016 UTC,https://arxiv.org/abs/1611.00712,259,
66,[Research] [1610.10099] Neural Machine Translation in Linear Time,https://www.reddit.com/r/MachineLearning/comments/5agopr/research_161010099_neural_machine_translation_in/," Abstract: We present a neural architecture for sequence processing. The ByteNet is a stack of two dilated convolutional neural networks, one to encode the source sequence and one to decode the target sequence, where the target network unfolds dynamically to generate variable length outputs. The ByteNet has two core properties: it runs in time that is linear in the length of the sequences and it preserves the sequences' temporal resolution. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent neural networks. The ByteNet also achieves a performance on raw character-level machine translation that approaches that of the best neural translation models that run in quadratic time. The implicit structure learnt by the ByteNet mirrors the expected alignments between the sequences. ",Tue Nov 1 02:20:59 2016 UTC,https://arxiv.org/abs/1610.10099,278,
4,[R][arXiv:1610.06998] Ranking of classification algorithms in terms of mean-standard deviation using A-TOPSIS,https://www.reddit.com/r/MachineLearning/comments/5a6xv7/rarxiv161006998_ranking_of_classification/," Abstract: In classification problems when multiples algorithms are applied to different benchmarks a difficult issue arises, i.e., how can we rank the algorithms? In machine learning it is common run the algorithms several times and then a statistic is calculated in terms of means and standard deviations. In order to compare the performance of the algorithms, it is very common to employ statistical tests. However, these tests may also present limitations, since they consider only the means and not the standard deviations of the obtained results. In this paper, we present the so called A-TOPSIS, based on TOPSIS (Technique for Order Preference by Similarity to Ideal Solution), to solve the problem of ranking and comparing classification algorithms in terms of means and standard deviations. We use two case studies to illustrate the A-TOPSIS for ranking classification algorithms and the results show the suitability of A-TOPSIS to rank the algorithms. The presented approach is general and can be applied to compare the performance of stochastic algorithms in machine learning. Finally, to encourage researchers to use the A-TOPSIS for ranking algorithms we also presented in this work an easy-to-use A-TOPSIS web framework. ",Sun Oct 30 16:54:50 2016 UTC,https://arxiv.org/ftp/arxiv/papers/1610/1610.06998.pdf,298,
17,[R] [1610.09296] Improving Sampling from Generative Autoencoders with Markov Chains,https://www.reddit.com/r/MachineLearning/comments/5a9dmy/r_161009296_improving_sampling_from_generative/," Abstract: We focus on generative autoencoders, such as variational or adversarial autoencoders, which jointly learn a generative model alongside an inference model. Generative autoencoders are those which are trained to softly enforce a prior on the latent distribution learned by the inference model. However, the inference model may not always map inputs to latent samples that are consistent with the prior. We formulate a Markov chain Monte Carlo (MCMC) sampling process, equivalent to iteratively encoding and decoding, which allows us to sample from the learned latent distribution. Using this, we can improve the quality of samples drawn from the model, especially when the learned distribution is far from the prior. Using MCMC sampling, we are able to reveal previously unseen differences between generative autoencoders trained either with or without a denoising criterion. ",Mon Oct 31 00:34:23 2016 UTC,https://arxiv.org/abs/1610.09296,295,
42,[1610.09038] Professor Forcing: A New Algorithm for Training Recurrent Networks,https://www.reddit.com/r/MachineLearning/comments/5a9zle/161009038_professor_forcing_a_new_algorithm_for/," Abstract: The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar. ",Mon Oct 31 02:43:28 2016 UTC,https://arxiv.org/abs/1610.09038,292,
24,[Research] [1610.09033] Operator Variational Inference,https://www.reddit.com/r/MachineLearning/comments/5ab2zg/research_161009033_operator_variational_inference/," Abstract: Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images. ",Mon Oct 31 07:52:46 2016 UTC,https://arxiv.org/abs/1610.09033,291,
53,[R][1610.09027] Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes [DeepMind],https://www.reddit.com/r/MachineLearning/comments/5abcd4/r161009027_scaling_memoryaugmented_neural/," Abstract: Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows --- limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs $1,\!000\times$ faster and with $3,\!000\times$ less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring $100,\!000$s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer. ",Mon Oct 31 09:32:45 2016 UTC,https://arxiv.org/abs/1610.09027,287,
20,[Research] [1610.09585] Conditional Image Synthesis With Auxiliary Classifier GANs,https://www.reddit.com/r/MachineLearning/comments/5ahi35/research_161009585_conditional_image_synthesis/," Abstract: Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data. ",Tue Nov 1 05:36:30 2016 UTC,https://arxiv.org/abs/1610.09585,280,
4,[R] [NIPS:ArXiv:1608.04042] Can Peripheral Representations Improve Clutter Metrics on Complex Scenes?,https://www.reddit.com/r/MachineLearning/comments/5a3a5o/r_nipsarxiv160804042_can_peripheral/," Abstract: Previous studies have proposed image-based clutter measures that correlate with human search times and/or eye movements. However, most models do not take into account the fact that the effects of clutter interact with the foveated nature of the human visual system: visual clutter further from the fovea has an increasing detrimental influence on perception. Here, we introduce a new foveated clutter model to predict the detrimental effects in target search utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz et al.) as our non foveated clutter model, and we stack a peripheral architecture on top of Feature Congestion for our foveated model. We introduce the Peripheral Integration Feature Congestion (PIFC) coefficient, as a fundamental ingredient of our model that modulates clutter as a non-linear gain contingent on eccentricity. We finally show that Foveated Feature Congestion (FFC) clutter scores r(44) = -0.82 correlate better with target detection (hit rate) than regular Feature Congestion r(44) = -0.19 in forced fixation search. Thus, our model allows us to enrich clutter perception research by computing fixation specific clutter maps. A toolbox for creating peripheral architectures: Piranhas: Peripheral Architectures for Natural, Hybrid and Artificial Systems will be made available. ",Sat Oct 29 23:25:32 2016 UTC,https://arxiv.org/pdf/1608.04042v1.pdf,301,
2,[arXiv:1610.07448] A Framework for Parallel and Distributed Training of Neural Networks,https://www.reddit.com/r/MachineLearning/comments/59nkaa/arxiv161007448_a_framework_for_parallel_and/," Abstract: The aim of this paper is to develop a general framework for training neural networks (NNs) in a distributed environment, where training data is partitioned over a set of agents that communicate with each other through a sparse, possibly time-varying, connectivity pattern. In such distributed scenario, the training problem can be formulated as the (regularized) optimization of a non-convex social cost function, given by the sum of local (non-convex) costs, where each agent contributes with a single error term defined with respect to its local dataset. To devise a flexible and efficient solution, we customize a recently proposed framework for non-convex optimization over networks, which hinges on a (primal) convexification-decomposition technique to handle non-convexity, and a dynamic consensus procedure to diffuse information among the agents. Several typical choices for the training criterion (e.g., squared loss, cross entropy, etc.) and regularization (e.g., $\ell_2$ norm, sparsity inducing penalties, etc.) are included in the framework and explored along the paper. Convergence to a stationary solution of the social non-convex problem is guaranteed under mild assumptions. Additionally, we show a principled way allowing each agent to exploit a multi-core architecture (e.g., a local cloud) in order to parallelize its local optimization step, resulting in strategies that are both distributed (across the agents) and parallel (inside each agent) in nature. A comprehensive set of experimental results validate the proposed approach. ",Thu Oct 27 11:57:36 2016 UTC,https://arxiv.org/abs/1610.07448,321,
18,[R] [1610.06402] A Growing Long-term Episodic & Semantic Memory,https://www.reddit.com/r/MachineLearning/comments/59obye/r_161006402_a_growing_longterm_episodic_semantic/," Abstract: The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this bounds the total amount of knowledge that can be learned and stored. Though this is not normally a problem for a neural network designed for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. To address this, we describe a lifelong learning system that leverages a fast, though non-differentiable, content-addressable memory which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge over many episodes for an unbounded number of domains. This opens the door for investigation into transfer learning, and leveraging prior knowledge that has been learned over a lifetime of experiences to new domains. ",Thu Oct 27 14:42:56 2016 UTC,https://arxiv.org/abs/1610.06402,317,
16,[Research] [1610.08123] Socratic Learning,https://www.reddit.com/r/MachineLearning/comments/59oxwn/research_161008123_socratic_learning/," Abstract: Modern machine learning techniques, such as deep learning, often use discriminative models that require large amounts of labeled data. An alternative approach is to use a generative model, which leverages heuristics from domain experts to train on unlabeled data. Domain experts often prefer to use generative models because they ""tell a story"" about their data. Unfortunately, generative models are typically less accurate than discriminative models. Several recent approaches combine both types of model to exploit their strengths. In this setting, a misspecified generative model can hurt the performance of subsequent discriminative training. To address this issue, we propose a framework called Socratic learning that automatically uses information from the discriminative model to correct generative model misspecification. Furthermore, this process provides users with interpretable feedback about how to improve their generative model. We evaluate Socratic learning on real-world relation extraction tasks and observe an immediate improvement in classification accuracy that could otherwise require several weeks of effort by domain experts. ",Thu Oct 27 16:31:15 2016 UTC,https://arxiv.org/abs/1610.08123,316,
4,[R] [1610.07675] Surprisal-Driven Zoneout,https://www.reddit.com/r/MachineLearning/comments/59j7t4/r_161007675_surprisaldriven_zoneout/," Abstract: We propose a novel method of regularization for recurrent neural networks called suprisal-driven zoneout. In this method, states zoneout (maintain their previous value rather than updating), when the suprisal (discrepancy between the last state's prediction and target) is small. Thus regularization is adaptive and input-driven on a per-neuron basis. We demonstrate the effectiveness of this idea by achieving state-of-the-art bits per character of 1.31 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to the best known highly-engineered compression methods. ",Wed Oct 26 18:33:04 2016 UTC,https://arxiv.org/abs/1610.07675,325,
39,[1610.07629] A Learned Representation For Artistic Style,https://www.reddit.com/r/MachineLearning/comments/59iiz9/161007629_a_learned_representation_for_artistic/," Abstract: The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style. ",Wed Oct 26 16:35:44 2016 UTC,https://arxiv.org/abs/1610.07629,323,
3,[R] [1610.08613] Can Active Memory Replace Attention?,https://www.reddit.com/r/MachineLearning/comments/59stao/r_161008613_can_active_memory_replace_attention/," Abstract: Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice. ",Fri Oct 28 05:16:08 2016 UTC,https://arxiv.org/abs/1610.08613,315,
7,[Research] Learning to Reason With Adaptive Computation,https://www.reddit.com/r/MachineLearning/comments/59sfz8/research_learning_to_reason_with_adaptive/," Abstract: Multi-hop inference is necessary for machine learning systems to successfully solve tasks such as Recognising Textual Entailment and Machine Reading. In this work, we demonstrate the effectiveness of adaptive computation for learning the number of inference steps required for examples of different complexity and that learning the correct number of inference steps is difficult. We introduce the first model involving Adaptive Computation Time which provides a small performance benefit on top of a similar model without an adaptive component as well as enabling considerable insight into the reasoning process of the model. ",Fri Oct 28 03:40:22 2016 UTC,https://arxiv.org/abs/1610.07647,314,
34,[R] [1610.06918] Learning to Protect Communications with Adversarial Neural Cryptography,https://www.reddit.com/r/MachineLearning/comments/59v9ua/r_161006918_learning_to_protect_communications/," Abstract: We ask whether neural networks can learn to use secret keys to protect information from other neural networks. Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary. Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks; instead, we train end-to-end, adversarially. We demonstrate that the neural networks can learn how to perform forms of encryption and decryption, and also how to apply these operations selectively in order to meet confidentiality goals. ",Fri Oct 28 16:12:57 2016 UTC,https://arxiv.org/abs/1610.06918,306,
33,[R] [1609.04309] Efficient softmax approximation for GPUs (Facebook AI Research),https://www.reddit.com/r/MachineLearning/comments/59dme6/r_160904309_efficient_softmax_approximation_for/," Abstract: We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. ",Tue Oct 25 21:16:00 2016 UTC,https://arxiv.org/abs/1609.04309,327,
62,[Research] The small universal perturbation perturbation causes the image to be misclassified,https://www.reddit.com/r/MachineLearning/comments/59yug9/research_the_small_universal_perturbation/," Abstract: Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images. ",Sat Oct 29 04:12:19 2016 UTC,http://arxiv.org/abs/1610.08401,303,
1,Stochastic Gradient MCMC with Stale Gradients,https://www.reddit.com/r/MachineLearning/comments/59f9w3/stochastic_gradient_mcmc_with_stale_gradients/," Abstract: Stochastic gradient MCMC (SG-MCMC) has played an important role in large-scale Bayesian learning, with well-developed theoretical convergence properties. In such applications of SG-MCMC, it is becoming increasingly popular to employ distributed systems, where stochastic gradients are computed based on some outdated parameters, yielding what are termed stale gradients. While stale gradients could be directly used in SG-MCMC, their impact on convergence properties has not been well studied. In this paper we develop theory to show that while the bias and MSE of an SG-MCMC algorithm depend on the staleness of stochastic gradients, its estimation variance (relative to the expected estimate, based on a prescribed number of samples) is independent of it. In a simple Bayesian distributed system with SG-MCMC, where stale gradients are computed asynchronously by a set of workers, our theory indicates a linear speedup on the decrease of estimation variance w.r.t. the number of workers. Experiments on synthetic data and deep neural networks validate our theory, demonstrating the effectiveness and scalability of SG-MCMC with stale gradients. ",Wed Oct 26 02:50:34 2016 UTC,http://arxiv.org/abs/1610.06664,335,
16,[R] Visual Explanations from Deep Networks via Gradient-based Localization,https://www.reddit.com/r/MachineLearning/comments/58xmnt/r_visual_explanations_from_deep_networks_via/," Abstract: We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing the regions of input that are ""important"" for predictions from these models - or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. Grad-CAM is a strict generalization of the Class Activation Mapping. Unlike CAM, Grad-CAM requires no re-training and is broadly applicable to any CNN-based architectures. We also show how Grad-CAM may be combined with existing pixel-space visualizations to create a high-resolution class-discriminative visualization (Guided Grad-CAM). We generate Grad-CAM and Guided Grad-CAM visual explanations to better understand image classification, image captioning, and visual question answering (VQA) models. In the context of image classification models, our visualizations (a) lend insight into their failure modes showing that seemingly unreasonable predictions have reasonable explanations, and (b) outperform pixel-space gradient visualizations (Guided Backpropagation and Deconvolution) on the ILSVRC-15 weakly supervised localization task. For image captioning and VQA, our visualizations expose the somewhat surprising insight that common CNN + LSTM models can often be good at localizing discriminative input image regions despite not being trained on grounded image-text pairs. Finally, we design and conduct human studies to measure if Guided Grad-CAM explanations help users establish trust in the predictions made by deep networks. Interestingly, we show that Guided Grad-CAM helps untrained users successfully discern a ""stronger"" deep network from a ""weaker"" one even when both networks make identical predictions. ",Sun Oct 23 07:44:00 2016 UTC,https://arxiv.org/abs/1610.02391,358,
28,[R] Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation,https://www.reddit.com/r/MachineLearning/comments/58kdpo/r_equilibrium_propagation_bridging_the_gap/," Abstract: We introduce Equilibrium Propagation (e-prop), a learning algorithm for energy-based models. This algorithm involves only one kind of neural computation both for the first phase (when the prediction is made) and the second phase (after the target is revealed) of training. Contrary to backpropagation in feedforward networks, there is no need for special computation in the second phase of our learning algorithm. Equilibrium Propagation combines features of Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: the algorithm computes the exact gradient of a well defined objective function. Because the objective function is defined in terms of local perturbations, the second phase of e-prop corresponds to only nudging the first-phase fixed point towards a configuration that has lower cost value. In the case of a multi-layer supervised neural network, the output units are slightly nudged towards their target, and the perturbation introduced at the output layer propagates backward in the network. The theory developed in this paper shows that the signal 'back-propagated' during this second phase actually contains information about the error derivatives, which we use to implement a learning rule proved to perform gradient descent with respect to the objective function. Thus, this work makes it more plausible that a mechanism similar to backpropagation could be implemented by brains. ",Fri Oct 21 00:23:10 2016 UTC,https://arxiv.org/abs/1602.05179,375,
18,[1610.06454v1] Reasoning with Memory Augmented Neural Networks for Language Comprehension,https://www.reddit.com/r/MachineLearning/comments/58n9hl/161006454v1_reasoning_with_memory_augmented/," Abstract: Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets. ",Fri Oct 21 13:41:34 2016 UTC,https://arxiv.org/abs/1610.06454v1,372,
44,[Research][1610.06258] Using Fast Weights to Attend to the Recent Past,https://www.reddit.com/r/MachineLearning/comments/58qjiw/research161006258_using_fast_weights_to_attend_to/," Abstract: Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These ""fast weights"" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns. ",Sat Oct 22 00:11:32 2016 UTC,https://arxiv.org/abs/1610.06258,367,
13,[Research] [1610.05683] Rejection Sampling Variational Inference,https://www.reddit.com/r/MachineLearning/comments/58fm3j/research_161005683_rejection_sampling_variational/," Abstract: Variational inference using the reparameterization trick has enabled large-scale approximate Bayesian inference in complex probabilistic models, leveraging stochastic optimization to sidestep intractable expectations. The reparameterization trick is applicable when we can simulate a random variable by applying a (differentiable) deterministic function on an auxiliary random variable whose distribution is fixed. For many distributions of interest (such as the gamma or Dirichlet), simulation of random variables relies on rejection sampling. The discontinuity introduced by the accept--reject step means that standard reparameterization tricks are not applicable. We propose a new method that lets us leverage reparameterization gradients even when variables are outputs of a rejection sampling algorithm. Our approach enables reparameterization on a larger class of variational distributions. In several studies of real and synthetic data, we show that the variance of the estimator of the gradient is significantly lower than other state-of-the-art methods. This leads to faster convergence of stochastic optimization variational inference. ",Thu Oct 20 07:22:35 2016 UTC,https://arxiv.org/abs/1610.05683,379,
14,[R] Learning in Implicit Generative Models,https://www.reddit.com/r/MachineLearning/comments/585wnd/r_learning_in_implicit_generative_models/," Abstract: Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination. ",Tue Oct 18 19:47:24 2016 UTC,https://arxiv.org/abs/1610.03483,394,
46,[R] Achieving Human Parity in Conversational Speech Recognition,https://www.reddit.com/r/MachineLearning/comments/58414p/r_achieving_human_parity_in_conversational_speech/," Abstract: Conversational speech recognition has served as a flagship speech recognition task since the release of the DARPA Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find that our latest automated system has reached human parity. The error rate of professional transcriptionists is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion where friends and family members have open-ended conversations. In both cases, our automated system establishes a new state-of-the-art, and edges past the human benchmark. This marks the first time that human parity has been reported for conversational speech. The key to our system's performance is the systematic use of convolutional and LSTM neural networks, combined with a novel spatial smoothing method and lattice-free MMI acoustic training. ",Tue Oct 18 14:21:11 2016 UTC,https://arxiv.org/abs/1610.05256,392,
6,[R] Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data,https://www.reddit.com/r/MachineLearning/comments/58e2o2/r_semisupervised_knowledge_transfer_for_deep/," Abstract: Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data. The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as ""teachers"" for a ""student"" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning. ",Thu Oct 20 01:14:25 2016 UTC,https://arxiv.org/abs/1610.05755,384,
27,[R] [1610.04490] Amortised MAP Inference for Image Super-Resolution: Connects GANs to MAP and Variational inference,https://www.reddit.com/r/MachineLearning/comments/57xaem/r_161004490_amortised_map_inference_for_image/," Abstract: Image Super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data, achieving particularly good results in photo-realistic texture SR. ",Mon Oct 17 13:23:44 2016 UTC,https://arxiv.org/abs/1610.04490,403,
19,[R] Temporal Ensembling for Semi-Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/57rq9f/r_temporal_ensembling_for_semisupervised_learning/," Abstract: In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally demonstrate good tolerance to incorrect labels. ",Sun Oct 16 14:50:33 2016 UTC,https://arxiv.org/abs/1610.02242,414,
22,[R] Sim-to-Real Robot Learning from Pixels with Progressive Nets (DeepMind),https://www.reddit.com/r/MachineLearning/comments/57w3fp/r_simtoreal_robot_learning_from_pixels_with/," Abstract: Applying end-to-end learning to solve complex, interactive, pixel-driven control tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms are too slow to achieve performance on a real robot, but their potential has been demonstrated in simulated environments. We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. The progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks, enabling a compositional, yet simple, approach to building complex skills. We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap. Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator. Moreover, rather than relying on model-based trajectory optimisation, the task learning is accomplished using only deep reinforcement learning and sparse rewards. ",Mon Oct 17 07:08:57 2016 UTC,https://arxiv.org/abs/1610.04286,405,
48,[R] Why Deep Neural Networks? [arXiv:1610.04161],https://www.reddit.com/r/MachineLearning/comments/57vzsc/r_why_deep_neural_networks_arxiv161004161/," Abstract: Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. In this paper, we show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of $\varepsilon$ uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on $\varepsilon$) require $\Omega(\text{poly}(1/\varepsilon))$ neurons while deep networks (i.e., networks whose depth grows with $1/\varepsilon$) require $\mathcal{O}(\text{polylog}(1/\varepsilon))$ neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on this simple observation that the binary approximation of a real number in the interval $[0,1]$ can be represented by a deep neural network which uses a ""small"" number of neurons. ",Mon Oct 17 06:34:44 2016 UTC,https://arxiv.org/abs/1610.04161,404,
11,[R]Big Batch SGD: Automated Inference using Adaptive Batch Sizes,https://www.reddit.com/r/MachineLearning/comments/58fbwj/rbig_batch_sgd_automated_inference_using_adaptive/," Abstract: Classical stochastic gradient methods for optimization rely on noisy gradient approximations that become progressively less accurate as iterates approach a solution. The large noise and small signal in the resulting gradients makes it difficult to use them for adaptive stepsize selection and automatic stopping. We propose alternative ""big batch"" SGD schemes that adaptively grow the batch size over time to maintain a nearly constant signal-to-noise ratio in the gradient approximation. The resulting methods have similar convergence rates to classical SGD methods without requiring convexity of the objective function. The high fidelity gradients enable automated learning rate selection and do not require stepsize decay. For this reason, big batch methods are easily automated and can run with little or no user oversight. ",Thu Oct 20 05:54:57 2016 UTC,http://arxiv.org/abs/1610.05792,381,Probably a good way for scaling up gradient based machine learning problems without performance drop.  
7,Hadamard Product for Low-rank Bilinear Pooling,https://www.reddit.com/r/MachineLearning/comments/57uph7/hadamard_product_for_lowrank_bilinear_pooling/," Abstract: Bilinear models provide rich representations compared with linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks with the state-of-the-art results on the VQA dataset, having a better parsimonious property. ",Mon Oct 17 01:01:43 2016 UTC,http://arxiv.org/abs/1610.04325,410,Welcome any feedbacks and questions! 
4,[Research] Generative Adversarial Nets from a Density Ratio Estimation Perspective,https://www.reddit.com/r/MachineLearning/comments/57na6g/research_generative_adversarial_nets_from_a/," Abstract: Generative adversarial networks (GANs) are successful deep generative models. GANs are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful. ",Sat Oct 15 18:09:14 2016 UTC,https://arxiv.org/abs/1610.02920,428,
32,[Research] Driving in the Matrix: Can Virtual Worlds Replace Human-Generated Annotations for Real World Tasks?,https://www.reddit.com/r/MachineLearning/comments/573a8e/research_driving_in_the_matrix_can_virtual_worlds/," Abstract: Deep learning has rapidly transformed the state of the art algorithms used to address a variety of problems in computer vision and robotics. These breakthroughs have however relied upon massive amounts of human annotated training data. This time-consuming process has begun impeding the progress of these deep learning efforts. This paper describes a method to incorporate photo-realistic computer images from a simulation engine to rapidly generate annotated data that can be used for training of machine learning algorithms. We demonstrate that a state of the art architecture, which is trained only using these synthetic annotations, performs better than the identical architecture trained on human annotated real-world data, when tested on the KITTI data set for vehicle detection. By training machine learning algorithms on a rich virtual world, this paper illustrates that real objects in real scenes can be learned and classified using synthetic data. This approach offers the possibility of accelerating deep learning's application to sensor based classification problems like those that appear in self-driving cars. ",Wed Oct 12 09:44:17 2016 UTC,https://arxiv.org/abs/1610.01983,454,
21,[Research] cleverhans v0.1: an adversarial machine learning library,https://www.reddit.com/r/MachineLearning/comments/56yaku/research_cleverhans_v01_an_adversarial_machine/," Abstract: cleverhans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the cleverhans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system. ",Tue Oct 11 14:37:27 2016 UTC,https://arxiv.org/abs/1610.00768,469,
51,[Research] Fully Character-Level Neural Machine Translation without Explicit Segmentation,https://www.reddit.com/r/MachineLearning/comments/5725kd/research_fully_characterlevel_neural_machine/," Abstract: Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment. ",Wed Oct 12 03:27:26 2016 UTC,https://arxiv.org/abs/1610.03017,458,
22,[1610.02357] Deep Learning with Separable Convolutions,https://www.reddit.com/r/MachineLearning/comments/56plkb/161002357_deep_learning_with_separable/," Abstract: We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the \textit{depthwise separable convolution} operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameter as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. ",Mon Oct 10 01:57:21 2016 UTC,https://arxiv.org/abs/1610.02357,491,
9,[Research] Supervision via Competition: Robot Adversaries for Learning Tasks,https://www.reddit.com/r/MachineLearning/comments/56nq5x/research_supervision_via_competition_robot/," Abstract: There has been a recent paradigm shift in robotics to data-driven learning for planning and control. Due to large number of experiences required for training, most of these approaches use a self-supervised paradigm: using sensors to measure success/failure. However, in most cases, these sensors provide weak supervision at best. In this work, we propose an adversarial learning framework that pits an adversary against the robot learning the task. In an effort to defeat the adversary, the original robot learns to perform the task with more robustness leading to overall improved performance. We show that this adversarial framework forces the the robot to learn a better grasping model in order to overcome the adversary. By grasping 82% of presented novel objects compared to 68% without an adversary, we demonstrate the utility of creating adversaries. We also demonstrate via experiments that having robots in adversarial setting might be a better learning strategy as compared to having collaborative multiple robots. ",Sun Oct 9 19:11:53 2016 UTC,https://arxiv.org/abs/1610.01685,499,
0,[Research][1610.02306] Optimization of Convolutional Neural Network using Microcanonical Annealing Algorithm,https://www.reddit.com/r/MachineLearning/comments/56tkpd/research161002306_optimization_of_convolutional/," Abstract: Convolutional neural network (CNN) is one of the most prominent architectures and algorithm in Deep Learning. It shows a remarkable improvement in the recognition and classification of objects. This method has also been proven to be very effective in a variety of computer vision and machine learning problems. As in other deep learning, however, training the CNN is interesting yet challenging. Recently, some metaheuristic algorithms have been used to optimize CNN using Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing and Harmony Search. In this paper, another type of metaheuristic algorithms with different strategy has been proposed, i.e. Microcanonical Annealing to optimize Convolutional Neural Network. The performance of the proposed method is tested using the MNIST and CIFAR-10 datasets. Although experiment results of MNIST dataset indicate the increase in computation time (1.02x - 1.38x), nevertheless this proposed method can considerably enhance the performance of the original CNN (up to 4.60\%). On the CIFAR10 dataset, currently, state of the art is 96.53\% using fractional pooling, while this proposed method achieves 99.14\%. ",Mon Oct 10 18:40:33 2016 UTC,https://arxiv.org/abs/1610.02306,492,
17,[1610.01644] Understanding intermediate layers using linear classifier probes,https://www.reddit.com/r/MachineLearning/comments/56el4x/161001644_understanding_intermediate_layers_using/," Abstract: Neural network models have a reputation for being black boxes. We propose a new method to understand better the roles and dynamics of the intermediate layers. This has direct consequences on the design of such models and it enables the expert to be able to justify certain heuristics (such as the auxiliary heads in the Inception model). Our method uses linear classifiers, referred to as ""probes"", where a probe can only use the hidden units of a given intermediate layer as discriminating features. Moreover, these probes cannot affect the training phase of a model, and they are generally added after training. They allow the user to visualize the state of the model at multiple steps of training. We demonstrate how this can be used to develop a better intuition about a known model and to diagnose potential problems. ",Sat Oct 8 00:33:52 2016 UTC,https://arxiv.org/abs/1610.01644,515,
38,[1610.01945] Connecting Generative Adversarial Networks and Actor-Critic Methods,https://www.reddit.com/r/MachineLearning/comments/569h88/161001945_connecting_generative_adversarial/," Abstract: Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities. ",Fri Oct 7 03:37:28 2016 UTC,https://arxiv.org/abs/1610.01945,520,
36,"[1609.08913] ""we must either continue to develop new learning methods year after year or move towards highly parameterized models that are both flexible and sensitive to their hyperparameters""",https://www.reddit.com/r/MachineLearning/comments/566eea/160908913_we_must_either_continue_to_develop_new/," Abstract: No Free Lunch theorems show that the average performance across any closed-under-permutation set of problems is fixed for all algorithms, under appropriate conditions. Extending these results, we demonstrate that the proportion of favorable problems is itself strictly bounded, such that no single algorithm can perform well over a large fraction of possible problems. Our results explain why we must either continue to develop new learning methods year after year or move towards highly parameterized models that are both flexible and sensitive to their hyperparameters. ",Thu Oct 6 16:58:02 2016 UTC,https://arxiv.org/abs/1609.08913,527,
10,Exploration Potential,https://www.reddit.com/r/MachineLearning/comments/561en3/exploration_potential/," Abstract: We introduce exploration potential, a quantity that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem's reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation. ",Wed Oct 5 19:49:08 2016 UTC,https://arxiv.org/pdf/1609.04994.pdf,540,
90,"Accelerating Deep Convolutional Networks using low-precision and sparsity - Intel paper, 2-bit ResNets",https://www.reddit.com/r/MachineLearning/comments/55sqv6/accelerating_deep_convolutional_networks_using/," Abstract: We explore techniques to significantly improve the compute efficiency and performance of Deep Convolution Networks without impacting their accuracy. To improve the compute efficiency, we focus on achieving high accuracy with extremely low-precision (2-bit) weight networks, and to accelerate the execution time, we aggressively skip operations on zero-values. We achieve the highest reported accuracy of 76.6% Top-1/93% Top-5 on the Imagenet object classification challenge with low-precision network\footnote{github release of the source code coming soon} while reducing the compute requirement by ~3x compared to a full-precision network that achieves similar accuracy. Furthermore, to fully exploit the benefits of our low-precision networks, we build a deep learning accelerator core, dLAC, that can achieve up to 1 TFLOP/mm^2 equivalent for single-precision floating-point operations (~2 TFLOP/mm^2 for half-precision). ",Tue Oct 4 09:50:42 2016 UTC,https://arxiv.org/abs/1610.00324,553,
13,[paper] Deep Reinforcement Learning for Robotic Manipulation,https://www.reddit.com/r/MachineLearning/comments/55r9s2/paper_deep_reinforcement_learning_for_robotic/," Abstract: Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations. ",Tue Oct 4 02:01:10 2016 UTC,https://arxiv.org/abs/1610.00633,567,
3,YouTube 8M: Large-Scale Video Classification from Google Research,https://www.reddit.com/r/MachineLearning/comments/55tu9i/youtube_8m_largescale_video_classification_from/," Abstract: Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets. In this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of ~8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. To get the videos and their labels, we used a YouTube video annotation system, which labels videos with their main topics. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. We trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using TensorFlow. We plan to release code for training a TensorFlow model and for computing metrics. ",Tue Oct 4 14:50:13 2016 UTC,https://arxiv.org/pdf/1609.08675.pdf,565,
65,DeepMind: Video Pixel Networks,https://www.reddit.com/r/MachineLearning/comments/55r81w/deepmind_video_pixel_networks/," Abstract: We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects. ",Tue Oct 4 01:49:17 2016 UTC,https://arxiv.org/abs/1610.00527,562,
9,X-CNN: Cross-modal Convolutional Neural Networks for Sparse Datasets,https://www.reddit.com/r/MachineLearning/comments/55turk/xcnn_crossmodal_convolutional_neural_networks_for/," Abstract: In this paper we propose cross-modal convolutional neural networks (X-CNNs), a novel biologically inspired type of CNN architectures, treating gradient descent-specialised CNNs as individual units of processing in a larger-scale network topology, while allowing for unconstrained information flow and/or weight sharing between analogous hidden layers of the network---thus generalising the already well-established concept of neural network ensembles (where information typically may flow only between the output layers of the individual networks). The constituent networks are individually designed to learn the output function on their own subset of the input data, after which cross-connections between them are introduced after each pooling operation to periodically allow for information exchange between them. This injection of knowledge into a model (by prior partition of the input data through domain knowledge or unsupervised methods) is expected to yield greatest returns in sparse data environments, which are typically less suitable for training CNNs. For evaluation purposes, we have compared a standard four-layer CNN as well as a sophisticated FitNet4 architecture against their cross-modal variants on the CIFAR-10 and CIFAR-100 datasets with differing percentages of the training data being removed, and find that at lower levels of data availability, the X-CNNs significantly outperform their baselines (typically providing a 2--6% benefit, depending on the dataset size and whether data augmentation is used), while still maintaining an edge on all of the full dataset tests. ",Tue Oct 4 14:53:09 2016 UTC,https://arxiv.org/abs/1610.00163,559,
17,DecomposeMe: Simplifying ConvNets for End-to-End Learning,https://www.reddit.com/r/MachineLearning/comments/55nfqa/decomposeme_simplifying_convnets_for_endtoend/," Abstract: Deep learning and convolutional neural networks (ConvNets) have been successfully applied to most relevant tasks in the computer vision community. However, these networks are computationally demanding and not suitable for embedded devices where memory and time consumption are relevant. In this paper, we propose DecomposeMe, a simple but effective technique to learn features using 1D convolutions. The proposed architecture enables both simplicity and filter sharing leading to increased learning capacity. A comprehensive set of large-scale experiments on ImageNet and Places2 demonstrates the ability of our method to improve performance while significantly reducing the number of parameters required. Notably, on Places2, we obtain an improvement in relative top-1 classification accuracy of 7.7\% with an architecture that requires 92% fewer parameters compared to VGG-B. The proposed network is also demonstrated to generalize to other tasks by converting existing networks. ",Mon Oct 3 12:36:01 2016 UTC,https://arxiv.org/abs/1606.05426,578,
1,"Universal classifier for Binary, Multi-class and Multi-label Classification",https://www.reddit.com/r/MachineLearning/comments/55mti5/universal_classifier_for_binary_multiclass_and/," Abstract: Classification involves the learning of the mapping function that associates input samples to corresponding target label. There are two major categories of classification problems: Single-label classification and Multi-label classification. Traditional binary and multi-class classifications are sub-categories of single-label classification. Several classifiers are developed for binary, multi-class and multi-label classification problems, but there are no classifiers available in the literature capable of performing all three types of classification. In this paper, a novel online universal classifier capable of performing all the three types of classification is proposed. Being a high speed online classifier, the proposed technique can be applied to streaming data applications. The performance of the developed classifier is evaluated using datasets from binary, multi-class and multi-label problems. The results obtained are compared with state-of-the-art techniques from each of the classification types. ",Mon Oct 3 08:59:53 2016 UTC,https://arxiv.org/abs/1609.00843,587,
27,Quantum-Chemical Insights from Deep Tensor Neural Networks,https://www.reddit.com/r/MachineLearning/comments/55k7m8/quantumchemical_insights_from_deep_tensor_neural/," Abstract: Learning from data has led to paradigm shifts in a multitude of disciplines, including web, text, and image search, speech recognition, as well as bioinformatics. Can machine learning enable similar breakthroughs in understanding quantum many-body systems? Here we develop an efficient deep learning approach that enables spatially and chemically resolved insights into quantum-mechanical observables of molecular systems. We unify concepts from many-body Hamiltonians with purpose-designed deep tensor neural networks (DTNN), which leads to size-extensive and uniformly accurate (1 kcal/mol) predictions in compositional and configurational chemical space for molecules of intermediate size. As an example of chemical relevance, the DTNN model reveals a classification of aromatic rings with respect to their stability -- a useful property that is not contained as such in the training dataset. Further applications of DTNN for predicting atomic energies and local chemical potentials in molecules, reliable isomer energies, and molecules with peculiar electronic structure demonstrate the high potential of machine learning for revealing novel insights into complex quantum-chemical systems. ",Sun Oct 2 20:55:22 2016 UTC,https://arxiv.org/pdf/1609.08259.pdf,583,
19,[1609.09049v1] Deep Reinforcement Learning for Tensegrity Robot Locomotion,https://www.reddit.com/r/MachineLearning/comments/55e7yc/160909049v1_deep_reinforcement_learning_for/," Abstract: Tensegrity robots, composed of rigid rods connected by elastic cables, have a number of unique properties that make them appealing for use as planetary exploration rovers. However, control of tensegrity robots remains a difficult problem due to their unusual structures and complex dynamics. In this work, we show how locomotion gaits can be learned automatically using a novel extension of mirror descent guided policy search (MDGPS) applied to periodic locomotion movements, and we demonstrate the effectiveness of our approach on tensegrity robot locomotion. We evaluate our method with real-world and simulated experiments on the SUPERball tensegrity robot, showing that the learned policies generalize to changes in system parameters, unreliable sensor measurements, and variation in environmental conditions, including varied terrains and a range of different gravities. Our experiments demonstrate that our method not only learns fast, power-efficient feedback policies for rolling gaits, but that these policies can succeed with only the limited onboard sensing provided by SUPERball's accelerometers. We compare the learned feedback policies to learned open-loop policies and hand-engineered controllers, and demonstrate that the learned policy enables the first continuous, reliable locomotion gait for the real SUPERball robot. ",Sat Oct 1 17:33:30 2016 UTC,http://arxiv.org/abs/1609.09049v1,595,
44,[1609.09475v1] Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge,https://www.reddit.com/r/MachineLearning/comments/55e7kk/160909475v1_multiview_selfsupervised_deep/," Abstract: Robot warehouse automation has attracted significant interest in recent years, perhaps most visibly in the Amazon Picking Challenge (APC). A fully autonomous warehouse pick-and-place system requires robust vision that reliably recognizes and locates objects amid cluttered environments, self-occlusions, sensor noise, and a large variety of objects. In this paper we present an approach that leverages multi-view RGB-D data and self-supervised, data-driven learning to overcome those difficulties. The approach was part of the MIT-Princeton Team system that took 3rd- and 4th- place in the stowing and picking tasks, respectively at APC 2016. In the proposed approach, we segment and label multiple views of a scene with a fully convolutional neural network, and then fit pre-scanned 3D object models to the resulting segmentation to get the 6D object pose. Training a deep neural network for segmentation typically requires a large amount of training data. We propose a self-supervised method to generate a large labeled dataset without tedious manual segmentation. We demonstrate that our system can reliably estimate the 6D pose of objects under a variety of scenarios. All code, data, and benchmarks are available at this http URL ",Sat Oct 1 17:31:08 2016 UTC,http://arxiv.org/abs/1609.09475v1,592,
40,HyperNetworks,https://www.reddit.com/r/MachineLearning/comments/5566fo/hypernetworks/," Abstract: This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of language modeling tasks with Character-Level Penn Treebank and Hutter Prize Wikipedia datasets, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters. ",Fri Sep 30 03:54:56 2016 UTC,https://arxiv.org/abs/1609.09106,610,
