votes,title,url,abstract,submitted,link,rank,top_comment
22,"[R] ""Learning to superoptimize programs"", Bunel et al 2016",https://www.reddit.com/r/MachineLearning/comments/5dv2iz/r_learning_to_superoptimize_programs_bunel_et_al/,"  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.
",Sat Nov 19 22:56:15 2016 UTC,http://openreview.net/forum?id=r1rz6U5lg,71,
18,"[R] ""Structured Attention Networks"" <-- can use its attention mechanism to segment and parse",https://www.reddit.com/r/MachineLearning/comments/5d8f0a/r_structured_attention_networks_can_use_its/,"Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.",Wed Nov 16 09:51:00 2016 UTC,http://openreview.net/forum?id=HkE0Nvqlg,107,
8,[R] Out-of-class novelty generation,https://www.reddit.com/r/MachineLearning/comments/5cfjgf/r_outofclass_novelty_generation/,"Recent advances in machine learning have brought the field closer to computational creativity research. From a creativity research point of view, this offers the potential to study creativity in relationship with knowledge acquisition. From a machine learning perspective, however, several aspects of creativity need to be better defined to allow the machine learning community to develop and test hypotheses in a systematic way. We propose an actionable definition of creativity as the generation of out-of-distribution novelty. We assess several  metrics designed for evaluating the quality of generative models on this new task. We also propose a new experimental setup. Inspired by the usual held-out validation, we hold out entire classes for evaluating the generative potential of models. The goal of the novelty generator is then to use training classes to build a model that can generate objects from future (hold-out) classes, unknown at training time - and thus, are novel with respect to the knowledge the model incorporates. Through extensive experiments on various types of generative models, we are able to find architectures and hyperparameter combinations which lead to out-of-distribution novelty.
",Fri Nov 11 17:03:21 2016 UTC,http://openreview.net/forum?id=ByEPMj5el,161,
6,[R] Variational Recurrent Adversarial Deep Domain Adaptation,https://www.reddit.com/r/MachineLearning/comments/5cghr4/r_variational_recurrent_adversarial_deep_domain/,"We study the problem of learning domain invariant representations for time series data while transferring the complex temporal latent dependencies between the domains. Our model termed as Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) is built atop a variational recurrent neural network (VRNN) and trains adversarially to capture complex temporal relationships that are domain-invariant. This is (as far as we know) the first to capture and transfer temporal latent dependencies in multivariate time-series data. Through experiments on real-world multivariate healthcare time-series datasets, we empirically demonstrate that learning temporal dependencies helps our model's ability to create domain-invariant representations, allowing our model to outperform current state-of-the-art deep domain adaptation approaches.",Fri Nov 11 19:51:43 2016 UTC,http://openreview.net/forum?id=rk9eAFcxg,162,
25,[Research] Generative Multi-Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/5btf4u/research_generative_multiadversarial_networks/,"Generative adversarial networks (GANs) are a framework for producing a generative model by way of a two-player minimax game.  In this paper, we propose the \emph{Generative Multi-Adversarial Network} (GMAN), a framework that extends GANs to multiple discriminators. In previous work, the successful training of GANs requires modifying the minimax objective to accelerate training early on. In contrast, GMAN can be reliably trained with the original, untampered objective. We explore a number of design perspectives with the discriminator role ranging from formidable adversary to forgiving teacher.  Image generation tasks comparing the proposed framework to standard GANs demonstrate GMAN produces higher quality samples in a fraction of the iterations when measured by a pairwise GAM-type metric.",Tue Nov 8 14:16:30 2016 UTC,http://openreview.net/forum?id=Byk-VI9eg,190,
7,"[R] ""Learning Curve Prediction With Bayesian Neural Networks"", Klein et al 2016 (hyperparameter optimization)",https://www.reddit.com/r/MachineLearning/comments/5bvvhw/r_learning_curve_prediction_with_bayesian_neural/,"Different neural network architectures, hyperparameters and training protocols lead to different performances as a function of time.
Human experts routinely inspect the resulting learning curves to quickly terminate runs with poor hyperparameter settings and thereby considerably speed up manual hyperparameter optimization. Exploiting the same information in automatic Bayesian hyperparameter optimization requires a probabilistic model of learning curves across hyperparameter settings. Here, we study the use of Bayesian neural networks for this purpose and improve their performance by a specialized learning curve layer.",Tue Nov 8 20:54:05 2016 UTC,http://openreview.net/forum?id=S11KBYclx,191,
7,[Research] Diet Networks: Thin Parameters for Fat Genomics,https://www.reddit.com/r/MachineLearning/comments/5bptvn/research_diet_networks_thin_parameters_for_fat/,"Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.",Mon Nov 7 23:54:08 2016 UTC,http://openreview.net/forum?id=Sk-oDY9ge,202,
31,[R] Outrageously Large Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5bhrck/r_outrageously_large_neural_networks/,"The capacity of a neural network to absorb information is limited by its number of parameters.  In this work, we present a new kind of layer, the Sparsely-Gated Mixture-of-Experts (MoE), which can be used to effectively increase model capacity with only a modest increase in computation. This layer consists of up to tens of thousands of feed-forward sub-networks (experts) containing a total of up to tens of billions of parameters.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the task of language modeling, where model capacity is critical for absorbing the vast quantities of world knowledge available in the training corpora.  We present new language model architectures where an MoE layer is inserted between stacked LSTMs, resulting in models with orders of magnitude more parameters than would otherwise be feasible. On language modeling and machine translation benchmarks, we achieve comparable or better results than state-of-the-art at lower computational cost, including test perplexity of 29.9 on the 1 Billion Word Language Modeling Benchmark and BLEU scores of 40.56  and 26.03 on the WMT'14 En to Fr and En to De datasets respectively.",Sun Nov 6 20:12:17 2016 UTC,http://openreview.net/forum?id=B1ckMDqlg,210,
39,[R] DeepCoder: Learning to Write Programs,https://www.reddit.com/r/MachineLearning/comments/5bh1om/r_deepcoder_learning_to_write_programs/,"We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the  program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites.",Sun Nov 6 18:02:45 2016 UTC,http://openreview.net/forum?id=ByldLrqlx,211,
35,"[R]SNAPSHOT ENSEMBLES: TRAIN 1, GET M FOR FREE",https://www.reddit.com/r/MachineLearning/comments/5bf6pl/rsnapshot_ensembles_train_1_get_m_for_free/,"Ensembles of neural networks are known to give far more robust and accurate predictions compared to any of its individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal to obtain ensembles of multiple  neural network at no additional training cost. We achieve this goal by letting a single neural network converge into several local minima along its optimization path and save the model parameters.  To obtain repeated rapid convergence we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as \emph{Snapshot Ensembling}, is surprisingly simple, yet effective.  We show in a series of experiments that our approach is compatible with  diverse network architectures and learning tasks. It consistently yields significantly lower error rates than state-of-the-art single  models at no additional training cost,  and almost matches the results of (far more expensive) independently trained network ensembles. On Cifar-10 and Cifar-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.",Sun Nov 6 10:36:53 2016 UTC,http://openreview.net/forum?id=BJYwwY9ll,213,
6,[R] Learning a Static Analyzer: A Case Study on a Toy Language,https://www.reddit.com/r/MachineLearning/comments/5bh11h/r_learning_a_static_analyzer_a_case_study_on_a/,"Static analyzers are meta-programs that analyze programs to detect
  potential errors or collect information. For example, they are used
  as security tools to detect potential buffer overflows. Also, they
  are used by compilers to verify that a program is well-formed and
  collect information to generate better code. In this paper, we
  address the following question: can a static analyzer be learned
  from data? More specifically, can we use deep learning to learn a
  static analyzer without the need for complicated feature
  engineering? We show that long short-term memory networks are able
  to learn a basic static analyzer for a simple toy language. However,
  pre-existing approaches based on feature engineering, hidden Markov
  models, or basic recurrent neural networks fail on such a simple
  problem. Finally, we show how to make such a tool usable by
  employing a language model to help the programmer detect where the
  reported errors are located.",Sun Nov 6 17:59:38 2016 UTC,http://openreview.net/forum?id=ry54RWtxx,216,
5,[R] Multi-task learning with deep model based reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/5bh0df/r_multitask_learning_with_deep_model_based/,"In recent years, model-free methods that use deep learning have achieved great success in many different reinforcement learning environments. Most successful approaches focus on solving a single task, while multi-task reinforcement learning remains an open problem. In this paper, we present a model based approach to deep reinforcement learning which we use to solve different tasks simultaneously. We show that our approach not only does not degrade but actually benefits from learning multiple tasks. For our model, we also present a new kind of recurrent neural network inspired by residual networks that decouples memory from computation allowing to model complex environments that do not require lots of memory. The code will be released before ICLR 2017.",Sun Nov 6 17:56:07 2016 UTC,http://openreview.net/forum?id=rJe-Pr9le,217,
181,"[R] LipNet, an end-to-end model with 93.4% accuracy in lip reading (previous state of the art 79.6%) - Univ. Oxford, Google Deepmind",https://www.reddit.com/r/MachineLearning/comments/5b8v8t/r_lipnet_an_endtoend_model_with_934_accuracy_in/,"Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). All existing works, however, perform only word classification, not sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, an LSTM recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first lipreading model to operate at sentence-level using a single end-to-end speaker-independent deep model to simultaneously learn spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 93.4% accuracy, outperforming experienced human lipreaders and the previous 79.6% state-of-the-art accuracy.",Sat Nov 5 07:25:42 2016 UTC,http://openreview.net/forum?id=BkjLkSqxg,220,
14,[R] Highway and Residual Networks learn Unrolled Iterative Estimation,https://www.reddit.com/r/MachineLearning/comments/5b7w82/r_highway_and_residual_networks_learn_unrolled/,"The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent.
While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.

In this report, we argue that this view is incomplete and does not adequately explain several recent findings.
We propose an alternative viewpoint based on unrolled iterative estimation---a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation.
We demonstrate that this viewpoint directly leads to the construction of highway and residual networks. 
Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.",Sat Nov 5 02:46:41 2016 UTC,http://openreview.net/forum?id=Skn9Shcxe,229,
8,[R] Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity,https://www.reddit.com/r/MachineLearning/comments/5bcuxx/r_symmetrybreaking_convergence_analysis_of/,"In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(x; w) = \sum_{j=1}^K \sigma(w_j \cdot x)$, where $\sigma(\cdot)$ is ReLU nonlinearity. We assume that the input $x$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $w*$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $w*$ with at least $(1-\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\sim 1/\sqrt{d}$) is used, verifying empirical practice. For networks with many ReLU nodes ($K \ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\{w*_j\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $w*$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.",Sat Nov 5 23:29:01 2016 UTC,http://openreview.net/forum?id=Hk85q85ee,221,
50,"[R] ""Neural Architecture Search with Reinforcement Learning"" <-- model that learned to design a SOTA language model",https://www.reddit.com/r/MachineLearning/comments/5b5022/r_neural_architecture_search_with_reinforcement/,"Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.84, which is only 0.1 percent worse and 1.2x faster than the current state-of-the-art model. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art.",Fri Nov 4 17:41:59 2016 UTC,http://openreview.net/forum?id=r1Ue8Hcxg,232,
13,[R] Neuro-Symbolic Program Synthesis,https://www.reddit.com/r/MachineLearning/comments/5bc1wx/r_neurosymbolic_program_synthesis/,"Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.",Sat Nov 5 20:46:04 2016 UTC,http://openreview.net/forum?id=rJ0JwFcex,222,
13,[R] Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening,https://www.reddit.com/r/MachineLearning/comments/5b5k7k/r_learning_to_play_in_a_day_faster_deep/,"We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time.  We evaluate the performance of our approach on the 49 games of the challenging Arcade  Learning Environment, and report significant improvements in both training time and accuracy.",Fri Nov 4 19:13:27 2016 UTC,http://openreview.net/forum?id=rJ8Je4clg,234,
6,[R] Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning,https://www.reddit.com/r/MachineLearning/comments/5bcwq7/r_training_agent_for_firstperson_shooter_game/,"In this paper, we propose a novel framework for training vision-based agent for First-Person Shooter (FPS) Game, in particular Doom.
Our framework combines the state-of-the-art reinforcement learning approach (Asynchronous Advantage Actor-Critic (A3C) model) with curriculum learning. Our model is simple in design and only uses game states from the AI side, rather than using opponents' information. On a known map, our agent won 10 out of the 11 attended games and the champion of Track1 in ViZDoom AI Competition 2016 by a large margin, 35\% higher score than the second place.",Sat Nov 5 23:39:23 2016 UTC,http://openreview.net/forum?id=Hk3mPK5gg,223,
38,[R] RenderGAN: Generating Realistic Labeled Data,https://www.reddit.com/r/MachineLearning/comments/5b3az6/r_rendergan_generating_realistic_labeled_data/,"Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the usage of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees.  A DCNN is trained on this data only. It performs better on a test set of real data than an equal DCNN trained on the limited amounts of real data available.",Fri Nov 4 12:39:20 2016 UTC,http://openreview.net/forum?id=BkGakb9lx,235,
12,[R] Topology and Geometry of Deep Rectified Network Optimization Landscapse,https://www.reddit.com/r/MachineLearning/comments/5b4xfe/r_topology_and_geometry_of_deep_rectified_network/,"The loss surface of deep neural networks has recently attracted interest 
in the optimization and machine learning communities as a prime example of 
high-dimensional non-convex problem. Some insights were recently gained using spin glass 
models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model.

In this work, we do not make any such approximation and study conditions 
on the data distribution and model architecture that prevent the existence 
of bad local minima. Our theoretical work quantifies and formalizes two 
important folklore facts: (i) the landscape of deep linear networks has a radically different topology 
from that of deep half-rectified ones, and (ii) that the energy landscape 
in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. Our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected, and we provide explicit bounds that reveal the aforementioned interplay.

The conditioning of gradient descent is the next challenge we address. 
We study this question through the geometry of the level sets, and we introduce
an algorithm to efficiently estimate the regularity of such sets on large-scale networks. 
Our empirical results show that these level sets remain connected throughout 
all the learning phase, suggesting a near convex behavior, but they become 
exponentially more curvy as the energy level decays, in accordance to what is observed in practice with 
very low curvature attractors.",Fri Nov 4 17:30:06 2016 UTC,http://openreview.net/forum?id=Bk0FWVcgx,236,
5,[R] Learning Python Code Suggestion With As Parse Pointer Network,https://www.reddit.com/r/MachineLearning/comments/5b023w/r_learning_python_code_suggestion_with_as_parse/,"To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.",Thu Nov 3 22:55:48 2016 UTC,http://openreview.net/forum?id=r1kQkVFgl,251,
21,[R] ICLR'17 submissions on OpenReview,https://www.reddit.com/r/MachineLearning/comments/5b38xc/r_iclr17_submissions_on_openreview/,,Fri Nov 4 12:26:34 2016 UTC,http://openreview.net/group?id=ICLR.cc/2017/conference,237,
20,[R] Learning to Compose Words into Sentences with Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/5b373g/r_learning_to_compose_words_into_sentences_with/,"We use reinforcement learning to learn
tree-structured neural networks for computing representations of natural language sentences.
In contrast with prior work on tree-structured models, in which the trees are either provided as input or
predicted using supervision from explicit treebank annotations,
the tree structures in this work are optimized to improve performance on a downstream task.
Experiments demonstrate the benefit of
learning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations.
We analyze the induced trees and show that while they discover
some linguistically intuitive structures (e.g., noun phrases, simple verb phrases),
they are different than conventional English syntactic structures.",Fri Nov 4 12:15:01 2016 UTC,http://openreview.net/forum?id=Skvgqgqxe,238,
9,[R] Nonparametric Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5b47tr/r_nonparametric_neural_networks/,,Fri Nov 4 15:32:10 2016 UTC,http://openreview.net/forum?id=ryHCQ79el,239,
7,[R] Learning Continuous Semantic Representations of Symbolic Expressions,https://www.reddit.com/r/MachineLearning/comments/5b3osw/r_learning_continuous_semantic_representations_of/,"The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.
",Fri Nov 4 13:58:51 2016 UTC,http://openreview.net/forum?id=B1vRTeqxg,240,
1,[R] LEARNING GRAPHICAL STATE TRANSITIONS,https://www.reddit.com/r/MachineLearning/comments/5abho3/r_learning_graphical_state_transitions/,"Graph-structured data is important in modeling relationships between multiple entities, and can be used to represent states of the world as well as many data structures. Li et al. (2016) describe a model known as a Gated Graph Sequence Neural Network (GGS-NN) that can transform graph-structured inputs into output sequences. In this work I introduce a set of graph-based transformations, which I combine to construct a versatile extension of GGS-NNs that uses graph-structured data as an intermediate representation. The model can learn to construct and modify graphs in sophisticated ways based on textual input, and also to use the graphs to produce a variety of outputs. For example, the model successfully learns to solve almost all of the bAbI tasks (Weston et al., 2016), and also discovers the rules governing graphical formulations of a simple cellular automaton and a family of Turing machines.
",Mon Oct 31 10:24:15 2016 UTC,http://openreview.net/forum?id=HJ0NvFzxl,297,
2,[R] Neural Data Filter for Bootstrapping Stochastic Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/5b4ht8/r_neural_data_filter_for_bootstrapping_stochastic/,"Mini-batch based Stochastic Gradient Descent(SGD) has been widely used to train deep neural networks efficiently. In this paper, we design a general framework to automatically and adaptively select training data for SGD. The framework is based on neural networks and we call it \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}). In Neural Data Filter, the whole training process of the original neural network is monitored and supervised by a deep reinforcement network, which controls whether to filter some data in sequentially arrived mini-batches so as to maximize future accumulative reward (e.g., validation accuracy). The SGD process accompanied with NDF is able to use less data and converge faster while achieving comparable accuracy as the standard SGD trained on the full dataset. Our experiments demonstrates that NDF significantly bootstraps SGD training for Recurrent Neural Network applied to sentiment classification.",Fri Nov 4 16:19:08 2016 UTC,http://openreview.net/forum?id=SyJNmVqgg,244,
2,[R] Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,https://www.reddit.com/r/MachineLearning/comments/5b22cj/r_tying_word_vectors_and_word_classifiers_a_loss/,"Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our LSTM model lowers the state of the art word-level perplexity on the Penn Treebank to 68.5.
",Fri Nov 4 06:19:25 2016 UTC,http://openreview.net/forum?id=r1aPbsFle,250,
